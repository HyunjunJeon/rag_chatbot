"""
BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê¸°ì¡´ì˜ merged JSON ë°ì´í„°ë¥¼ LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
KiwiBM25Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ BM25 ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš© (slack_qa_merged)
    python document_processing/rebuild_bm25_for_chatbot.py

    # ì •ë¦¬ëœ ë°ì´í„°ë¡œ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    python document_processing/rebuild_bm25_for_chatbot.py \
        --input-dir document_chunks/slack_qa_cleaned \
        --output-dir sparse_index/kiwi_bm25_slack_qa_v2_cleaned

    # Top-K ì„¤ì •
    python document_processing/rebuild_bm25_for_chatbot.py \
        --input-dir document_chunks/slack_qa_cleaned \
        --output-dir sparse_index/kiwi_bm25_slack_qa_v2_cleaned \
        -k 20
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "app"))

# LangChain Document import
from langchain_core.documents import Document
from tqdm import tqdm

# KiwiBM25Retrieverë¥¼ ì§ì ‘ import (ì„¤ì • íŒŒì¼ ë¡œë“œ ìš°íšŒ)
import importlib.util

# kiwi_bm25_retriever ëª¨ë“ˆ ì§ì ‘ ë¡œë“œ
spec = importlib.util.spec_from_file_location(
    "kiwi_bm25_retriever",
    PROJECT_ROOT / "app" / "naver_connect_chatbot" / "rag" / "retriever" / "kiwi_bm25_retriever.py"
)
kiwi_module = importlib.util.module_from_spec(spec)
sys.modules['kiwi_bm25_retriever'] = kiwi_module
spec.loader.exec_module(kiwi_module)

KiwiBM25Retriever = kiwi_module.KiwiBM25Retriever
get_default_important_pos = kiwi_module.get_default_important_pos

__all__ = ["rebuild_bm25_index"]


def load_merged_json(json_path: Path) -> dict[str, Any]:
    """
    merged JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        json_path: JSON íŒŒì¼ ê²½ë¡œ

    ë°˜í™˜ê°’:
        JSON ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    with open(json_path, encoding="utf-8") as f:
        return json.load(f)


def convert_to_documents(merged_dir: Path) -> list[Document]:
    """
    merged JSON ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ LangChain Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        merged_dir: merged JSON ë””ë ‰í† ë¦¬ ê²½ë¡œ

    ë°˜í™˜ê°’:
        Document ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸ“‚ ë””ë ‰í† ë¦¬: {merged_dir}")

    # JSON íŒŒì¼ ì°¾ê¸°
    json_files = sorted(merged_dir.glob("*_merged.json"))
    print(f"   ë°œê²¬ëœ íŒŒì¼: {len(json_files)}ê°œ")

    documents: list[Document] = []

    for json_file in tqdm(json_files, desc="íŒŒì¼ ì²˜ë¦¬"):
        # _summary.jsonì€ ê±´ë„ˆë›°ê¸°
        if json_file.name == "_summary.json":
            continue

        try:
            data = load_merged_json(json_file)
            course = data["course"]

            for qa in data["qa_pairs"]:
                question = qa["question"]
                answers = qa["answers"]

                # ê° ë‹µë³€ë§ˆë‹¤ Document ìƒì„±
                for idx, answer in enumerate(answers):
                    # Document ID ìƒì„±
                    doc_id = (
                        f"{qa['generation']}_{course}_{qa['date']}_"
                        f"{question['timestamp']}_a{idx}"
                    )

                    # page_content: ì§ˆë¬¸ê³¼ ë‹µë³€ ê²°í•©
                    page_content = f"ì§ˆë¬¸: {question['text']}\në‹µë³€: {answer['text']}"

                    # metadata: ê²€ìƒ‰ í•„í„°ë§ ë° ì¶”ì ìš©
                    metadata = {
                        "doc_id": doc_id,
                        "course": course,
                        "generation": qa["generation"],
                        "date": qa["date"],
                        "question_text": question["text"],
                        "answer_text": answer["text"],
                        "question_user": question.get("user_name") or question["user"],
                        "answer_user": answer.get("user_name") or answer["user"],
                        "is_bot": answer["is_bot"],
                        "question_timestamp": question["timestamp"],
                        "answer_timestamp": answer["timestamp"],
                    }

                    # Document ìƒì„±
                    doc = Document(page_content=page_content, metadata=metadata)
                    documents.append(doc)

        except Exception as e:
            print(f"\n   âœ— {json_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    print(f"\n   ì´ ë¬¸ì„œ ìˆ˜: {len(documents):,}ê°œ")

    return documents


def rebuild_bm25_index(
    merged_dir: Path | str,
    output_dir: Path | str,
    k: int = 10,
    typos: str = "basic_with_continual_and_lengthening",
) -> KiwiBM25Retriever:
    """
    BM25 ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        merged_dir: merged JSON ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        k: ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        typos: Kiwi ì˜¤íƒ€ êµì • ëª¨ë“œ

    ë°˜í™˜ê°’:
        KiwiBM25Retriever ì¸ìŠ¤í„´ìŠ¤
    """
    merged_dir = Path(merged_dir)
    output_dir = Path(output_dir)

    print("=" * 80)
    print("ğŸš€ BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘")
    print("=" * 80)

    # 1. Document ë³€í™˜
    print("\n[1] JSON ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜")
    documents = convert_to_documents(merged_dir)

    if not documents:
        raise ValueError("ë³€í™˜ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 2. KiwiBM25Retriever ìƒì„±
    print("\n[2] KiwiBM25Retriever ìƒì„±")
    print(f"   K: {k}")
    print(f"   ì˜¤íƒ€ êµì •: {typos}")
    print("   í’ˆì‚¬ í•„í„°: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì˜ì–´, í•œì, ìˆ«ì")

    retriever = KiwiBM25Retriever.from_documents(
        documents=documents,
        k=k,
        model_type="knlm",  # KNLM ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        typos=None,  # ì˜¤íƒ€ êµì • ë¹„í™œì„±í™”
        important_pos=get_default_important_pos(),
        load_default_dict=True,  # ê¸°ë³¸ ì‚¬ì „ í™œì„±í™”
        load_typo_dict=False,  # ì˜¤íƒ€ ì‚¬ì „ ë¹„í™œì„±í™”
        load_multi_dict=False,  # ë‹¤ì–´ì ˆ ì‚¬ì „ ë¹„í™œì„±í™”
        num_workers=0,  # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‹¤í–‰
        auto_save=True,
        save_path=output_dir,
        save_user_dict=True,
    )

    print("\n   âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    print(f"      ì €ì¥ ìœ„ì¹˜: {output_dir}")

    # 3. í†µê³„ ì¶œë ¥
    print("\n[3] ì¸ë±ìŠ¤ í†µê³„")
    print(f"   ì´ ë¬¸ì„œ ìˆ˜: {len(retriever.docs):,}ê°œ")

    # ê³¼ì •ë³„ í†µê³„
    course_counts: dict[str, int] = {}
    for doc in retriever.docs:
        course = doc.metadata["course"]
        course_counts[course] = course_counts.get(course, 0) + 1

    print(f"   ê³¼ì • ìˆ˜: {len(course_counts)}ê°œ")
    for course, count in sorted(course_counts.items()):
        print(f"      - {course}: {count:,}ê°œ")

    # 4. ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n[4] ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    test_queries = [
        "GPU ë©”ëª¨ë¦¬ ë¶€ì¡±",
        "ë°ì´í„° ì¦ê°•",
        "optimizer",
    ]

    for query in test_queries:
        print(f"\n   ì¿¼ë¦¬: '{query}'")
        results = retriever.invoke(query)
        
        if results:
            top_result = results[0]
            print(f"      - ìƒìœ„ ê²°ê³¼: {top_result.metadata['course']}")
            print(f"      - ì§ˆë¬¸: {top_result.metadata['question_text'][:50]}...")
            print(f"      - ë‹µë³€: {top_result.metadata['answer_text'][:50]}...")
            if "score" in top_result.metadata:
                print(f"      - ì ìˆ˜: {top_result.metadata['score']:.4f}")

    print("\n" + "=" * 80)
    print("âœ… BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ!")
    print("=" * 80)

    return retriever


def parse_args() -> argparse.Namespace:
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ (Kiwi í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜)"
    )

    parser.add_argument(
        "--input-dir",
        type=str,
        default="document_chunks/slack_qa_merged",
        help="ì…ë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (merged JSON íŒŒì¼ë“¤) (ê¸°ë³¸ê°’: document_chunks/slack_qa_merged)",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="sparse_index/kiwi_bm25_slack_qa",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (BM25 ì¸ë±ìŠ¤ ì €ì¥) (ê¸°ë³¸ê°’: sparse_index/kiwi_bm25_slack_qa)",
    )

    parser.add_argument(
        "-k",
        "--top-k",
        type=int,
        default=10,
        help="ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)",
    )

    return parser.parse_args()


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    args = parse_args()

    # ê²½ë¡œ ì„¤ì • (ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
    merged_dir = PROJECT_ROOT / args.input_dir
    output_dir = PROJECT_ROOT / args.output_dir

    print(f"\nğŸ“‹ ì„¤ì •:")
    print(f"   ì…ë ¥ ë””ë ‰í† ë¦¬: {merged_dir}")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   Top-K: {args.top_k}")

    # ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
    try:
        retriever = rebuild_bm25_index(
            merged_dir=merged_dir,
            output_dir=output_dir,
            k=args.top_k,
        )

        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰: python tests/test_integration_retriever.py")
        print("   2. Hybrid Retriever êµ¬ì„± ë° í…ŒìŠ¤íŠ¸")
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print(f"   - {output_dir / 'bm25_index.pkl'}")
        print(f"   - {output_dir / 'user_dict.txt'}")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

