"""
ê°•ì˜ ë…¹ì·¨ë¡ ì²­í‚¹ ëª¨ë“ˆ.

processed_combined ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼(ê°•ì˜ ë…¹ì·¨ë¡)ì„
RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

íŒŒì¼ëª…ì´ ê°•ì˜ëª…ì´ë¯€ë¡œ ë©”íƒ€ë°ì´í„°ë¡œ ì¤‘ìš”í•˜ê²Œ í™œìš©í•©ë‹ˆë‹¤.
"""

import hashlib
import json
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

from langchain_text_splitters import RecursiveCharacterTextSplitter

from ..common.versioning import create_chunk_version_metadata


@dataclass
class LectureTranscriptChunk:
    """
    ê°•ì˜ ë…¹ì·¨ë¡ì—ì„œ ì¶”ì¶œí•œ ì²­í¬.

    Attributes:
        id: ê³ ìœ  ì‹ë³„ì
        content: ì²­í¬ ë‚´ìš©
        metadata: ë©”íƒ€ë°ì´í„°
    """

    id: str
    content: str
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def token_estimate(self) -> int:
        """í† í° ìˆ˜ ì¶”ì • (í•œê¸€ ê¸°ì¤€)."""
        korean_chars = len(re.findall(r"[ê°€-í£]", self.content))
        other_chars = len(self.content) - korean_chars
        return int(korean_chars / 1.5 + other_chars / 4)

    @property
    def char_count(self) -> int:
        """ë¬¸ì ìˆ˜."""
        return len(self.content)

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜."""
        return {
            "id": self.id,
            "content": self.content,
            "metadata": self.metadata,
            "token_estimate": self.token_estimate,
            "char_count": self.char_count,
        }


@dataclass
class ParsedTranscript:
    """
    íŒŒì‹±ëœ ê°•ì˜ ë…¹ì·¨ë¡.

    Attributes:
        file_path: ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        lecture_name: ê°•ì˜ëª… (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
        source_file: ì›ë³¸ ì†ŒìŠ¤ íŒŒì¼ëª…
        text: ì „ì²´ ë…¹ì·¨ë¡ í…ìŠ¤íŠ¸
        course: ê³¼ëª©ëª… (ì¶”ì¶œ)
        lecture_num: ê°•ì˜ ë²ˆí˜¸ (ì¶”ì¶œ)
        lecture_title: ê°•ì˜ ì œëª© (ì¶”ì¶œ)
    """

    file_path: Path
    lecture_name: str
    source_file: str
    text: str
    course: str = ""
    lecture_num: str = ""
    lecture_title: str = ""


class LectureTranscriptChunker:
    """
    ê°•ì˜ ë…¹ì·¨ë¡ì„ RAGìš© ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤.

    ì²­í‚¹ ì „ëµ:
    1. RecursiveCharacterTextSplitterë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
    2. íŒŒì¼ëª…ì—ì„œ ê³¼ëª©/ê°•ì˜ë²ˆí˜¸/ì œëª© ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    3. ì»¨í…ìŠ¤íŠ¸ í—¤ë” ì¶”ê°€ (ê³¼ëª©, ê°•ì˜ëª…)

    ì˜ˆì‹œ:
        ```python
        chunker = LectureTranscriptChunker(chunk_size=1000)
        chunks = chunker.process_file(Path("(1ê°•) CV.json"))

        for chunk in chunks:
            print(f"[{chunk.id}] {chunk.content[:100]}...")
        ```
    """

    # í•œêµ­ì–´ì— ì í•©í•œ êµ¬ë¶„ì (ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  ìš°ì„ )
    DEFAULT_SEPARATORS = [
        "\n\n",  # ë¬¸ë‹¨
        "\n",  # ì¤„ë°”ê¿ˆ
        "ë‹¤. ",  # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²°
        ". ",  # ì˜ì–´ ë¬¸ì¥ ì¢…ê²°
        "ìš”. ",  # ì¡´ëŒ“ë§ ì¢…ê²°
        "ì£ . ",  # êµ¬ì–´ì²´ ì¢…ê²°
        ", ",  # ì‰¼í‘œ
        " ",  # ê³µë°±
        "",  # ê¸€ì
    ]

    # ê³¼ëª©ëª… íŒ¨í„´ë“¤ (íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
    COURSE_PATTERNS = [
        # [ê³¼ëª©] í˜•ì‹: [RecSys ì´ë¡ ], [AI Math], [MRC]
        r"^\[([^\]]+)\]",
        # (ê³¼ëª©) í˜•ì‹ (ê°•ì˜ë²ˆí˜¸ ì œì™¸)
        r"^\((?!\d+ê°•)([^)]+)\)",
    ]

    # ê°•ì˜ ë²ˆí˜¸ íŒ¨í„´ë“¤
    LECTURE_NUM_PATTERNS = [
        r"\((\d+ê°•)\)",  # (1ê°•)
        r"\((\d+-\d+ê°•)\)",  # (8-1ê°•)
        r"(\d+ê°•)[\s_]",  # 1ê°• ë˜ëŠ” 1ê°•_
        r"_(\d+ê°•)_",  # _10ê°•_
    ]

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 150,
        min_chunk_size: int = 100,
    ) -> None:
        """
        LectureTranscriptChunker ì´ˆê¸°í™”.

        Args:
            chunk_size: ì²­í¬ ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ 1000)
            chunk_overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í† í° ìˆ˜ (ê¸°ë³¸ 150)
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸° (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size

        # í† í° â†’ ë¬¸ì ë³€í™˜ (í•œê¸€ ê¸°ì¤€ ì•½ 2ì/í† í°)
        char_size = int(chunk_size * 2)
        char_overlap = int(chunk_overlap * 2)

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=char_size,
            chunk_overlap=char_overlap,
            separators=self.DEFAULT_SEPARATORS,
            length_function=len,
        )

    def process_file(self, file_path: Path) -> list[LectureTranscriptChunk]:
        """
        ë‹¨ì¼ JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            file_path: JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            LectureTranscriptChunk ë¦¬ìŠ¤íŠ¸
        """
        # JSON íŒŒì‹±
        transcript = self._load_transcript(file_path)
        if not transcript or not transcript.text.strip():
            return []

        # ì²­í‚¹
        chunks = self._chunk_transcript(transcript)

        return chunks

    def process_directory(
        self,
        directory: Path,
        verbose: bool = True,
    ) -> list[LectureTranscriptChunk]:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            directory: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬
            verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€

        Returns:
            ëª¨ë“  LectureTranscriptChunk ë¦¬ìŠ¤íŠ¸
        """
        json_files = sorted(directory.glob("*.json"))

        if verbose:
            print(f"\nğŸ“ {directory.name}: {len(json_files)}ê°œ íŒŒì¼")

        all_chunks: list[LectureTranscriptChunk] = []
        skipped = 0

        for json_file in json_files:
            chunks = self.process_file(json_file)
            if chunks:
                all_chunks.extend(chunks)
                if verbose:
                    print(f"   âœ“ {json_file.stem[:50]}... â†’ {len(chunks)}ì²­í¬")
            else:
                skipped += 1
                if verbose:
                    print(f"   âœ— {json_file.stem[:50]}... (ë¹ˆ íŒŒì¼)")

        if verbose:
            print(f"\n   ì´: {len(all_chunks)}ì²­í¬ ({skipped}ê°œ ìŠ¤í‚µ)")

        return all_chunks

    def _load_transcript(self, file_path: Path) -> ParsedTranscript | None:
        """JSON íŒŒì¼ì„ ë¡œë“œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            with open(file_path, encoding="utf-8") as f:
                data = json.load(f)

            lecture_name = data.get("lecture_name", file_path.stem)
            source_file = data.get("source_file", "")
            text = data.get("text", "")

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if len(text) < 50:
                return None

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            course, lecture_num, lecture_title = self._extract_metadata(lecture_name)

            return ParsedTranscript(
                file_path=file_path,
                lecture_name=lecture_name,
                source_file=source_file,
                text=text,
                course=course,
                lecture_num=lecture_num,
                lecture_title=lecture_title,
            )

        except Exception as e:
            print(f"   âš ï¸ íŒŒì‹± ì˜¤ë¥˜ {file_path.name}: {e}")
            return None

    def _extract_metadata(self, lecture_name: str) -> tuple[str, str, str]:
        """
        ê°•ì˜ëª…ì—ì„œ ê³¼ëª©, ê°•ì˜ë²ˆí˜¸, ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ì˜ˆì‹œ:
        - "[RecSys ì´ë¡ ] (2ê°•) ì¶”ì²œ ì‹œìŠ¤í…œ Basic 2"
          â†’ ("RecSys ì´ë¡ ", "2ê°•", "ì¶”ì²œ ì‹œìŠ¤í…œ Basic 2")
        - "(1ê°•) Introduction to Computer Vision"
          â†’ ("Computer Vision", "1ê°•", "Introduction to Computer Vision")
        - "GenAI CV part1"
          â†’ ("GenAI CV", "", "GenAI CV part1")

        Returns:
            (ê³¼ëª©ëª…, ê°•ì˜ë²ˆí˜¸, ê°•ì˜ì œëª©) íŠœí”Œ
        """
        course = ""
        lecture_num = ""
        lecture_title = lecture_name

        # 1. [ê³¼ëª©] íŒ¨í„´ ì¶”ì¶œ
        for pattern in self.COURSE_PATTERNS:
            match = re.search(pattern, lecture_name)
            if match:
                course = match.group(1).strip()
                # ê³¼ëª©ëª… ì œê±°í•œ ë‚˜ë¨¸ì§€ê°€ ì œëª©
                lecture_title = lecture_name[match.end() :].strip()
                break

        # 2. ê°•ì˜ ë²ˆí˜¸ ì¶”ì¶œ
        for pattern in self.LECTURE_NUM_PATTERNS:
            match = re.search(pattern, lecture_name)
            if match:
                lecture_num = match.group(1)
                # ê°•ì˜ë²ˆí˜¸ ì œê±°
                lecture_title = re.sub(pattern, "", lecture_title).strip()
                break

        # 3. ê³¼ëª©ì´ ì—†ìœ¼ë©´ ê°•ì˜ ì œëª©ì—ì„œ ì¶”ë¡ 
        if not course:
            course = self._infer_course(lecture_name)

        # ì œëª© ì •ë¦¬ (ì•ë’¤ íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        lecture_title = re.sub(r"^[\s\-_]+|[\s\-_]+$", "", lecture_title)

        return course, lecture_num, lecture_title

    def _infer_course(self, lecture_name: str) -> str:
        """
        íŒŒì¼ëª…ì—ì„œ ê³¼ëª©ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

        í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘:
        """
        lecture_lower = lecture_name.lower()

        # í‚¤ì›Œë“œ â†’ ê³¼ëª© ë§¤í•‘
        course_keywords = {
            "computer vision": "Computer Vision",
            "cv": "Computer Vision",
            "segmentation": "Computer Vision",
            "detection": "Computer Vision",
            "3d understanding": "Computer Vision",
            "3d human": "Computer Vision",
            "generative model": "Generative Model",
            "genai": "Generative AI",
            "diffusion": "Generative Model",
            "vae": "Generative Model",
            "nlp": "NLP",
            "transformer": "NLP",
            "bert": "NLP",
            "language model": "NLP",
            "tokenization": "NLP",
            "word embedding": "NLP",
            "seq2seq": "NLP",
            "attention": "NLP",
            "mrc": "MRC",
            "passage retrieval": "MRC",
            "recsys": "RecSys",
            "recommender": "RecSys",
            "recommendation": "RecSys",
            "collaborative filtering": "RecSys",
            "pytorch": "PyTorch",
            "tensor": "PyTorch",
            "neural network": "Deep Learning Basic",
            "back propagation": "Deep Learning Basic",
            "linear regression": "Machine Learning",
            "classification": "Machine Learning",
            "ai math": "AI Math",
            "ì„ í˜•ëŒ€ìˆ˜": "AI Math",
            "ë²¡í„°": "AI Math",
            "í–‰ë ¬": "AI Math",
            "ê²½ì‚¬í•˜ê°•ë²•": "AI Math",
            "í™•ë¥ ë¡ ": "AI Math",
            "í†µê³„í•™": "AI Math",
            "ml lifecycle": "ML Engineering",
            "streamlit": "ML Engineering",
            "ë°ì´í„° ì¦ê°•": "Data Engineering",
            "ì „ì²˜ë¦¬": "Data Engineering",
            "ê²½ì§„ëŒ€íšŒ": "Competition",
            "object detection": "Object Detection",
        }

        for keyword, course in course_keywords.items():
            if keyword in lecture_lower:
                return course

        # íŒŒì¼ëª… ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ ì‹œë„ (ì˜ˆ: "GenAI CV part1" â†’ "GenAI CV")
        # part/Partë¡œ ë¶„ë¦¬
        part_match = re.split(r"\s+part\s*\d*", lecture_name, flags=re.IGNORECASE)
        if len(part_match) > 1 and part_match[0].strip():
            return part_match[0].strip()

        return "ê¸°íƒ€"

    def _chunk_transcript(self, transcript: ParsedTranscript) -> list[LectureTranscriptChunk]:
        """ë…¹ì·¨ë¡ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = self._preprocess_text(transcript.text)

        # ì»¨í…ìŠ¤íŠ¸ í—¤ë” ìƒì„±
        header = self._create_header(transcript)

        # RecursiveCharacterTextSplitterë¡œ ë¶„í• 
        text_chunks = self.text_splitter.split_text(text)

        # ì²­í¬ ê°ì²´ë¡œ ë³€í™˜
        chunks: list[LectureTranscriptChunk] = []
        for idx, chunk_text in enumerate(text_chunks):
            # ì²« ì²­í¬ì—ëŠ” í—¤ë” ì¶”ê°€
            if idx == 0:
                content = f"{header}\n\n{chunk_text}"
            else:
                # ì´í›„ ì²­í¬ì—ëŠ” ê°„ëµí•œ ì°¸ì¡°ë§Œ
                short_header = f"[{transcript.course}] {transcript.lecture_name}"
                content = f"{short_header}\n\n{chunk_text}"

            chunk = self._create_chunk(
                transcript=transcript,
                content=content,
                chunk_idx=idx,
                total_chunks=len(text_chunks),
            )
            chunks.append(chunk)

        # ì‘ì€ ì²­í¬ ë³‘í•©
        chunks = self._merge_small_chunks(chunks)

        return chunks

    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬."""
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r" {2,}", " ", text)
        # ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        text = re.sub(r"\n{3,}", "\n\n", text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        return text

    def _create_header(self, transcript: ParsedTranscript) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í—¤ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        parts = []

        if transcript.course:
            parts.append(f"ê³¼ëª©: {transcript.course}")
        if transcript.lecture_num:
            parts.append(f"ê°•ì˜: {transcript.lecture_num}")
        if transcript.lecture_title:
            parts.append(f"ì œëª©: {transcript.lecture_title}")

        if parts:
            return f"[ê°•ì˜ ë…¹ì·¨ë¡] {' | '.join(parts)}"
        return f"[ê°•ì˜ ë…¹ì·¨ë¡] {transcript.lecture_name}"

    def _create_chunk(
        self,
        transcript: ParsedTranscript,
        content: str,
        chunk_idx: int,
        total_chunks: int,
    ) -> LectureTranscriptChunk:
        """ì²­í¬ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        chunk_id = self._generate_chunk_id(transcript, chunk_idx)

        metadata = {
            "doc_type": "lecture_transcript",
            "source_file": str(transcript.file_path.name),
            "lecture_name": transcript.lecture_name,
            "course": transcript.course,
            "lecture_num": transcript.lecture_num,
            "lecture_title": transcript.lecture_title,
            "chunk_idx": chunk_idx,
            "total_chunks": total_chunks,
        }

        # ë²„ì „ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        version_meta = create_chunk_version_metadata(
            source_file=transcript.file_path,
            include_hash=True,
        )
        metadata.update(version_meta)

        return LectureTranscriptChunk(
            id=chunk_id,
            content=content,
            metadata=metadata,
        )

    def _generate_chunk_id(self, transcript: ParsedTranscript, chunk_idx: int) -> str:
        """ì²­í¬ ê³ ìœ  ID ìƒì„±."""
        # ê³¼ëª© ìŠ¬ëŸ¬ê·¸
        course_slug = self._slugify(transcript.course) if transcript.course else "etc"

        # ê°•ì˜ ìŠ¬ëŸ¬ê·¸
        lecture_slug = self._slugify(transcript.lecture_name)[:30]

        # íŒŒì¼ í•´ì‹œ
        hash_input = f"{transcript.file_path}_{chunk_idx}".encode()
        short_hash = hashlib.md5(hash_input).hexdigest()[:6]

        return f"transcript_{course_slug}_{lecture_slug}_c{chunk_idx:03d}_{short_hash}"

    def _slugify(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ID ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜."""
        # í•œê¸€ ìœ ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r"[^\wê°€-í£\s-]", "", text)
        text = re.sub(r"[-\s]+", "_", text)
        return text.lower().strip("_")

    def _merge_small_chunks(
        self, chunks: list[LectureTranscriptChunk]
    ) -> list[LectureTranscriptChunk]:
        """ì‘ì€ ì²­í¬ë¥¼ ì´ì „ ì²­í¬ì— ë³‘í•©í•©ë‹ˆë‹¤."""
        if not chunks or len(chunks) < 2:
            return chunks

        merged: list[LectureTranscriptChunk] = []
        min_chars = self.min_chunk_size * 2  # í† í° â†’ ë¬¸ì

        for chunk in chunks:
            if not merged:
                merged.append(chunk)
                continue

            # í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©
            if chunk.char_count < min_chars:
                prev = merged[-1]
                new_content = f"{prev.content}\n\n{chunk.content}"

                # ë³‘í•© í›„ í¬ê¸° ì²´í¬ (ë„ˆë¬´ í¬ì§€ ì•Šìœ¼ë©´ ë³‘í•©)
                if len(new_content) <= self.chunk_size * 2 * 1.3:
                    merged[-1] = LectureTranscriptChunk(
                        id=prev.id,
                        content=new_content,
                        metadata=prev.metadata,
                    )
                    continue

            merged.append(chunk)

        return merged


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python -m lecture_transcript.lecture_transcript_chunker <directory>")
        sys.exit(1)

    directory = Path(sys.argv[1])
    if not directory.exists():
        print(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
        sys.exit(1)

    chunker = LectureTranscriptChunker(chunk_size=1000, chunk_overlap=150)

    print("=" * 70)
    print("ğŸ™ï¸  ê°•ì˜ ë…¹ì·¨ë¡ ì²­í‚¹")
    print("=" * 70)

    all_chunks = chunker.process_directory(directory, verbose=True)

    print("\n" + "=" * 70)
    print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")

    # ìƒ˜í”Œ ì¶œë ¥
    if all_chunks:
        print("\nğŸ“„ ìƒ˜í”Œ ì²­í¬:")
        for chunk in all_chunks[:2]:
            print(f"\n--- {chunk.id} ---")
            print(f"ê³¼ëª©: {chunk.metadata.get('course')}")
            print(f"ê°•ì˜: {chunk.metadata.get('lecture_name')}")
            print(f"í† í°: ~{chunk.token_estimate}")
            print(f"ë‚´ìš©: {chunk.content[:300]}...")
