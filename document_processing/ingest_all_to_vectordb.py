"""
ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ì—¬ VectorDB + BM25ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

Slack Q&A + Notebook + PDF + Weekly Mission ì²­í¬ë¥¼ í•˜ë‚˜ì˜ Collectionì— ì €ì¥í•©ë‹ˆë‹¤.

í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°:
- DocumentLoader ê¸°ë³¸ í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì—¬ ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€
- í˜„ì¬ ì§€ì›: notebook, slack_qa, pdf, weekly_mission

ì¸ë±ìŠ¤:
- VectorDB (Qdrant): Dense ì„ë² ë”© ê²€ìƒ‰
- BM25 (Kiwi): Sparse í‚¤ì›Œë“œ ê²€ìƒ‰
"""

import argparse
import hashlib
import json
import re
import sys
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_naver import ClovaXEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, PointStruct, VectorParams
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë° .env ë¡œë“œ
PROJECT_ROOT = Path(__file__).parent.parent
load_dotenv(PROJECT_ROOT / ".env")

# ë²„ì „ ê´€ë¦¬ ëª¨ë“ˆ ë¡œë“œ
from document_processing.common.versioning import (
    SCHEMA_VERSION,
    PIPELINE_VERSION,
    CORPUS_VERSION,
    get_current_timestamp,
)

sys.path.insert(0, str(PROJECT_ROOT / "app"))

try:
    import importlib.util

    spec = importlib.util.spec_from_file_location(
        "kiwi_bm25_retriever",
        PROJECT_ROOT
        / "app"
        / "naver_connect_chatbot"
        / "rag"
        / "retriever"
        / "kiwi_bm25_retriever.py",
    )
    kiwi_module = importlib.util.module_from_spec(spec)
    sys.modules["kiwi_bm25_retriever"] = kiwi_module
    spec.loader.exec_module(kiwi_module)

    KiwiBM25Retriever = kiwi_module.KiwiBM25Retriever
    get_default_important_pos = kiwi_module.get_default_important_pos
    BM25_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ KiwiBM25Retriever ë¡œë“œ ì‹¤íŒ¨: {e}")
    KiwiBM25Retriever = None
    get_default_important_pos = None
    BM25_AVAILABLE = False

__all__ = ["UnifiedVectorDBIngestion", "DocumentLoader"]


# =============================================================================
# ë°ì´í„° ì†ŒìŠ¤ë³„ Loader (í™•ì¥ ê°€ëŠ¥)
# =============================================================================


class DocumentLoader(ABC):
    """
    ë°ì´í„° ì†ŒìŠ¤ë³„ Document Loader ê¸°ë³¸ í´ë˜ìŠ¤.

    ìƒˆ ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€ ì‹œ ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
        ```python
        class PDFLoader(DocumentLoader):
            doc_type = "pdf"
            default_dir = "document_chunks/pdf_chunks"

            def load(self, directory: Path) -> list[Document]:
                # PDF ì²­í¬ ë¡œë“œ ë¡œì§
                ...
        ```
    """

    doc_type: str = "unknown"  # ë¬¸ì„œ íƒ€ì… (í•„í„°ë§ìš©)
    default_dir: str = ""  # ê¸°ë³¸ ë””ë ‰í† ë¦¬

    @abstractmethod
    def load(self, directory: Path) -> list[Document]:
        """Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        pass


class NotebookLoader(DocumentLoader):
    """ë…¸íŠ¸ë¶ ì²­í¬ Loader."""

    doc_type = "notebook"
    default_dir = "document_chunks/notebook_chunks"

    def load(self, directory: Path) -> list[Document]:
        """Notebook ì²­í¬ë¥¼ Documentë¡œ ë³€í™˜."""
        print(f"\nğŸ““ Notebook ì²­í¬ ë¡œë“œ: {directory}")

        documents: list[Document] = []
        chunk_files = sorted(directory.glob("*_chunks.json"))
        print(f"   íŒŒì¼: {len(chunk_files)}ê°œ")

        for chunk_file in chunk_files:
            if chunk_file.name in ("_summary.json", "all_notebook_chunks.json"):
                continue

            try:
                with open(chunk_file, encoding="utf-8") as f:
                    data = json.load(f)

                for chunk in data.get("chunks", []):
                    doc = self._to_document(chunk)
                    documents.append(doc)

            except Exception as e:
                print(f"   âœ— {chunk_file.name}: {e}")

        print(f"   ì´ ë¬¸ì„œ: {len(documents):,}ê°œ")
        return documents

    def _to_document(self, chunk: dict[str, Any]) -> Document:
        """NotebookChunk dictë¥¼ Documentë¡œ ë³€í™˜."""
        metadata = chunk.get("metadata", {})

        doc_metadata = {
            "doc_id": chunk["id"],
            "doc_type": self.doc_type,
            "source_file": metadata.get("source_file", ""),
            "course": metadata.get("course", ""),
            "topic": metadata.get("topic", ""),
            "subcourse": metadata.get("subcourse", ""),
            "difficulty": metadata.get("difficulty", ""),
            "file_type": metadata.get("file_type", ""),
            "keywords": metadata.get("keywords", []),
        }

        return Document(page_content=chunk["content"], metadata=doc_metadata)


class SlackQALoader(DocumentLoader):
    """
    Slack Q&A Loader.

    í’ˆì§ˆ í•„í„°ë§:
    - ì´ëª¨ì§€ ì½”ë“œ ì œê±° (:emoji:)
    - Slack ë©˜ì…˜ ì œê±° (<@USER_ID>)
    - ì§§ì€ ë‹µë³€ í•„í„°ë§
    - í–‰ì •ì  ë‚´ìš© í•„í„°ë§
    """

    doc_type = "slack_qa"
    default_dir = "document_chunks/slack_qa_auto_filtered"

    # í•„í„°ë§ ì„¤ì •
    MIN_ANSWER_LENGTH = 50  # ë‹µë³€ ìµœì†Œ ê¸¸ì´
    MIN_QUESTION_LENGTH = 15  # ì§ˆë¬¸ ìµœì†Œ ê¸¸ì´

    # í–‰ì •ì  í‚¤ì›Œë“œ (ì´ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ì œì™¸)
    ADMIN_KEYWORDS = [
        "ì¶œì„",
        "ì§€ê°",
        "ì¡°í‡´",
        "íŒ€ë¹Œë”©",
        "íŒ€ ë°°ì •",
        "íŒ€ì› ëª¨ì§‘",
        "ì œì¶œ ê¸°í•œ",
        "ë§ˆê°ì¼",
        "ë§ˆê° ì¼ì •",
        "zoom ë§í¬",
        "ì¤Œ ë§í¬",
        "ì˜¤í”¼ìŠ¤ì•„ì›Œ",
        "ì˜¤í”¼ìŠ¤ ì•„ì›Œ",
        "í”¼ì–´ì„¸ì…˜",
    ]

    # ë‹¨ìˆœ ì‘ë‹µ íŒ¨í„´ (ì´ê²ƒë§Œ ìˆìœ¼ë©´ ì œì™¸)
    SIMPLE_RESPONSE_PATTERNS = [
        r"^ê°ì‚¬í•©ë‹ˆë‹¤\.?!?$",
        r"^ë„¤\.?!?$",
        r"^ë„¤ë„¤\.?!?$",
        r"^ì•Œê²ŸìŠµë‹ˆë‹¤\.?!?$",
        r"^í™•ì¸í–ˆìŠµë‹ˆë‹¤\.?!?$",
        r"^í™•ì¸í–ˆì–´ìš”\.?!?$",
        r"^í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤\.?!?$",
        r"^í•´ê²°ëìŠµë‹ˆë‹¤\.?!?$",
    ]

    def __init__(self) -> None:
        """SlackQALoader ì´ˆê¸°í™”."""
        self._stats = {
            "total_qa": 0,
            "filtered_emoji": 0,
            "filtered_short": 0,
            "filtered_admin": 0,
            "filtered_simple": 0,
            "passed": 0,
        }

    def load(self, directory: Path) -> list[Document]:
        """Slack Q&Aë¥¼ Documentë¡œ ë³€í™˜ (í•„í„°ë§ ì ìš©)."""
        print(f"\nğŸ’¬ Slack Q&A ë¡œë“œ: {directory}")

        documents: list[Document] = []
        json_files = sorted(directory.glob("*_merged.json"))
        print(f"   íŒŒì¼: {len(json_files)}ê°œ")

        for json_file in json_files:
            if json_file.name == "_summary.json":
                continue

            try:
                with open(json_file, encoding="utf-8") as f:
                    data = json.load(f)

                course = data["course"]

                for qa in data["qa_pairs"]:
                    docs = self._qa_to_documents(qa, course)
                    documents.extend(docs)

            except Exception as e:
                print(f"   âœ— {json_file.name}: {e}")

        # í•„í„°ë§ í†µê³„ ì¶œë ¥
        print(f"   í•„í„°ë§ ê²°ê³¼:")
        print(f"      ì „ì²´ Q&A: {self._stats['total_qa']:,}ê°œ")
        print(f"      ì§§ì€ ë‹µë³€ ì œì™¸: {self._stats['filtered_short']:,}ê°œ")
        print(f"      í–‰ì • ë‚´ìš© ì œì™¸: {self._stats['filtered_admin']:,}ê°œ")
        print(f"      ë‹¨ìˆœ ì‘ë‹µ ì œì™¸: {self._stats['filtered_simple']:,}ê°œ")
        print(f"   â†’ ìµœì¢… ë¬¸ì„œ: {len(documents):,}ê°œ")
        return documents

    def _clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ì œ:
        - Slack ì´ëª¨ì§€ ì½”ë“œ ì œê±° (:emoji:)
        - Slack ë©˜ì…˜ ì œê±° (<@USER_ID>)
        - ì—°ì† ê³µë°± ì •ë¦¬
        """
        # Slack ì´ëª¨ì§€ ì½”ë“œ ì œê±°: :emoji_name:
        text = re.sub(r":[a-zA-Z0-9_+-]+:", "", text)
        # Slack ë©˜ì…˜ ì œê±°: <@U1234567>
        text = re.sub(r"<@[A-Z0-9]+>", "", text)
        # Slack ì±„ë„ ë§í¬ ì œê±°: <#C1234567|channel-name>
        text = re.sub(r"<#[A-Z0-9]+\|[^>]+>", "", text)
        # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+", " ", text).strip()
        return text

    def _should_filter(self, question_text: str, answer_text: str) -> str | None:
        """
        í•„í„°ë§ ì—¬ë¶€ íŒë‹¨.

        Returns:
            í•„í„°ë§ ì‚¬ìœ  (í•„í„°ë§ ì•ˆí•  ê²½ìš° None)
        """
        # 1. ì§§ì€ ë‹µë³€ í•„í„°ë§
        if len(answer_text) < self.MIN_ANSWER_LENGTH:
            return "short_answer"

        # 2. ì§§ì€ ì§ˆë¬¸ í•„í„°ë§
        if len(question_text) < self.MIN_QUESTION_LENGTH:
            return "short_question"

        # 3. í–‰ì •ì  í‚¤ì›Œë“œ í•„í„°ë§
        combined = (question_text + " " + answer_text).lower()
        for keyword in self.ADMIN_KEYWORDS:
            if keyword in combined:
                return "admin_content"

        # 4. ë‹¨ìˆœ ì‘ë‹µ íŒ¨í„´ í•„í„°ë§
        for pattern in self.SIMPLE_RESPONSE_PATTERNS:
            if re.match(pattern, answer_text.strip()):
                return "simple_response"

        return None

    def _qa_to_documents(self, qa: dict[str, Any], course: str) -> list[Document]:
        """QA í˜ì–´ë¥¼ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í•„í„°ë§ ì ìš©)."""
        documents: list[Document] = []

        question = qa["question"]
        answers = qa["answers"]
        generation = qa["generation"]
        date = qa["date"]

        # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì •ì œ
        clean_question = self._clean_text(question["text"])

        for idx, answer in enumerate(answers):
            self._stats["total_qa"] += 1

            # ë‹µë³€ í…ìŠ¤íŠ¸ ì •ì œ
            clean_answer = self._clean_text(answer["text"])

            # í•„í„°ë§ ì²´í¬
            filter_reason = self._should_filter(clean_question, clean_answer)
            if filter_reason:
                if filter_reason in ("short_answer", "short_question"):
                    self._stats["filtered_short"] += 1
                elif filter_reason == "admin_content":
                    self._stats["filtered_admin"] += 1
                elif filter_reason == "simple_response":
                    self._stats["filtered_simple"] += 1
                continue

            self._stats["passed"] += 1

            doc_id = f"slack_{generation}_{course}_{date}_{question['timestamp']}_a{idx}"

            content = (
                f"[ê³¼ì •: {course}] [ê¸°ìˆ˜: {generation}ê¸°]\n\n"
                f"ì§ˆë¬¸: {clean_question}\n\n"
                f"ë‹µë³€: {clean_answer}"
            )

            metadata = {
                "doc_id": doc_id,
                "doc_type": self.doc_type,
                "course": course,
                "generation": generation,
                "date": date,
                "question_text": clean_question,
                "answer_text": clean_answer,
                "question_user": question.get("user_name") or question["user"],
                "answer_user": answer.get("user_name") or answer["user"],
                "is_bot": answer["is_bot"],
                "source_file": qa.get("source_file", ""),
            }

            documents.append(Document(page_content=content, metadata=metadata))

        return documents


class PDFLoader(DocumentLoader):
    """PDF ê°•ì˜ ìë£Œ Loader."""

    doc_type = "pdf"
    default_dir = "document_chunks/pdf_chunks"

    def load(self, directory: Path) -> list[Document]:
        """PDF ì²­í¬ë¥¼ Documentë¡œ ë³€í™˜."""
        print(f"\nğŸ“„ PDF ì²­í¬ ë¡œë“œ: {directory}")

        documents: list[Document] = []
        chunk_files = sorted(directory.glob("*_chunks.json"))
        print(f"   íŒŒì¼: {len(chunk_files)}ê°œ")

        for chunk_file in chunk_files:
            if chunk_file.name in ("_summary.json", "all_pdf_chunks.json"):
                continue

            try:
                with open(chunk_file, encoding="utf-8") as f:
                    data = json.load(f)

                for chunk in data.get("chunks", []):
                    doc = self._to_document(chunk)
                    documents.append(doc)

            except Exception as e:
                print(f"   âœ— {chunk_file.name}: {e}")

        print(f"   ì´ ë¬¸ì„œ: {len(documents):,}ê°œ")
        return documents

    def _to_document(self, chunk: dict[str, Any]) -> Document:
        """PDFChunk dictë¥¼ Documentë¡œ ë³€í™˜."""
        metadata = chunk.get("metadata", {})

        doc_metadata = {
            "doc_id": chunk["id"],
            "doc_type": self.doc_type,
            "source_file": metadata.get("source_file", ""),
            "course": metadata.get("course", ""),
            "lecture_num": metadata.get("lecture_num", ""),
            "lecture_title": metadata.get("lecture_title", ""),
            "instructor": metadata.get("instructor", ""),
            "page_start": metadata.get("page_start", 0),
            "page_end": metadata.get("page_end", 0),
        }

        return Document(page_content=chunk["content"], metadata=doc_metadata)


class WeeklyMissionLoader(DocumentLoader):
    """ì£¼ê°„ ë¯¸ì…˜ Loader."""

    doc_type = "weekly_mission"
    default_dir = "document_chunks/mission_chunks"

    def load(self, directory: Path) -> list[Document]:
        """ì£¼ê°„ ë¯¸ì…˜ ì²­í¬ë¥¼ Documentë¡œ ë³€í™˜."""
        print(f"\nğŸ¯ ì£¼ê°„ ë¯¸ì…˜ ì²­í¬ ë¡œë“œ: {directory}")

        documents: list[Document] = []
        chunk_files = sorted(directory.glob("*_chunks.json"))
        print(f"   íŒŒì¼: {len(chunk_files)}ê°œ")

        for chunk_file in chunk_files:
            if chunk_file.name in ("_summary.json", "all_mission_chunks.json"):
                continue

            try:
                with open(chunk_file, encoding="utf-8") as f:
                    data = json.load(f)

                for chunk in data.get("chunks", []):
                    doc = self._to_document(chunk)
                    documents.append(doc)

            except Exception as e:
                print(f"   âœ— {chunk_file.name}: {e}")

        print(f"   ì´ ë¬¸ì„œ: {len(documents):,}ê°œ")
        return documents

    def _to_document(self, chunk: dict[str, Any]) -> Document:
        """MissionChunk dictë¥¼ Documentë¡œ ë³€í™˜."""
        metadata = chunk.get("metadata", {})

        doc_metadata = {
            "doc_id": chunk["id"],
            "doc_type": self.doc_type,
            "source_file": metadata.get("source_file", ""),
            "course": metadata.get("course", ""),
            "week": metadata.get("week", ""),
            "mission_name": metadata.get("mission_name", ""),
            "instructor": metadata.get("instructor", ""),
            "chunk_type": metadata.get("chunk_type", ""),  # problem / rubric
        }

        return Document(page_content=chunk["content"], metadata=doc_metadata)


class LectureTranscriptLoader(DocumentLoader):
    """ê°•ì˜ ë…¹ì·¨ë¡ Loader."""

    doc_type = "lecture_transcript"
    default_dir = "document_chunks/lecture_transcript_chunks"

    def load(self, directory: Path) -> list[Document]:
        """ê°•ì˜ ë…¹ì·¨ë¡ ì²­í¬ë¥¼ Documentë¡œ ë³€í™˜."""
        print(f"\nğŸ™ï¸ ê°•ì˜ ë…¹ì·¨ë¡ ë¡œë“œ: {directory}")

        documents: list[Document] = []
        chunk_files = sorted(directory.glob("*_chunks.json"))
        print(f"   íŒŒì¼: {len(chunk_files)}ê°œ")

        for chunk_file in chunk_files:
            if chunk_file.name in ("_summary.json",):
                continue

            try:
                with open(chunk_file, encoding="utf-8") as f:
                    data = json.load(f)

                for chunk in data.get("chunks", []):
                    doc = self._to_document(chunk)
                    documents.append(doc)

            except Exception as e:
                print(f"   âœ— {chunk_file.name}: {e}")

        print(f"   ì´ ë¬¸ì„œ: {len(documents):,}ê°œ")
        return documents

    def _to_document(self, chunk: dict[str, Any]) -> Document:
        """LectureTranscriptChunk dictë¥¼ Documentë¡œ ë³€í™˜."""
        metadata = chunk.get("metadata", {})

        doc_metadata = {
            "doc_id": chunk["id"],
            "doc_type": self.doc_type,
            "source_file": metadata.get("source_file", ""),
            "lecture_name": metadata.get("lecture_name", ""),
            "course": metadata.get("course", ""),
            "lecture_num": metadata.get("lecture_num", ""),
            "lecture_title": metadata.get("lecture_title", ""),
            "chunk_idx": metadata.get("chunk_idx", 0),
            "total_chunks": metadata.get("total_chunks", 1),
        }

        return Document(page_content=chunk["content"], metadata=doc_metadata)


# =============================================================================
# Loader ë ˆì§€ìŠ¤íŠ¸ë¦¬
# =============================================================================

# ì§€ì›í•˜ëŠ” ëª¨ë“  Loader (ì—¬ê¸°ì— ì¶”ê°€)
REGISTERED_LOADERS: dict[str, type[DocumentLoader]] = {
    "notebook": NotebookLoader,
    "slack_qa": SlackQALoader,
    "pdf": PDFLoader,
    "weekly_mission": WeeklyMissionLoader,
    "lecture_transcript": LectureTranscriptLoader,
}


# =============================================================================
# í†µí•© ì¸ì œìŠ¤íŠ¸
# =============================================================================


class UnifiedVectorDBIngestion:
    """
    ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ì—¬ VectorDBì— ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤.

    í™•ì¥ ë°©ë²•:
    1. DocumentLoader ìƒì† í´ë˜ìŠ¤ ìƒì„±
    2. REGISTERED_LOADERSì— ë“±ë¡
    3. ingest_all() í˜¸ì¶œ ì‹œ ìë™ í¬í•¨

    ì˜ˆì‹œ:
        ```python
        ingestion = UnifiedVectorDBIngestion(
            qdrant_url="http://localhost:6333",
            collection_name="naver_connect_docs",
        )

        ingestion.create_collection(recreate=True)
        ingestion.ingest_all(sources=["notebook", "slack_qa"])
        ```
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "naver_connect_docs",
        embedding_model: str = "bge-m3",
        batch_size: int = 100,
        embedding_batch_size: int = 16,
    ) -> None:
        """
        ì´ˆê¸°í™”.

        ë§¤ê°œë³€ìˆ˜:
            qdrant_url: Qdrant ì„œë²„ URL
            collection_name: ì»¤ë ‰ì…˜ ì´ë¦„ (í†µí•©)
            embedding_model: NAVER ì„ë² ë”© ëª¨ë¸ (bge-m3 ë“±)
            batch_size: VectorDB ë°°ì¹˜ í¬ê¸°
            embedding_batch_size: ì„ë² ë”© API ë°°ì¹˜ í¬ê¸°
        """
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.embedding_batch_size = embedding_batch_size

        # Qdrant í´ë¼ì´ì–¸íŠ¸
        print(f"ğŸ”Œ Qdrant ì—°ê²°: {qdrant_url}")
        self.client = QdrantClient(url=qdrant_url)

        # NAVER Embedding V2 (ClovaXEmbeddings)
        print(f"ğŸ¤– NAVER Embedding: {embedding_model}")
        self.embeddings = ClovaXEmbeddings(model=embedding_model)
        # bge-m3: 1024ì°¨ì›
        self.vector_size = 1024
        print(f"   ë²¡í„° ì°¨ì›: {self.vector_size}")

    def create_collection(self, recreate: bool = False) -> None:
        """ì»¬ë ‰ì…˜ ìƒì„±."""
        print(f"\nğŸ“¦ ì»¬ë ‰ì…˜: {self.collection_name}")

        collections = self.client.get_collections().collections
        exists = any(c.name == self.collection_name for c in collections)

        if exists:
            if recreate:
                print("   ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ...")
                self.client.delete_collection(self.collection_name)
            else:
                print("   âš ï¸ ì´ë¯¸ ì¡´ì¬. recreate=Trueë¡œ ì¬ìƒì„± ê°€ëŠ¥")
                return

        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE),
        )
        print("   âœ… ìƒì„± ì™„ë£Œ")

    # =========================================================================
    # í†µí•© ì¸ì œìŠ¤íŠ¸ (Loader íŒ¨í„´ ì‚¬ìš©)
    # =========================================================================

    def ingest_all(
        self,
        sources: list[str] | None = None,
        custom_dirs: dict[str, Path | str] | None = None,
        build_bm25: bool = True,
        bm25_output_dir: Path | str | None = None,
    ) -> dict[str, int]:
        """
        ë“±ë¡ëœ ë°ì´í„° ì†ŒìŠ¤ë¥¼ VectorDB + BM25ì— ì €ì¥í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            sources: ë¡œë“œí•  ì†ŒìŠ¤ ëª©ë¡ (Noneì´ë©´ êµ¬í˜„ëœ ëª¨ë“  ì†ŒìŠ¤)
                     ì˜ˆ: ["notebook", "slack_qa"]
            custom_dirs: ì†ŒìŠ¤ë³„ ì»¤ìŠ¤í…€ ë””ë ‰í† ë¦¬
                     ì˜ˆ: {"notebook": "/path/to/chunks"}
            build_bm25: BM25 ì¸ë±ìŠ¤ ìƒì„± ì—¬ë¶€
            bm25_output_dir: BM25 ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ

        ë°˜í™˜ê°’:
            ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜

        ì˜ˆì‹œ:
            ```python
            # ëª¨ë“  ì†ŒìŠ¤ ë¡œë“œ (VectorDB + BM25)
            ingestion.ingest_all()

            # íŠ¹ì • ì†ŒìŠ¤ë§Œ, BM25 ì—†ì´
            ingestion.ingest_all(sources=["notebook"], build_bm25=False)
            ```
        """
        # ê¸°ë³¸ê°’: ëª¨ë“  êµ¬í˜„ëœ ì†ŒìŠ¤
        if sources is None:
            sources = ["notebook", "slack_qa", "pdf", "weekly_mission", "lecture_transcript"]

        if custom_dirs is None:
            custom_dirs = {}

        base_dir = Path(__file__).parent.parent / "document_chunks"

        all_documents: list[Document] = []
        stats: dict[str, int] = {}

        print(f"\nğŸ“‹ ë¡œë“œí•  ì†ŒìŠ¤: {sources}")

        for source_name in sources:
            if source_name not in REGISTERED_LOADERS:
                print(f"\nâš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤: {source_name}")
                stats[source_name] = 0
                continue

            # Loader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            loader_class = REGISTERED_LOADERS[source_name]
            loader = loader_class()

            # ë””ë ‰í† ë¦¬ ê²°ì •
            if source_name in custom_dirs:
                directory = Path(custom_dirs[source_name])
            else:
                directory = base_dir / loader.default_dir.split("/")[-1]

            # ë¡œë“œ
            if directory.exists():
                docs = loader.load(directory)
                all_documents.extend(docs)
                stats[source_name] = len(docs)
            else:
                print(f"\nâš ï¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {directory}")
                stats[source_name] = 0

        # í†µê³„ ì¶œë ¥
        print(f"\n{'=' * 60}")
        print(f"ğŸ“Š ë¡œë“œ ì™„ë£Œ: ì´ {len(all_documents):,}ê°œ")
        for source, count in stats.items():
            print(f"   - {source}: {count:,}ê°œ")
        print(f"{'=' * 60}")

        if not all_documents:
            print("\nâš ï¸ ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return stats

        # 1. VectorDBì— ì €ì¥
        self._ingest_documents(all_documents)

        # 2. BM25 ì¸ë±ìŠ¤ ìƒì„±
        if build_bm25:
            self._build_bm25_index(all_documents, bm25_output_dir)

        stats["total"] = len(all_documents)
        return stats

    def _build_bm25_index(
        self,
        documents: list[Document],
        output_dir: Path | str | None = None,
    ) -> None:
        """
        Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            documents: Document ë¦¬ìŠ¤íŠ¸
            output_dir: ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ
        """
        if not BM25_AVAILABLE:
            print("\nâš ï¸ KiwiBM25Retrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. BM25 ì¸ë±ìŠ¤ ìƒì„± ê±´ë„ˆëœ€.")
            return

        if output_dir is None:
            output_dir = PROJECT_ROOT / "sparse_index" / "unified_bm25"
        else:
            output_dir = Path(output_dir)

        print(f"\n{'=' * 60}")
        print("ğŸ” BM25 ì¸ë±ìŠ¤ ìƒì„± (Kiwi í˜•íƒœì†Œ ë¶„ì„)")
        print(f"{'=' * 60}")
        print(f"   ë¬¸ì„œ ìˆ˜: {len(documents):,}ê°œ")
        print(f"   ì €ì¥ ìœ„ì¹˜: {output_dir}")

        try:
            retriever = KiwiBM25Retriever.from_documents(
                documents=documents,
                k=10,
                model_type="knlm",
                typos=None,
                important_pos=get_default_important_pos(),
                load_default_dict=True,
                load_typo_dict=False,
                load_multi_dict=False,
                num_workers=0,
                auto_save=True,
                save_path=output_dir,
                save_user_dict=True,
            )

            print("\n   âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            print(f"   ì´ ë¬¸ì„œ: {len(retriever.docs):,}ê°œ")

            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_query = "PyTorch Tensor ìƒì„±"
            results = retriever.invoke(test_query)
            if results:
                print(f"\n   ğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{test_query}'")
                print(f"      ìƒìœ„ ê²°ê³¼: {results[0].page_content[:100]}...")

        except Exception as e:
            print(f"\n   âœ— BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

    def _split_long_documents(
        self,
        documents: list[Document],
        max_chars: int = 1500,
        overlap: int = 150,
    ) -> list[Document]:
        """ê¸´ ë¬¸ì„œë¥¼ RecursiveCharacterTextSplitterë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        # í•œêµ­ì–´ì— ì í•©í•œ êµ¬ë¶„ì ì„¤ì •
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=max_chars,
            chunk_overlap=overlap,
            separators=["\n\n", "\n", ". ", "ã€‚", "? ", "! ", ", ", " ", ""],
            length_function=len,
        )

        result: list[Document] = []
        split_count = 0

        for doc in documents:
            if len(doc.page_content) <= max_chars:
                result.append(doc)
                continue

            # ê¸´ ë¬¸ì„œ ë¶„í• 
            split_count += 1
            chunks = text_splitter.split_text(doc.page_content)

            for i, chunk in enumerate(chunks):
                new_metadata = doc.metadata.copy()
                new_metadata["chunk_idx"] = i
                new_metadata["total_chunks"] = len(chunks)
                new_metadata["original_doc_id"] = new_metadata.get("doc_id", "")
                new_metadata["doc_id"] = f"{new_metadata.get('doc_id', '')}__chunk_{i}"

                result.append(Document(page_content=chunk, metadata=new_metadata))

        if split_count > 0:
            print(
                f"   âœ‚ï¸ {split_count}ê°œ ë¬¸ì„œ â†’ {len(result) - len(documents) + split_count}ê°œ ì²­í¬ë¡œ ë¶„í• "
            )

        return result

    def _ingest_documents(self, documents: list[Document]) -> None:
        """Document ë¦¬ìŠ¤íŠ¸ë¥¼ VectorDBì— ì €ì¥ (ë¬´í•œ ì¬ì‹œë„, ì ˆëŒ€ ë©ˆì¶”ì§€ ì•ŠìŒ)."""
        # ê¸´ ë¬¸ì„œ ë¶„í•  (1500ì ì²­í¬)
        max_chunk_chars = 1500
        print(f"\nğŸ“„ ë¬¸ì„œ ë¶„í•  ì¤‘... (ìµœëŒ€ {max_chunk_chars}ì/ì²­í¬)")
        documents = self._split_long_documents(documents, max_chars=max_chunk_chars)
        print(f"   ì´ ë¬¸ì„œ: {len(documents):,}ê°œ")

        # NAVER API rate limitì´ ì—„ê²©í•˜ë¯€ë¡œ ì‘ì€ ë°°ì¹˜ + ì¶©ë¶„í•œ ì§€ì—°
        batch_size = min(self.embedding_batch_size, 2)  # ìµœëŒ€ 2ê°œì”© (ì•ˆì „í•˜ê²Œ)
        delay_between_batches = 5.0  # ë°°ì¹˜ ê°„ 5ì´ˆ ëŒ€ê¸°

        print("\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (NAVER Embedding V2)")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}, ë°°ì¹˜ ê°„ ì§€ì—°: {delay_between_batches}s")
        print("   âš ï¸ ë¬´í•œ ì¬ì‹œë„ ëª¨ë“œ: ì ˆëŒ€ ë©ˆì¶”ì§€ ì•ŠìŒ")

        all_points: list[PointStruct] = []
        total_batches = (len(documents) + batch_size - 1) // batch_size

        for batch_idx, i in enumerate(range(0, len(documents), batch_size)):
            batch = documents[i : i + batch_size]
            texts = [doc.page_content for doc in batch]

            attempt = 0
            while True:  # ë¬´í•œ ì¬ì‹œë„
                try:
                    vectors = self.embeddings.embed_documents(texts)

                    for doc, vector in zip(batch, vectors):
                        point_id = self._generate_point_id(doc)
                        payload = self._create_payload(doc)
                        point = PointStruct(
                            id=point_id,
                            vector=vector,
                            payload=payload,
                        )
                        all_points.append(point)

                    # ì„±ê³µ! ì§„í–‰ë¥  ì¶œë ¥
                    progress = (batch_idx + 1) / total_batches * 100
                    print(
                        f"\r   [{batch_idx + 1}/{total_batches}] {progress:.1f}% - {len(all_points)}ê°œ ì™„ë£Œ",
                        end="",
                        flush=True,
                    )

                    # ë‹¤ìŒ ë°°ì¹˜ ì „ ëŒ€ê¸°
                    time.sleep(delay_between_batches)
                    break  # ì„±ê³µ ì‹œ ë‹¤ìŒ ë°°ì¹˜ë¡œ

                except Exception as e:
                    attempt += 1
                    error_msg = str(e)

                    if "429" in error_msg or "rate" in error_msg.lower():
                        # Rate limit - ì¶©ë¶„íˆ ê¸´ ëŒ€ê¸° (ìµœëŒ€ 120ì´ˆ)
                        wait_time = min(120, (2 ** min(attempt, 6)) * 2)
                        print(f"\n   â³ Rate limit, {wait_time}s ëŒ€ê¸° (ì‹œë„ {attempt})")
                        time.sleep(wait_time)
                    else:
                        # ê¸°íƒ€ ì˜¤ë¥˜ - 30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        print(f"\n   âš ï¸ ì˜¤ë¥˜: {error_msg[:100]}, 30s í›„ ì¬ì‹œë„ (ì‹œë„ {attempt})")
                        time.sleep(30)

        print(f"\n\nğŸ’¾ ì €ì¥ ì¤‘... ({len(all_points):,}ê°œ)")
        self._batch_upsert(all_points)
        print("âœ… ëª¨ë“  ì„ë² ë”© ì™„ë£Œ!")

    def _generate_point_id(self, doc: Document) -> str:
        """Documentì—ì„œ ê³ ìœ  ID ìƒì„±."""
        doc_id = doc.metadata.get("doc_id", "")
        if doc_id:
            return hashlib.md5(doc_id.encode()).hexdigest()
        # fallback: content hash
        return hashlib.md5(doc.page_content[:500].encode()).hexdigest()

    def _create_payload(self, doc: Document) -> dict[str, Any]:
        """Documentì—ì„œ payload ìƒì„± (ë¼ì¸ë¦¬ì§€ í•„ë“œ í¬í•¨)."""
        payload = doc.metadata.copy()

        # keywordsê°€ ë¦¬ìŠ¤íŠ¸ë©´ ë¬¸ìì—´ë¡œ ë³€í™˜ (Qdrant í˜¸í™˜)
        if "keywords" in payload and isinstance(payload["keywords"], list):
            payload["keywords_list"] = payload["keywords"]
            payload["keywords"] = ", ".join(payload["keywords"])

        # content ì €ì¥ (ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë°”ë¡œ ì ‘ê·¼)
        payload["content"] = doc.page_content

        # ë¼ì¸ë¦¬ì§€ í•„ë“œ ì¶”ê°€ (ë²„ì „ ì¶”ì ìš©)
        payload["schema_version"] = SCHEMA_VERSION
        payload["pipeline_version"] = PIPELINE_VERSION
        payload["corpus_version"] = CORPUS_VERSION
        payload["ingested_at"] = get_current_timestamp()

        # pipeline_traceê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ê°€
        if "pipeline_trace" not in payload:
            doc_type = payload.get("doc_type", "unknown")
            payload["pipeline_trace"] = [
                f"{doc_type}_loaded",
                "filtered_v2",
                "chunked",
                "ingested",
            ]

        return payload

    def _batch_upsert(self, points: list[PointStruct]) -> None:
        """ë°°ì¹˜ ì €ì¥."""
        batches = [points[i : i + self.batch_size] for i in range(0, len(points), self.batch_size)]

        for batch in tqdm(batches, desc="ì €ì¥"):
            self.client.upsert(collection_name=self.collection_name, points=batch)

        print(f"   âœ… {len(points):,}ê°œ ì €ì¥ ì™„ë£Œ")

    def get_collection_info(self) -> None:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥."""
        try:
            info = self.client.get_collection(self.collection_name)
            print(f"\nğŸ“Š ì»¬ë ‰ì…˜: {self.collection_name}")
            print(f"   ë¬¸ì„œ ìˆ˜: {info.points_count:,}")
            print(f"   ë²¡í„° ì°¨ì›: {self.vector_size}")
        except Exception as e:
            print(f"   âœ— ì¡°íšŒ ì‹¤íŒ¨: {e}")

    def test_search(self, query: str, doc_type: str | None = None, limit: int = 5) -> None:
        """í…ŒìŠ¤íŠ¸ ê²€ìƒ‰."""
        from qdrant_client.models import FieldCondition, Filter, MatchValue

        print(f"\nğŸ” ê²€ìƒ‰: {query}")
        if doc_type:
            print(f"   í•„í„°: doc_type={doc_type}")

        query_vector = self.embeddings.embed_query(query)

        query_filter = None
        if doc_type:
            query_filter = Filter(
                must=[FieldCondition(key="doc_type", match=MatchValue(value=doc_type))]
            )

        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_vector,
            query_filter=query_filter,
            limit=limit,
        ).points

        print(f"\n   ê²°ê³¼: {len(results)}ê°œ")
        for i, r in enumerate(results, 1):
            print(f"\n   [{i}] ìœ ì‚¬ë„: {r.score:.3f}")
            print(f"       íƒ€ì…: {r.payload.get('doc_type')}")
            print(f"       ê³¼ëª©: {r.payload.get('course')}")
            content = r.payload.get("content", "")[:150]
            print(f"       ë‚´ìš©: {content}...")


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description="í†µí•© VectorDB + BM25 ì¸ì œìŠ¤íŠ¸")
    parser.add_argument(
        "--qdrant-url",
        default="http://localhost:6333",
        help="Qdrant URL",
    )
    parser.add_argument(
        "--collection",
        default="naver_connect_docs",
        help="ì»¬ë ‰ì…˜ ì´ë¦„",
    )
    parser.add_argument(
        "--model",
        default="bge-m3",
        help="NAVER ì„ë² ë”© ëª¨ë¸ (bge-m3 ë“±)",
    )
    parser.add_argument("--recreate", action="store_true", help="ì»¬ë ‰ì…˜ ì¬ìƒì„±")
    parser.add_argument("--test", action="store_true", help="í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    parser.add_argument(
        "--sources",
        type=str,
        nargs="+",
        default=None,
        help="ë¡œë“œí•  ì†ŒìŠ¤ (notebook, slack_qa, pdf, weekly_mission)",
    )
    parser.add_argument(
        "--no-bm25",
        action="store_true",
        help="BM25 ì¸ë±ìŠ¤ ìƒì„± ê±´ë„ˆë›°ê¸°",
    )
    parser.add_argument(
        "--bm25-dir",
        type=str,
        default="./sparse_index/unified_bm25",
        help="BM25 ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ",
    )

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸš€ í†µí•© ì¸ë±ìŠ¤ ìƒì„± (VectorDB + BM25)")
    print("=" * 80)
    print(f"   ì§€ì› ì†ŒìŠ¤: {list(REGISTERED_LOADERS.keys())}")
    print(f"   BM25 ì‚¬ìš©: {'âŒ ë¹„í™œì„±í™”' if args.no_bm25 else 'âœ… í™œì„±í™”'}")

    ingestion = UnifiedVectorDBIngestion(
        qdrant_url=args.qdrant_url,
        collection_name=args.collection,
        embedding_model=args.model,
    )

    ingestion.create_collection(recreate=args.recreate)

    stats = ingestion.ingest_all(
        sources=args.sources,
        build_bm25=not args.no_bm25,
        bm25_output_dir=args.bm25_dir,
    )

    print("\n" + "=" * 80)
    print("ğŸ“Š ê²°ê³¼")
    print("=" * 80)
    for source, count in stats.items():
        print(f"   {source}: {count:,}ê°œ")

    ingestion.get_collection_info()

    if args.test:
        print("\n" + "=" * 80)
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (VectorDB)")
        print("=" * 80)
        ingestion.test_search("PyTorch Tensor ìƒì„± ë°©ë²•", doc_type="notebook")
        ingestion.test_search("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±", doc_type="slack_qa")

    print("\n" + "=" * 80)
    print("âœ… ì™„ë£Œ!")
    print("=" * 80)
    print("\nìƒì„±ëœ ì¸ë±ìŠ¤:")
    print(f"   ğŸ“¦ VectorDB: {args.collection} (Qdrant)")
    if not args.no_bm25:
        bm25_path = args.bm25_dir or "./sparse_index/unified_bm25"
        print(f"   ğŸ” BM25: {bm25_path} (Kiwi)")


if __name__ == "__main__":
    main()
