"""
PDF ì²­í‚¹ ëª¨ë“ˆ.

LangChainì˜ RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ RAG ì‹œìŠ¤í…œìš© ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
"""

import hashlib
import re
from dataclasses import dataclass, field
from typing import Any

from langchain_text_splitters import RecursiveCharacterTextSplitter

from .pdf_loader import ParsedPDF, PDFPage
from ..common.versioning import create_chunk_version_metadata
from ..common.filters import (
    is_toc_page,
    remove_copyright_notices,
    remove_headers_footers,
)


@dataclass
class PDFChunk:
    """
    PDFì—ì„œ ì¶”ì¶œí•œ í•˜ë‚˜ì˜ ì²­í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤.

    Attributes:
        id: ê³ ìœ  ì‹ë³„ì
        content: ì²­í¬ ë‚´ìš©
        metadata: ì²­í¬ ë©”íƒ€ë°ì´í„°
    """

    id: str
    content: str
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def token_estimate(self) -> int:
        """
        ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •.

        í•œê¸€ì€ ì•½ 1.5 ê¸€ìë‹¹ 1 í† í°, ì˜ë¬¸ì€ ì•½ 4 ê¸€ìë‹¹ 1 í† í°ìœ¼ë¡œ ê³„ì‚°.
        """
        korean_chars = len(re.findall(r"[ê°€-í£]", self.content))
        other_chars = len(self.content) - korean_chars
        return int(korean_chars / 1.5 + other_chars / 4)

    @property
    def char_count(self) -> int:
        """ë¬¸ì ìˆ˜."""
        return len(self.content)

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜."""
        return {
            "id": self.id,
            "content": self.content,
            "metadata": self.metadata,
            "token_estimate": self.token_estimate,
            "char_count": self.char_count,
        }


class PDFChunker:
    """
    PDFë¥¼ RAGìš© ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤.

    LangChainì˜ RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬
    ì˜ë¯¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

    ì²­í‚¹ ì „ëµ:
    1. ì „ì²´ PDF í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©
    2. RecursiveCharacterTextSplitterë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
    3. ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ë¶€ì—¬

    ì˜ˆì‹œ:
        ```python
        chunker = PDFChunker(chunk_size=1000, chunk_overlap=100)
        chunks = chunker.chunk_pdf(parsed_pdf)

        for chunk in chunks:
            print(f"[{chunk.id}] {chunk.content[:100]}...")
        ```
    """

    # í•œêµ­ì–´ì— ì í•©í•œ êµ¬ë¶„ì (ìš°ì„ ìˆœìœ„ ìˆœ)
    DEFAULT_SEPARATORS = [
        "\n\n\n",  # í˜ì´ì§€/ì„¹ì…˜ êµ¬ë¶„
        "\n\n",  # ë‹¨ë½ êµ¬ë¶„
        "\n",  # ì¤„ë°”ê¿ˆ
        ".\n",  # ë¬¸ì¥ ë + ì¤„ë°”ê¿ˆ
        ". ",  # ë¬¸ì¥ ë
        "ã€‚",  # í•œ/ì¤‘/ì¼ ë§ˆì¹¨í‘œ
        "? ",  # ë¬¼ìŒí‘œ
        "! ",  # ëŠë‚Œí‘œ
        ";\n",  # ì„¸ë¯¸ì½œë¡  + ì¤„ë°”ê¿ˆ
        "; ",  # ì„¸ë¯¸ì½œë¡ 
        ", ",  # ì‰¼í‘œ
        " ",  # ê³µë°±
        "",  # ë¬¸ì ë‹¨ìœ„ (ìµœí›„ ìˆ˜ë‹¨)
    ]

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 100,
        separators: list[str] | None = None,
        length_function: str = "tokens",
        enable_filtering: bool = True,
    ) -> None:
        """
        PDFChunker ì´ˆê¸°í™”.

        Args:
            chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸° (í† í° ë˜ëŠ” ë¬¸ì)
            chunk_overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
            separators: í…ìŠ¤íŠ¸ ë¶„í•  êµ¬ë¶„ì ë¦¬ìŠ¤íŠ¸
            length_function: ê¸¸ì´ ì¸¡ì • ë°©ì‹ ("tokens" ë˜ëŠ” "chars")
            enable_filtering: í•„í„°ë§ í™œì„±í™” (ëª©ì°¨/ì €ì‘ê¶Œ/í—¤ë”í‘¸í„° ì œê±°)
        """
        self.enable_filtering = enable_filtering
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or self.DEFAULT_SEPARATORS
        self.length_function = length_function

        # í† í° ê¸°ë°˜ì´ë©´ ë¬¸ì ìˆ˜ë¡œ ë³€í™˜ (í•œê¸€ 1.5ì/í† í°, ì˜ë¬¸ 4ì/í† í° â†’ í‰ê·  2.5ì/í† í°)
        if length_function == "tokens":
            char_size = int(chunk_size * 2.5)
            char_overlap = int(chunk_overlap * 2.5)
        else:
            char_size = chunk_size
            char_overlap = chunk_overlap

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=char_size,
            chunk_overlap=char_overlap,
            separators=self.separators,
            length_function=len,
            is_separator_regex=False,
        )

    def chunk_pdf(self, pdf: ParsedPDF) -> list[PDFChunk]:
        """
        PDFë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            pdf: íŒŒì‹±ëœ PDF

        Returns:
            PDFChunk ë¦¬ìŠ¤íŠ¸
        """
        # ë¹ˆ PDF ì²´í¬
        if not pdf.non_empty_pages:
            return []

        # í•„í„°ë§: ëª©ì°¨ í˜ì´ì§€ ì œì™¸
        pages_to_process = pdf.pages
        if self.enable_filtering:
            pages_to_process = [page for page in pdf.pages if not is_toc_page(page.text)]

        if not pages_to_process:
            return []

        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•© (í˜ì´ì§€ êµ¬ë¶„ì í¬í•¨)
        full_text = self._combine_pages(pages_to_process)

        # í•„í„°ë§: ì €ì‘ê¶Œ ê³ ì§€ ë° í—¤ë”/í‘¸í„° ì œê±°
        if self.enable_filtering:
            full_text = remove_copyright_notices(full_text)
            full_text = remove_headers_footers(full_text)

        if not full_text.strip():
            return []

        # RecursiveCharacterTextSplitterë¡œ ë¶„í• 
        text_chunks = self.text_splitter.split_text(full_text)

        # PDFChunk ê°ì²´ë¡œ ë³€í™˜
        chunks: list[PDFChunk] = []
        for idx, text in enumerate(text_chunks):
            chunk = self._create_chunk(
                pdf=pdf,
                content=text,
                chunk_idx=idx,
                total_chunks=len(text_chunks),
            )
            chunks.append(chunk)

        return chunks

    def chunk_pdfs(self, pdfs: list[ParsedPDF]) -> list[PDFChunk]:
        """
        ì—¬ëŸ¬ PDFë¥¼ ì¼ê´„ ì²­í‚¹í•©ë‹ˆë‹¤.

        Args:
            pdfs: ParsedPDF ë¦¬ìŠ¤íŠ¸

        Returns:
            ëª¨ë“  PDFì˜ PDFChunk ë¦¬ìŠ¤íŠ¸
        """
        all_chunks: list[PDFChunk] = []

        for pdf in pdfs:
            chunks = self.chunk_pdf(pdf)
            all_chunks.extend(chunks)

        return all_chunks

    def _combine_pages(self, pages: list[PDFPage]) -> str:
        """
        í˜ì´ì§€ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©.

        í˜ì´ì§€ êµ¬ë¶„ì„ ìœ„í•´ ì¶©ë¶„í•œ ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            pages: PDFPage ë¦¬ìŠ¤íŠ¸

        Returns:
            ê²°í•©ëœ í…ìŠ¤íŠ¸
        """
        texts: list[str] = []

        for page in pages:
            if page.is_empty:
                continue

            # í˜ì´ì§€ í…ìŠ¤íŠ¸ ì •ë¦¬
            text = page.text_content.strip()

            # í˜ì´ì§€ ë²ˆí˜¸ ì£¼ì„ ì¶”ê°€ (ê²€ìƒ‰ ì‹œ ì°¸ì¡°ìš©)
            page_text = f"[Page {page.page_num}]\n{text}"
            texts.append(page_text)

        return "\n\n\n".join(texts)

    def _create_chunk(
        self,
        pdf: ParsedPDF,
        content: str,
        chunk_idx: int,
        total_chunks: int,
    ) -> PDFChunk:
        """
        ì²­í¬ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            pdf: ì›ë³¸ PDF
            content: ì²­í¬ ë‚´ìš©
            chunk_idx: ì²­í¬ ì¸ë±ìŠ¤
            total_chunks: ì „ì²´ ì²­í¬ ìˆ˜

        Returns:
            PDFChunk ê°ì²´
        """
        # ì²­í¬ ID ìƒì„±
        chunk_id = self._generate_chunk_id(pdf, chunk_idx)

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "source_file": str(pdf.file_path),
            "file_name": pdf.file_path.name,
            "course": pdf.course,
            "lecture_num": pdf.lecture_num,
            "topic": pdf.topic,
            "instructor": pdf.instructor,
            "chunk_idx": chunk_idx,
            "total_chunks": total_chunks,
            "doc_type": "pdf",
        }

        # ë²„ì „ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        version_meta = create_chunk_version_metadata(
            source_file=pdf.file_path,
            include_hash=True,
        )
        metadata.update(version_meta)

        return PDFChunk(
            id=chunk_id,
            content=content,
            metadata=metadata,
        )

    def _generate_chunk_id(self, pdf: ParsedPDF, chunk_idx: int) -> str:
        """
        ì²­í¬ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        í˜•ì‹: pdf_{course}_{lecture_num}_{chunk_idx}_{hash}

        Args:
            pdf: ì›ë³¸ PDF
            chunk_idx: ì²­í¬ ì¸ë±ìŠ¤

        Returns:
            ê³ ìœ  ID ë¬¸ìì—´
        """
        # ê³¼ëª©ëª… ì •ê·œí™” (ê³µë°± â†’ ì–¸ë”ìŠ¤ì½”ì–´, ì†Œë¬¸ì)
        course_slug = pdf.course.lower().replace(" ", "_").replace("-", "_")
        if not course_slug:
            course_slug = "unknown"

        # ì§§ì€ í•´ì‹œ (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
        hash_input = f"{pdf.file_path}_{chunk_idx}".encode()
        short_hash = hashlib.md5(hash_input).hexdigest()[:6]

        lecture_str = f"lecture{pdf.lecture_num:02d}" if pdf.lecture_num else "lecture00"

        return f"pdf_{course_slug}_{lecture_str}_c{chunk_idx:03d}_{short_hash}"


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    import sys
    from .pdf_loader import PDFLoader

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python -m pdf.pdf_chunker <pdf_path>")
        sys.exit(1)

    pdf_path = sys.argv[1]

    # PDF ë¡œë“œ
    loader = PDFLoader(verbose=True)
    pdf = loader.load_from_file(pdf_path)

    # ì²­í‚¹ (1000 í† í°)
    chunker = PDFChunker(chunk_size=1000, chunk_overlap=100)
    chunks = chunker.chunk_pdf(pdf)

    print(f"\n{'=' * 60}")
    print(f"ğŸ“„ íŒŒì¼: {pdf.file_path.name}")
    print(f"ğŸ“š ê³¼ëª©: {pdf.course} | ğŸ“– {pdf.lecture_num}ê°• | ğŸ“ {pdf.topic}")
    print(f"ğŸ“ƒ í˜ì´ì§€: {pdf.total_pages}ê°œ â†’ ğŸ§© ì²­í¬: {len(chunks)}ê°œ")
    print(f"{'=' * 60}\n")

    # ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
    for i, chunk in enumerate(chunks[:5]):
        print(f"--- Chunk {i} ({chunk.token_estimate} tokens) ---")
        preview = chunk.content[:300]
        print(preview)
        if len(chunk.content) > 300:
            print("...")
        print()
