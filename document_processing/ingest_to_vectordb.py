"""
Slack Q&A ë°ì´í„°ë¥¼ Qdrant VectorDBì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

ì§ˆë¬¸-ë‹µë³€ í˜ì–´ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ìƒì„±í•˜ê³ , í’ë¶€í•œ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
"""

import hashlib
import json
from datetime import datetime
from pathlib import Path
from typing import Any
from uuid import uuid4

from openrouter_embeddings import OpenRouterEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance,
    FieldCondition,
    Filter,
    MatchValue,
    PointStruct,
    VectorParams,
)
from tqdm import tqdm

__all__ = ["QAVectorDBIngestion"]


class QAVectorDBIngestion:
    """
    Slack Q&A ë°ì´í„°ë¥¼ Qdrant VectorDBì— ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤.

    ì˜ˆì‹œ:
        ```python
        ingestion = QAVectorDBIngestion(
            qdrant_url="http://localhost:6333",
            collection_name="slack_qa",
            embedding_model="jhgan/ko-sroberta-multitask"
        )

        ingestion.create_collection()
        ingestion.ingest_from_directory("document_chunks/slack_qa_merged/")
        ```
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "slack_qa",
        embedding_model: str = "qwen/qwen3-embedding-4b",
        batch_size: int = 100,
        embedding_batch_size: int = 16,
    ) -> None:
        """
        ì´ˆê¸°í™”.

        ë§¤ê°œë³€ìˆ˜:
            qdrant_url: Qdrant ì„œë²„ URL
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            batch_size: VectorDB ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
            embedding_batch_size: ì„ë² ë”© API ë°°ì¹˜ í¬ê¸°
        """
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.embedding_batch_size = embedding_batch_size

        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        print(f"ğŸ”Œ Qdrant ì—°ê²° ì¤‘: {qdrant_url}")
        self.client = QdrantClient(url=qdrant_url)

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {embedding_model}")
        self.embedding_model = OpenRouterEmbeddings(model=embedding_model)
        self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
        print(f"   ë²¡í„° ì°¨ì›: {self.vector_size}")

    def create_collection(self, recreate: bool = False) -> None:
        """
        Qdrant ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            recreate: Trueë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
        """
        print(f"\nğŸ“¦ ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
        collections = self.client.get_collections().collections
        collection_exists = any(c.name == self.collection_name for c in collections)

        if collection_exists:
            if recreate:
                print(f"   ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘...")
                self.client.delete_collection(self.collection_name)
            else:
                print(f"   âš ï¸  ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. recreate=Trueë¡œ ì¬ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return

        # ì»¬ë ‰ì…˜ ìƒì„±
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE),
        )
        print(f"   âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")

    def _generate_doc_id(self, qa_data: dict[str, Any]) -> str:
        """
        ë¬¸ì„œ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            qa_data: Q&A ë°ì´í„°

        ë°˜í™˜ê°’:
            ê³ ìœ  ID ë¬¸ìì—´
        """
        # ìƒì„±, ê³¼ì •, ë‚ ì§œ, íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  ID ìƒì„±
        unique_str = (
            f"{qa_data['generation']}_"
            f"{qa_data['course']}_"
            f"{qa_data['date']}_"
            f"{qa_data['question']['timestamp']}_"
            f"{qa_data.get('answer_index', 0)}"
        )
        return hashlib.md5(unique_str.encode()).hexdigest()

    def _parse_course_name(self, course: str) -> tuple[str, str]:
        """
        ê³¼ì •ëª…ì„ íŒŒì‹±í•˜ì—¬ ë ˆë²¨ê³¼ ì£¼ì œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            course: ê³¼ì •ëª… (ì˜ˆ: "level2_cv")

        ë°˜í™˜ê°’:
            (ë ˆë²¨, ì£¼ì œ) íŠœí”Œ
        """
        parts = course.split("_", 1)
        if len(parts) == 2:
            return parts[0], parts[1]
        return course, "unknown"

    def _create_embedding_text(self, qa_data: dict[str, Any]) -> str:
        """
        ì„ë² ë”©í•  í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            qa_data: Q&A ë°ì´í„°

        ë°˜í™˜ê°’:
            ì„ë² ë”©ìš© í…ìŠ¤íŠ¸
        """
        course = qa_data["course"]
        generation = qa_data["generation"]
        question_text = qa_data["question"]["text"]
        answer_text = qa_data["answer"]["text"]
        answer_user = qa_data["answer"].get("user_name") or qa_data["answer"]["user"]

        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
        text = f"""ê³¼ì •: {course}
ê¸°ìˆ˜: {generation}ê¸°

ì§ˆë¬¸: {question_text}

ë‹µë³€: {answer_text}

ì‘ì„±ì: {answer_user}"""

        return text

    def _create_payload(self, qa_data: dict[str, Any]) -> dict[str, Any]:
        """
        Qdrant payloadë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            qa_data: Q&A ë°ì´í„°

        ë°˜í™˜ê°’:
            payload ë”•ì…”ë„ˆë¦¬
        """
        course = qa_data["course"]
        course_level, course_topic = self._parse_course_name(course)
        date_str = qa_data["date"]

        # ë‚ ì§œ íŒŒì‹±
        try:
            date_obj = datetime.strptime(date_str, "%Y-%m-%d")
            year = date_obj.year
            year_month = date_obj.strftime("%Y-%m")
            timestamp = int(date_obj.timestamp())
        except (ValueError, TypeError):
            year = 0
            year_month = ""
            timestamp = 0

        # ë°˜ì‘ ì •ë³´
        reactions = qa_data["answer"].get("metadata", {}).get("reactions") or []
        reaction_count = sum(r.get("count", 0) for r in reactions) if reactions else 0
        has_reactions = reaction_count > 0

        # Payload ìƒì„±
        payload = {
            # í•„í„°ë§ í•µì‹¬ í•„ë“œ
            "course": course,
            "course_level": course_level,
            "course_topic": course_topic,
            "generation": qa_data["generation"],
            # ì‹œê°„ ì •ë³´
            "date": date_str,
            "year": year,
            "year_month": year_month,
            "timestamp": timestamp,
            # ë¬¸ì„œ íƒ€ì…
            "doc_type": "qa_pair",
            "has_bot_answer": qa_data["answer"]["is_bot"],
            # í’ˆì§ˆ ì§€í‘œ
            "has_reactions": has_reactions,
            "reaction_count": reaction_count,
            "answer_count": qa_data.get("answer_count", 1),
            "answer_index": qa_data.get("answer_index", 0),
            # í…ìŠ¤íŠ¸ í•„ë“œ
            "question_text": qa_data["question"]["text"],
            "answer_text": qa_data["answer"]["text"],
            "question_user": qa_data["question"].get("user_name")
            or qa_data["question"]["user"],
            "answer_user": qa_data["answer"].get("user_name") or qa_data["answer"]["user"],
            # ì¶”ì  ì •ë³´
            "thread_id": qa_data.get("thread_id", ""),
            "qa_id": qa_data.get("qa_id", ""),
            "source_file": qa_data["source_file"],
        }

        return payload

    def _process_merged_file(self, file_path: Path) -> list[dict[str, Any]]:
        """
        ë³‘í•©ëœ JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Q&A í˜ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            file_path: JSON íŒŒì¼ ê²½ë¡œ

        ë°˜í™˜ê°’:
            ì²˜ë¦¬ëœ Q&A í˜ì–´ ë¦¬ìŠ¤íŠ¸
        """
        with open(file_path, encoding="utf-8") as f:
            data = json.load(f)

        qa_pairs: list[dict[str, Any]] = []
        course = data["course"]

        for qa in data["qa_pairs"]:
            question = qa["question"]
            answers = qa["answers"]
            answer_count = len(answers)

            # ê° ë‹µë³€ë§ˆë‹¤ ë³„ë„ ë¬¸ì„œ ìƒì„±
            for idx, answer in enumerate(answers):
                # Thread ID ìƒì„±
                thread_id = f"{qa['generation']}_{course}_{qa['date']}_{question['timestamp']}"

                # QA ID ìƒì„±
                qa_id = f"{thread_id}_a{idx}"

                qa_pair = {
                    "generation": qa["generation"],
                    "date": qa["date"],
                    "source_file": qa["source_file"],
                    "course": course,
                    "question": question,
                    "answer": answer,
                    "answer_count": answer_count,
                    "answer_index": idx,
                    "thread_id": thread_id,
                    "qa_id": qa_id,
                }

                qa_pairs.append(qa_pair)

        return qa_pairs

    def ingest_from_directory(self, directory: Path | str) -> dict[str, int]:
        """
        ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  ë³‘í•© íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ VectorDBì— ì €ì¥í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            directory: ë³‘í•© íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ

        ë°˜í™˜ê°’:
            íŒŒì¼ë³„ ì²˜ë¦¬ ê°œìˆ˜ ë”•ì…”ë„ˆë¦¬
        """
        directory = Path(directory)
        print(f"\nğŸ“‚ ë””ë ‰í† ë¦¬ ì²˜ë¦¬: {directory}")

        # JSON íŒŒì¼ ì°¾ê¸°
        json_files = sorted(directory.glob("*_merged.json"))
        print(f"   ë°œê²¬ëœ íŒŒì¼: {len(json_files)}ê°œ")

        if not json_files:
            print("   âš ï¸  ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        stats: dict[str, int] = {}
        all_points: list[PointStruct] = []

        # ëª¨ë“  Q&A í˜ì–´ ìˆ˜ì§‘
        all_qa_pairs: list[tuple[Path, dict[str, Any]]] = []
        for json_file in json_files:
            try:
                qa_pairs = self._process_merged_file(json_file)
                stats[json_file.name] = len(qa_pairs)
                for qa_pair in qa_pairs:
                    all_qa_pairs.append((json_file, qa_pair))
            except Exception as e:
                print(f"\n   âœ— {json_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"\n   ì´ Q&A í˜ì–´: {len(all_qa_pairs):,}ê°œ")

        # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
        print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ë°°ì¹˜ í¬ê¸°: {self.embedding_batch_size})")
        for i in tqdm(
            range(0, len(all_qa_pairs), self.embedding_batch_size),
            desc="ì„ë² ë”© ë°°ì¹˜",
        ):
            batch = all_qa_pairs[i : i + self.embedding_batch_size]

            # ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±
            embedding_texts = [self._create_embedding_text(qa) for _, qa in batch]

            try:
                # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
                vectors = self.embedding_model.encode(
                    embedding_texts, batch_size=self.embedding_batch_size, convert_to_numpy=True
                )

                # í¬ì¸íŠ¸ ìƒì„±
                for (json_file, qa_pair), vector in zip(batch, vectors):
                    payload = self._create_payload(qa_pair)
                    point_id = self._generate_doc_id(qa_pair)
                    point = PointStruct(
                        id=point_id, vector=vector.tolist(), payload=payload
                    )
                    all_points.append(point)

            except Exception as e:
                print(f"\n   âœ— ë°°ì¹˜ {i//self.embedding_batch_size + 1} ì‹¤íŒ¨: {e}")
                # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ê°œë³„ ì²˜ë¦¬ ì‹œë„
                for json_file, qa_pair in batch:
                    try:
                        embedding_text = self._create_embedding_text(qa_pair)
                        vector = self.embedding_model.encode(
                            embedding_text, convert_to_numpy=True
                        )
                        payload = self._create_payload(qa_pair)
                        point_id = self._generate_doc_id(qa_pair)
                        point = PointStruct(
                            id=point_id, vector=vector.tolist(), payload=payload
                        )
                        all_points.append(point)
                    except Exception as e2:
                        print(f"\n   âœ— ê°œë³„ ì²˜ë¦¬ë„ ì‹¤íŒ¨: {e2}")
                        continue

        # ë°°ì¹˜ ì €ì¥
        print(f"\nğŸ’¾ VectorDBì— ì €ì¥ ì¤‘... (ì´ {len(all_points)}ê°œ ë¬¸ì„œ)")
        self._batch_upsert(all_points)

        return stats

    def _batch_upsert(self, points: list[PointStruct]) -> None:
        """
        í¬ì¸íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            points: ì €ì¥í•  í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        total = len(points)
        batches = [points[i : i + self.batch_size] for i in range(0, total, self.batch_size)]

        for batch in tqdm(batches, desc="ë°°ì¹˜ ì €ì¥"):
            self.client.upsert(collection_name=self.collection_name, points=batch)

        print(f"   âœ… {total}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

    def get_collection_info(self) -> None:
        """ì»¬ë ‰ì…˜ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        try:
            info = self.client.get_collection(self.collection_name)
            print(f"\nğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´: {self.collection_name}")
            print(f"   ë²¡í„° ê°œìˆ˜: {info.points_count:,}")
            print(f"   ë²¡í„° ì°¨ì›: {self.vector_size}")
            print(f"   ê±°ë¦¬ ì¸¡ì •: Cosine")
        except Exception as e:
            print(f"   âœ— ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    def test_search(self, query: str, course: str | None = None, limit: int = 5) -> None:
        """
        í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            course: ê³¼ì • í•„í„° (ì„ íƒ)
            limit: ê²°ê³¼ ê°œìˆ˜
        """
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
        print(f"   ì¿¼ë¦¬: {query}")
        if course:
            print(f"   ê³¼ì • í•„í„°: {course}")

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vector = self.embedding_model.encode(query, convert_to_numpy=True).tolist()

        # í•„í„° ìƒì„±
        query_filter = None
        if course:
            query_filter = Filter(
                must=[FieldCondition(key="course", match=MatchValue(value=course))]
            )

        # ê²€ìƒ‰ ìˆ˜í–‰
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_vector,
            query_filter=query_filter,
            limit=limit,
        ).points

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n   ê²°ê³¼: {len(results)}ê°œ")
        for i, result in enumerate(results, 1):
            print(f"\n   [{i}] ìœ ì‚¬ë„: {result.score:.3f}")
            print(f"       ê³¼ì •: {result.payload['course']} ({result.payload['generation']}ê¸°)")
            print(f"       ë‚ ì§œ: {result.payload['date']}")
            print(f"       ì§ˆë¬¸: {result.payload['question_text'][:100]}...")
            print(f"       ë‹µë³€: {result.payload['answer_text'][:100]}...")


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜."""
    import argparse

    parser = argparse.ArgumentParser(description="Slack Q&Aë¥¼ VectorDBì— ì €ì¥")
    parser.add_argument(
        "--input-dir",
        type=str,
        default="/Users/jhj/Desktop/personal/naver_connect_chatbot/document_chunks/slack_qa_merged",
        help="ì…ë ¥ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument("--qdrant-url", type=str, default="http://localhost:6333", help="Qdrant URL")
    parser.add_argument(
        "--collection", type=str, default="slack_qa", help="ì»¬ë ‰ì…˜ ì´ë¦„"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="qwen/qwen3-embedding-4b",
        help="ì„ë² ë”© ëª¨ë¸",
    )
    parser.add_argument("--recreate", action="store_true", help="ì»¬ë ‰ì…˜ ì¬ìƒì„±")
    parser.add_argument("--test", action="store_true", help="í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰")

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸš€ Slack Q&A VectorDB ì €ì¥ ì‹œì‘")
    print("=" * 80)

    # Ingestion ê°ì²´ ìƒì„±
    ingestion = QAVectorDBIngestion(
        qdrant_url=args.qdrant_url,
        collection_name=args.collection,
        embedding_model=args.model,
    )

    # ì»¬ë ‰ì…˜ ìƒì„±
    ingestion.create_collection(recreate=args.recreate)

    # ë°ì´í„° ì €ì¥
    stats = ingestion.ingest_from_directory(args.input_dir)

    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ì²˜ë¦¬ í†µê³„")
    print("=" * 80)
    total_docs = sum(stats.values())
    print(f"ì²˜ë¦¬ëœ íŒŒì¼: {len(stats)}ê°œ")
    print(f"ìƒì„±ëœ ë¬¸ì„œ: {total_docs:,}ê°œ")

    # ì»¬ë ‰ì…˜ ì •ë³´
    ingestion.get_collection_info()

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    if args.test:
        print("\n" + "=" * 80)
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
        print("=" * 80)
        ingestion.test_search("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ í•´ê²°", course="level2_cv")

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()

