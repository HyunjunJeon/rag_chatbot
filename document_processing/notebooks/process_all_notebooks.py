"""
ëª¨ë“  Jupyter Notebook íŒŒì¼ì„ ì¼ê´„ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸.

practice/ ë° home_work/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .ipynb íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬
RAGìš© ì²­í¬ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ì¦ë¶„ ì—…ë°ì´íŠ¸ (ê¸°ë³¸)
    python process_all_notebooks.py

    # ì „ì²´ ì¬ì²˜ë¦¬
    python process_all_notebooks.py --recreate

    # ë³€ê²½ì‚¬í•­ë§Œ í™•ì¸ (dry-run)
    python process_all_notebooks.py --dry-run
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path

# common ëª¨ë“ˆ importì„ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
DOC_PROCESSING_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(DOC_PROCESSING_DIR))

from notebook_chunker import NotebookChunk, NotebookChunker
from notebook_loader import FileType, NotebookLoader, ParsedNotebook
from common.versioning import VersionInfo, save_version_file, load_version_file


def process_notebooks(
    input_dirs: list[Path],
    output_dir: Path,
    solution_only: bool = True,
    max_tokens: int = 500,
    incremental: bool = True,
    dry_run: bool = False,
) -> dict[str, any]:
    """
    ì—¬ëŸ¬ ë””ë ‰í† ë¦¬ì˜ ë…¸íŠ¸ë¶ì„ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        input_dirs: ì…ë ¥ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        solution_only: ì •ë‹µ íŒŒì¼ë§Œ ì½”ë“œ í¬í•¨
        max_tokens: ì²­í¬ ìµœëŒ€ í† í° ìˆ˜
        incremental: Trueë©´ ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬
        dry_run: Trueë©´ ë³€ê²½ì‚¬í•­ë§Œ í™•ì¸ (ì‹¤ì œ ì²˜ë¦¬ ì•ˆí•¨)

    Returns:
        ì²˜ë¦¬ í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    print("=" * 80)
    print("ğŸ““ Jupyter Notebook ì¼ê´„ ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    print(f"\nì…ë ¥ ë””ë ‰í† ë¦¬: {[str(d) for d in input_dirs]}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ì •ë‹µ íŒŒì¼ë§Œ ì½”ë“œ í¬í•¨: {solution_only}")
    print(f"ìµœëŒ€ í† í°: {max_tokens}")
    mode_str = "ğŸ”„ ì¦ë¶„ ì—…ë°ì´íŠ¸" if incremental else "ğŸ” ì „ì²´ ì¬ì²˜ë¦¬"
    if dry_run:
        mode_str += " (DRY-RUN)"
    print(f"ğŸ“Œ ëª¨ë“œ: {mode_str}\n")

    # ë¡œë”ì™€ ì²­ì»¤ ì´ˆê¸°í™”
    loader = NotebookLoader()
    chunker = NotebookChunker(
        max_tokens=max_tokens,
        min_tokens=50,
        include_outputs=True,
        max_output_lines=30,
        solution_only=solution_only,
    )

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)

    # ê¸°ì¡´ ë²„ì „ ì •ë³´ ë¡œë“œ (ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©)
    version_file = output_dir / "_version.json"
    existing_version = load_version_file(version_file) if incremental else None

    # í†µê³„
    stats = {
        "total_notebooks": 0,
        "solution_notebooks": 0,
        "problem_notebooks": 0,
        "total_chunks": 0,
        "by_course": {},
        "by_difficulty": {},
        "failed_files": [],
    }

    all_chunks: list[NotebookChunk] = []
    all_notebooks: list[ParsedNotebook] = []
    all_loaded_notebooks: list[ParsedNotebook] = []

    # ëª¨ë“  ë…¸íŠ¸ë¶ ë¨¼ì € ë¡œë“œ
    for input_dir in input_dirs:
        if not input_dir.exists():
            print(f"âš ï¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_dir}")
            continue

        try:
            notebooks = loader.load_from_directory(input_dir, recursive=True, solution_only=False)
            all_loaded_notebooks.extend(notebooks)
        except Exception as e:
            print(f"âŒ ë””ë ‰í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")

    print(f"\nğŸ“¥ ë°œê²¬ëœ ë…¸íŠ¸ë¶: {len(all_loaded_notebooks)}ê°œ")

    # ì¦ë¶„ ì—…ë°ì´íŠ¸: ë³€ê²½ëœ íŒŒì¼ë§Œ í•„í„°ë§
    if existing_version and incremental:
        current_files = {nb.file_path.name: nb.file_path for nb in all_loaded_notebooks}
        new_files, changed_files, deleted_files = existing_version.get_changed_files(current_files)

        print(f"\nğŸ“Š ë³€ê²½ ê°ì§€ ê²°ê³¼:")
        print(f"   ğŸ†• ìƒˆ íŒŒì¼: {len(new_files)}ê°œ")
        print(f"   âœï¸  ë³€ê²½ë¨: {len(changed_files)}ê°œ")
        print(f"   ğŸ—‘ï¸  ì‚­ì œë¨: {len(deleted_files)}ê°œ")

        if dry_run:
            if new_files:
                print(f"\n   ìƒˆ íŒŒì¼: {new_files[:5]}{'...' if len(new_files) > 5 else ''}")
            if changed_files:
                print(f"   ë³€ê²½ íŒŒì¼: {changed_files[:5]}{'...' if len(changed_files) > 5 else ''}")
            print("\nâœ… DRY-RUN ì™„ë£Œ. ì‹¤ì œ ì²˜ë¦¬ëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {
                "mode": "dry_run",
                "new": len(new_files),
                "changed": len(changed_files),
                "deleted": len(deleted_files),
            }

        # ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ í•„í„°ë§
        files_to_process = set(new_files + changed_files)
        if not files_to_process:
            print("\nâœ… ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {"mode": "incremental", "changes": 0}

        notebooks_to_process = [
            nb for nb in all_loaded_notebooks if nb.file_path.name in files_to_process
        ]
        print(f"\n   â†’ ì²˜ë¦¬ ëŒ€ìƒ: {len(notebooks_to_process)}ê°œ íŒŒì¼")
    else:
        notebooks_to_process = all_loaded_notebooks

    # ê° ë…¸íŠ¸ë¶ ì²˜ë¦¬
    for notebook in notebooks_to_process:
        stats["total_notebooks"] += 1

        # íŒŒì¼ íƒ€ì… í†µê³„
        if notebook.file_type == FileType.SOLUTION:
            stats["solution_notebooks"] += 1
        elif notebook.file_type == FileType.PROBLEM:
            stats["problem_notebooks"] += 1

        # ê³¼ëª©ë³„ í†µê³„
        course = notebook.course or "ê¸°íƒ€"
        if course not in stats["by_course"]:
            stats["by_course"][course] = {"notebooks": 0, "chunks": 0}
        stats["by_course"][course]["notebooks"] += 1

        # ë‚œì´ë„ë³„ í†µê³„
        difficulty = notebook.difficulty.value
        if difficulty not in stats["by_difficulty"]:
            stats["by_difficulty"][difficulty] = 0
        stats["by_difficulty"][difficulty] += 1

        # ì²­í‚¹
        try:
            chunks = chunker.chunk_notebook(notebook)

            if chunks:
                all_chunks.extend(chunks)
                all_notebooks.append(notebook)
                stats["total_chunks"] += len(chunks)
                stats["by_course"][course]["chunks"] += len(chunks)

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                print(
                    f"  âœ“ {notebook.file_path.name}: "
                    f"{len(chunks)}ê°œ ì²­í¬ ({notebook.file_type.value})"
                )
            else:
                print(f"  â—‹ {notebook.file_path.name}: ì²­í¬ ì—†ìŒ")

        except Exception as e:
            print(f"  âœ— {notebook.file_path.name}: ì˜¤ë¥˜ - {e}")
            stats["failed_files"].append({"file": str(notebook.file_path), "error": str(e)})

    # ì²­í¬ ì €ì¥ (ê³¼ëª©ë³„ë¡œ ë¶„ë¦¬)
    save_chunks_by_course(all_chunks, output_dir)

    # ì „ì²´ ì²­í¬ ì €ì¥
    all_chunks_file = output_dir / "all_notebook_chunks.json"
    save_all_chunks(all_chunks, all_chunks_file)

    # í†µê³„ ì €ì¥
    stats["processed_at"] = datetime.now().isoformat()
    stats_file = output_dir / "_summary.json"
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    # ë²„ì „ íŒŒì¼ ì €ì¥
    save_version_info(all_notebooks, all_chunks, output_dir)

    # ìµœì¢… í†µê³„ ì¶œë ¥
    print_final_stats(stats)

    return stats


def save_version_info(
    notebooks: list[ParsedNotebook],
    chunks: list[NotebookChunk],
    output_dir: Path,
) -> None:
    """
    ë²„ì „ ì •ë³´ë¥¼ _version.jsonì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        notebooks: ì²˜ë¦¬ëœ ë…¸íŠ¸ë¶ ë¦¬ìŠ¤íŠ¸
        chunks: ìƒì„±ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    version_info = VersionInfo(
        total_chunks=len(chunks),
    )

    # ê° ë…¸íŠ¸ë¶ì˜ ì²­í¬ ìˆ˜ ê³„ì‚°
    chunks_by_file: dict[str, int] = {}
    for chunk in chunks:
        source_file = chunk.metadata.get("source_file", "")
        if source_file:
            file_name = Path(source_file).name
            chunks_by_file[file_name] = chunks_by_file.get(file_name, 0) + 1

    # ì›ë³¸ íŒŒì¼ ì •ë³´ ì¶”ê°€
    for notebook in notebooks:
        file_name = notebook.file_path.name
        chunk_count = chunks_by_file.get(file_name, 0)
        if chunk_count > 0:
            version_info.add_source_file(
                file_name=file_name,
                file_path=notebook.file_path,
                chunk_count=chunk_count,
            )

    # ì €ì¥
    version_file = output_dir / "_version.json"
    save_version_file(version_file, version_info)
    print(f"\nğŸ“ ë²„ì „ ì •ë³´ ì €ì¥: {version_file.name}")


def save_chunks_by_course(chunks: list[NotebookChunk], output_dir: Path) -> None:
    """
    ì²­í¬ë¥¼ ê³¼ëª©ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        chunks: ì „ì²´ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    # ê³¼ëª©ë³„ ê·¸ë£¹í™”
    by_course: dict[str, list[NotebookChunk]] = {}

    for chunk in chunks:
        course = chunk.metadata.get("course", "ê¸°íƒ€") or "ê¸°íƒ€"
        if course not in by_course:
            by_course[course] = []
        by_course[course].append(chunk)

    # ê° ê³¼ëª©ë³„ íŒŒì¼ ì €ì¥
    for course, course_chunks in by_course.items():
        # íŒŒì¼ëª… ì •ë¦¬
        safe_course = course.replace("/", "_").replace(" ", "_")
        output_file = output_dir / f"{safe_course}_chunks.json"

        data = {
            "course": course,
            "total_chunks": len(course_chunks),
            "chunks": [chunk.to_dict() for chunk in course_chunks],
        }

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ“„ ì €ì¥: {output_file.name} ({len(course_chunks)}ê°œ ì²­í¬)")


def save_all_chunks(chunks: list[NotebookChunk], output_file: Path) -> None:
    """
    ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        chunks: ì „ì²´ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    data = {
        "total_chunks": len(chunks),
        "generated_at": datetime.now().isoformat(),
        "chunks": [chunk.to_dict() for chunk in chunks],
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“¦ ì „ì²´ ì²­í¬ ì €ì¥: {output_file.name}")


def print_final_stats(stats: dict) -> None:
    """ìµœì¢… í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\n{'=' * 80}")
    print("ğŸ“Š ìµœì¢… í†µê³„")
    print(f"{'=' * 80}")

    print("\nğŸ““ ë…¸íŠ¸ë¶ ì²˜ë¦¬:")
    print(f"   ì „ì²´: {stats['total_notebooks']}ê°œ")
    print(f"   ì •ë‹µ íŒŒì¼: {stats['solution_notebooks']}ê°œ")
    print(f"   ë¬¸ì œ íŒŒì¼: {stats['problem_notebooks']}ê°œ")

    print("\nğŸ“ ì²­í¬ ìƒì„±:")
    print(f"   ì „ì²´: {stats['total_chunks']}ê°œ")

    print("\nğŸ“š ê³¼ëª©ë³„:")
    for course, data in sorted(stats["by_course"].items()):
        print(f"   {course}: {data['notebooks']}ê°œ ë…¸íŠ¸ë¶ â†’ {data['chunks']}ê°œ ì²­í¬")

    print("\nâ­ ë‚œì´ë„ë³„:")
    for difficulty, count in sorted(stats["by_difficulty"].items()):
        print(f"   {difficulty}: {count}ê°œ")

    if stats["failed_files"]:
        print(f"\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼: {len(stats['failed_files'])}ê°œ")
        for fail in stats["failed_files"][:5]:
            print(f"   - {fail['file']}: {fail['error']}")

    print(f"\n{'=' * 80}")


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜."""
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent.parent.parent

    parser = argparse.ArgumentParser(
        description="Jupyter Notebookì„ ì²­í‚¹í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=base_dir / "document_chunks" / "notebook_chunks",
        help="ì²­í¬ ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=500,
        help="ì²­í¬ ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 500)",
    )
    parser.add_argument(
        "--include-problems",
        action="store_true",
        help="ë¬¸ì œ íŒŒì¼ì˜ ì½”ë“œë„ í¬í•¨",
    )
    parser.add_argument(
        "--recreate",
        action="store_true",
        help="ì „ì²´ ì¬ì²˜ë¦¬ (ê¸°ì¡´ ì²­í¬ ë¬´ì‹œ)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ë³€ê²½ì‚¬í•­ë§Œ í™•ì¸ (ì‹¤ì œ ì²˜ë¦¬ ì•ˆí•¨)",
    )

    args = parser.parse_args()

    input_dirs = [
        base_dir / "original_documents" / "practice",
        base_dir / "original_documents" / "home_work",
    ]

    # ì²˜ë¦¬ ì‹¤í–‰
    process_notebooks(
        input_dirs=input_dirs,
        output_dir=args.output_dir,
        solution_only=not args.include_problems,
        max_tokens=args.max_tokens,
        incremental=not args.recreate,
        dry_run=args.dry_run,
    )


if __name__ == "__main__":
    main()
