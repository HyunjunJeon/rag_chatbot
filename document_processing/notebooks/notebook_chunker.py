"""
Jupyter Notebook ì²­í‚¹ ëª¨ë“ˆ.

íŒŒì‹±ëœ ë…¸íŠ¸ë¶ì—ì„œ RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì˜ë¯¸ìˆëŠ” ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì„¹ì…˜ ë‹¨ìœ„ë¡œ ê´€ë ¨ ì…€ë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
"""

import hashlib
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

from .notebook_loader import (
    CellType,
    FileType,
    NotebookCell,
    ParsedNotebook,
)
from ..common.versioning import create_chunk_version_metadata
from ..common.filters import remove_copyright_notices


@dataclass
class NotebookChunk:
    """
    ë…¸íŠ¸ë¶ì—ì„œ ì¶”ì¶œí•œ í•˜ë‚˜ì˜ ì²­í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤.

    Attributes:
        id: ê³ ìœ  ì‹ë³„ì
        content: ì²­í¬ ë‚´ìš© (ë§ˆí¬ë‹¤ìš´ + ì½”ë“œ ì¡°í•©)
        metadata: ì²­í¬ ë©”íƒ€ë°ì´í„°
    """

    id: str
    content: str
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def token_estimate(self) -> int:
        """ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (í•œê¸€ ê¸°ì¤€ ê¸€ììˆ˜ / 1.5)."""
        # í•œê¸€ì€ ëŒ€ëµ 1.5 ê¸€ìë‹¹ 1 í† í°, ì˜ë¬¸ì€ 4 ê¸€ìë‹¹ 1 í† í°
        korean_chars = len(re.findall(r"[ê°€-í£]", self.content))
        other_chars = len(self.content) - korean_chars
        return int(korean_chars / 1.5 + other_chars / 4)

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜."""
        return {
            "id": self.id,
            "content": self.content,
            "metadata": self.metadata,
            "token_estimate": self.token_estimate,
        }


class NotebookChunker:
    """
    ë…¸íŠ¸ë¶ì„ RAGìš© ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤.

    ì²­í‚¹ ì „ëµ:
    1. ì„¹ì…˜ ê¸°ë°˜ ë¶„í•  (H2/H3 í—¤ë”© ê¸°ì¤€)
    2. ê´€ë ¨ ì…€ ê·¸ë£¹í™” (ë§ˆí¬ë‹¤ìš´ + ì½”ë“œ + ì¶œë ¥)
    3. í¬ê¸° ì œí•œ (max_tokens)
    4. ë©”íƒ€ë°ì´í„° ë³´ì¡´

    ì˜ˆì‹œ:
        ```python
        chunker = NotebookChunker(max_tokens=500)
        chunks = chunker.chunk_notebook(parsed_notebook)

        for chunk in chunks:
            print(f"[{chunk.id}] {chunk.content[:100]}...")
        ```
    """

    # ì˜ë¯¸ì—†ëŠ” ì½”ë“œ íŒ¨í„´ (ë¬¸ì œ íŒŒì¼ì˜ ë¹ˆ ì½”ë“œ)
    EMPTY_CODE_PATTERNS = [
        r"^\s*#\s*TODO",
        r"^\s*pass\s*$",
        r"^\s*\.\.\.\s*$",
        r"^#.*ì‘ì„±.*ì½”ë“œ",
        r"^#.*ì—¬ê¸°ì—.*ì‘ì„±",
        r"^#.*your\s+code",
    ]

    # importë§Œ ìˆëŠ” ì½”ë“œ íŒ¨í„´
    IMPORT_ONLY_PATTERN = r"^(\s*(import|from)\s+.+\n?)+$"

    def __init__(
        self,
        max_tokens: int = 500,
        min_tokens: int = 50,
        include_outputs: bool = True,
        max_output_lines: int = 30,
        solution_only: bool = True,
        enable_filtering: bool = True,
    ) -> None:
        """
        NotebookChunker ì´ˆê¸°í™”.

        Args:
            max_tokens: ì²­í¬ ìµœëŒ€ í† í° ìˆ˜
            min_tokens: ì²­í¬ ìµœì†Œ í† í° ìˆ˜ (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ë‹¤ìŒ ì„¹ì…˜ê³¼ ë³‘í•©)
            include_outputs: ì½”ë“œ ì¶œë ¥ í¬í•¨ ì—¬ë¶€
            max_output_lines: ì¶œë ¥ ìµœëŒ€ ë¼ì¸ ìˆ˜
            solution_only: ì •ë‹µ íŒŒì¼ë§Œ ì½”ë“œ í¬í•¨
            enable_filtering: í•„í„°ë§ í™œì„±í™” (ì €ì‘ê¶Œ ê³ ì§€ ì œê±°)
        """
        self.max_tokens = max_tokens
        self.min_tokens = min_tokens
        self.include_outputs = include_outputs
        self.max_output_lines = max_output_lines
        self.solution_only = solution_only
        self.enable_filtering = enable_filtering

    def chunk_notebook(self, notebook: ParsedNotebook) -> list[NotebookChunk]:
        """
        ë…¸íŠ¸ë¶ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            notebook: íŒŒì‹±ëœ ë…¸íŠ¸ë¶

        Returns:
            NotebookChunk ë¦¬ìŠ¤íŠ¸
        """
        # ë¹ˆ ë…¸íŠ¸ë¶ ì²´í¬
        if not notebook.non_empty_cells:
            return []

        # ì„¹ì…˜ë³„ë¡œ ì…€ ê·¸ë£¹í™”
        sections = self._split_into_sections(notebook)

        # ê° ì„¹ì…˜ì„ ì²­í¬ë¡œ ë³€í™˜
        chunks: list[NotebookChunk] = []
        for section_idx, section_cells in enumerate(sections):
            section_chunks = self._create_chunks_from_section(
                notebook=notebook,
                section_cells=section_cells,
                section_idx=section_idx,
            )
            chunks.extend(section_chunks)

        # ë„ˆë¬´ ì‘ì€ ì²­í¬ ë³‘í•©
        chunks = self._merge_small_chunks(chunks)

        return chunks

    def chunk_notebooks(self, notebooks: list[ParsedNotebook]) -> list[NotebookChunk]:
        """
        ì—¬ëŸ¬ ë…¸íŠ¸ë¶ì„ ì¼ê´„ ì²­í‚¹í•©ë‹ˆë‹¤.

        Args:
            notebooks: ParsedNotebook ë¦¬ìŠ¤íŠ¸

        Returns:
            ëª¨ë“  ë…¸íŠ¸ë¶ì˜ NotebookChunk ë¦¬ìŠ¤íŠ¸
        """
        all_chunks: list[NotebookChunk] = []

        for notebook in notebooks:
            chunks = self.chunk_notebook(notebook)
            all_chunks.extend(chunks)

        return all_chunks

    def _split_into_sections(self, notebook: ParsedNotebook) -> list[list[NotebookCell]]:
        """
        ë…¸íŠ¸ë¶ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        H2 ë˜ëŠ” H3 í—¤ë”©ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Returns:
            ì„¹ì…˜ë³„ ì…€ ë¦¬ìŠ¤íŠ¸
        """
        sections: list[list[NotebookCell]] = []
        current_section: list[NotebookCell] = []

        for cell in notebook.cells:
            # ìƒˆ ì„¹ì…˜ ì‹œì‘ ì¡°ê±´: H1, H2, H3 í—¤ë”©
            if cell.is_heading and cell.heading_level and cell.heading_level <= 3:
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section:
                    sections.append(current_section)
                current_section = [cell]
            else:
                current_section.append(cell)

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section:
            sections.append(current_section)

        return sections

    def _create_chunks_from_section(
        self,
        notebook: ParsedNotebook,
        section_cells: list[NotebookCell],
        section_idx: int,
    ) -> list[NotebookChunk]:
        """
        í•˜ë‚˜ì˜ ì„¹ì…˜ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            notebook: ì›ë³¸ ë…¸íŠ¸ë¶
            section_cells: ì„¹ì…˜ ë‚´ ì…€ë“¤
            section_idx: ì„¹ì…˜ ì¸ë±ìŠ¤

        Returns:
            NotebookChunk ë¦¬ìŠ¤íŠ¸
        """
        chunks: list[NotebookChunk] = []
        content_parts: list[str] = []
        cell_indices: list[int] = []
        cell_types: list[str] = []

        for cell in section_cells:
            # ë¹ˆ ì…€ ìŠ¤í‚µ
            if cell.is_empty:
                continue

            # ì…€ íƒ€ì…ë³„ ì²˜ë¦¬
            if cell.cell_type == CellType.MARKDOWN:
                content_parts.append(cell.source.strip())
                cell_indices.append(cell.cell_index)
                cell_types.append("markdown")

            elif cell.cell_type == CellType.CODE:
                # ì½”ë“œ í¬í•¨ ì¡°ê±´ í™•ì¸
                if self._should_include_code(cell, notebook.file_type):
                    # ì˜ë¯¸ì—†ëŠ” ì½”ë“œ í•„í„°ë§
                    if not self._is_empty_code(cell.source):
                        code_block = f"```python\n{cell.source.strip()}\n```"
                        content_parts.append(code_block)
                        cell_indices.append(cell.cell_index)
                        cell_types.append("code")

                        # ì¶œë ¥ í¬í•¨
                        if self.include_outputs:
                            output_text = cell.get_output_text(self.max_output_lines)
                            if output_text.strip():
                                output_block = f"```\n# Output:\n{output_text.strip()}\n```"
                                content_parts.append(output_block)
                                cell_types.append("output")

            # í† í° ì œí•œ ì²´í¬ - ë„ˆë¬´ ì»¤ì§€ë©´ ì²­í¬ ìƒì„±
            combined = "\n\n".join(content_parts)
            estimated_tokens = self._estimate_tokens(combined)

            if estimated_tokens > self.max_tokens and len(content_parts) > 1:
                # ë§ˆì§€ë§‰ ë¶€ë¶„ ì œì™¸í•˜ê³  ì²­í¬ ìƒì„±
                chunk_content = "\n\n".join(content_parts[:-1])
                chunk = self._create_chunk(
                    notebook=notebook,
                    content=chunk_content,
                    section_idx=section_idx,
                    chunk_idx=len(chunks),
                    cell_indices=cell_indices[:-1],
                    cell_types=cell_types[:-1],
                )
                chunks.append(chunk)

                # ë§ˆì§€ë§‰ ë¶€ë¶„ìœ¼ë¡œ ìƒˆ ì‹œì‘
                content_parts = [content_parts[-1]]
                cell_indices = [cell_indices[-1]] if cell_indices else []
                cell_types = [cell_types[-1]] if cell_types else []

        # ë‚¨ì€ ë‚´ìš©ìœ¼ë¡œ ì²­í¬ ìƒì„±
        if content_parts:
            chunk_content = "\n\n".join(content_parts)
            chunk = self._create_chunk(
                notebook=notebook,
                content=chunk_content,
                section_idx=section_idx,
                chunk_idx=len(chunks),
                cell_indices=cell_indices,
                cell_types=cell_types,
            )
            chunks.append(chunk)

        return chunks

    def _create_chunk(
        self,
        notebook: ParsedNotebook,
        content: str,
        section_idx: int,
        chunk_idx: int,
        cell_indices: list[int],
        cell_types: list[str],
    ) -> NotebookChunk:
        """
        ì²­í¬ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            notebook: ì›ë³¸ ë…¸íŠ¸ë¶
            content: ì²­í¬ ë‚´ìš©
            section_idx: ì„¹ì…˜ ì¸ë±ìŠ¤
            chunk_idx: ì²­í¬ ì¸ë±ìŠ¤ (ì„¹ì…˜ ë‚´)
            cell_indices: í¬í•¨ëœ ì…€ ì¸ë±ìŠ¤ë“¤
            cell_types: í¬í•¨ëœ ì…€ íƒ€ì…ë“¤ (ë¯¸ì‚¬ìš©, í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)

        Returns:
            NotebookChunk ê°ì²´
        """
        # ê³ ìœ  ID ìƒì„±
        chunk_id = self._generate_chunk_id(notebook, section_idx, chunk_idx)

        # í•„í„°ë§: ì €ì‘ê¶Œ ê³ ì§€ ì œê±°
        if self.enable_filtering:
            content = remove_copyright_notices(content)

        # ìƒëŒ€ ê²½ë¡œ ìƒì„± (original_documents ê¸°ì¤€)
        source_path = str(notebook.file_path)
        if "original_documents" in source_path:
            relative_path = source_path.split("original_documents/")[-1]
        else:
            relative_path = notebook.file_path.name

        # subcourse ì¶”ì¶œ (topicì—ì„œ ë” ì„¸ë¶€ì ì¸ ì •ë³´)
        subcourse = self._extract_subcourse(notebook.file_path, notebook.topic)

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(content, notebook)

        # Content ì•ì— í‚¤ì›Œë“œ í—¤ë” ì¶”ê°€ (ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ)
        if keywords:
            keywords_header = f"[í‚¤ì›Œë“œ: {', '.join(keywords)}]\n\n"
            enriched_content = keywords_header + content
        else:
            enriched_content = content

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "doc_type": "notebook",
            "source_file": relative_path,
            "course": notebook.course,
            "topic": notebook.topic,
            "subcourse": subcourse,
            "difficulty": notebook.difficulty.value,
            "file_type": notebook.file_type.value,
            "keywords": keywords,
            "section_idx": section_idx,
            "chunk_idx": chunk_idx,
            "cell_range": [min(cell_indices), max(cell_indices)] if cell_indices else [],
        }

        # ë²„ì „ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        version_meta = create_chunk_version_metadata(
            source_file=notebook.file_path,
            include_hash=True,
        )
        metadata.update(version_meta)

        return NotebookChunk(id=chunk_id, content=enriched_content, metadata=metadata)

    def _generate_chunk_id(self, notebook: ParsedNotebook, section_idx: int, chunk_idx: int) -> str:
        """ì²­í¬ ê³ ìœ  ID ìƒì„±."""
        # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ prefix
        parts = []
        if notebook.course:
            parts.append(self._slugify(notebook.course))
        if notebook.topic:
            parts.append(self._slugify(notebook.topic))

        prefix = "_".join(parts) if parts else "notebook"

        # ì„¹ì…˜/ì²­í¬ ì¸ë±ìŠ¤ ì¶”ê°€
        base_id = f"{prefix}_s{section_idx:02d}_c{chunk_idx:02d}"

        # ì§§ì€ í•´ì‹œ ì¶”ê°€ (ì¶©ëŒ ë°©ì§€)
        content_hash = hashlib.md5(str(notebook.file_path).encode()).hexdigest()[:6]

        return f"{base_id}_{content_hash}"

    def _slugify(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ URL/ID ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜."""
        # í•œê¸€ ìœ ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r"[^\wê°€-í£\s-]", "", text)
        text = re.sub(r"[-\s]+", "_", text)
        return text.lower().strip("_")[:30]

    def _should_include_code(self, cell: NotebookCell, file_type: FileType) -> bool:
        """
        ì½”ë“œ ì…€ì„ í¬í•¨í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.

        ë¬¸ì œ íŒŒì¼ì˜ ì½”ë“œëŠ” ëŒ€ë¶€ë¶„ ë¹ˆ í…œí”Œë¦¿ì´ë¯€ë¡œ ì œì™¸í•©ë‹ˆë‹¤.
        """
        if not self.solution_only:
            return True

        # ì •ë‹µ íŒŒì¼ë§Œ ì½”ë“œ í¬í•¨
        return file_type == FileType.SOLUTION

    def _is_empty_code(self, source: str) -> bool:
        """ì˜ë¯¸ì—†ëŠ” ë¹ˆ ì½”ë“œì¸ì§€ í™•ì¸."""
        source = source.strip()

        # ë¹ˆ ë¬¸ìì—´
        if not source:
            return True

        # ë¹ˆ ì½”ë“œ íŒ¨í„´ ì²´í¬
        for pattern in self.EMPTY_CODE_PATTERNS:
            if re.search(pattern, source, re.MULTILINE | re.IGNORECASE):
                return True

        # importë§Œ ìˆëŠ” ì½”ë“œ
        if re.match(self.IMPORT_ONLY_PATTERN, source, re.MULTILINE):
            # importë§Œ ìˆìœ¼ë©´ ì œì™¸
            return True

        return False

    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì •."""
        korean_chars = len(re.findall(r"[ê°€-í£]", text))
        other_chars = len(text) - korean_chars
        return int(korean_chars / 1.5 + other_chars / 4)

    def _extract_subcourse(self, file_path: Path, topic: str) -> str:
        """
        íŒŒì¼ ê²½ë¡œì—ì„œ ì„¸ë¶€ ê³¼ëª©(subcourse)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ì˜ˆì‹œ:
        - "(ê¸°ë³¸-1) Tensorì˜ ìƒì„±, ì¡°ì‘, ì—°ì‚°" â†’ "Tensorì˜ ìƒì„±, ì¡°ì‘, ì—°ì‚°"
        - "(ì‹¬í™”-1) BERT4Rec" â†’ "BERT4Rec"
        """
        # í´ë”ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
        for part in file_path.parts:
            # (ê¸°ë³¸-1) ë˜ëŠ” (ì‹¬í™”-1) íŒ¨í„´
            match = re.match(r"^\([ê¸°ë³¸ì‹¬í™”]-?\d*\)\s*(.+)$", part)
            if match:
                return match.group(1).strip()

        # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
        stem = file_path.stem
        # (ì •ë‹µ), (ë¬¸ì œ) ì œê±°
        stem = re.sub(r"\s*\((ì •ë‹µ|ë¬¸ì œ|í•´ì„¤)\)\s*", "", stem)
        # (ê¸°ë³¸-1), (ì‹¬í™”-1) íŒ¨í„´ ë§¤ì¹­
        match = re.match(r"^\([ê¸°ë³¸ì‹¬í™”]-?\d*\)\s*(.+)$", stem)
        if match:
            return match.group(1).strip()

        # topicì´ ìˆìœ¼ë©´ topic ì‚¬ìš©
        if topic and topic != file_path.stem:
            return topic

        return ""

    def _extract_keywords(self, content: str, notebook: ParsedNotebook) -> list[str]:
        """
        ì²­í¬ ë‚´ìš©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ì¶”ì¶œ ëŒ€ìƒ:
        - ì£¼ìš” ê¸°ìˆ  ìš©ì–´ (PyTorch, Tensor, BERT ë“±)
        - ì˜ë¬¸ ëŒ€ë¬¸ìë¡œ ëœ ì•½ì–´ (CUDA, GPU, MLP ë“±)
        - ì½”ë“œì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” í´ë˜ìŠ¤/í•¨ìˆ˜ëª…
        """
        keywords: set[str] = set()

        # 1. ê³¼ëª©/ì£¼ì œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        if notebook.course:
            keywords.add(notebook.course)
        if notebook.topic:
            # ê´„í˜¸ ì œê±°í•˜ê³  ì¶”ê°€
            topic_clean = re.sub(r"^\([^)]+\)\s*", "", notebook.topic)
            if topic_clean:
                keywords.add(topic_clean)

        # 2. ê¸°ìˆ  ìš©ì–´ íŒ¨í„´ ë§¤ì¹­
        tech_patterns = [
            r"\b(PyTorch|TensorFlow|Keras|NumPy|Pandas)\b",
            r"\b(Tensor|Matrix|Vector|Array)\b",
            r"\b(BERT|GPT|Transformer|Attention|RNN|LSTM|GRU|CNN)\b",
            r"\b(Linear|Conv2d|BatchNorm|Dropout|Softmax|ReLU)\b",
            r"\b(Adam|SGD|Optimizer|Loss|CrossEntropy)\b",
            r"\b(GPU|CUDA|CPU)\b",
            r"\b(Embedding|Tokenizer|Encoder|Decoder)\b",
            r"\b(FAISS|BM25|TF-IDF|Retrieval|Retriever)\b",
            r"\b(KorQuAD|SQuAD|ODQA|MRC)\b",
            r"\b(RecSys|Recommendation|Collaborative|Filtering)\b",
            r"\b(Segmentation|Detection|Classification)\b",
            r"\b(VAE|GAN|Diffusion|Contrastive)\b",
            r"\b(HCCF|SASRec|BERT4Rec|DeepFM|NCF)\b",
        ]

        for pattern in tech_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                # ì›ë˜ ì¼€ì´ìŠ¤ ìœ ì§€
                keywords.add(match)

        # 3. ì˜ë¬¸ ëŒ€ë¬¸ì ì•½ì–´ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
        acronyms = re.findall(r"\b([A-Z]{3,})\b", content)
        for acronym in acronyms:
            if acronym not in {"TODO", "NOTE", "FIXME", "OUTPUT"}:
                keywords.add(acronym)

        # 4. ì¤‘ë³µ ì œê±° ë° ì •ë ¬ (ìµœëŒ€ 8ê°œ)
        keywords_list = sorted(keywords, key=lambda x: (-len(x), x.lower()))
        return keywords_list[:8]

    def _merge_small_chunks(self, chunks: list[NotebookChunk]) -> list[NotebookChunk]:
        """
        ë„ˆë¬´ ì‘ì€ ì²­í¬ë¥¼ ì´ì „ ì²­í¬ì™€ ë³‘í•©í•©ë‹ˆë‹¤.

        Args:
            chunks: ì›ë³¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            return chunks

        merged: list[NotebookChunk] = []

        for chunk in chunks:
            if not merged:
                merged.append(chunk)
                continue

            # í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì‘ê³ , ì´ì „ ì²­í¬ì™€ ê°™ì€ íŒŒì¼ì¸ ê²½ìš° ë³‘í•©
            if chunk.token_estimate < self.min_tokens and merged[-1].metadata.get(
                "source_file"
            ) == chunk.metadata.get("source_file"):
                # ì´ì „ ì²­í¬ì— ë³‘í•©
                prev = merged[-1]
                new_content = f"{prev.content}\n\n{chunk.content}"

                # ë³‘í•© í›„ í¬ê¸° ì²´í¬
                if self._estimate_tokens(new_content) <= self.max_tokens * 1.2:
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (í‚¤ì›Œë“œ ë³‘í•©)
                    new_metadata = prev.metadata.copy()
                    prev_keywords = set(prev.metadata.get("keywords", []))
                    chunk_keywords = set(chunk.metadata.get("keywords", []))
                    new_metadata["keywords"] = list(prev_keywords | chunk_keywords)[:8]

                    merged[-1] = NotebookChunk(
                        id=prev.id,
                        content=new_content,
                        metadata=new_metadata,
                    )
                    continue

            merged.append(chunk)

        return merged


def main() -> None:
    """í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ìš© ë©”ì¸ í•¨ìˆ˜."""
    from .notebook_loader import NotebookLoader

    print("=" * 80)
    print("ğŸ“ NotebookChunker í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    loader = NotebookLoader()
    chunker = NotebookChunker(max_tokens=500, solution_only=True)

    # PROJECT_ROOT ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
    project_root = Path(__file__).parent.parent.parent

    # í…ŒìŠ¤íŠ¸ ê²½ë¡œ
    test_dirs = [
        project_root / "original_documents" / "practice",
        project_root / "original_documents" / "home_work",
    ]

    for test_dir in test_dirs:
        if test_dir.exists():
            print(f"\nğŸ“ {test_dir.name} í´ë” ì²˜ë¦¬ ì¤‘...")

            # ì •ë‹µ íŒŒì¼ë§Œ ë¡œë“œ
            notebooks = loader.load_from_directory(test_dir, recursive=True, solution_only=True)
            print(f"   ì •ë‹µ ë…¸íŠ¸ë¶: {len(notebooks)}ê°œ")

            # ì²­í‚¹
            all_chunks = chunker.chunk_notebooks(notebooks)
            print(f"   ìƒì„±ëœ ì²­í¬: {len(all_chunks)}ê°œ")

            # ìƒ˜í”Œ ì¶œë ¥
            for chunk in all_chunks[:2]:
                print(f"\n   ğŸ“„ {chunk.id}")
                print(f"      ê³¼ëª©: {chunk.metadata.get('course')}")
                print(f"      ì£¼ì œ: {chunk.metadata.get('topic')}")
                print(f"      ì„¸ë¶€ê³¼ëª©: {chunk.metadata.get('subcourse')}")
                print(f"      í† í°: ~{chunk.token_estimate}")
                print(f"      í‚¤ì›Œë“œ: {chunk.metadata.get('keywords')}")
                print(f"      ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {chunk.content[:100]}...")


if __name__ == "__main__":
    main()
