"""
ì²˜ë¦¬ëœ ì²­í¬ í’ˆì§ˆ ì ê²€ ëª¨ë“ˆ.

ì ê²€ ëŒ€ìƒ:
- ì²­í¬ ë©”íƒ€ë°ì´í„° ì¼ê´€ì„±
- ì²­í¬ ì½˜í…ì¸  í’ˆì§ˆ
- ì¤‘ë³µ ì²­í¬ íƒì§€
- Slack QA í’ˆì§ˆ í‰ê°€ ê²°ê³¼

ì ê²€ í•­ëª©:
- doc_type, schema_version ë“± í•„ìˆ˜ í•„ë“œ ì¡´ì¬
- ì²­í¬ ê¸¸ì´ ë¶„í¬ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì²­í¬ íƒì§€)
- ë¹ˆ page_content íƒì§€
- í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ íƒì§€

ì‚¬ìš© ì˜ˆ:
    ```python
    from document_processing.audit.auditors.chunk_auditor import ChunkAuditor

    auditor = ChunkAuditor(verbose=True)
    result = await auditor.audit()
    print(result.model_dump_json(indent=2))
    ```
"""

import hashlib
import json
import logging
from collections import Counter
from pathlib import Path
from typing import Any

from document_processing.audit.auditors.base import BaseAuditor
from document_processing.audit.models.audit_result import LayerResult, LayerStats, Severity
from document_processing.common.versioning import SCHEMA_VERSION, PIPELINE_VERSION

logger = logging.getLogger(__name__)


class ChunkAuditor(BaseAuditor):
    """ì²˜ë¦¬ëœ ì²­í¬ í’ˆì§ˆ ì ê²€ê¸°."""

    layer_name = "chunks"

    # ì ê²€ ëŒ€ìƒ ë””ë ‰í† ë¦¬
    TRANSCRIPT_CHUNKS_DIR = "document_chunks/lecture_transcript_chunks"
    SLACK_QA_SCORED_DIR = "document_chunks/slack_qa_scored"

    # ì²­í¬ ê¸¸ì´ ì„ê³„ê°’
    MIN_CHUNK_LENGTH = 50  # 50ì ë¯¸ë§Œì€ ë„ˆë¬´ ì§§ìŒ
    MAX_CHUNK_LENGTH = 5000  # 5000ì ì´ˆê³¼ëŠ” ë„ˆë¬´ ê¹€
    WARN_SHORT_LENGTH = 100  # 100ì ë¯¸ë§Œì€ ê²½ê³ 

    # í•„ìˆ˜ ë©”íƒ€ë°ì´í„° í•„ë“œ
    REQUIRED_METADATA_FIELDS = ["doc_type"]
    OPTIONAL_METADATA_FIELDS = ["schema_version", "pipeline_version", "source_file"]

    async def audit(self) -> LayerResult:
        """ì²­í¬ í’ˆì§ˆ ì ê²€ ì‹¤í–‰."""
        result = self.create_result()
        self.start_timer()

        stats_extra: dict[str, Any] = {
            "length_distribution": {},
            "metadata_coverage": {},
            "duplicates": {},
            "by_doc_type": {},
        }

        all_chunks: list[dict[str, Any]] = []
        total_items = 0
        checked_items = 0
        passed_items = 0
        failed_items = 0

        # 1. ê°•ì˜ ë…¹ìŒ ì²­í¬ ì ê²€
        transcript_chunks = await self._load_transcript_chunks()
        all_chunks.extend(transcript_chunks)

        # 2. Slack QA ì²­í¬ ì ê²€
        slack_chunks = await self._load_slack_qa_chunks()
        all_chunks.extend(slack_chunks)

        total_items = len(all_chunks)
        stats_extra["total_chunks"] = total_items

        # 3. ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± ì ê²€
        metadata_stats = self._audit_metadata(all_chunks)
        stats_extra["metadata_coverage"] = metadata_stats
        failed_items += metadata_stats.get("missing_required", 0)

        # 4. ì²­í¬ ê¸¸ì´ ì ê²€
        length_stats = self._audit_chunk_lengths(all_chunks)
        stats_extra["length_distribution"] = length_stats
        failed_items += length_stats.get("too_short", 0) + length_stats.get("too_long", 0)

        # 5. ì¤‘ë³µ ì²­í¬ íƒì§€
        duplicate_stats = self._detect_duplicates(all_chunks)
        stats_extra["duplicates"] = duplicate_stats

        # 6. doc_typeë³„ í†µê³„
        doc_type_stats = self._analyze_by_doc_type(all_chunks)
        stats_extra["by_doc_type"] = doc_type_stats

        # 7. ë¹ˆ ì½˜í…ì¸  ì ê²€
        empty_count = self._count_empty_content(all_chunks)
        stats_extra["empty_content"] = empty_count
        failed_items += empty_count

        checked_items = total_items
        passed_items = checked_items - failed_items

        # í†µê³„ ì—…ë°ì´íŠ¸
        result.total_items = total_items
        result.stats = LayerStats(
            total_items=total_items,
            checked_items=checked_items,
            passed_items=passed_items,
            failed_items=failed_items,
            extra=stats_extra,
        )

        return self.finalize_result()

    async def _load_transcript_chunks(self) -> list[dict[str, Any]]:
        """ê°•ì˜ ë…¹ìŒ ì²­í¬ ë¡œë“œ."""
        chunks_dir = self.resolve_path(self.TRANSCRIPT_CHUNKS_DIR)
        all_chunks: list[dict[str, Any]] = []

        if not chunks_dir.exists():
            self.add_issue(
                severity=Severity.WARNING,
                category="directory",
                message=f"ê°•ì˜ ë…¹ìŒ ì²­í¬ ë””ë ‰í† ë¦¬ ì—†ìŒ: {chunks_dir}",
            )
            return all_chunks

        json_files = list(chunks_dir.glob("*_chunks.json"))
        self.log_progress(0, len(json_files), "Loading transcript chunks...")

        for i, json_path in enumerate(json_files):
            self.log_progress(i + 1, len(json_files), json_path.name)

            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # ì²­í¬ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ì§€ì›)
                chunks = data if isinstance(data, list) else data.get("chunks", [])

                for chunk in chunks:
                    if isinstance(chunk, dict):
                        chunk["_source_file"] = str(json_path)
                        chunk["_doc_type"] = "lecture_transcript"
                        all_chunks.append(chunk)

            except Exception as e:
                self.add_issue(
                    severity=Severity.WARNING,
                    category="parse",
                    message=f"ì²­í¬ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}",
                    file_path=str(json_path),
                )

        return all_chunks

    async def _load_slack_qa_chunks(self) -> list[dict[str, Any]]:
        """Slack QA ì²­í¬ ë¡œë“œ."""
        chunks_dir = self.resolve_path(self.SLACK_QA_SCORED_DIR)
        all_chunks: list[dict[str, Any]] = []

        if not chunks_dir.exists():
            self.add_issue(
                severity=Severity.WARNING,
                category="directory",
                message=f"Slack QA ì²­í¬ ë””ë ‰í† ë¦¬ ì—†ìŒ: {chunks_dir}",
            )
            return all_chunks

        json_files = list(chunks_dir.glob("*.json"))
        self.log_progress(0, len(json_files), "Loading Slack QA chunks...")

        for i, json_path in enumerate(json_files):
            self.log_progress(i + 1, len(json_files), json_path.name)

            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # qa_pairs ë°°ì—´ ì²˜ë¦¬
                qa_pairs = data.get("qa_pairs", [])
                for qa in qa_pairs:
                    chunk = {
                        "page_content": self._format_qa_content(qa),
                        "metadata": qa.get("evaluation", {}),
                        "_source_file": str(json_path),
                        "_doc_type": "slack_qa",
                    }
                    all_chunks.append(chunk)

            except Exception as e:
                self.add_issue(
                    severity=Severity.WARNING,
                    category="parse",
                    message=f"Slack QA íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}",
                    file_path=str(json_path),
                )

        return all_chunks

    def _format_qa_content(self, qa: dict) -> str:
        """Q&A ìŒì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·."""
        question = qa.get("question", {})
        question_text = question.get("text", "") if isinstance(question, dict) else str(question)

        answers = qa.get("answers", [])
        answer_texts = []
        for ans in answers:
            if isinstance(ans, dict):
                answer_texts.append(ans.get("text", ""))
            else:
                answer_texts.append(str(ans))

        return f"Q: {question_text}\nA: {' '.join(answer_texts)}"

    def _audit_metadata(self, chunks: list[dict[str, Any]]) -> dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¼ê´€ì„± ì ê²€."""
        stats = {
            "total": len(chunks),
            "missing_required": 0,
            "missing_optional": {},
            "version_mismatch": 0,
            "field_coverage": {},
        }

        field_counts: Counter[str] = Counter()

        for chunk in chunks:
            metadata = chunk.get("metadata", {})

            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            for field in self.REQUIRED_METADATA_FIELDS:
                if field in metadata or f"_{field}" in chunk:
                    field_counts[field] += 1
                else:
                    stats["missing_required"] += 1

            # ì„ íƒ í•„ë“œ í™•ì¸
            for field in self.OPTIONAL_METADATA_FIELDS:
                if field in metadata:
                    field_counts[field] += 1

            # ë²„ì „ ì¼ì¹˜ í™•ì¸
            schema_ver = metadata.get("schema_version", "")
            pipeline_ver = metadata.get("pipeline_version", "")

            if schema_ver and schema_ver != SCHEMA_VERSION:
                stats["version_mismatch"] += 1
            if pipeline_ver and pipeline_ver != PIPELINE_VERSION:
                stats["version_mismatch"] += 1

        # í•„ë“œë³„ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        total = len(chunks) if chunks else 1
        for field, count in field_counts.items():
            stats["field_coverage"][field] = round(count / total * 100, 2)

        # ì´ìŠˆ ì¶”ê°€
        if stats["missing_required"] > 0:
            self.add_issue(
                severity=Severity.WARNING,
                category="metadata",
                message=f"í•„ìˆ˜ ë©”íƒ€ë°ì´í„° í•„ë“œ ëˆ„ë½: {stats['missing_required']}ê°œ ì²­í¬",
                details={"missing_fields": self.REQUIRED_METADATA_FIELDS},
            )

        if stats["version_mismatch"] > 0:
            self.add_issue(
                severity=Severity.INFO,
                category="metadata",
                message=f"ë²„ì „ ë¶ˆì¼ì¹˜ ì²­í¬: {stats['version_mismatch']}ê°œ (í˜„ì¬: schema={SCHEMA_VERSION}, pipeline={PIPELINE_VERSION})",
            )

        return stats

    def _audit_chunk_lengths(self, chunks: list[dict[str, Any]]) -> dict[str, Any]:
        """ì²­í¬ ê¸¸ì´ ë¶„í¬ ë¶„ì„."""
        stats = {
            "total": len(chunks),
            "too_short": 0,
            "too_long": 0,
            "warning_short": 0,
            "min_length": float("inf"),
            "max_length": 0,
            "avg_length": 0,
            "distribution": {
                "0-50": 0,
                "50-100": 0,
                "100-500": 0,
                "500-1000": 0,
                "1000-3000": 0,
                "3000-5000": 0,
                "5000+": 0,
            },
        }

        total_length = 0
        short_chunks: list[str] = []
        long_chunks: list[str] = []

        for chunk in chunks:
            content = chunk.get("page_content", "") or chunk.get("content", "")
            length = len(content)
            total_length += length

            # ìµœì†Œ/ìµœëŒ€
            stats["min_length"] = min(stats["min_length"], length)
            stats["max_length"] = max(stats["max_length"], length)

            # ë¶„í¬
            if length < 50:
                stats["distribution"]["0-50"] += 1
            elif length < 100:
                stats["distribution"]["50-100"] += 1
            elif length < 500:
                stats["distribution"]["100-500"] += 1
            elif length < 1000:
                stats["distribution"]["500-1000"] += 1
            elif length < 3000:
                stats["distribution"]["1000-3000"] += 1
            elif length < 5000:
                stats["distribution"]["3000-5000"] += 1
            else:
                stats["distribution"]["5000+"] += 1

            # ì„ê³„ê°’ í™•ì¸
            if length < self.MIN_CHUNK_LENGTH:
                stats["too_short"] += 1
                short_chunks.append(chunk.get("_source_file", "unknown"))
            elif length < self.WARN_SHORT_LENGTH:
                stats["warning_short"] += 1

            if length > self.MAX_CHUNK_LENGTH:
                stats["too_long"] += 1
                long_chunks.append(chunk.get("_source_file", "unknown"))

        # í‰ê·  ê³„ì‚°
        if chunks:
            stats["avg_length"] = round(total_length / len(chunks), 2)

        # ë¬´í•œëŒ€ ì²˜ë¦¬
        if stats["min_length"] == float("inf"):
            stats["min_length"] = 0

        # ì´ìŠˆ ì¶”ê°€
        if stats["too_short"] > 0:
            self.add_issue(
                severity=Severity.WARNING,
                category="length",
                message=f"ë„ˆë¬´ ì§§ì€ ì²­í¬ ({self.MIN_CHUNK_LENGTH}ì ë¯¸ë§Œ): {stats['too_short']}ê°œ",
                details={"sample_files": list(set(short_chunks))[:5]},
            )

        if stats["too_long"] > 0:
            self.add_issue(
                severity=Severity.WARNING,
                category="length",
                message=f"ë„ˆë¬´ ê¸´ ì²­í¬ ({self.MAX_CHUNK_LENGTH}ì ì´ˆê³¼): {stats['too_long']}ê°œ",
                details={"sample_files": list(set(long_chunks))[:5]},
            )

        return stats

    def _detect_duplicates(self, chunks: list[dict[str, Any]]) -> dict[str, Any]:
        """ì¤‘ë³µ ì²­í¬ íƒì§€ (í•´ì‹œ ê¸°ë°˜)."""
        stats = {
            "total": len(chunks),
            "unique": 0,
            "duplicates": 0,
            "duplicate_groups": 0,
        }

        # ì½˜í…ì¸  í•´ì‹œ ê³„ì‚°
        hash_to_chunks: dict[str, list[int]] = {}

        for i, chunk in enumerate(chunks):
            content = chunk.get("page_content", "") or chunk.get("content", "")
            content_hash = hashlib.md5(content.encode("utf-8")).hexdigest()

            if content_hash not in hash_to_chunks:
                hash_to_chunks[content_hash] = []
            hash_to_chunks[content_hash].append(i)

        # ì¤‘ë³µ ì¹´ìš´íŠ¸
        for indices in hash_to_chunks.values():
            if len(indices) == 1:
                stats["unique"] += 1
            else:
                stats["duplicate_groups"] += 1
                stats["duplicates"] += len(indices) - 1  # ì›ë³¸ ì œì™¸

        # ì´ìŠˆ ì¶”ê°€
        if stats["duplicates"] > 0:
            duplicate_rate = round(stats["duplicates"] / len(chunks) * 100, 2)
            self.add_issue(
                severity=Severity.WARNING if duplicate_rate > 5 else Severity.INFO,
                category="duplicate",
                message=f"ì¤‘ë³µ ì²­í¬ ë°œê²¬: {stats['duplicates']}ê°œ ({duplicate_rate}%)",
                details={"duplicate_groups": stats["duplicate_groups"]},
            )

        return stats

    def _analyze_by_doc_type(self, chunks: list[dict[str, Any]]) -> dict[str, Any]:
        """doc_typeë³„ í†µê³„ ë¶„ì„."""
        stats: dict[str, dict[str, Any]] = {}

        for chunk in chunks:
            doc_type = (
                chunk.get("metadata", {}).get("doc_type")
                or chunk.get("_doc_type")
                or "unknown"
            )

            if doc_type not in stats:
                stats[doc_type] = {"count": 0, "total_length": 0}

            stats[doc_type]["count"] += 1

            content = chunk.get("page_content", "") or chunk.get("content", "")
            stats[doc_type]["total_length"] += len(content)

        # í‰ê·  ê¸¸ì´ ê³„ì‚°
        for doc_type, data in stats.items():
            if data["count"] > 0:
                data["avg_length"] = round(data["total_length"] / data["count"], 2)
            else:
                data["avg_length"] = 0

        return stats

    def _count_empty_content(self, chunks: list[dict[str, Any]]) -> int:
        """ë¹ˆ ì½˜í…ì¸  ì²­í¬ ì¹´ìš´íŠ¸."""
        empty_count = 0

        for chunk in chunks:
            content = chunk.get("page_content", "") or chunk.get("content", "")
            if not content or not content.strip():
                empty_count += 1

        if empty_count > 0:
            self.add_issue(
                severity=Severity.WARNING,
                category="content",
                message=f"ë¹ˆ ì½˜í…ì¸  ì²­í¬: {empty_count}ê°œ",
            )

        return empty_count


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.INFO)

    async def main():
        auditor = ChunkAuditor(verbose=True)
        result = await auditor.audit()

        print("\n" + "=" * 60)
        print("ì²­í¬ í’ˆì§ˆ ì ê²€ ê²°ê³¼")
        print("=" * 60)
        print(f"ìƒíƒœ: {result.status}")
        print(f"ì „ì²´ ì²­í¬: {result.total_items}")
        print(f"ì´ìŠˆ: {len(result.issues)}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {result.duration_seconds:.2f}ì´ˆ")

        print("\nğŸ“Š í†µê³„:")
        for key, value in result.stats.extra.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")

        if result.issues:
            print("\nâš ï¸ ë°œê²¬ëœ ì´ìŠˆ:")
            for issue in result.issues:
                print(f"  {issue}")

    asyncio.run(main())
