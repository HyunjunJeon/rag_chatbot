"""
ì¸ë±ìŠ¤ ë™ê¸°í™” ì ê²€ ëª¨ë“ˆ.

ì ê²€ ëŒ€ìƒ:
- BM25 ì¸ë±ìŠ¤ (sparse_index/unified_bm25/)
- Qdrant ë²¡í„° DB (vdb_store/collections/)

ì ê²€ í•­ëª©:
- ì¸ë±ìŠ¤ ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€
- ì¸ë±ì‹±ëœ ë¬¸ì„œ ìˆ˜ ì¼ì¹˜
- í† í°í™” í’ˆì§ˆ (Kiwi í˜•íƒœì†Œ ë¶„ì„)
- ë²¡í„° ì°¨ì› ì¼ì¹˜

ì‚¬ìš© ì˜ˆ:
    ```python
    from document_processing.audit.auditors.index_auditor import IndexAuditor

    auditor = IndexAuditor(verbose=True)
    result = await auditor.audit()
    print(result.model_dump_json(indent=2))
    ```
"""

import json
import logging
import pickle
from pathlib import Path
from typing import Any

from document_processing.audit.auditors.base import BaseAuditor
from document_processing.audit.models.audit_result import LayerResult, LayerStats, Severity

logger = logging.getLogger(__name__)


class IndexAuditor(BaseAuditor):
    """ì¸ë±ìŠ¤ ë™ê¸°í™” ì ê²€ê¸°."""

    layer_name = "indexes"

    # ì¸ë±ìŠ¤ ê²½ë¡œ (base_path ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
    BM25_INDEX_DIR = "sparse_index/unified_bm25"
    BM25_INDEX_FILE = "bm25_index.pkl"
    QDRANT_STORE_DIR = "vdb_store"
    QDRANT_COLLECTION_NAME = "naver_connect_docs"

    # ê¸°ëŒ€ ê°’
    EXPECTED_EMBEDDING_DIM = 1024  # BGE-M3 ì°¨ì›

    async def audit(self) -> LayerResult:
        """ì¸ë±ìŠ¤ ë™ê¸°í™” ì ê²€ ì‹¤í–‰."""
        result = self.create_result()
        self.start_timer()

        stats_extra: dict[str, Any] = {
            "bm25": {},
            "qdrant": {},
            "sync_status": {},
        }

        total_items = 0
        checked_items = 0
        passed_items = 0
        failed_items = 0

        # 1. BM25 ì¸ë±ìŠ¤ ì ê²€
        bm25_stats = await self._audit_bm25_index()
        stats_extra["bm25"] = bm25_stats
        total_items += 1
        checked_items += 1
        if bm25_stats.get("loaded"):
            passed_items += 1
        else:
            failed_items += 1

        # 2. Qdrant ë²¡í„° DB ì ê²€
        qdrant_stats = await self._audit_qdrant_store()
        stats_extra["qdrant"] = qdrant_stats
        total_items += 1
        checked_items += 1
        if qdrant_stats.get("accessible"):
            passed_items += 1
        else:
            failed_items += 1

        # 3. ë™ê¸°í™” ìƒíƒœ ì ê²€
        sync_stats = self._check_sync_status(bm25_stats, qdrant_stats)
        stats_extra["sync_status"] = sync_stats

        # í†µê³„ ì—…ë°ì´íŠ¸
        result.total_items = total_items
        result.stats = LayerStats(
            total_items=total_items,
            checked_items=checked_items,
            passed_items=passed_items,
            failed_items=failed_items,
            extra=stats_extra,
        )

        return self.finalize_result()

    async def _audit_bm25_index(self) -> dict[str, Any]:
        """BM25 ì¸ë±ìŠ¤ ì ê²€."""
        stats: dict[str, Any] = {
            "loaded": False,
            "path": None,
            "file_size_bytes": 0,
            "document_count": 0,
            "vocab_size": 0,
            "sample_tokens": [],
        }

        index_dir = self.resolve_path(self.BM25_INDEX_DIR)
        index_file = index_dir / self.BM25_INDEX_FILE
        stats["path"] = str(index_file)

        if not index_dir.exists():
            self.add_issue(
                severity=Severity.CRITICAL,
                category="index",
                message=f"BM25 ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ì—†ìŒ: {index_dir}",
            )
            return stats

        if not index_file.exists():
            self.add_issue(
                severity=Severity.CRITICAL,
                category="index",
                message=f"BM25 ì¸ë±ìŠ¤ íŒŒì¼ ì—†ìŒ: {index_file}",
            )
            return stats

        # íŒŒì¼ í¬ê¸°
        stats["file_size_bytes"] = index_file.stat().st_size
        stats["file_size_mb"] = round(stats["file_size_bytes"] / (1024 * 1024), 2)

        # ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        load_result = await self._load_bm25_index(index_file)

        if load_result["success"]:
            stats["loaded"] = True
            stats["document_count"] = load_result.get("doc_count", 0)
            stats["vocab_size"] = load_result.get("vocab_size", 0)
            stats["sample_tokens"] = load_result.get("sample_tokens", [])
            stats["avg_doc_length"] = load_result.get("avg_doc_length", 0)
        else:
            self.add_issue(
                severity=Severity.CRITICAL,
                category="index",
                message=f"BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {load_result.get('error')}",
                file_path=str(index_file),
            )

        # ì‚¬ìš©ì ì‚¬ì „ í™•ì¸
        user_dict_path = index_dir / "user_dict.txt"
        if user_dict_path.exists():
            stats["has_user_dict"] = True
            with open(user_dict_path, "r", encoding="utf-8") as f:
                stats["user_dict_entries"] = len(f.readlines())
        else:
            stats["has_user_dict"] = False

        return stats

    async def _load_bm25_index(self, index_path: Path) -> dict[str, Any]:
        """BM25 ì¸ë±ìŠ¤ ë¡œë“œ ë° ê²€ì¦."""

        def load_index() -> dict[str, Any]:
            try:
                with open(index_path, "rb") as f:
                    data = pickle.load(f)

                # KiwiBM25Retrieverì˜ ì €ì¥ í˜•ì‹ í™•ì¸
                if isinstance(data, dict):
                    # ìƒˆ í˜•ì‹: {"bm25": BM25Okapi, "docs": [...], "doc_ids": [...]}
                    bm25 = data.get("bm25")
                    docs = data.get("docs", [])
                    doc_ids = data.get("doc_ids", [])

                    doc_count = len(docs) if docs else (len(doc_ids) if doc_ids else 0)

                    # BM25 ê°ì²´ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    vocab_size = 0
                    avg_doc_length = 0
                    sample_tokens: list[str] = []

                    if bm25 is not None:
                        if hasattr(bm25, "idf"):
                            vocab_size = len(bm25.idf)
                        if hasattr(bm25, "avgdl"):
                            avg_doc_length = round(bm25.avgdl, 2)
                        if hasattr(bm25, "doc_freqs"):
                            # ìƒìœ„ ë¹ˆë„ í† í° ìƒ˜í”Œë§
                            sorted_tokens = sorted(
                                bm25.doc_freqs.items(),
                                key=lambda x: x[1],
                                reverse=True,
                            )
                            sample_tokens = [t[0] for t in sorted_tokens[:10]]

                    return {
                        "success": True,
                        "doc_count": doc_count,
                        "vocab_size": vocab_size,
                        "avg_doc_length": avg_doc_length,
                        "sample_tokens": sample_tokens,
                    }

                else:
                    # êµ¬ í˜•ì‹: ì§ì ‘ BM25 ê°ì²´
                    return {
                        "success": True,
                        "doc_count": getattr(data, "corpus_size", 0),
                        "vocab_size": len(getattr(data, "idf", {})),
                        "avg_doc_length": round(getattr(data, "avgdl", 0), 2),
                        "sample_tokens": [],
                    }

            except Exception as e:
                return {"success": False, "error": str(e)}

        return await self.run_in_executor(load_index)

    async def _audit_qdrant_store(self) -> dict[str, Any]:
        """Qdrant ë²¡í„° DB ì ê²€."""
        stats: dict[str, Any] = {
            "accessible": False,
            "path": None,
            "collection_exists": False,
            "point_count": 0,
            "vector_dim": 0,
            "config": {},
        }

        store_dir = self.resolve_path(self.QDRANT_STORE_DIR)
        stats["path"] = str(store_dir)

        if not store_dir.exists():
            self.add_issue(
                severity=Severity.CRITICAL,
                category="vectordb",
                message=f"Qdrant ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ ì—†ìŒ: {store_dir}",
            )
            return stats

        # ì»¬ë ‰ì…˜ ë””ë ‰í† ë¦¬ í™•ì¸
        collection_dir = store_dir / "collections" / self.QDRANT_COLLECTION_NAME
        if not collection_dir.exists():
            self.add_issue(
                severity=Severity.CRITICAL,
                category="vectordb",
                message=f"Qdrant ì»¬ë ‰ì…˜ ì—†ìŒ: {self.QDRANT_COLLECTION_NAME}",
            )
            return stats

        stats["collection_exists"] = True

        # config.json ì½ê¸°
        config_file = collection_dir / "config.json"
        if config_file.exists():
            config_result = await self._load_qdrant_config(config_file)
            if config_result["success"]:
                stats["accessible"] = True
                stats["config"] = config_result.get("config", {})
                stats["vector_dim"] = config_result.get("vector_dim", 0)

                # ë²¡í„° ì°¨ì› ê²€ì¦
                if stats["vector_dim"] != self.EXPECTED_EMBEDDING_DIM:
                    self.add_issue(
                        severity=Severity.WARNING,
                        category="vectordb",
                        message=f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: {stats['vector_dim']} (ê¸°ëŒ€ê°’: {self.EXPECTED_EMBEDDING_DIM})",
                    )
            else:
                self.add_issue(
                    severity=Severity.WARNING,
                    category="vectordb",
                    message=f"Qdrant ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {config_result.get('error')}",
                )

        # í¬ì¸íŠ¸ ìˆ˜ ì¶”ì • (ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì—ì„œ)
        point_count = await self._estimate_point_count(collection_dir)
        stats["point_count"] = point_count

        # ì €ì¥ì†Œ í¬ê¸°
        total_size = sum(f.stat().st_size for f in collection_dir.rglob("*") if f.is_file())
        stats["storage_size_bytes"] = total_size
        stats["storage_size_mb"] = round(total_size / (1024 * 1024), 2)

        return stats

    async def _load_qdrant_config(self, config_path: Path) -> dict[str, Any]:
        """Qdrant ì»¬ë ‰ì…˜ ì„¤ì • ë¡œë“œ."""

        def load_config() -> dict[str, Any]:
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config = json.load(f)

                # ë²¡í„° ì°¨ì› ì¶”ì¶œ
                params = config.get("params", {})
                vectors = params.get("vectors", {})

                # ë‹¨ì¼ ë²¡í„° ë˜ëŠ” ëª…ëª…ëœ ë²¡í„° ì²˜ë¦¬
                if isinstance(vectors, dict):
                    if "size" in vectors:
                        vector_dim = vectors["size"]
                    else:
                        # ëª…ëª…ëœ ë²¡í„°ì˜ ê²½ìš° ì²« ë²ˆì§¸ ê²ƒ ì‚¬ìš©
                        first_vec = next(iter(vectors.values()), {})
                        vector_dim = first_vec.get("size", 0) if isinstance(first_vec, dict) else 0
                else:
                    vector_dim = 0

                return {
                    "success": True,
                    "config": config,
                    "vector_dim": vector_dim,
                }

            except Exception as e:
                return {"success": False, "error": str(e)}

        return await self.run_in_executor(load_config)

    async def _estimate_point_count(self, collection_dir: Path) -> int:
        """Qdrant í¬ì¸íŠ¸ ìˆ˜ ì¶”ì •."""

        def estimate() -> int:
            # payload_index_pointer.jsonì—ì„œ ì¶”ì •
            pointer_file = collection_dir / "payload_index_pointer.json"
            if pointer_file.exists():
                try:
                    with open(pointer_file, "r") as f:
                        data = json.load(f)
                        # í¬ì¸íŠ¸ ìˆ˜ ê´€ë ¨ í•„ë“œ ì°¾ê¸°
                        if "points_count" in data:
                            return data["points_count"]
                except Exception:
                    pass

            # ì„¸ê·¸ë¨¼íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì¶”ì •
            segments_dir = collection_dir / "0" / "segments"
            if segments_dir.exists():
                # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ í¬ê¸°ë¡œ ëŒ€ëµì ìœ¼ë¡œ ì¶”ì •
                segment_dirs = [d for d in segments_dir.iterdir() if d.is_dir()]
                return len(segment_dirs) * 1000  # ë§¤ìš° ëŒ€ëµì ì¸ ì¶”ì •

            return 0

        return await self.run_in_executor(estimate)

    def _check_sync_status(
        self,
        bm25_stats: dict[str, Any],
        qdrant_stats: dict[str, Any],
    ) -> dict[str, Any]:
        """BM25ì™€ Qdrant ê°„ ë™ê¸°í™” ìƒíƒœ ì ê²€."""
        sync_status: dict[str, Any] = {
            "in_sync": False,
            "bm25_doc_count": bm25_stats.get("document_count", 0),
            "qdrant_point_count": qdrant_stats.get("point_count", 0),
            "difference": 0,
        }

        bm25_count = sync_status["bm25_doc_count"]
        qdrant_count = sync_status["qdrant_point_count"]

        if bm25_count == 0 or qdrant_count == 0:
            self.add_issue(
                severity=Severity.WARNING,
                category="sync",
                message="ì¸ë±ìŠ¤ ì¤‘ í•˜ë‚˜ê°€ ë¹„ì–´ìˆì–´ ë™ê¸°í™” ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŒ",
                details={
                    "bm25_count": bm25_count,
                    "qdrant_count": qdrant_count,
                },
            )
            return sync_status

        difference = abs(bm25_count - qdrant_count)
        sync_status["difference"] = difference

        # 10% ì´ë‚´ ì°¨ì´ëŠ” í—ˆìš©
        tolerance = max(bm25_count, qdrant_count) * 0.1

        if difference <= tolerance:
            sync_status["in_sync"] = True
        else:
            sync_status["in_sync"] = False
            self.add_issue(
                severity=Severity.WARNING,
                category="sync",
                message=f"BM25ì™€ Qdrant ë¬¸ì„œ ìˆ˜ ë¶ˆì¼ì¹˜: BM25={bm25_count}, Qdrant={qdrant_count} (ì°¨ì´: {difference})",
                details={
                    "bm25_count": bm25_count,
                    "qdrant_count": qdrant_count,
                    "difference": difference,
                    "tolerance": round(tolerance),
                },
            )

        return sync_status


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.INFO)

    async def main():
        auditor = IndexAuditor(verbose=True)
        result = await auditor.audit()

        print("\n" + "=" * 60)
        print("ì¸ë±ìŠ¤ ë™ê¸°í™” ì ê²€ ê²°ê³¼")
        print("=" * 60)
        print(f"ìƒíƒœ: {result.status}")
        print(f"ì´ìŠˆ: {len(result.issues)}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {result.duration_seconds:.2f}ì´ˆ")

        print("\nğŸ“Š BM25 ì¸ë±ìŠ¤:")
        for key, value in result.stats.extra.get("bm25", {}).items():
            print(f"  {key}: {value}")

        print("\nğŸ“Š Qdrant ë²¡í„° DB:")
        for key, value in result.stats.extra.get("qdrant", {}).items():
            if key != "config":
                print(f"  {key}: {value}")

        print("\nğŸ“Š ë™ê¸°í™” ìƒíƒœ:")
        for key, value in result.stats.extra.get("sync_status", {}).items():
            print(f"  {key}: {value}")

        if result.issues:
            print("\nâš ï¸ ë°œê²¬ëœ ì´ìŠˆ:")
            for issue in result.issues:
                print(f"  {issue}")

    asyncio.run(main())
