"""
ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ì ê²€ ëª¨ë“ˆ.

ì ê²€ ëŒ€ìƒ:
- ì „ì²´ ë¬¸ì„œ í†µê³„ (ì›ë³¸ vs ì²˜ë¦¬ë¨ vs ì¸ë±ìŠ¤)
- doc_typeë³„ ë¶„í¬
- ì½”ìŠ¤ë³„/ê¸°ìˆ˜ë³„ ë¶„í¬
- Slack QA í’ˆì§ˆ í‰ê°€ ê²°ê³¼

ì ê²€ í•­ëª©:
- ì´ìƒì¹˜ íƒì§€ (ê·¹ë‹¨ì  ì²­í¬ ê¸¸ì´)
- ë©”íƒ€ë°ì´í„° ëˆ„ë½ ë¹„ìœ¨
- ì¤‘ë³µ ë¬¸ì„œ ë¹„ìœ¨
- í’ˆì§ˆ ë“±ê¸‰ ë¶„í¬

ì‚¬ìš© ì˜ˆ:
    ```python
    from document_processing.audit.auditors.quality_auditor import QualityAuditor

    auditor = QualityAuditor(verbose=True)
    result = await auditor.audit()
    print(result.model_dump_json(indent=2))
    ```
"""

import json
import logging
from collections import Counter
from typing import Any

from document_processing.audit.auditors.base import BaseAuditor
from document_processing.audit.models.audit_result import LayerResult, LayerStats, Severity

logger = logging.getLogger(__name__)


class QualityAuditor(BaseAuditor):
    """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ì ê²€ê¸°."""

    layer_name = "quality"

    # ë°ì´í„° ê²½ë¡œ
    SLACK_QA_SCORED_DIR = "document_chunks/slack_qa_scored"
    TRANSCRIPT_CHUNKS_DIR = "document_chunks/lecture_transcript_chunks"

    # í’ˆì§ˆ ì„ê³„ê°’
    LOW_QUALITY_THRESHOLD = 0.1  # ì €í’ˆì§ˆ ë°ì´í„° 10% ì´ˆê³¼ ì‹œ ê²½ê³ 
    METADATA_MISSING_THRESHOLD = 0.05  # ë©”íƒ€ë°ì´í„° ëˆ„ë½ 5% ì´ˆê³¼ ì‹œ ê²½ê³ 

    async def audit(self) -> LayerResult:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì ê²€ ì‹¤í–‰."""
        result = self.create_result()
        self.start_timer()

        stats_extra: dict[str, Any] = {
            "overall_summary": {},
            "slack_qa_quality": {},
            "distribution_by_doc_type": {},
            "distribution_by_course": {},
            "anomalies": {},
        }

        # 1. ì „ì²´ ë°ì´í„° ìš”ì•½
        overall_summary = await self._compute_overall_summary()
        stats_extra["overall_summary"] = overall_summary

        # 2. Slack QA í’ˆì§ˆ ë¶„ì„
        slack_quality = await self._analyze_slack_qa_quality()
        stats_extra["slack_qa_quality"] = slack_quality

        # 3. doc_typeë³„ ë¶„í¬
        doc_type_dist = await self._analyze_doc_type_distribution()
        stats_extra["distribution_by_doc_type"] = doc_type_dist

        # 4. ì½”ìŠ¤ë³„ ë¶„í¬
        course_dist = await self._analyze_course_distribution()
        stats_extra["distribution_by_course"] = course_dist

        # 5. ì´ìƒì¹˜ íƒì§€
        anomalies = await self._detect_anomalies()
        stats_extra["anomalies"] = anomalies

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._compute_quality_score(stats_extra)
        stats_extra["quality_score"] = quality_score

        # í†µê³„ ì—…ë°ì´íŠ¸
        total_items = overall_summary.get("total_documents", 0)
        result.total_items = total_items
        result.stats = LayerStats(
            total_items=total_items,
            checked_items=total_items,
            passed_items=int(total_items * (quality_score / 100)),
            failed_items=int(total_items * (1 - quality_score / 100)),
            extra=stats_extra,
        )

        return self.finalize_result()

    async def _compute_overall_summary(self) -> dict[str, Any]:
        """ì „ì²´ ë°ì´í„° ìš”ì•½ ê³„ì‚°."""
        summary: dict[str, Any] = {
            "total_documents": 0,
            "by_source": {},
        }

        # Slack QA
        slack_dir = self.resolve_path(self.SLACK_QA_SCORED_DIR)
        if slack_dir.exists():
            slack_count = 0
            for json_file in slack_dir.glob("*.json"):
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        slack_count += len(data.get("qa_pairs", []))
                except Exception:
                    pass
            summary["by_source"]["slack_qa"] = slack_count
            summary["total_documents"] += slack_count

        # Transcript chunks
        transcript_dir = self.resolve_path(self.TRANSCRIPT_CHUNKS_DIR)
        if transcript_dir.exists():
            transcript_count = 0
            for json_file in transcript_dir.glob("*_chunks.json"):
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        chunks = data if isinstance(data, list) else data.get("chunks", [])
                        transcript_count += len(chunks)
                except Exception:
                    pass
            summary["by_source"]["lecture_transcript"] = transcript_count
            summary["total_documents"] += transcript_count

        return summary

    async def _analyze_slack_qa_quality(self) -> dict[str, Any]:
        """Slack QA í’ˆì§ˆ ë¶„ì„."""
        quality_stats: dict[str, Any] = {
            "total_qa_pairs": 0,
            "evaluated": 0,
            "quality_distribution": {
                "high": 0,
                "medium": 0,
                "low": 0,
                "remove": 0,
            },
            "score_distribution": {
                "completeness": {"avg": 0, "min": 5, "max": 1},
                "context_independence": {"avg": 0, "min": 5, "max": 1},
                "technical_accuracy": {"avg": 0, "min": 5, "max": 1},
            },
            "low_quality_rate": 0,
        }

        slack_dir = self.resolve_path(self.SLACK_QA_SCORED_DIR)
        if not slack_dir.exists():
            return quality_stats

        all_scores: dict[str, list[int]] = {
            "completeness": [],
            "context_independence": [],
            "technical_accuracy": [],
        }

        for json_file in slack_dir.glob("*.json"):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                qa_pairs = data.get("qa_pairs", [])
                quality_stats["total_qa_pairs"] += len(qa_pairs)

                for qa in qa_pairs:
                    evaluation = qa.get("evaluation", {})
                    if not evaluation:
                        continue

                    quality_stats["evaluated"] += 1

                    # ì „ì²´ í’ˆì§ˆ ë“±ê¸‰
                    overall = evaluation.get("overall_quality", "").lower()
                    if overall in quality_stats["quality_distribution"]:
                        quality_stats["quality_distribution"][overall] += 1

                    # ê°œë³„ ì ìˆ˜
                    for dim in ["completeness", "context_independence", "technical_accuracy"]:
                        dim_data = evaluation.get(dim, {})
                        if isinstance(dim_data, dict) and "score" in dim_data:
                            score = dim_data["score"]
                            all_scores[dim].append(score)

            except Exception as e:
                logger.warning(f"Slack QA íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {json_file}, {e}")

        # í†µê³„ ê³„ì‚°
        for dim, scores in all_scores.items():
            if scores:
                quality_stats["score_distribution"][dim] = {
                    "avg": round(sum(scores) / len(scores), 2),
                    "min": min(scores),
                    "max": max(scores),
                }

        # ì €í’ˆì§ˆ ë¹„ìœ¨
        total_evaluated = quality_stats["evaluated"]
        if total_evaluated > 0:
            low_count = (
                quality_stats["quality_distribution"]["low"]
                + quality_stats["quality_distribution"]["remove"]
            )
            quality_stats["low_quality_rate"] = round(low_count / total_evaluated * 100, 2)

            if quality_stats["low_quality_rate"] > self.LOW_QUALITY_THRESHOLD * 100:
                self.add_issue(
                    severity=Severity.WARNING,
                    category="quality",
                    message=f"ì €í’ˆì§ˆ Slack QA ë¹„ìœ¨ì´ ë†’ìŒ: {quality_stats['low_quality_rate']}%",
                    details=quality_stats["quality_distribution"],
                )

        return quality_stats

    async def _analyze_doc_type_distribution(self) -> dict[str, Any]:
        """doc_typeë³„ ë¶„í¬ ë¶„ì„."""
        distribution: dict[str, int] = Counter()

        # Slack QA
        slack_dir = self.resolve_path(self.SLACK_QA_SCORED_DIR)
        if slack_dir.exists():
            for json_file in slack_dir.glob("*.json"):
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        distribution["slack_qa"] += len(data.get("qa_pairs", []))
                except Exception:
                    pass

        # Transcript
        transcript_dir = self.resolve_path(self.TRANSCRIPT_CHUNKS_DIR)
        if transcript_dir.exists():
            for json_file in transcript_dir.glob("*_chunks.json"):
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        chunks = data if isinstance(data, list) else data.get("chunks", [])
                        distribution["lecture_transcript"] += len(chunks)
                except Exception:
                    pass

        return dict(distribution)

    async def _analyze_course_distribution(self) -> dict[str, Any]:
        """ì½”ìŠ¤ë³„ ë¶„í¬ ë¶„ì„."""
        distribution: dict[str, int] = Counter()

        # Slack QA - íŒŒì¼ëª…ì—ì„œ ì½”ìŠ¤ ì¶”ì¶œ
        slack_dir = self.resolve_path(self.SLACK_QA_SCORED_DIR)
        if slack_dir.exists():
            for json_file in slack_dir.glob("*.json"):
                # íŒŒì¼ëª… ì˜ˆ: level2_nlp_merged.json
                course = json_file.stem.replace("_merged", "")
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        distribution[course] += len(data.get("qa_pairs", []))
                except Exception:
                    pass

        # Transcript - íŒŒì¼ëª…ì—ì„œ ì½”ìŠ¤ ì¶”ì¶œ
        transcript_dir = self.resolve_path(self.TRANSCRIPT_CHUNKS_DIR)
        if transcript_dir.exists():
            for json_file in transcript_dir.glob("*_chunks.json"):
                # íŒŒì¼ëª… ì˜ˆ: [NLP ì´ë¡ ] (8ê°•) Transformer_chunks.json
                filename = json_file.stem
                course = self._extract_course(filename)
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        chunks = data if isinstance(data, list) else data.get("chunks", [])
                        distribution[course] += len(chunks)
                except Exception:
                    pass

        return dict(distribution)

    def _extract_course(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì½”ìŠ¤ëª… ì¶”ì¶œ."""
        if filename.startswith("["):
            end = filename.find("]")
            if end > 0:
                return filename[1:end]
        return "general"

    async def _detect_anomalies(self) -> dict[str, Any]:
        """ì´ìƒì¹˜ íƒì§€."""
        anomalies: dict[str, Any] = {
            "empty_content": 0,
            "extremely_short": 0,
            "extremely_long": 0,
            "missing_metadata": 0,
            "suspicious_patterns": [],
        }

        # Transcript chunks ì ê²€
        transcript_dir = self.resolve_path(self.TRANSCRIPT_CHUNKS_DIR)
        if transcript_dir.exists():
            for json_file in transcript_dir.glob("*_chunks.json"):
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        chunks = data if isinstance(data, list) else data.get("chunks", [])

                        for chunk in chunks:
                            content = chunk.get("page_content", "") or chunk.get("content", "")

                            # ë¹ˆ ì½˜í…ì¸ 
                            if not content.strip():
                                anomalies["empty_content"] += 1

                            # ê·¹ë‹¨ì  ê¸¸ì´
                            length = len(content)
                            if length < 20:
                                anomalies["extremely_short"] += 1
                            elif length > 10000:
                                anomalies["extremely_long"] += 1

                            # ë©”íƒ€ë°ì´í„° ëˆ„ë½
                            metadata = chunk.get("metadata", {})
                            if not metadata:
                                anomalies["missing_metadata"] += 1

                except Exception:
                    pass

        # ì´ìƒì¹˜ ì´ìŠˆ ì¶”ê°€
        if anomalies["empty_content"] > 0:
            self.add_issue(
                severity=Severity.WARNING,
                category="anomaly",
                message=f"ë¹ˆ ì½˜í…ì¸  ì²­í¬: {anomalies['empty_content']}ê°œ",
            )

        if anomalies["extremely_short"] > 0:
            self.add_issue(
                severity=Severity.INFO,
                category="anomaly",
                message=f"ê·¹ë‹¨ì ìœ¼ë¡œ ì§§ì€ ì²­í¬ (20ì ë¯¸ë§Œ): {anomalies['extremely_short']}ê°œ",
            )

        if anomalies["extremely_long"] > 0:
            self.add_issue(
                severity=Severity.INFO,
                category="anomaly",
                message=f"ê·¹ë‹¨ì ìœ¼ë¡œ ê¸´ ì²­í¬ (10000ì ì´ˆê³¼): {anomalies['extremely_long']}ê°œ",
            )

        return anomalies

    def _compute_quality_score(self, stats: dict[str, Any]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)."""
        score = 100.0

        # Slack QA í’ˆì§ˆ ë°˜ì˜
        slack_quality = stats.get("slack_qa_quality", {})
        low_quality_rate = slack_quality.get("low_quality_rate", 0)
        score -= low_quality_rate * 0.5  # ì €í’ˆì§ˆ ë¹„ìœ¨ë§Œí¼ ê°ì 

        # ì´ìƒì¹˜ ë°˜ì˜
        anomalies = stats.get("anomalies", {})
        total_docs = stats.get("overall_summary", {}).get("total_documents", 1)

        empty_rate = anomalies.get("empty_content", 0) / total_docs * 100
        score -= empty_rate * 2  # ë¹ˆ ì½˜í…ì¸ ëŠ” 2ë°° ê°ì 

        missing_meta_rate = anomalies.get("missing_metadata", 0) / total_docs * 100
        score -= missing_meta_rate * 0.5

        # ë²”ìœ„ ì œí•œ
        return max(0, min(100, round(score, 2)))


# =============================================================================
# CLI í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.INFO)

    async def main():
        auditor = QualityAuditor(verbose=True)
        result = await auditor.audit()

        print("\n" + "=" * 60)
        print("ë°ì´í„° í’ˆì§ˆ ì ê²€ ê²°ê³¼")
        print("=" * 60)
        print(f"ìƒíƒœ: {result.status}")
        print(f"ì „ì²´ ë¬¸ì„œ: {result.total_items}")
        print(f"í’ˆì§ˆ ì ìˆ˜: {result.stats.extra.get('quality_score', 0)}/100")
        print(f"ì´ìŠˆ: {len(result.issues)}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {result.duration_seconds:.2f}ì´ˆ")

        print("\nğŸ“Š ì „ì²´ ìš”ì•½:")
        for key, value in result.stats.extra.get("overall_summary", {}).items():
            print(f"  {key}: {value}")

        print("\nğŸ“Š Slack QA í’ˆì§ˆ:")
        for key, value in result.stats.extra.get("slack_qa_quality", {}).items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")

        print("\nğŸ“Š doc_typeë³„ ë¶„í¬:")
        for key, value in result.stats.extra.get("distribution_by_doc_type", {}).items():
            print(f"  {key}: {value}")

        if result.issues:
            print("\nâš ï¸ ë°œê²¬ëœ ì´ìŠˆ:")
            for issue in result.issues:
                print(f"  {issue}")

    asyncio.run(main())
