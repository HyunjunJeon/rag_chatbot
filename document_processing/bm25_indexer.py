"""
BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆ.

Hybrid Searchë¥¼ ìœ„í•œ BM25 ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import pickle
from pathlib import Path
from typing import Any

from kiwipiepy import Kiwi
from rank_bm25 import BM25Okapi
from tqdm import tqdm

__all__ = ["BM25Indexer"]


class BM25Indexer:
    """
    BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ê²€ìƒ‰í•˜ëŠ” í´ë˜ìŠ¤.

    ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë¶„ë¦¬í•˜ì—¬ ì¸ë±ì‹±í•˜ê³ , ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì—¬ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì˜ˆì‹œ:
        ```python
        indexer = BM25Indexer()

        # ì¸ë±ìŠ¤ ìƒì„±
        indexer.build_index_from_directory(
            "document_chunks/slack_qa_merged/"
        )

        # ì¸ë±ìŠ¤ ì €ì¥
        indexer.save_index("bm25_index.pkl")

        # ê²€ìƒ‰
        results = indexer.search(
            query="GPU ë©”ëª¨ë¦¬ ë¶€ì¡±",
            question_weight=0.7,
            answer_weight=0.3,
            limit=10
        )
        ```
    """

    def __init__(self) -> None:
        """ì´ˆê¸°í™”."""
        print("ğŸ”§ BM25Indexer ì´ˆê¸°í™” ì¤‘...")

        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°
        print("   Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì¤‘...")
        self.kiwi = Kiwi()

        # BM25 ì¸ë±ìŠ¤ (ì§ˆë¬¸, ë‹µë³€, í†µí•©)
        self.bm25_question: BM25Okapi | None = None
        self.bm25_answer: BM25Okapi | None = None
        self.bm25_combined: BM25Okapi | None = None

        # ë¬¸ì„œ ID ë§¤í•‘
        self.doc_ids: list[str] = []

        # ì›ë³¸ ë¬¸ì„œ ì €ì¥ (ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜ìš©)
        self.documents: list[dict[str, Any]] = []

        print("   âœ… ì´ˆê¸°í™” ì™„ë£Œ")

    def tokenize(self, text: str) -> list[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            text: í† í°í™”í•  í…ìŠ¤íŠ¸

        ë°˜í™˜ê°’:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        # Kiwië¡œ í˜•íƒœì†Œ ë¶„ì„
        tokens = self.kiwi.tokenize(text)

        # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì˜ì–´, ìˆ«ìë§Œ ì¶”ì¶œ
        result = []
        for token in tokens:
            if token.tag in ["NNG", "NNP", "VV", "VA", "SL", "SN"]:  # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì˜ì–´, ìˆ«ì
                result.append(token.form.lower())

        return result

    def _process_merged_file(self, file_path: Path) -> list[dict[str, Any]]:
        """
        ë³‘í•©ëœ JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            file_path: JSON íŒŒì¼ ê²½ë¡œ

        ë°˜í™˜ê°’:
            ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        with open(file_path, encoding="utf-8") as f:
            data = json.load(f)

        documents: list[dict[str, Any]] = []
        course = data["course"]

        for qa in data["qa_pairs"]:
            question = qa["question"]
            answers = qa["answers"]

            # ê° ë‹µë³€ë§ˆë‹¤ ë¬¸ì„œ ìƒì„±
            for idx, answer in enumerate(answers):
                # ë¬¸ì„œ ID ìƒì„±
                doc_id = (
                    f"{qa['generation']}_{course}_{qa['date']}_"
                    f"{question['timestamp']}_a{idx}"
                )

                # í† í°í™”
                question_tokens = self.tokenize(question["text"])
                answer_tokens = self.tokenize(answer["text"])
                combined_tokens = question_tokens + answer_tokens

                doc = {
                    "doc_id": doc_id,
                    "generation": qa["generation"],
                    "course": course,
                    "date": qa["date"],
                    "question_text": question["text"],
                    "answer_text": answer["text"],
                    "question_tokens": question_tokens,
                    "answer_tokens": answer_tokens,
                    "combined_tokens": combined_tokens,
                    "question_user": question.get("user_name") or question["user"],
                    "answer_user": answer.get("user_name") or answer["user"],
                    "is_bot": answer["is_bot"],
                }

                documents.append(doc)

        return documents

    def build_index_from_directory(self, directory: Path | str) -> None:
        """
        ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì½ì–´ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            directory: JSON íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        directory = Path(directory)
        print(f"\nğŸ“‚ BM25 ì¸ë±ìŠ¤ ìƒì„±: {directory}")

        # JSON íŒŒì¼ ì°¾ê¸°
        json_files = sorted(directory.glob("*_merged.json"))
        print(f"   ë°œê²¬ëœ íŒŒì¼: {len(json_files)}ê°œ")

        # ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
        all_documents: list[dict[str, Any]] = []

        for json_file in tqdm(json_files, desc="íŒŒì¼ ì²˜ë¦¬"):
            try:
                documents = self._process_merged_file(json_file)
                all_documents.extend(documents)
            except Exception as e:
                print(f"\n   âœ— {json_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"\n   ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents):,}ê°œ")

        # ë¬¸ì„œ ì €ì¥
        self.documents = all_documents
        self.doc_ids = [doc["doc_id"] for doc in all_documents]

        # í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±
        print("   BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        question_corpus = [doc["question_tokens"] for doc in all_documents]
        answer_corpus = [doc["answer_tokens"] for doc in all_documents]
        combined_corpus = [doc["combined_tokens"] for doc in all_documents]

        # BM25 ì¸ë±ìŠ¤ ìƒì„±
        self.bm25_question = BM25Okapi(question_corpus)
        self.bm25_answer = BM25Okapi(answer_corpus)
        self.bm25_combined = BM25Okapi(combined_corpus)

        print("   âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    def save_index(self, output_path: Path | str) -> None:
        """
        BM25 ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        output_path = Path(output_path)
        print(f"\nğŸ’¾ BM25 ì¸ë±ìŠ¤ ì €ì¥: {output_path}")

        data = {
            "bm25_question": self.bm25_question,
            "bm25_answer": self.bm25_answer,
            "bm25_combined": self.bm25_combined,
            "doc_ids": self.doc_ids,
            "documents": self.documents,
        }

        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "wb") as f:
            pickle.dump(data, f)

        file_size = output_path.stat().st_size / 1024 / 1024
        print(f"   âœ… ì €ì¥ ì™„ë£Œ: {file_size:.2f} MB")

    def load_index(self, index_path: Path | str) -> None:
        """
        ì €ì¥ëœ BM25 ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            index_path: ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
        """
        index_path = Path(index_path)
        print(f"\nğŸ“¥ BM25 ì¸ë±ìŠ¤ ë¡œë“œ: {index_path}")

        if not index_path.exists():
            raise FileNotFoundError(f"ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_path}")

        with open(index_path, "rb") as f:
            data = pickle.load(f)

        self.bm25_question = data["bm25_question"]
        self.bm25_answer = data["bm25_answer"]
        self.bm25_combined = data["bm25_combined"]
        self.doc_ids = data["doc_ids"]
        self.documents = data["documents"]

        print(f"   âœ… ë¡œë“œ ì™„ë£Œ: {len(self.documents):,}ê°œ ë¬¸ì„œ")

    def search(
        self,
        query: str,
        question_weight: float = 0.7,
        answer_weight: float = 0.3,
        limit: int = 10,
        course_filter: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        BM25ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        ë§¤ê°œë³€ìˆ˜:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            question_weight: ì§ˆë¬¸ ê°€ì¤‘ì¹˜ (0~1)
            answer_weight: ë‹µë³€ ê°€ì¤‘ì¹˜ (0~1)
            limit: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            course_filter: ê³¼ì • í•„í„° (ì„ íƒ)

        ë°˜í™˜ê°’:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if self.bm25_question is None or self.bm25_answer is None:
            raise ValueError("ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_index() ë˜ëŠ” load_index()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self.tokenize(query)

        # BM25 ì ìˆ˜ ê³„ì‚°
        question_scores = self.bm25_question.get_scores(query_tokens)
        answer_scores = self.bm25_answer.get_scores(query_tokens)

        # ê°€ì¤‘ì¹˜ ì ìš©
        combined_scores = (
            question_weight * question_scores + answer_weight * answer_scores
        )

        # ê³¼ì • í•„í„° ì ìš©
        if course_filter:
            for i, doc in enumerate(self.documents):
                if doc["course"] != course_filter:
                    combined_scores[i] = -1  # ì œì™¸

        # ìƒìœ„ ë¬¸ì„œ ì„ íƒ
        import numpy as np

        top_indices = np.argsort(combined_scores)[::-1][:limit]

        # ê²°ê³¼ ìƒì„±
        results = []
        for idx in top_indices:
            if combined_scores[idx] > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ
                doc = self.documents[idx].copy()
                doc["bm25_score"] = float(combined_scores[idx])
                results.append(doc)

        return results

    def get_stats(self) -> dict[str, Any]:
        """
        ì¸ë±ìŠ¤ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ë°˜í™˜ê°’:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not self.documents:
            return {"total_documents": 0}

        # ê³¼ì •ë³„ í†µê³„
        course_counts: dict[str, int] = {}
        for doc in self.documents:
            course = doc["course"]
            course_counts[course] = course_counts.get(course, 0) + 1

        return {
            "total_documents": len(self.documents),
            "unique_courses": len(course_counts),
            "by_course": course_counts,
        }


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜."""
    import argparse

    parser = argparse.ArgumentParser(description="BM25 ì¸ë±ìŠ¤ ìƒì„±")
    # PROJECT_ROOT ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
    project_root = Path(__file__).parent.parent

    parser.add_argument(
        "--input-dir",
        type=str,
        default=str(project_root / "document_chunks" / "slack_qa_merged"),
        help="ì…ë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: document_chunks/slack_qa_merged)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=str(project_root / "document_chunks" / "bm25_index.pkl"),
        help="ì¶œë ¥ íŒŒì¼ (ê¸°ë³¸ê°’: document_chunks/bm25_index.pkl)",
    )
    parser.add_argument("--test", action="store_true", help="í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰")

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸš€ BM25 ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘")
    print("=" * 80)

    # ì¸ë±ì„œ ìƒì„±
    indexer = BM25Indexer()

    # ì¸ë±ìŠ¤ ë¹Œë“œ
    indexer.build_index_from_directory(args.input_dir)

    # í†µê³„ ì¶œë ¥
    stats = indexer.get_stats()
    print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„")
    print(f"   ì´ ë¬¸ì„œ: {stats['total_documents']:,}ê°œ")
    print(f"   ê³¼ì • ìˆ˜: {stats['unique_courses']}ê°œ")

    # ì €ì¥
    indexer.save_index(args.output)

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    if args.test:
        print("\n" + "=" * 80)
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
        print("=" * 80)

        test_queries = [
            ("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±", "level2_cv"),
            ("ë°ì´í„° ì¦ê°• ê¸°ë²•", "level2_cv"),
            ("optimizer ì„ íƒ", None),
        ]

        for query, course in test_queries:
            print(f"\nğŸ” ì¿¼ë¦¬: {query}")
            if course:
                print(f"   ê³¼ì •: {course}")

            results = indexer.search(
                query=query,
                question_weight=0.7,
                answer_weight=0.3,
                limit=3,
                course_filter=course,
            )

            for i, result in enumerate(results, 1):
                print(f"\n   [{i}] BM25 ì ìˆ˜: {result['bm25_score']:.3f}")
                print(f"       ê³¼ì •: {result['course']} ({result['generation']}ê¸°)")
                print(f"       ì§ˆë¬¸: {result['question_text'][:80]}...")
                print(f"       ë‹µë³€: {result['answer_text'][:80]}...")

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()

