{
  "lecture_name": "[MLforRecSys] (1강) 최신 RecSys 동향 및 통계학 기본",
  "source_file": "[MLforRecSys] (1강) 최신 RecSys 동향 및 통계학 기본_94.mp4_2025-12-04-104430905.json",
  "text": "네 안녕하십니까? 네 저는 연세대학교 응용통계학과 및 통계 데이터 사이언스학과 교수 송경우라고 합니다. 네 먼저 이렇게 추천 시스템과 관련한 머신러닝 강의 통해서 만나 뵙게 되어 굉장히 반갑습니다. 저는 본 강의의 제목처럼 여러분들께서 이러한 렉시스 내용을 이해하기 위해서 필요한 굉장히 중요한 머신러닝의 개념들을 학습해 나가는 강의로 10주 동안 진행을 할 계획입니다. 그래서 그와 관련해서 먼저 첫 주차에는 지금 최신 렉시스 동향이 어떻고 그다음에 그러한 내용을 이해하기 위해서는 우리가 어떠한 머신러닝과 통계학과 관련된 내용이 필요한지 저희가 한번 살펴보도록 하겠습니다. 먼저 첫 번째로는 최신 렉시스 동향입니다. 즉 저희가 지금 어떻게 렉시스 연구가 발전이 되고 있고 그러한 내용들을 이해하기 위해서 어떤 기술이 필요하고 그래서 우리가 어떠한 내용을 다룰 것인지 그러한 흐름으로 저희가 한번 살펴보도록 하겠습니다. 자 추천 시스템은 여러분들 아시다시피 뭐 국가 상관없이 분야 상관없이 굉장히 여러 곳에서 활용이 되고 있는 것을 알 수가 있습니다. 실생활에 가장 많이 활용되는 머신러닝 알고리즘 중에 하나라고 볼 수도 있습니다. 그리고 그러한 추천 시스템은 다음과 같이 굉장히 개략적으로 표현하면 쉘로우 모델 딥 모델 라시 스케일의 제널티 모델 기반으로 계속해서 발전이 되고 있습니다. 자 먼저 쉘로우 모델 같은 경우는 다음과 같이 우리가 뭐 뉴런네 토크를 쓰거나 그런 게 아니라 행렬 분해를 통해서 우리가 추천 시스템을 활용하는 것입니다. 즉 다음과 같이 이 유저가 이 영화에 대해서 클릭한 정보들이 있다고 할 때 이러한 정보들을 좀 잘 모델링 할 수 있도록 하려면 어떻게 우리가 행렬을 분해를 해야 될까 즉 이러한 행렬을 어떠한 두 곱의 행렬로 분해를 해야 될지 최적화를 시키고 나면 이러한 빈칸들에 대해서도 우리가 값을 추론할 수 있다라는 연구가 되겠습니다. 그다음에 두 번째는 뉴럴 네트워크를 활용한 이제 모델입니다. 다음 한번 보시면 왼쪽은 오트랙으로 유명한 알고리즘 중의 하나인데요. 한번 보시다시피 인풋으로는 알이라는 게 들어갑니다. 이 알이라는 거는 레이팅 매트릭스라고 생각하시면 되겠습니다. 즉 여기서는 우리가 아이 번째 아이템에 대해서 우리가 그 아이템을 소비한 유저들을 이렇게 막 넣을 수가 있겠죠 그렇죠 그러면 우리가 그 유저가 실제로 아이템을 소비했는지 안 했는지 1 0 이런 바이너리한 값으로 우리가 들어가게 됩니다. 그러고 나서 오토 인코더 구조가 뭐예요? 인풋을 그대로 우리가 리컨스럭션 하는 게 목표입니다. 그러면은 여기서 우리가 이러한 인풋을 잘 리컨스럭션 하는 오토 인코더 뉴럴 네트워크를 학습하는 겁니다. 그렇게 되면은 이러한 레이팅 매트릭스가 1로 적혀 있는 것들을 우리가 최대한 잘 복원하도록 학습을 하면은 이런 이제 0과 관련된 관측이 안 된 데이터도 얘가 0인지 1인지 우리가 자연스럽게 복원이 된다는 겁니다. 즉 전체적인 흐름은 앞서 봤던 이러한 매트릭스 팩토레이제션과 유사하죠. 얘도 관측이 된 데이터를 우리가 잘 추론할 수 있도록 학습을 시키게 되면은 그때는 이런 비어 있는 거에 대해서 우리 모델이 예측 값을 내보내게 된다. 그럴듯한 예측값을 내보내게 된다 였고 이 또한 마찬가지입니다. 그다음에 이러한 뉴럴 네트워크를 더 깊게 쌓아서 우리가 레이팅을 예측하는 뉴럴 엠프라는 모델들도 존재합니다. 자 그다음에 최근에는 라지 스케일 모델 기반으로도 굉장히 많이 활용이 되고 있습니다. 여러분들께서 티5라는 모델은 이미 익숙하실 것 같은데요. 티5는 우리가 텍스트를 넣었을 때 그 텍스트를 번역을 한다든지 아니면 감정 분류를 한다든지 요약을 한다든지 여러 가지 테스크를 하나의 모델로 다 할 수 있는 통합적인 모델입니다. 그거의 추천 시스템 버전이 p5입니다. 여기서는 특정한 유저가 이러한 순서대로 아이템을 구매를 했을 때 자 그다음에 구매해야 할 아이템은 무엇인가요? 추천을 하기도 하고요. 그다음에 여기서는 특정한 유저가 특정한 아이템에 어떠한 별점을 줄까요? 그다음에 특정한 유저가 특정한 아이템을 구매했는데 그 이유가 뭘까요? 그런 것들을 설명하는 것까지 다양한 아웃풋을 내보내는 게 하나의 모델로 가능해진다라는 것이고요. 이러한 텍스트 기반의 모델도 있고 그다음에 텍스트랑 이미지를 모두 다 다룰 수 있는 멀티 모델 기반으로도 또 활용이 되기도 합니다. 왜 특정 아이템을 산다고 할 때 그 아이템의 텍스트와 관련된 속성뿐만 아니라 그 아이템의 디자인이라든지 그러한 이미지와 관련된 속성도 굉장히 중요하기 때문입니다. 최근에는 이미지 생성 쪽에서 많이 활용되고 있는 디퓨전 모델 기반으로 또 이런 추천 시스템이 많이 연구가 되고 있습니다. 자 이러한 이미지 생성은 여러분들 가장 대표적인 알고리즘으로 어떤 게 떠오르시나요? 아마 떠오르는 것 중에 하나가 달리 3 그다음에 스테이블 디퓨션 등이 있으실 겁니다. 자 그러한 알고리즘들의 근간에는 디퓨전 모델이라는 게 존재를 합니다. 디퓨션 모델이라 함은 이러한 이미지를 우리가 노이즈를 점점 주고 그다음에 그러한 노이즈를 어떻게 줬는지를 우리가 만약에 추론할 수 있으면 그 노이즈를 반대로 우리가 빼기만 하면은 다시 원래의 이미지로 돌아오게 되겠죠. 그러면 우리가 이미지를 노이즈로 보냈다가 노이즈에서 다시 원래 이미지로 복원도 가능해지는 겁니다. 그러면은 새로운 이미지를 만들고 싶다 그러면은 이 노이즈를 우리가 그냥 랜덤하게 주고 나서 뭔가 그럴듯한 이미지가 나오도록 우리 모델이 알아서 노이즈를 빼주게 되니까 새로운 이미지도 생성 또한 가능하게 되는 것입니다. 사실 얘는 저희가 나중에 살펴볼 베니션 오토 인코더 또는 오토 인코더의 보다 더 여러 스텝을 거치게 되는 버전으로도 볼 수가 있어요. 혹시 베니션 오토 인코더가 이미 익숙하신 분들은 그 베니션 오토인코더를 그냥 여러 번 즉 디노이징 하는 거를 즉 이미지 생성하는 디코더의 이 스텝을 여러 번 하는 버전으로 디퓨션 모델을 이해하셔도 됩니다. 자 이러한 알고리즘을 이용해서 추천 시스템에서도 많이 활용이 되고 있습니다. 자 디퓨션 모델을 추천 시스템에 활용한 것인데요. 결국 다음과 같은 디퓨션 모델이라 함은 이러한 이미지를 우리가 점점 더 그럴듯하게 생성해 나가는 과정입니다. 즉 무슨 말이냐면 여기서도 우리가 특정한 유저랑 특정한 아이템들 간의 관계된 일부의 관측값만 주어져 있다고 할 때 그거를 우리가 노이즈를 주고 다시 디노이징 해 나가면서 굉장히 그럴듯한 레이팅 매트릭스로 다시 만들어 나가는 겁니다. 즉 오토랙과 사실은 컨셉은 또 유사합니다. 우리가 레이팅 매트릭스를 주고 다시 레이팅 매트릭스를 우리가 복원시키는 건데 관측된 값에 대해서 잘 복원시키게 하면은 관측이 안 된 값도 이 전반적으로 이 맥락이 자연스럽게 관측이 안 된 것도 우리가 복원이 잘 된다라는 의미로 진행이 되고 있습니다. 저희가 이러한 알고리즘 자체들을 보지는 않을 거예요. 왜 이러한 것들은 렉시스의 최신 동향이고 나중에 렉시스와 관련된 여러분들 코스에서 다루게 되실 겁니다. 본 강의는 이러한 알고리즘들을 이해함에 있어서 근간이 되는 머신러닝 알고리즘들을 저희가 다룰 겁니다. 네 그다음에 또한 더 나아가서 우리가 추천 시스템에서 또 중요하게 다뤄지고 있는 테스크의 근간이 되는 알고리즘들도 볼 겁니다. 추천 시스템에서 우리가 아이템을 잘 추천하는 것뿐만 아니라 그 추천이 왜 일어났는지 어떠한 유저들 간의 관계 아이템들 간의 관계 또는 그 유저가 어떠한 어떤 특성을 좋아했었는지 그런 설명성도 우리가 제공하는 것이 굉장히 중요합니다. 왜냐하면 그런 거를 우리가 잘 설명하는 게 또 유저가 실제로 그 아이템을 클릭함에 있어서 굉장히 높은 긍정적인 효과를 야기할 수 있기 때문입니다. 또한 최근에 또 많이 활용되고 있는 연구 분야가 바이러스를 줄이고 편향성을 줄이고 인과관계를 잘 고려하는 것입니다. 자 추천 시스템 데이터는 기본적으로 편향성이 가미된 데이터가 대부분입니다. 왜냐하면 우리가 결국 학습시키고자 하는 데이터가 이미 애초에 편향되어 있을 가능성이 크기 때문입니다. 우리가 특정 유튜브와 같은 플랫폼에서 일어나는 추천 시스템 알고리즘을 고도화한다고 가정하겠습니다. 그러면 우리가 학습시킬 데이터는 당연히 유튜브 데이터일 거고 유튜브 데이터에서 어떤 유저가 어떤 영상을 클릭했는지일 겁니다. 하지만 그 데이터는 이미 평형되어 있습니다. 왜냐하면 유튜버가 특정한 알고리즘에 의해서 특정한 유저에게 특정한 영상을 추천해 주는 알고리즘이 이미 존재했고 그 알고리즘에 기반해서 데이터가 구축되었기 때문입니다. 그러면 그러한 편향성을 우리가 어떻게 해결할 것인가 또 유사하게 우리가 특정한 영상을 클릭했다고 하더라도 그 영상이 좋아해서 클릭했을 수도 있고 또는 그냥 그 영상이 유명했기 때문에 생각 없이 클릭을 하고 금방 닫았을 가능성도 있습니다. 그 까닭에 그러한 여러 가지 편향성을 우리가 어떻게 제거할 수 있을 것인지 그러한 방법 중의 하나로 인과성에 대해서 저희가 살펴볼 것입니다. 즉 저희가 앞에서 본 거는 추천 시스템에 대한 최근 동향들을 살펴본 것이고요. 그러한 동향들을 우리가 잘 이해하기 위해서 어떤 내용들을 살펴볼 것인가를 지금부터 보도록 하겠습니다. 자 데이터 사이언스 또는 머신 러닝이라고 하면은 다음과 같이 우리가 분류를 할 수가 있을 것입니다. 즉 y를 잘 예측하는 것 그다음에 그러한 y를 잘 예측하는 거 더 나아가서 인과성을 고려해서 우리가 편향적인 걸 제거하고 인과적인 효과를 고려해서 와인을 잘 예측하는 거가 되겠습니다. 그다음에 이러한 와인을 잘 예측하는 것에 대해서는 우리가 데이터가 일단 어떻게 생겼는지 그다음에 좋은 모델을 학습시키는 거 그다음에 로스 함수를 잘 짜는 거 그다음에 예측하고 나서 왜 그렇게 예측했는지 설명하는 거가 되겠습니다. 자 그중에서 저희는 다음에 네 가지 파트 노란색으로 된 거를 보도록 하겠습니다. 그 이유는 무엇이냐면 다른 부분들도 굉장히 사실 중요하지만 다른 부분들은 이미 많은 다른 강좌들에서 제공을 하고 있는 경우가 많기 때문입니다. 그래서 다른 강좌에서 제공을 하지 않으면서 추천 시스템의 최근 동향을 이해함에 있어서 가장 근간이 되는 알고리즘들만 선별적으로 저희가 다루도록 하겠습니다. 첫 번째 제노티 모델 밸류에이션 그다음에 다음과 같은 인과성 추론 각각에 대해서는 이제 저희가 다음과 같은 본 주차를 포함해서 10주 차에 걸쳐서 강의를 진행을 할 예정입니다. 그리고 각각에 대해서 저희가 조금만 더 자세히 살펴보고 지나가도록 하겠습니다. 자 생성 모델이라 함은 우리가 이러한 스 콤마 y 또는 x에 대한 데이터 분포를 우리가 추정하는 것입니다. 즉 슈퍼바이스 러닝이라고 하면은 엑스에서 와이로 가는 관계를 우리가 추론하는 것이라고 볼 수 있겠죠. 자 생성 모델이라 함은 다음과 같은 엑스라는 샘플이 나올 어떤 분포에서 나왔을지 그 분포 자체를 우리가 추론하는 것을 목표로 합니다. 또는 데이터뿐만 아니라 그 데이터의 레이블도 같이 있을 수가 있겠죠. 그러면 그러한 레이블도 우리가 같이 추론하는 것을 목표로 합니다. 자 쉽게 말해서 이러한 검은색 데이터가 샘플로 주어졌을 때 이 검은색 샘플 데이터가 어떠한 분포에서 나왔을지 여기서는 정답은 회색 분포입니다. 이 회색 분포를 근데 실제로 우리는 알지 못하는 경우가 많겠죠. 그랬을 때 이 검은색 데이터 샘플만 보고 이 회색 분포를 추론하고 싶은 겁니다. 왜 그러면 우리가 새로운 데이터도 우리가 막 샘플링을 할 수가 있게 되니까 그렇죠 자 우리가 이런 비행기 데이터가 있다고 할 때 이러한 비행기 데이터가 샘플로 우리가 관측할 뿐이지 그러한 비행기 데이터가 어떠한 분포에서 나왔는지 우리는 알지 못합니다. 하지만 어떠한 분포에서 나왔을지 우리가 추론을 하게 되면 새로운 비행기 데이터도 우리가 샘플링을 할 수 있게 됩니다. 그래서 우리가 생성 모델을 배우게 되는 것이고 또 생성 모델은 또 다른 장점도 있습니다. 생성 모델이라 함은 이러한 데이터가 결국 어떠한 과정에 의해서 어떠한 분포로 생성이 됐는지를 나타냅니다. 그 까닭에 어떠한 과정으로 인해서 이 데이터가 생성됐는지를 우리가 알게 되니까 뭐 예를 들어서 이러한 사람 얼굴 데이터는 성별 콧수염 또는 안경 여러 가지 요소로 인해서 얘가 생성이 됐다 라고 한다면 코스 형은 그대로 둔 상황에서 우리가 성별만 바꾼다든지 그러한 요소도 우리가 쉽게 컨트롤해서 우리가 원하는 데이터를 생성할 수 있고 또는 반대로 특정한 요소를 제거하거나 바꾸는 것도 가능하다는 겁니다. 그러면 우리가 컨트롤러 빌리티가 높은 이미지 생성뿐만 아니라 바이러스가 제거된 데이터도 쉽게 만들 수가 있다는 것입니다. 그 까닭에 생성 모델 기반의 추천 시스템 을 저희가 한번 살펴볼 예정입니다. 그래서 저희는 생성 모델이 무엇인지 살펴보고 마지막에 생성 모델 기반의 추천 시스템도 살펴볼 예정이고요. 자 하지만 이제 그런 생성 모델을 우리가 학습시키기 위해서는 좀 별도의 과정이 필요합니다. 여기 나와 있는 데이터 샘플만 가지고 이 회색 분포를 추론하는 게 쉬운 과정만은 있지는 않겠죠. 그래서 그러한 데이터가 인기가 주어졌을 때 그러한 데이터를 가장 잘 설명할 수 있는 모델의 파라미터를 추론하는 방법으로 우리는 생성 모델을 학습할 것입니다. 예를 들어서 얘는 우리가 정규 분포에서 나갔다고 할 때 정규 분포의 밈과 코베리언스가 어떤 게 가장 이 데이터를 가장 잘 설명할 수 있냐 그러한 것들을 우리가 학습하게 됩니다. 그래서 하지만 이게 쉽지는 않아요. 그래서 그거를 우리가 어떻게 근사해서 추론할 것인지 그거를 나타내는 게 베리에이션 인퍼런스라고 부르고 결론적으로는 이 식만 우리가 학습을 하게 됩니다. 근데 이 식의 의미가 무엇이고 이러한 유도 과정이 어떻게 되는지는 저희가 조금 있다가 네 차후 주차에서 학습을 진행을 하도록 하겠습니다. 그다음에 이런 생성 모델을 학습하는 두 번째 방법 베르션 인퍼런스가 첫 번째 방법이었고 두 번째 방법은 다음과 같은 이제 엠씨엠씨 방법을 통해서 이러한 모집단의 분포가 샘플만 가지고 이런 모집단의 분포가 어떻게 생겼을지 그리고 그러한 모집단의 분포에서 데이터를 어떻게 샘플링 할 수 있는지 저희가 추론할 수 있는 두 번째 방법 엠씨엠씨 또한 살펴볼 예정입니다. 자 마지막 세 번째는 데이터의 가치 측정과 인과 추론입니다. 자 데이터의 가치 측정은 트레잉 데이터 인스턴스마다의 가치를 측정하는 방법이에요. 즉 여기서는 트레이닝 데이터로 우리가 머신러닝 모델을 보통 학습을 하고 테스트 데이터에 대해서 우리 모델의 예측치를 평가하게 됩니다. 즉 테스트 데이터에 대한 우리 모델은 결국 트레이닝 데이터에서 학습된 널리지를 기반으로 진행이 되는 것이죠. 자 그 까닭에 이러한 테스트 데이터에 대해서 우리 모델이 강아지라고 판단했다 또는 고양이라고 판단했다 또는 특정한 아이템을 추천했다라고 하면은 그 이유를 알기 위해서는 트레인 데이터를 살펴봐야 된다는 겁니다. 그쵸 왜 우리의 모델은 트레잉 데이터로 학습된 거니까 즉 우리가 시험장에 가서 특정한 문제를 풀었는데 그 문제를 맞췄는지 아니면 틀렸는지 왜 틀렸는지 왜 맞췄는지 설명하려면 우리가 그동안 어떻게 공부했는지를 봐야지 그게 가능한 것이죠. 그걸 우리는 익스플레너빌리티라고 부릅니다. 그래서 테스트 데이터 x에 대해서 우리 모델이 아웃풋 y를 도출했는데 왜 아웃풋 와라고 도출했는지를 우리는 알고 싶은 거예요. 근데 그걸 하기 위한 게 데이터 어트리뷰션 또는 데이터 밸류에이션입니다. 즉 예를 들어서 특정한 유저에게 다섯 번째 영화를 추천했습니다. 그 이유가 뭐냐 아 해당 유저가 과거에 관측했던 영화들이 대략 이런 영화들과 유사한 영화들이었기 때문이다라고 우리가 설명할 수 있다면 굉장히 좋겠죠. 그런 것들을 우리가 어떻게 할 건지 데이터 가치 측정으로 가능합니다. 더 나아가서 다음과 같이 모델을 진단함에 있어서도 가능합니다. 즉 우리 모델이 특정한 성능이 좋지 않다 특정한 학습 데이터를 학습시키고 테스트를 해봤더니 성능이 너무 좋지 않더라라고 하면은 우리가 학습 데이터 중에서 가치가 없는 데이터는 제거하고 다시 학습을 할 수도 있겠죠. 그러면 그때 어떻게 가치가 있고 없는 데이터를 자동으로 판단할 것인가를 결정하는 요소가 될 것입니다. 자 그다음에 데이터와 설명과 관련된 또는 인과성과 관련된 마지막 파트입니다. 자 여기서 우리가 다음과 같은 문제를 푼다고 할게요. 몸 상태가 괜찮은 환자 그다음에 심각한 환자 두 환자가 있고 약이 2개가 있다고 하겠습니다. 그때 이 약을 취했을 때는 몸 상태가 괜찮으신 분들은 1400명 중에서 210명이 사망했고 그다음에 100명 중에서는 30명이 사망했다는 뜻입니다. 비라는 약은 이렇고요. 자 그때 우리가 결국 에라는 약이 좋은지 비라는 약이 좋은지 어떤 약이 좋은지 우리가 추천하고 싶다는 겁니다. 어떤 양이 좋은지 자 얘는 우리가 그 약 추천뿐만 아니라 다양한 또 다른 추천에서도 활용될 수가 있습니다. 아이템의 퀄리티가 이럴 때 가격을 올릴지 말지 가격을 올렸을 때 즉 유저가 클릭을 몇 번 할 건지 그런 것들이 만약에 계산이 서면 결국 가격을 올리는 게 좋은가 안 올리는 게 좋은가 우리가 테스트를 해볼 수 있겠죠. 그렇죠 ab 테스트에서도 많이 활용될 수 있는 문제입니다. 여러분 그러면 여기서 a라는 약이 좋을까요 아니면 b라는 약이 좋을까요? 어떻게 생각하면은 굉장히 자명한 문제라고 생각할 수도 있습니다. 그렇죠 b라는 약이 애초에 사망률이 더 높으니까 a라는 약이 좋은 거 아닙니까라고 생각을 할 수가 있겠죠. 그렇죠 하지만 그거는 우리가 사실은 알 수 없다예요. 정답은 문제 상황에 따라서 우리가 a라는 약이 좋을 때도 있고 b라는 약이 좋을 때도 사실 있습니다. 즉 인과성을 고려하지 않으면 이러한 상황에서 인과성을 고려하지 않으면 우리는 잘못된 의사 결정을 내리게 됩니다. 즉 그 말은 우리가 잘못된 가격 설정을 하거나 잘못된 아이템을 추천하게 될 가능성이 크다는 거예요. 그래서 이러한 문제를 어떻게 풀 건지 즉 이런 데이터가 있을 때 실제로 b라는 약이 사망에 미치는 인과적인 효과가 얼마인지 얘는 인과적인 효과가 아닙니다. 상관성을 고려한 효과인 거고요. 인과성을 고려한 효과인지를 어떻게 계산할 거냐를 저희가 살펴볼 겁니다. 자 그렇게 인과성을 보는 게 한 가지가 있고 그다음에 두 번째는 변수들 사이에 인과 관계 그래프를 추론하는 게 하나 있습니다. 실제로 우리가 새로운 영상을 하나 제작을 한다고 가정해 보겠습니다. 그러면은 그때 우리의 가장 큰 목표는 유저의 클릭수를 가장 많이 받을 수 있는 영화를 우리가 만드는 거예요. 그때 기획 단계에서 우리가 좀 더 머신 러닝스럽게 기획을 한다라고 가정하겠습니다. 굉장히 어려운 문제지만 만약에 에멜스럽게 기획을 한다라고 치면은 실제로 이 클릭 수를 높이기 위해서는 어떠한 종류의 영상을 그렇죠 뭐 다큐를 할 건지 뭘 할 건지 그다음에 2 30대 중에서 어떠한 유저를 우리가 타깃으로 할 것이며 그다음에 영상을 우리가 나중에 풀었을 때 그 영상의 가격은 얼마로 할 거고 그다음에 언제 풀 것이며 그런 것들을 우리가 다 고려해야 되는 거예요. 그렇죠 그러면 그때 실제로 이 클릭에 인과적인 효과를 미치는 애도 있고 아닌 애도 있을 거예요. 그렇죠 그러면 그런 것들을 우리가 자동으로 판단을 하고 싶다는 겁니다. 만약에 아이템 타입이 이 클릭에 인과적인 영향을 미치는지 안 미치는지 미친다면 어떻게 미치는지 그래서 우리가 어떠한 아이템 타입을 어떠한 영화 타입을 만들어야지 좋을지 그렇죠 그런 것들을 우리가 자동으로 하고 싶다는 겁니다. 실제로 여러분 넷플릭스에서 가장 유명한 영상 혹시 여러분들께서 생각했을 때 가장 유명한 영상이 뭐라고 생각하시나요? 뭐 굉장히 유명한 영상들이 많습니다. 그렇죠 오징어 게임도 있고 굉장히 많은데 지금의 넷플릭스를 있게 만든 가장 또 유명한 영상 중에 하나는 이제 하우스 오브 카드라고 우리가 볼 수가 있습니다. 그런 하우스 오브 카드는 굉장히 전략적으로 그 구체적인 알고리즘은 공개가 되어 있지는 않지만 굉장히 전략적으로 하우스 오브 카드를 이제 기획 단계에서부터 전략적인 단계로 많은 사람들의 관심을 받도록 기획이 된 것으로 알려져 있고요. 그런 걸 할 때 다음과 같은 인과성이 굉장히 널리 활용될 수가 있습니다. 그 까닭에 저희는 다음과 같은 관측 데이터 즉 실제로 특정한 유저가 뭐 20대의 유저 3차 유저일 수도 있고요. 그러한 유저들이 과거에 어떤 영화 장르들을 봤었는지 또는 어떤 영화 감독들의 작품을 봤었는지 그러한 관측 데이터들을 기반으로 해서 영화 장르 영화 감독 그다음에 클릭수 연령 그런 것들 간의 인과관계를 저희가 추론하는 것을 목표로 합니다. 왜 그렇게 되면은 우리가 이러한 클릭수를 올리기 위해서는 어떠한 절차로 마케팅을 해야 되는지 또는 어떠한 절차로 영화 기획을 해야 되는지를 저희가 정할 수가 있기 때문입니다. 여러분 지금까지는 저희가 앞으로 본 강의를 포함해서 10주 동안 어떠한 내용을 다룰 것인지에 대해서 저희가 간략하게 한번 살펴봤고요. 지금부터는 그러면 그러한 내용을 우리가 이해하기 위한 기본적인 통계학 내용을 한번 살펴보고 넘어가도록 하겠습니다. 저희가 뒤에서 계속해서 이런 통계에 관련된 내용들이 나올 거예요. 그리고 그때마다 제가 다시 한 번 리마인드를 드릴 겁니다. 하지만 그전에 저희가 좀 기본적인 내용들을 한번 짚고 넘어가도록 하겠습니다. 뒤에서 저희가 계속해서 랜덤 베리블이라는 말이 많이 나오고 여러분들께서도 이미 학부 1 2학년 네 통계학 또는 뭐 확률과 관련된 내용들을 배우시면서 랜덤 베리어블이라는 말을 수없이 많이 들었을 거예요. 그거를 한 번만 더 정리하고 넘어가겠습니다. 자 랜덤 베리어블이라는 것은 그냥 뭐냐 함수라고 생각하시면 돼요. 함수 함수라고 하면 그냥 뭐예요? x에서 뭔가 y로 보내주는 게 함수잖아요 뭔가 매핑시켜주는 거죠 인풋에서 아웃풋으로 매핑시켜주는 게 함수입니다. 여기서 랜덤 베리어블이라는 것은 인풋으로 뭐가 들어오냐면 샘플 스페이스가 들어와요. 샘플 스페이스는 뭐냐면 그냥 가능한 모든 경우의 수라고 생각하시면 됩니다. 가능한 모든 경우의 수가 그다음에 얘가 인풋이고 아웃풋은 뭐다 리얼 밸류 실수라고 생각하시면 됩니다. 그냥 숫자 하나라고 생각하시면 돼요. 자 무슨 말이냐 만약에 우리가 주사위를 2개 던집니다. 그리고 주사위를 2개 던졌을 때 그때 우리가 랜덤 베어러브를 스라고 할게요. 그러면은 우리가 뭐라고 부를 수 있을까요? 주사위를 두 개 던졌으면 가능한 모든 조합의 수가 어떻게 됩니까? 이런 아이콤마 제라고 우리가 볼 수 있겠죠 1 콤마 1 1 콤마 2 그렇죠 이 콤마 1 2 콤마 2 콤마 3 그걸 우리가 샘플 스페이스라고 부릅니다. 모든 조합 그렇죠 그러면은 랜덤 베레블이라는 건 뭐다 그러한 조합이 들어왔을 때 이러한 조합이 들어왔을 때 그거를 우리가 특정한 실수 값 하나로 보내주는 함수를 그냥 랜덤 VR이라고 한다는 겁니다. 그렇죠 그랬을 때 그러면 여기서는 x 아이콘만 제가 주어지면 아이콘만 제가 인풋으로 주어지면 그때 우리가 x를 다음같이 썸으로 표현했으니까 아이 플러스 제로 보내주는 함수를 x라는 랜덤 베리어블로 정의한 겁니다. 즉 스라는 랜덤 베리블은 아이 콤마 제가 있을 때 아이 플러스 제로 보내주는 함수라고 볼 수 있는 거죠. 그렇죠 그때 그러면은 실제로 x가 2라는 값을 가질 수도 있고 그렇죠 1 콤마 1을 가지게 되면 또는 3이라는 값을 가질 수 있습니다. 1 콤마 2나 2 콤마 1을 가지면 그렇죠 그래서 각각에 대한 우리가 확률 값을 이렇게 정의할 수가 있게 되는 거예요. 그렇죠 얘가 우리가 랜덤 베리어블이라고 부르는 것입니다. 자 그다음에 또 많이 여러분들께서 들어보셨을 분포에 대한 정의와 여러 가지 좀 정리들을 보고 넘어가도록 하겠습니다. 우리가 여기서는 분포라고 했을 때 분포는 굉장히 다양한 분포가 존재하지만 여기서 우리가 디스크릿 한 거를 한번 일단은 보고 넘어가겠습니다. 앞에서 봤던 랜덤 베리어블이라는 것은 이러한 랜덤 베리어블이라는 것은 우리가 디스크릿 이산적이라고 하기도 하고 또는 연속적이라고 하기도 합니다. 즉 무슨 말이냐면 discret 하다는 말은 앞서서 우리가 본 것처럼 가질 수 있는 결국 우리가 고려해야 되는 가지수가 한정적일 때 그때 우리는 디스크리다다라고 말을 하고 있습니다. 아시겠죠? 자 그다음에 여기서는 우리가 확률 값은 우리가 이렇게 정의를 또 할 수가 있겠죠. 즉 여기서는 어떤 말입니까? 우리가 이런 확률 값을 정의한다고 하면 x가 3 x라는 랜덤 베리어블이 그렇죠 3 1 확률값이라는 거는 우리가 이러한 샘플 스페이스에서 이 3이 나오게 되는 1 콤마 2 2 콤마 1 그거에 대한 우리가 빈도수를 서메이션 한 거 전체의 가지수에 인해서 그렇게 정의할 수 있다 라는 얘기를 우리가 하는 것이고요. 그때 디스트리비션on이라는 게 뭐냐라고 하면은 그때 확률 분포라는 거는 모든 y에 대해서 그걸 우리가 다 정의해 준 것을 확률 분포라고 합니다. 무슨 말이냐면 우리가 여기서 x는 2부터 x는 3 쭉쭉쭉 가서 x가 뭐까지 나옵니까? x는 12까지 나오겠죠 이 랜덤 베리블이 가질 수 있는 모든 것에 대해서 우리가 다 나타낸 거를 뭐 이렇게 포뮬러 공식으로 나타낼 수도 있고 또는 테이블로 나타낼 수도 있고 뭐 그래프로 그렇죠 이런 바 그래프로 나타날 수도 있겠죠. 그런 것들을 우리가 확률 분포라고 말합니다. 그리고 그러한 우리가 그 확률 분포 중에서도 인상 확률 분포는 다음과 같이 0에서 1 사이이고 그다음에 썸에이션은 1로 우리가 정의를 할 수가 있습니다. 자 이거를 우리가 연속적인 확률 분포도 사실 정리를 할 수가 있는데요. 근데 그거는 보다 더 엄밀하게 하려면 좀 수학적으로 깊게 들어가기 때문에 우리가 여기서 이상 확률 분포에 대해서만 먼저 정리를 하고 넘어가도록 하겠습니다. 자 그러면 이상 확률 분포를 우리가 배웠으니까 이거에 몇 가지 예시도 우리가 한번 보고 넘어가도록 하겠습니다. 자 이상 확률 분포에서 가장 유명한 한 가지죠. 바이노미 분포 한번 보고 넘어가도록 할게요. 바이노미 분포는 우리가 쉽게 생각해서 결국 0 또는 1이라고 생각할 수가 있겠습니다. 그렇죠 즉 동전을 우리가 던져 가지고 앞면이 나오냐 뒷면이 나오냐인 겁니다. 자 그랬을 때 예를 들어서 우리가 무 한 번 던지지 않고 결국 엠 개의 동전을 만약에 던진다고 생각합시다. 그렇죠 n개의 동전을 던져가지고 그 각각의 동전이 결국 앞면이 나오느냐 뒷면이 나오느냐 두 가지 경우의 가지수가 존재하고 그다음에 앞면이 나올 확률 또는 성공할 확률을 우리가 p 그러면 뒷면이 나올 확률을 우리가 1만 스피라고 볼 수가 있겠죠. 그렇죠 그러면 이러한 사건을 우리가 어떻게 모델링 할 것인가에 대한 게 바이노밀 분포입니다. 여기서 여러분들께서 베르누이 분포도 많이 들어봤을 것 같은데요. 베르누이 분포는 한 번의 실험만 하는 경우를 의미하는 것이고 바이노미얼은 그걸 우리가 n 번에 걸쳐서 우리가 연속적으로 실험을 독립적으로 시행한다라는 뜻입니다. 자 여러분들께서 그 아아디라는 것을 많이 보셨을 것 같은데요. 아아디라는 것은 우리가 아이덴티컬 그다음에 인디펜던트의 약자입니다. 무슨 말이냐면 우리가 여기서 바이노미얼이라는 것은 n번을 우리가 반복한다라고 말씀드리지 않았습니까? 베르누이는 동전을 한 번 던져서 앞면 또는 뒷면 인 거고 바이노미얼은 동전을 n번 던져서 앞면 또는 뒷면이 몇 번 나오냐인 겁니다. 자 그러면 동전을 n번 던지는데 n번 던질 때 우리가 동일한 환경에서 던진다는 뜻이에요. 즉 우리가 예를 들어서 처음 던진 동전과 두 번째 던진 동전이 뭔가 다르다든지 그런 게 아니라 바람이 부는 동전은 다 동일하다고 우리가 가정하자는 겁니다. 인디펜던트라는 거는 처음 던진 사건과 두 번째 동전을 던진 사건은 이제 우리가 독립 사건이라는 뜻입니다. 즉 처음 던졌을 때 앞면이 나왔어요. 그러면 두 번째 던졌을 때는 뒷면이 나온다 그런 게 아니라 처음 던졌을 때 앞면이 나오든 뒷면이 나오든 상관없이 두 번째 던진 것도 독립적으로 우리가 계산을 한다라는 뜻입니다. 그다음에 바이노미에서는 우리가 보통 랜덤 베리를 성공한 횟수 또는 안면이 나온 횟수로 정의를 합니다. 즉 바이노미얼이라는 거는 우리가 동전을 n번 던져 가지고 그랬을 때 우리가 동전의 앞면이 나오는 횟수 니가 몇 번이냐 그거에 대한 확률값이 얼마냐를 우리가 모델링하는 것이 되겠죠. 그랬을 때 우리가 동전을 만약에 n번 던졌다고 하겠습니다. 동전을 n번 던져가지고 그중에서 와이번 안면이 나왔다고 한번 가정을 한번 해볼게요. 자 그러면은 엠번 중에서 일단 와이번 성공하는 것은 우리가 이렇게 조합으로 표현할 수 있습니다. 엔 콤비네이션 와라고 표현하기도 하고 또는 이렇게 우리가 노테이션을 쓰기도 하죠. 그렇죠 n번 중에서 와 번 성공하는 경우는 만약에 y가 n이라고 치면 y가 n이라 치면 뭐예요? m 벡토리얼의 n 벡토리얼을 나눠주는 거죠. n 마이너스 n 벡토리얼 0 팩토리얼은 1이니까 그러면 엠 팩토리얼을 엠팩토리얼로 나누는 거니까 그냥 1입니다. 엔번 중에서 엔번 성공하는 경우는 한 번이잖아요. 모든 동전이 다 안면이 나오는 겁니다. 자 근데 엔번 중에서 엔 마이너스 한 번 성공하는 거 엔 마이너스 한 번 앞면이 나온다라는 말은 한 번 뒷면이 나온다는 거예요. 그 가지수는 총 n번이 있겠죠 그렇죠 자 그래서 그 가지수가 여기에 나와 있는 것이고 그다음에 안면이 나올 확률 또 뒷면이 나올 확률 앞면이 근데 y번 나올 확률 뒷면이 n 마이너스 y번 나올 확률 q는 1 마이너스 p라고 생각하시면 되겠죠. 그래서 바이용률 분포는 이렇게 우리가 식을 세울 수가 있습니다. 얘가 결국 바이노밀 분포라고 우리가 생각할 수가 있겠죠. 그렇죠 자 여기서 그러면 우리가 바이노밀만 있냐 아니죠 유니폼도 굉장히 많이 쓰이는 분포 중에 하나입니다. 만약에 버스가 8시에서 8시 10분 사이에 랜덤하게 온다 또는 8시부터 그러면 8시 2분까지 랜덤하게 온다 뭐 그런 것들을 우리가 모델링 할 때 여기서 균등하게 8시부터 8시 10분까지 올 수 있는 그 분포와 확률이 똑같다라고 하면 그걸 모델링하기 위한 분포는 그러니까 다음과 같은 유니폼 분포로 많이 모델링을 할 수가 있겠죠. 유니폼 분포는 다음과 같이 버스로 치면은 8시부터 8시 10분 사이에 나올 수 있는 이제 확률 값은 쫙 있는 것이고 그렇죠 그다음에 그거를 우리가 그 외에는 다 0을 이렇게 부여하게 됩니다. 자 실제로 이러한 우리가 그 컨티너스한 확률 분포를 모델링하기 위해서는 젠시티라는 것도 우리가 사실 정의를 하고 넘어가야 되는데 그거는 좀 많은 이제 개념을 요구하다 보니까 우리가 앞에서 봤던 디스크릿과 유사하게 먼저 컨티넌스에 대해서도 정의를 이렇게 하고 넘어가도록 하겠습니다. 즉 이 랜덤 베리어블이 가질 수 있는 모든 조합에 대해서 우리가 다음과 같은 PDF 값을 앞에서 유닛 디스크릿한 경우에는 우리가 그걸 피엠프라고 메스라고 불렀고요. 그다음에 컨센서스일 때는 우리가 그걸 덴스트라고 부릅니다. 자 그럼 PDF 값을 이렇게 정의하는 거를 우리가 확률 분포라고 말을 할 수가 있겠습니다. 그다음에 또 많이 쓰이는 게 또 뭐가 있어요? 가우시안 분포 또는 로말 분포 또는 정규 분포 다 같은 말입니다. 그것이 존재하겠죠 이런 이제 밸류 쉐입이 있고 이러한 특정한 값에서 평균에서 가장 큰 값을 가지고 있고 그 외에는 이렇게 시멘트릭한 구조를 갖고 있습니다. 자 얘는 특징이 한 가지 뭐냐면 마이너스 무한에서부터 무한 사이의 값을 우리가 가지게 됩니다. 자 무슨 말이냐면은 우리가 다음과 같은 정규 분포에서 하나 샘플링을 한다 값을라고 하면은 여기서 랜덤하게 값이 하나 샘플링 되는 거예요. 그러면 마이너스 2가 될 수도 있고 얘가 0이라고 했을 때 마이너스 2가 나올 수도 있고 매우 드물지만 마이너스 10이 나올 수도 있고 또는 플러스 10이 나올 수도 있고 그렇죠 그러한 것들을 우리가 모델링하는 것이 되겠습니다. 자 그다음에 또 굉장히 많이 쓰이는 확률 분포가 뭐가 있습니까? 베타 분포라는 게 존재합니다. 자 베타 분포는 0에서 1 사이의 값을 샘플링 함에 있어서 굉장히 유익한 분포입니다. 자 베타 분포는 다음과 같이 좀 형태가 좀 복잡하게 쓰였습니다. 이 베타 분포의 형태까지 여러분들께서 외우실 필요는 없고요. 다만 베타 분포가 어떻게 생겼는지는 여러분들이 아시면 좋을 것 같아요. 베타 분포는 0에서 1 사이에서 정의가 됩니다. 무슨 말이냐면 확률 분포는 여기 앞에서 봤던 가오시 분포는 마이너스 무한에서 무한까지 값을 모델링 하는 거예요. 그런데 여기 나와 있는 베타 분포는 뭐예요? 0에서 1 사이의 값을 가지게 됩니다. 그렇죠 그 까닭에 특정한 확률 값을 우리가 모델링할 때 실제로 이 베타 분포를 또 많이 쓰게 돼요. 0에서 1 사이의 값만 딱 가지게 되니까 그리고 우리가 가오션 분포는 여러분들 아시다시피 민과 베리언스가 모델링함에 있어서 중요하죠. 즉 이 민과 베리언스가 딱 정해지면 가우지 분포가 이렇게 생겼는지 아니면 이렇게 생겼는지 아니면 이렇게 한쪽으로 여기 좀 더 이쪽으로 오른쪽으로 가 있는 시멘트 간 분포인지 그런 것들이 정해지는데 베타 분포는 알파랑 베타에 따라서 이렇게 되기도 하고 이렇게 되기도 하고 이렇게 한쪽으로 치우쳐지기도 하고 굉장히 다양한 형태를 가지게 됩니다. 그리고 알파랑 베타가 1이 되면은 유니폼 분포로 또 가게 돼요. 그래서 굉장히 많이 활용이 되고 있다라는 거를 알고 계시면 좋을 것 같고요. 저희가 이 분포 각각에 대해서는 저희가 뒤에서 다시 한 번 활용하는 것을 보게 될 겁니다. 그다음에 또 통계학에서 굉장히 중요한 정리 중에 하나를 저희가 짚고 넘어가도록 하겠습니다. 자 센트럴 리미 페어룸 또는 이제 중심 극한 정리라고 부르는 것입니다. 자 이거를 한번 보게 되면은 만약에 와원부터 yn까지 여러분 우리가 랜덤하게 샘플링 된 것이라고 가정하겠습니다. 즉 우리가 n개 샘플을 뽑은 거예요. 쉽게 말해서 우리가 여기에서 랜덤하게 만약에 샘플을 만약에 n개 뽑는다라고 하면 0.5에서 뭔가 이 부근에서 값이 많이 뽑히겠죠 그렇죠 그다음에 요 부분에서 또 일부 뽑힐 거고 가우시안에서 우리가 값을 뽑게 되면은 이 평균 주변에서 또 대부분의 샘플이 뽑히게 되고 그다음에 이 양극단에 소재 또 약간 뽑히게 되고 유니폼은 유니폼하게 값이 샘플링 되게 되겠죠. 여기서 우리가 샘플링한다 그러면 0도 몇 개 나오고 1도 몇 개 나오고 이도 몇 개 나오고 그렇게 될 겁니다. 자 어떤 분포에서 뽑혔는지 몰라요 그렇죠 어떤 분포에서 뽑혔는지 모릅니다. 근데 어딘가에서 뽑혔다고 할게요. 어딘가에서 못 뺐습니다. 근데 이제 밍이 이렇고 베리언스가 이렇대요. 어떤 분포인지 모르겠지만 그러면은 이것들의 평균 요것들의 우리가 평균을 구할 수 있겠죠 n분의 1을 하고 이 전체 n개를 더한 거 그거를 우리가 와 바라고 하겠습니다. 그러면 그 와 바에 대한 기댓값과 베리언스는 이렇게 나와요 이렇게 나온다 라는 것이 알려져 있습니다. 근데 이 y 바가 정규 분포를 따른다 그런 말은 없어요. 우리는 모르는 거예요. 근데 만약에 이 y들이 정규 분포에서 나왔다라고 가정하겠습니다. 아시겠죠? 즉 위에보다 조금만 더 우리가 문제가 약간 쉽게 바뀐 거예요. 왜 이제 어떤 분포에서 나왔다는 것도 아니까 그때는 이 와 바가 정규 분포를 따르면서 기댓값은 이렇게 되는 거예요. 그렇죠 즉 우리가 정규 분포에서 나왔다라는 가정이 없으면 이 와 바라는 것은 어떤 분포를 따르는지는 몰라요. 기댓값이 뭐고 분산이 뭔지만 알지 그렇죠 어떤 분포인지 모릅니다. 근데 어떤 분포에서 나왔는지는 알 수는 없지만 샘플이 만약에 개수가 무지 많다 이 n이 무지 많다 라고 하면은 그때는 이 와바가 또 정규 분포를 따르게 된다는 거예요. 사실 두 번째가 가장 어쩌면 쉬운 거죠. 그렇죠 정규 분포에서 내가 나왔을 것이다. 근데 굉장히 비현실적인 왜 그래요? 여러분 한번 보시면은 앞서서 우리가 본 것처럼 우리는 이러한 샘플들이 어떤 분포에서 나왔는지 몰라요. 대부분의 많은 경우에 한번 생각해 보시면 사람 얼굴 이미지를 생각하면 사람 얼굴이 어떤 분포에서 이 사람 얼굴이라는 데이터가 나왔는지 몰라요. 사람 얼굴이라는 그 샘플들만 우리가 갖고 있는 것뿐입니다. 자 즉 그러한 샘플만 있을 때 우리가 어떻게 하면은 그 분포를 추론할 수 있을까라고 했을 때 사실 방법이 없습니다. 근데 우리가 이러한 샘플들이 무수히 많으면 그러한 샘플들에 대한 그 y br 즉 이 샘플들에 대한 평균값이죠. 그럼 y 바는 우리가 노멀 분포를 따른다라고 우리가 가정해도 된다는 겁니다. 언제 사실 이 샘플 데이터가 무수히 많을 때 그리고 사실은 또 어떤 가정이 있냐면 이 각각의 샘플들이 IID일 때 즉 샘플이 뽑힌 게 동일한 분포에서 랜덤하게 우리가 독립적으로 뽑혔을 때를 의미합니다. 그래서 이러한 센트럴 리미 시어럼에 대한 예제를 한번 시각화해서 한번 보면은 또 많이 쓰이는 분포 중에 하나가 다음과 같은 지수 분포가 있습니다. 이런 지수 분포는 다음과 같은 이제 익스포넨셜 형태로 많이 활용이 되고 있고 여러분들께서 아마 학부 1 2학년 때 한번 보셨을 것입니다. 자 여기서 지수 분포 자체를 정의는 이렇게 이때까지만 하고 넘어갈게요. 저희가 이걸 뒤에서 쓰거나 그러진 않을 거거든요. 그랬을 때 자 지수 분포가 우리가 이렇게 정의가 된다라고 했습니다. 그쵸 지수 분포가 이렇게 정리가 됐습니다. 자 그러면은 우리가 만약에 여기서 엔이 5라고 한번 가정을 하겠습니다. 지금 무슨 말이냐하면은 이러한 지수 분포를 따르는 생긴 게 이렇게 생겼습니다. 자 이러한 지수 분포를 따르는 거를 우리가 5개를 샘플링 했다고 할게요. 즉 앞에서는 n개가 샘플링 됐던 거고 그때 n이 5라는 겁니다. 그럼 5개를 샘플링하면 와1 y2 y3 y4 와5가 있는 겁니다. 그러면 그것도 평균 낼 수 있겠죠 그러면 뭐가 됩니까? 하나의 와 바가 나오는 거예요. 그렇죠 그다음에 그 과정을 우리가 천 번 반복하면 와 바가 천 개 나오는 거예요. 그렇죠 그걸 시각화하면 이렇게 나온다 라는 거예요. 즉 천 개를 우리가 시각화한 겁니다. 비율로 그렇죠 그러면 지금 여기서는 쉽게 말하면 여기가 대략 20%니까 여기가 대략 한 천에 대한 20% 200개가 여기에 몰려 있다고 해석할 수 있겠죠. 근데 이걸 엔을 늘리면 즉 우리가 여기 나와 있는 익스프렌셜 분포에서 그 변수를 25개를 랜덤하게 샘플링했습니다. 그러고 나서 그거를 평균 쳐요. 그러면 하나의 와 바가 되는 거예요. 그러면 그 y 바를 그 과정을 우리가 다시 천 번 반복하면 yb가 천 개 나오는 거예요. 그렇죠 그러면 그때는 이렇게 나온다 점점 정규 분포에 가까워지는 걸 볼 수 있죠. 정규 분포라 함은 이러한 스큐니스가 없는 거 시멘트이라는 거죠. 이렇게 되는 겁니다. 그리고 얘를 우리가 좀 더 뭐 있어 보이게 수식으로 정리하면은 이겁니다. 지금 무슨 말이냐면 되게 막 복잡하게 적혀 있는데 결국 어떤 말이냐 우리가 아이아이디하게 이런 엔개의 샘플링을 했습니다. 그러면은 그거에 대한 평균값 y 반은 결국 어떤 분포를 따릅니까라고 하면 정규 분포를 따릅니다라고 말하는 거예요. 그게 사실 전부입니다. 그게 센트럴 리미스라는 가끔씩 착각하시는 분들이 있어요. 여기서 무수히 많은 샘플을 뽑으면 이 샘플 자체가 정규 분포를 따른다라고 잘못 해석하시는 분들이 있는데 그게 아니죠. 지금 보면은 뭐가 정규분포를 따라요? y 바가 따르는 거예요. 즉 이 샘플들의 평균 이 정규 분포를 따르는 겁니다. 그래서 한번 보시면 우리가 여기서 y를 25개 샘플링 하면은 그 25개를 시각화한 게 아닙니다. y를 25개 뽑았으면 y를 평균 내서 하나의 와 바로 만들었어요. 그 y 바를 동일하게 천 개를 만들어서 그러면 사실은 2만 5천개가 샘플링 됐다고 볼 수 있겠죠. 그렇죠 25개 샘플링에서 와이바 하나 25개 샘플링에서 와이바 하나 그렇게 해서 천개를 우리가 시각화하면 이런 정규 분포를 점점 자르더라라는 게 센터럴 리니시어론입니다. 네 이제부터는 그 통계학과 관련된 또 기본적인 내용들을 조금만 더 살펴보도록 하겠습니다. 자 여러분 저희가 머신러닝 알고리즘을 계속해서 배워나가면서 또는 여러분들께서 본 커리큘럼을 따라가면서 라이클루드라는 말을 굉장히 많이 보게 될 겁니다. 라이클루드라는 것은 어떤 거냐면요. 우리한테 만약에 엔게이 샘플이 주어졌다고 가정하겠습니다. 즉 샘플링이 된 거예요. n게 이제 관측 값이 n게이 데이터가 이제 우리가 수집이 된 겁니다. 그랬을 때 그런 n게이 데이터가 결국 나오게 될 경향성을 우리는 이 라이클루드라고 말을 합니다. 이 라이클루드라는 거는 사실은 확률이라는 거랑은 약간 좀 다르긴 합니다. 사실은 여기서는 우리가 여러분들께서 그 가오샴 분포 같은 컨티넌스 값에 대해서는 PDF 값을 우리가 다음과 같이 사실은 정의를 했었습니다. 그렇죠 그다음에 뭐 베타도 마찬가지고요. 그다음에 우리가 다음과 같은 바이노미 같은 경우는 또 이러한 피엠프 형태로 또 우리가 정의를 했었습니다. 즉 피고 프고 그건 중요하지 않고요. 보통 디스크릿 할 때는 우리가 피 또는 이제 컨티넌스 할 때는 우리가 프라고 많이 나타내고 있습니다. 자 여러분 그 라이클루즈라는 거는 쉽게 말해서 경향성인 건데 그 데이터가 이제 나오게 될 경향성인 건데 그거는 다음과 같은 거으로 결국 나오게 됩니다. 즉 만약에 우리가 가온 시험 분포에서 인기가 나왔다라고 가정하겠습니다. 독립적으로 그러면은 실제로 가온 시험 분포에서 그 각각의 샘플이 나오게 될 경향성을 우리가 다 곱하면 돼요. 어떻게 이거를 구하냐라고 하면은 실제로 이 가우시안 PDF에다가 이 와라는 값을 딱 넣어주면은 묘하고 시그마는 우리가 안다고 가정하면 이 와라는 거를 딱 넣어주면은 그때 이거에 대한 값이 나오게 됩니다. 근데 우리가 얘를 왜 확률이라고 하지 않냐라고 하면은 실제로 여러분 가우시안 PDF는 이 1보다 클 수가 있어요. 얘가 엄청 그 분산이 어떠냐에 따라서 그래서 라이클루드랑 이제 확률이랑은 다르다. 이 라이클루드라는 거는 경향성인 건데 결국 이러한 데이터 샘플이 나오게 될 경향성으로 여러분들께서 생각하시면 됩니다. 자 그리고 여기 나와 있는 세터가 뭐냐라고 하면 세타가 결국 파라미터예요. 우리가 최적화시키고 싶은 파라미터입니다. 즉 무슨 말이냐면 우리가 이러한 샘플들이 가우샴 분포에서 만약에 나왔다라고 가정하겠습니다. 그러면은 앞서서 말씀드린 것처럼 우리가 라이크드를 구할 수가 있어요. 어떻게 와에다가 우리한테 주어진 샘플 값을 딱 넣으면 그러면 뮤랑 시그마가 우리가 안다고 가정만 하면 이 값을 계산할 수 있겠죠. 그렇죠 근데 실제로는 이 뮤랑 시그마를 우리가 많은 경우에 모른다고 했습니다. 샘플만 알지 그렇죠 그러면은 뮤랑 시그마가 얼마가 되어야지 가장 이 값이 클 것인가 즉 데이터가 나오게 될 경향성을 가장 잘 설명하는 이 세타 뮤하고 시그마가 뭐냐 가오시일 경우에 이 파라미터가 뭐냐를 우리는 이제 추정하고 싶은 게 목표예요. 그래서 막 수식이 되게 막 복잡하게 써져 있는데 결국에 우리는 뭘 하고 싶은 건지 딱 결론만 여러분들이 기억하시면 되거든요. 그 결론은 뭐냐면 이러한 데이터가 n게이 데이터가 만약에 주어졌을 때 그 n게이 데이터를 가장 잘 설명할 수 있는 모델 파라미터를 우리가 최대화하는 겁니다. 그래서 여기서는 한번 보시면 여기서 예제는 우리한테 만약에 m개 관측 값이 주어졌어요 m개 이미지가 주어졌다라고 할 때 그 m개 이미지를 가장 잘 설명할 수 있는 세타라는 파라미터를 찾는 것이다. 즉 쉽게 말해서 다시 한 번 말씀드리면 우리한테 주어진 데이터가 만약에 정규 분포에서 나왔다고 하면은 그 정규 분포를 그 정규 분포에 대한 민하고 코베런스를 우리가 추론하는 겁니다. 이 데이터를 가장 잘 설명하기 위한 용도라는 뜻입니다. 자 얘가 우리는 라이클리브라고 불러요. 자 그래서 우리는 그 여러분들께서 엠멜리 맥시멈 라이클로드 에스티메이터라는 거를 여러분들께서 아마 이제 들어 보셨을 가능성이 가장 큰데 얘가 얘는 특별한 게 아니라 이러한 라이킬로드를 가장 최대화시켜 주는 파라미터를 맥시멈 라이크 로드의 에스티미터라고 부릅니다. 그래서 이러한 에밀리 같은 경우는 사실 뭐 여러 가지 좋은 성질들을 갖고 있다고 하지만 가장 중요한 거는 데이터 개수가 사실 무수히 많을 때는 그때는 이런 에밀리가 실제로 실제 그라운 트루 파라미터로 근사한다라는 것이 또 한 가지 중요한 성실이 됩니다. 즉 무슨 말이냐면 우리한테 주어진 데이터가 이렇게 m기가 있을 때 그 데이터 m개를 가장 잘 설명하는 정규 분포의 민과 코비런스를 찾겠다. 그러면 처음에는 그게 부정확할 수 있지만 그 엠이라는 데이터 개수가 만약에 많아지면 많아질수록 우리가 추정한 미나노 코비넌스도 점점 더 실제에 정확해진다라는 뜻입니다. 자 지금까지 본 거는 우리가 하나의 세터만 본 거예요. 지금 여기서의 세타는 파라미터 또는 지금의 예시에서는 계속해서는 민하고 코베리언스라고 생각하시면 됩니다. 정규 분포의 민하고 코베리언스 자 그러면은 우리한테 주어진 데이터를 가장 최대화하는 민 값 하나 코베리언스 값 하나를 우리가 찾는 거예요. 그렇죠 근데 우리가 특정한 가정을 할 수도 있는 겁니다. 아 이러한 미는 대략 뭐 0에서 5 사이에 있을 것 같다라든지 그러한 우리의 믿음을 넣을 수도 있는 거예요. 그렇죠 그러한 믿음을 우리는 프라이어라고 부릅니다. 즉 파라미터 세타 여기서는 우리가 미하우 코비런스가 되는 것이고 베이지 뉴럴넷이라는 거는 이 세터가 웨이트 파라미터 w가 되는 거지 않습니까? 그 w에다가 우리가 특정한 분포를 가정하는 거예요. 일단 우리는 베이지 뉴로넨 토크는 하지 않고 여기서는 미하고 코비런스에 대해서 우리가 범위를 주는 겁니다. 민이라는 건 이런 범위에 있을 것이다. 코비런스는 이런 범위에 있을 것이다라고 자 그러면은 여기 첫 번째 파트가 뭐예요? 라이클루드라고 볼 수 있죠 엠게 데이터에 대한 라이클로드 거기다가 이제 프라이어를 곱한다고 하겠습니다. 그렇죠 자 그러고 나서 이 아래에다가 이런 데이터를 우리가 쓰게 되면은 조건부 확률 베이즈 룰에 따라서 이러한 포스테리어가 나오게 됩니다. 라이클로드랑 다른 건 뭐예요? 이러한 스가 이제 기본으로 주어지는 겁니다. 그리고 세터를 추정하는 거예요. 자 무슨 말이냐면 라이키루드는 이러한 데이터를 가장 잘 생성할 법한 데이터를 추론하는 게 라이클루드였고 포스테리어라는 것은 자 이러한 데이터가 관측됐습니다. 근데 우리는 이 세타에 대한 우리만의 믿음도 있어요. 이 세타는 뭐 어떤 범위에서 어떤 범위 사이에 있을 것이다 또는 어떤 분포를 따를 것이다라는 그러한 믿음까지 같이 결합된 형태를 우리는 포스트 이오라고 부릅니다. 왜 얘는 다시 쓰면 이렇게 써지니까 여기 나와 있는 프라이어랑 이런 라이키루드로 우리가 써지게 되니까요. 즉 이러한 이제 포스테리어라는 것이 뭔지까지도 우리는 한번 보게 됐습니다. 그래서 이러한 포스테리어는 우리가 추정을 하는 게 쉽지가 않아요. 왜 이러한 분모를 한번 보시게 되시면 이러한 분모는 우리가 다시 쓰게 되면은 이러한 뭔가 여러 가지의 우리가 곱으로 사실 곱에 대한 적분 값으로 쓰게 되거든요. 그래서 얘의 정확한 포스테리어를 우리가 알 수는 없는 경우가 많아요. 그 까닭에 우리가 배려자 엠퍼런스나 엠씨엠씨와 같은 방법론을 저희가 나중에 배우게 될 겁니다. 지금까지 결국 정리하면 뭐냐면 MLE 맥시멈 라이클루드 에스티메이터라는 거는 우리한테 관측된 데이터가 있을 때 우리가 특정한 믿음을 하나도 가지지 않습니다. 그냥 딱 데이터만 보고 그 데이터를 가장 잘 설명하는 파라미터 민과 코베리언스 여기서는 가우시안으로 예시를 들면 민과 코베런스를 추정하는 겁니다. 근데 우리는 만약에 알아요. 뭘 그러한 민과 코베런스가 가우시안 분포를 따를 거다 아니면 어떠한 범위 안에 있을 거다라는 가정을 우리는 알고 있습니다. 그러면 우리의 그런 믿음을 모델한테 넣고 싶죠. 그거를 우리는 프라이어라고 한다고 했습니다. 그렇죠 그러면은 그런 라이 클리우드랑 프라이어를 우리가 같이 고려하 그러면은 이러한 데이터가 관측이 됐고 우리의 믿음은 이런데 그때 최적의 미음과 코베런스는 뭔지 왜 관측값이 틀릴 수도 있으니까 또는 우리의 믿음이 틀릴 수도 있으니까 그 두 가지를 같이 고려한 것을 우리는 포스테리어라고 부릅니다. 자 예시로 한번 볼게요. 우리가 흔히 많이 보는 리니어 리그레이션 세팅을 한번 보겠습니다. 리니어 리그레션 세팅 데이터 x가 주어져 있고 여기에 블라는 파라미터를 곱해서 y라는 걸 우리가 추론하는 겁니다. 그렇죠 자 그때 이 와라는 것은 우리가 가우시안 분포에서 이렇게 나온다고 가정하겠습니다. 얘를 민으로 하고 그다음에 아이덴티티 코비런스 또는 베리언스 1을 갖고 있는 걸로 생각하시면 됩니다. 그러면은 여기서 이렇게 우리가 쓸 수가 있겠죠. 가오션 분포에 대한 PDF 값을 우리가 분모를 무시하고 비량할 관계로 쓰면 이렇게 쓸 수가 있습니다. 그쵸 여기서 이 w라는 거를 우리가 w에 대한 믿음을 하나도 주지 않고 얘를 그냥 가장 맥시마이즈 시키는 w를 찾는다 라고 하면은 그때는 우리가 MLE를 구하는 것이고 근데 우리가 이 w라는 게 가오션 분포를 따를 것이다. 이 w라는 게 특정한 이 뮤제라는 값을 기본으로 해서 그 양옆에 특정한 범위 안에 존재할 것이다라는 가정이 있으면 이런 프라이어를 우리가 줄 수 있는 겁니다. 더블유는 이러한 정규 분포를 따를 것이다라는 프라이어를 주는 것이죠. 그러면 우리는 이러한 에멜린 이러한 라이클리드와 이러한 프라이어를 결합한 포스테리어를 추론할 수 있는 거예요. 그러면 그때 포스테리어는 이건 여러분들께서 한번 계산을 한번 해보세요. 얘랑 얘를 두개를 곱한 다음에 얘를 더블유에 대해서 한번 정리를 한번 묶어 보세요. 그러면은 이런 형태로 나옵니다. 즉 다시 가오신이 나오게 돼요. 그러면은 우리는 얘를 맥시마이즈 시키는 w를 찾게 되면 어떤 게 됩니까? 그때는 우리의 믿음에 기반하면서도 우리한테 관측된 데이터에도 잘 설명할 수 있는 w 파라미터를 찾게 된다고 보시면 됩니다. 여기서의 블가 앞에서는 세타에 대응된다라고 사실 보셔도 되겠죠. 그리고 실제로 여러분들께서 이미 엠에피라는 이러한 포스테리어를 이미 쓰고 있습니다. 언제 쓰고 있냐면 실제로 여러분들께서는 뉴럴 네트워크를 학습하거나 할 때 다음과 같이 로그 라이클루드를 맥시마이즈 하면서 뒤에 웨이트에 대한 엘투 놈을 우리가 페널티로 주는 경우가 많으실 거예요. 사실은 그게 이러한 가우시안을 우리가 이 이 셋탑 또는 이 블는 가우시안 분포를 따른다. 0 콤마 1이라는 가우시안 분포를 따른다라는 가정이랑도 연결됩니다. 자 그 부분 여러분 한번 고민을 한번 해보세요. 힌트를 잠깐 드리면 p 세터를 우리가 가우시안 분포 0 콤마 1을 따르는 가우시안 분포로 쓰게 되면은 그렇죠 익스포넨셜 형태의 뭔가 세타 마이너스 0의 제곱 형태로 써지게 됩니다. 로그랑 익스포넨셜 없어지니까 남는 거는 마이너스 세타의 제곱만 남게 돼요. 그게 l2는 패널티를 주는 것과 동일한 형태가 되는 거죠. 사실은 그래서 이미 여러분들은 프라이어 모델링은 이미 알고 있다 라는 점입니다. 여기까지 해서 우리가 통계학에 대한 기본적인 내용들을 봤는데 여러분들께서 기억하셔야 되는 건 랜덤 베리블이 뭔지 분포라는 게 뭔지 그다음에 분포의 다양한 종류 바이노미얼 유니폼 그다음에 베타 가오잔드 그쵸 그다음에 그런 분포들의 연결 관계와 특징을 저희가 한번 봤고 마지막으로는 라이클리우드 프라이어 포스테리어까지 우리가 한번 살펴봤습니다. 네 여러분 그러면은 고생 많으셨습니다."
}