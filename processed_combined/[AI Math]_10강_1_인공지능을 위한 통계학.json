{
  "lecture_name": "[AI Math]_10강_1_인공지능을 위한 통계학",
  "source_file": "[AI Math]_10강_1_인공지능을 위한 통계학_84.mp4_2025-12-04-103957903.json",
  "text": "네 안녕하세요 메스메릭스포 RT피셜 인텔레전스 10강 인공지능을 위한 통계학 강의를 맡게 된 고려대학교 통계학과 임성빈 교수입니다. 오늘 이번 시간에는 9강에서 배웠던 기초적인 확률론을 기반으로 이제 인공지능에서 필요한 통계학 기본 개념들을 한번 살펴보도록 하겠습니다. 먼저 통계학이 어떤 학문인지를 좀 소개해 드리려고 합니다. 사실 통계학의 어떠한 정의는 좀 넓은 편입니다마는 기본적으로 통계적 모델링이란 적절한 가정 위에서 확률 분포를 추정 인퍼런스 하는 기법이고 이 기법 자체는 사실 기계 학습과 통계학이 공통적으로 추구하는 목표다라고 얘기할 수 있겠습니다. 즉 적절한 과정을 위해서 어떤 데이터를 관찰할 수 있는 확률 분포를 추정하고 이 추정된 정보로 이제 여러 가지 예측을 하거나 또는 어떤 분석을 하거나 또는 의사 결정을 하는 데 필요한 정보를 우리가 추출하는 학문이 되는 것이고요. 이거는 기계학습과 통계학이 공통적으로 추구하는 목표라서 방법론적으로 겹치는 부분들이 상당히 많습니다. 이때 어떤 데이터가 특정 확률 분포를 따른다고 우리가 어 선언적으로 미리 아프로어를 가정한다고 했을 때에 그 분포를 추정하는 방법을 우리는 이제 파라미트릭 모수적 방법론이라고 합니다. 어 지금 이렇게 하단에서 보시다시피 사실 저희가 사용할 수 있는 파라메트릭 분포가 굉장히 여러 종류가 있는데요. 사실은 이것보다도 훨씬 더 많습니다. 이렇게 여러 가지의 분포를 가지고 우리가 어떤 데이터를 관찰했을 때 아 이 데이터는 어떤 분포를 따르는 것 같다라고 가정을 했을 때 그 가정 위에서 분포를 측정하는 거는 우리가 이제 모수적 방법론이라고 하는데 우리가 보통 통계학 학부 시간 때 배우는 방법론들은 주로 이러한 모수적 방법론들을 많이 쓰고는 합니다. 사실 우리가 쓰는 선형 회귀 분석 같은 경우도 모수적 방법론에 속한다고 얘기할 수가 있겠습니다. 이런 모수적 방법론의 예시를 좀 한번 살펴보도록 하겠습니다. 우리가 사실 통계학 시간 때 많이 배우기는 합니다만 어 가령 이제 데이터가 이와 같은 특징들을 지니면 선험적으로 모델링 할 수가 있는데요. 가령 데이터가 어떻게 두 개의 값 즉 0 또는 1만 가지는 패턴을 가지고 있는 경우에는 베르누이 분포로 모델링 해 볼 수가 있겠고요. 만약에 0 또는 1 또는 이렇게 2개의 값만 가지는 경우가 아니라 좀 더 여러 개의 하지만 이산적인 값을 가지는 경우에는 우리가 카테고리 분포로 모델링 할 수가 있겠습니다. 어 그리고 이제 데이터가 이산적인 경우가 아니라 연속 확률 분포로 지난 시간에 배웠던 연속적인 케이스를 살펴보시게 되면은 데이터가 만약 0보다 크거나 같고 1보다 작거나 같은 경우에 값을 가지는 경우에는 그때는 베타 분포 즉 베타 디스트리뷰션으로 모델링 할 수가 있게 되는 것이고요. 만약에 데이터가 0과 1 사이의 범위가 아니라 0 이상의 값을 가지는 경우에 즉 0서부터 무한대까지 모든 값을 가질 수 있다고 하면은 그때는 0보다 큰 범위에서 정의가 되는 감마 분포나 또는 지수 분포나 로그 정규 분포를 사용해 볼 수도 있겠고요. 만약에 데이터가 실수 전체에서 값을 가지는 즉 전 범위에서 값을 가지는 경우에는 여러분이 잘 알고 계시는 정규 분포나 또는 라플라스 분포 같은 경우를 사용할 수가 있겠습니다. 우리가 어떤 특정 확률 분포를 따른다고 가정하고 그 가정 위에서 확률 분포를 모델링하고 추론할 수가 있고요. 이런 추론들을 사용하는 게 모수적 방법론에 속한다라고 이해하시면 되겠습니다. 네 문제는 이제 이런 확률 분포마다 모수를 추정하는 방법론들이 다 각각 개별적으로 있는데 이런 데이터를 통해서 우리가 어 추정을 알맞게 했는지 검증하는 과정이 좀 필요합니다. 이거를 이제 테스팅 과정이라고 하는데요. 이 테스팅 과정을 통해서 우리가 실제로 적절한 분포를 선택했는지 그런 가정을 우리가 검증할 필요가 있겠습니다. 가령 여러분들이 어떤 데이터를 정규 분포로 모델링을 했다고 하게 되면은 어 그냥 평균과 분산만 추정하고 아 이 분포는 정규 분포요라고 결론을 내리면 안 되겠죠 이 정규 분포가 실제로 정규 분포의 패턴을 띠는지 우리가 정규성 검정 즉 노멀리티 테스팅을 할 수가 있고 이를 통해서 우리가 해당 확률 분포에 대한 가정이 맞는지도 우리가 검정할 수가 있고 그 검정이 맞다라고 한다면 우리가 이 가정 위에서 데이터의 확률 분포를 추정할 수가 있는 것이죠. 모수적 방법론은 이런 식으로 우리가 관찰한 데이터의 특징을 기반으로 추정하는 기법이 되겠습니다. 하지만 우리가 이제 유한한 개수의 데이터만 관찰해서는 사실 모집단의 분포를 정확하게 알아내는 것은 불가능하고요. 그리고 어떤 데이터의 변수가 하이드 맨저인 경우에는 사실 모수적 분포를 사용하는 게 쉽지 않을 수도 있습니다. 이런 경우에는 우리가 근사적으로 확률 분포를 추정하는 기법을 써야 될 수가 있습니다. 만약에 특정 확률 분포를 가정하지 않고 여러분이 데이터에 따라 모델의 구조 또는 일단 모수의 개수가 유연하게 바뀌게 되는 경우에는 우리가 모수적이 아닌 즉 비모수 넌 페라메트릭 방법론이라고 부르게 되고요. 우리가 기계 학습에서 배우게 되는 상당수의 방법론은 통계학에서는 이제 비모수 방법론이라고 부르고 우리가 실제로 기계 학습에 많은 기법들은 어 비모수 기법으로 우리가 분류하고 배우게 되는 것입니다. 다만 우리가 통계학에서 배우는 게 꼭 모수적 방법론만 있는 것은 아니고요. 이런 비모수 방법론에 대한 연구도 통계학에서 많이 연구가 되어 있고 기계 학습에 많은 연구들이 이런 바탕 위에서 진행이 되었다는 것을 여러분들께서 이해하시면 되겠습니다. 네 이런 통계적 기계 학습 또는 추정에서 가장 많이 사용되는 학습 원리를 여러분들께 소개해 드리려고 합니다. 어 가장 많이 사용되는 것 중 하나가 바로 맥시멈 라이클리우드 에스티메이션 즉 최대 가능도 추정법이라고 부르는 에메리 추정 방법입니다. 어 먼저 우리가 라이클리우드라는 게 뭔가를 좀 설명을 드려봐야 될 것 같은데요. 라이클리우드 즉 가능도 함수라는 것은 이렇게 엘이라는 기호를 쓰게 될 텐데 먼저 데이터가 주어져 있는 경우에 그 데이터를 가지고 해당 모수가 실제로 적절한지 아닌지를 평가를 하게 될 때 쓰는 것인데요. 좀 더 정확히 말하자면 어떤 파라미터 모수 세터를 따르는 확률 분포에서 해당 데이터 x를 관찰할 가능성을 의미합니다. 수식적으로 쓰게 되면은 세타라는 파라미터를 설정했을 때 해당 확률 분포에서 스를 관찰할 에비던스 즉 스를 관찰할 가능성을 우리가 라이클리우드라고 설정하게 되는 것이고 이때 이 가능성을 가장 극대화시키는 파라미터 세터를 찾는 걸 바로 에멜리 맥시멈 라이클리우드 프린스플이라고 부르는 것이죠. 정리하자면 어떤 파라미터를 썼을 때 이 데이터를 관찰할 확률이 가장 클 것인가 그 파라미터 세터를 찾는 원칙을 우리가 에밀리라고 부르게 되는 것이죠. 기계 학습의 원리에서 가장 어떻게 말하자면 적절한 원리라고 볼 수가 있는데요. 왜냐하면 어떤 파라미터를 썼을 때 이 데이터를 관찰할 확률이 낮다면 이 파라미터를 사용했을 때 데이터를 관찰할 가능성이 낮다면 그 페라미터를 사용하게 될 어떤 정당성이 떨어지겠죠. 그렇기 때문에 에메리는 이 페라미터를 사용했을 때 지금 현재 관찰되는 데이터를 우리가 실제로 보게 될 확률이 가장 높다라는 어떤 원리를 가지고 그 페라미터를 추정하는 방법이다. 그래서 통계학적으로 굉장히 많이 사용되고 어떻게 말하자면은 기계 학습 방법론에서 가장 근간이 되는 원칙이다라고 이해를 하시면 되겠습니다. 네 문제는 이 라이 클리우드 함수를 가지고 실제 최대화를 우리가 수행하게 되긴 하는데요. 근데 만약에 데이터를 우리가 독립적으로 추출했다 즉 독립적으로 이 데이터를 데이터 분포로부터 관찰했다라고 가정하게 되면은 우리가 실제로 이 라이클리우드 함수를 데이터의 개수에 따라서 데이터의 어떤 이 관찰하는 인덱스에 따라서 우리가 곱으로 쪼갤 수가 있게 됩니다. 독립적으로 추출했기 때문에 이 데이터를 관찰하게 되는 확률이 독립 사건으로 우리가 이해할 수 있으니까 해당 밀도 함수 또는 확률 질량 함수를 이런 식으로 곱셈으로 쪼갤 수가 있게 되는 것이죠. 이렇게 곱셈으로 쪼개지는 나이클리우드에 대해서 우리가 어떤 식으로 변환할 수가 있냐면은 로그를 씌워서 그 로그를 씌운 녀석을 가지고 우리가 최대화를 수행할 수가 있습니다. 왜냐하면 로그 함수는 단조 증가 즉 모노토닝 인크리징 함수이기 때문에 라이클리우드에 로그를 씌워서 최대화를 수행하나 아니면 라이클리우드 그 자체를 최대화를 수행하나 모두 같은 어 세터를 얻기 때문에 그렇습니다. 근데 그렇다면 우리가 왜 로그를 쓰는 것일까요? 네 실제로 우리가 라이클리우드 계산을 했을 때 독립적으로 관찰되는 데이터에 대해서 이런 확률 분포 밀도 함수나 에비던스 함수를 곱해주게 되면은 사실 0에서 1 사이의 확률 값을 곱해주는 것이기 때문에 데이터 개수가 많아졌을 때는 어 이 곱셈의 값이 매우 0 근처로 떨어지게 됩니다. 근데 우리가 주로 컴퓨터를 통해서 최적화를 수행하기 때문에요. 0 근처의 값으로 떨어지게 되면은 실제로 컴퓨터는 0으로 인식할 확률이 크기 때문에 최적화 과정에서 좀 불편함을 볼 수가 있기 때문에 이거를 로그를 취해서 곱셈이 아닌 덧셈으로 변환하는 걸 통해서 어 좀 더 컴퓨테이션 전산학적 관점에서 안정성을 추구하게 되는 것이죠. 그래서 실제로 통계학에서는 그냥 가능도 자체를 최대화하는 것보다 데이터를 독립적으로 관찰했다는 전제하에서 로그 가능도를 최대화시키는 방법을 수행하게 됩니다. 그래서 우리는 실제로 로그 가능도를 최대화하는 방법을 찾게 되는 것이고 로그 가능도의 경우에는 각각의 데이터에 대해서 로그 가능도를 덧셈을 취한 이 로그 가능도를 최적화하는 방식으로 우리가 최적 파라미터 세터를 찾는다라는 거를 기억하시면 좋겠습니다. 네 다시 한번 정리를 드리자면 왜 로그 가능도를 사용하냐면요 일단 로그 가능도로 최적화된 모수 세터를 찾는 게 실제 가능도를 최적화하는 가장 최적화된 세터를 찾는 것과 같은 문제가 되고요. 데이터가 만약에 수억 단위가 되면은 컴퓨터의 정확도로는 우리가 로그 가능도를 계산하는 것이 가능도를 계산하는 것보다 좀 더 정확한 최적화가 가능하기 때문에 그렇습니다. 그리고 가능도의 값이 어 로그 가원도로 바꿨을 때 좀 더 단위나 연산적 측면에서 훨씬 더 효율적인 연산이 되기 때문에 우리가 최적화 관점에서는 좀 더 수치적으로 안정된 장점을 누릴 수가 있고 특히 경사하강법으로 가능도를 추적할 때는 미분 연산을 쓰게 되는데요 로그 가능도를 사용해서 경사 하강법을 사용하게 되면은 실제 연산량이 선형적으로 콤플렉스 티가 줄어들기 때문에 즉 오엔 제곱에서 5엔으로 줄어들기 때문에 훨씬 더 유리한 연산이 됩니다. 즉 데이터의 개수가 증가했을 때 엔 제곱이 아니라 엔으로 줄어들기 때문에 어 훨씬 더 유리한 연산 최적화에 장점을 누릴 수가 있게 되고요. 그래서 대부분의 이제 손실 함수의 경우에는 경사강법을 쓰기 때문에 우리가 라이 클리우드에 로그를 씌운 다음 거기다가 음수를 취해줘서 우리가 negative 롤크렐라이클루드로 바꿔주고 거기서 경사 감법을 취해 주게 됩니다. 네 그러면 우리가 이 최대 가능도 추정법을 이용해서 실제로 통계학에서 어떻게 사용되는지를 좀 살펴보도록 하겠습니다. 가장 대표적인 예로 정규 분포를 한번 살펴볼 건데요. 정규 분포를 따르는 어떤 확률변수 x로부터 독립적인 표본 x1 xn까지 총 n개의 데이터를 독립적으로 얻었다고 가정해 보겠습니다. 이때 이 데이터를 기반으로 해서 최대 가능도 추정법을 이용해 이 정규 분포의 모수 정규분포의 경우는 모수가 2개죠 평균과 분산 평균과 분산을 한번 가장 어 최대 가능도의 기반으로 해서 가장 적절한 모수를 추정해 보자라는 문제를 풀어보도록 하겠습니다. 자 먼저 라이클리우드 맥시마이제이션을 우리가 수행해야 되니까요. 라이클리우드 함수가 어떻게 나오는지를 한번 관찰해야 될 텐데 우리는 현재 이 확률 변수의 스가 정규 분포를 따른다라는 걸 알고 있기 때문에 주어진 모수에 대해서 실제로 밀도 함수를 수직적으로 쓸 수가 있겠죠. 네 이거를 수식적으로 바꿔주게 되면은 우리가 평균 뮤 그리고 분산 시그마 스퀘어가 주어졌을 때의 확률 밀도 함수에 가지고 우리가 그 로그 라이클 라이클리우드 최대화를 수행하는 문제로 이해할 수가 있겠습니다. 자 그렇다면 이거를 이제 로그라이클리오드로 변환해 보겠습니다. 로그 라이클리오드로 변환하려면 주어진 보수 뷰랑 시그마 제곱에 대해서 각각의 데이터가 관찰된 확률 밀도 함수 값을 계산해야 됩니다. 네 계산하는 것은 어렵지 않습니다. 우리가 알고 있는 정규 분포의 밀도 함수에다가 유랑 시그마 제곱을 넣고 그리고 x자리에다가 데이터의 값인 스아 값을 넣으면 됩니다. 자 그러면은 주어진 스아이에 대한 에비던스 값을 계산하게 되는 것이죠. 그리고 이 밀도 함수에다가 로그를 취해 주게 되는데요. 로그의 성질은 곱셈을 덧셈으로 바꿔주는 것이죠. 그래서 루트 2파이 시그마 제곱 분의 1과 그리고 지수 함수 2 투더 2 시그마 제곱 분의 x아 빼기 뷰의 제곱을 어 이렇게 곱해져 있는 두 개를 로그를 취해 주게 되면은 나눠주기 이게 두 개의 항으로 덧셈으로 나눠 줄 수가 있게 되는데 어 지금은 이제 분수가 이제 들어가 있다 보니까 분수인 경우에는 우리가 뺄셈으로 바뀌어지게 돼서 이 로그 라이클리우드의 최종 값은 마이너스 2분의 1 곱하기 로그 2파이 시그마 스퀘어 빼기 아는 14부터 n까지의 합으로 바뀌어지게 되는데 이때 이 시그마 제곱 분의 XI 빼기 평균의 제곱의 합을 우리가 얻게 되는 것입니다. 그렇다면 이제 이 로그라이 클리우드를 가장 최적화하는 뮤랑 시그마 제곱을 한번 구해보도록 하겠습니다. 이거를 최적화하는 방법은요 네 미분을 통해서 최적화를 할 수가 있습니다. 그래서 각각의 로그라이클리오에 대해서 각 파라미터 모수인 뮤람 그리고 시그마에 대해서 미분을 한번 취해 보겠습니다. 미분을 취해줬을 때 그 미분값이 0이 되는 지점을 찾아주게 되면은 우리는 가장 최적화된 파라미터를 찾았다고 결론을 내릴 수가 있게 되는 것이죠. 그래서 한번 미분을 한번 수행해 보겠습니다. 먼저 뮤에 대한 미분을 수행해 보게 되면은 이와 같이 마이너스 마이너스 에서부터 n까지 서메이션 그리고 시그마 제곱 분의 스와 빼기 뮤가 나오게 되고요. 시그마에 대한 로그라이크로 된 미분을 취해주게 되면은 하단의 식처럼 마이너스 시그마 엔 더하기 시그마 3승 분의 1 곱하기 아는 14부터 n까지 엑스알 빼기 뷰의 제곱이 나오게 됩니다. 자 조금 복잡해 보이지만 이렇게 계산된 미분 값들이 모두 0이 되는 뮤랑 시그마를 찾으면 되겠습니다. 자 이거를 한번 찾아보시게 되면요. 먼저 상단의 식에서 시그마 제곱은 그 스와 빼기 뮤 전부에다 사실 곱해져 있는데 이 양변에 시그마 제곱을 곱해주게 되면은 사실 0에다 곱해지고 하니까 좌변은 0 그대로이고요. 우변은 XI 빼기 뷰의 서메이션만 살아남게 되겠죠. 그래서 얘네들을 정리해 주게 되면은 뮤는 XI들의 합을 n분의 1로 나눠준 즉 산술 평균의 공식을 얻게 됩니다. 달리 말하자면 정규분포의 기댓값을 우리가 기댓값에 해당하는 뉴에 최대 가능도 추정법 원리에 따라서 가장 잘 추정한 모수 추정은 바로 뭐냐면은 사실은 데이터에서 산수 평균을 계산한 걸 가지고 우리가 사용하게 되면은 가장 최대 가능도 추정법 원리에 근거하여 추정한 값이다라는 걸 알 수 있게 되는 것이죠. 그래서 산소 평균을 쓰는 것이 실제로 에밀리 원칙하에서도 가장 좋은 추정 방법이다라는 걸 알 수가 있는 것이고요. 어 이 똑같은 원리를 이용해서 우리가 시그마에 대해서도 한번 계산해 볼 수가 있는데 이거를 한번 여러분이 풀어보시면 시그마의 경우에는 XY 빼기 뮤의 제곱을 가지고 다 더해서 n분의 1로 나눠준 예 이 공식이 시그마에 대한 가장 최대 가능도 추정법을 근거로 얻은 어 최선의 파라미터다라는 것입니다. 네 근데 잘 보시면 이 시그마에 대한 가장 최적화된 추정 에밀리 추정 방법은요 잘 보시면 어 불편 추정량 성질을 만족하지 않습니다. 불편 추정량이 되려면 원래는 n이 아니라 n 마이너스 1로 나눠줘야 되는데요. 네 한 가지 주의하실 점은 우리가 불편 추정량을 만족하는 것은 ML리에서는 항상 보장하지는 않는다라는 사실을 여러분들께서 기억하셔야 되겠습니다. 네 이번에는 카테고리 분포에서의 최대 가능도 추정법을 활용한 예시를 한번 살펴보겠습니다. 네 카테고리 분포는 앞에서도 설명드렸듯이 n개의 여러 개의 경우의 수를 가지는 어 케이스에 부터 해서 우리가 모델링을 할 때 쓸 수 있다고 말씀을 드렸는데요. 자 이 예제의 경우에는 어 전부 약간 d 개의 케이스를 가지는 그래서 p원서부터 PD까지 총 2개의 케이스를 가지는 카테고리 분포를 저희가 선정해 보겠습니다. 즉 이 카테고리 분포는 p14부터 PD까지 즉 d개의 페라미터를 가지는 분포라고 볼 수 있을 것이고요. 이때 이 분포로부터 확률 변수 x로부터 우리가 표본 x1부터 xn까지 총 n개 데이터를 얻었다고 가정하겠습니다. 자 그때 최대 가능도 추정법을 이용해서 우리가 모수를 한번 추정하겠습니다. 자 앞에서와 마찬가지로 우리가 라이클리우드 함수를 한번 구해봐야 되는데요. 카테고리 분포의 라이클리우드 함수는 우리가 어떻게 상상해 볼 수가 있냐면은 이 카테고리 분포의 모수에 해당하는 p1 그리고 PD에 대해서 우리가 각각의 표본에서 얻은 이 x1과 xn에 해당하는 표본들에서 얻게 되는 케이스들을 우리가 나눠볼 수가 있는데 가령 데이터 x1에서 얻은 케이스가 14부터 d까지 가질 수 있는 모든 어 케이스 중에서 가령 뭐 케 값의 값을 가졌고 나머지가 아니다라고 전제했을 때 이 스 1의 케이 번트 값을 우리가 피케다의 지수승으로 씌워서 우리가 이 나이클리오드를 정의할 수가 있게 되는데요. 무슨 뜻이냐면 만약에 첫 번째 표본에서 전체 d개의 케이스 중에서 그 한 가지 케이스만 1이고 나머지는 다 0이 되잖아요 그러면 어 이 그 케이스에 해당하는 어 x아 값만 XI의 두 번째 값만 1이고 나머지는 다 0이니까 이 모수 PK의 xik 승은 xik가 1일 때만 PK 값이 살아남고 xik가 0일 때는 PK 값이 1로 바뀌기 때문에 곱셈에서 1을 곱하는 거는 아무런 변화를 주지 않죠. 그래서 xyk가 1인 경우에만 이 PK 값이 의미가 있는 경우로 우리가 상상해 볼 수가 있겠죠. 정리하자면 우리가 어떤 표본 스원서부터 스까지 주어져 있을 때 이 표본들이 어떤 케이스에 해당하느냐에 따라 그 케이스에 해당하는 파라미터 PK만 값이 살아남고 나머지는 이제 곱셈에서 활용되지 않는 라이클리우드를 정리해서 쓸 수가 있게 됩니다. 네 이렇게 했을 때 우리가 해당 카테고리 분포의 모수로부터 해당 데이터를 관찰하게 되는 확률을 우리가 모델링 할 수가 있게 되는 것이고요. 이거를 이제 모든 데이터에 대해서 다 곱해주는 게 라이클 리드의 정의가 되겠습니다. 자 그렇다면 이 라이클리우드에다가 로그를 채워주게 되면은 네 로그 라이클리우드를 우리가 계산할 수가 있게 되겠죠 한 가지 여러분들께서 유의하셔야 될 점은 카테고리의 분포의 모수는 정규 분포의 모수랑 달리 다 더했을 때 1이 돼야 된다라는 제약 조건이 있습니다. 그래서 이 제약식 하에서 카테고리 분포의 맥시멈 라이크 리어드 에스티메이션을 수행해야 됩니다. 자 그래서 라이클리우드가 이렇게 두 개의 곱셈으로 주어져 있을 때 피케의 스아케 승을 아이와 케이의 두 인덱스에 대해서 곱을 취해 줬을 때 이거를 로그로 취해 주게 되면요. 모든 거에 대한 곱셈을 다 덧셈으로 바꿔줄 수가 있으니까 케이와 아이에 대한 덧셈으로 다 바꿔줄 수가 있게 되겠죠. 자 이때 PK의 xik 승을 로그로 씌워 주게 되면은 xik 승은 로그의 바깥에 곱해져서 로그 PK에다가 xik를 곱해주는 형태로 표현이 되게 됩니다. 이때 로그 PK에다가 xik를 우리가 곱해주는 형태가 되는데요. 이때 덧셈의 순서를 아이랑 케의 순서를 바꿔주게 되면은 xik를 아에 대한 인덱스에 대한 덧셈을 먼저 수행해 주게 됩니다. 이 덧셈의 의미는 무엇이냐 실제 우리가 얻은 데이터로서부터 이 k 번째 경우에 해당하는 값이 1인 모든 케이스들을 관찰하게 되는 것이니까 어 우리가 가지고 있는 전체 데이터 중에서 어 케이스가 케이 번째에 해당하는 데이터가 얼마큼 있는지를 카운팅하게 되는 것이죠. 그 숫자를 우리가 NK라고 하겠습니다. 즉 XI 콤마 케를 아는 14부터 n까지 다 더해주게 되면은 케 번째 경우의 수를 가지게 되는 데이터의 숫자가 되는 것이죠. 이게 엔케로 우리가 쓰겠습니다. 그러면은 이 로그 라이클리우드의 수식은 간단하게 정리해서 케이는 14부터 d까지 즉 모든 케이스에 대해서 로그 피케에다가 이 케이 번째 경우가 발생할 데이터의 숫자 엔케를 곱해줘서 우리가 로그 라이클 리드를 얻게 되는 것이죠. 이때 제약식은 피케를 전부 다 더해줬을 때 1이 돼야 된다는 제약식에 만족해야 됩니다. 이렇게 제약식이 있는 경우에 최적화를 수행할 때는요. 예전에 저희가 배웠던 라그랑주 승수법을 이용해서 제약식이 있는 최적화 문제를 풀 수 있다고 배웠습니다. 어 저희가 라그랑주 승수법을 배우지 않았지만 그냥 이런 경우에 라그랑주 승수법을 쓸 수 있다까지만 여러분들이 기억하시고요. 어 이 라그랑즈 승수법을 이용해서 위에 제약식이 있는 최적화식을 한번 풀어보시게 되면은 다음과 같이 라그랑지 승수법 목적식을 우리가 얻게 됩니다. 이 람다 같은 경우에는 라그랑지 승수법에 해당하는 개수 람다라고 보시면 되고요. 승수라고도 부릅니다. 이때 이 라그랑지 승수법으로 얻게 되는 목적식에서 우리가 각각의 PK에 대한 미분 즉 각각의 페라미터에 대한 미분을 취해줬을 때 그 미분 값이 0이 되는 PK를 구해주면 우리가 최적 즉 맥시멈 라이크드 에스티메이션에 해당하는 최적 페라미터를 구할 수가 있게 됩니다. 앞에서 정규 분포의 경우에는 우리가 어 제약식이 없었기 때문에 이 경우에는 미분했을 때 그냥 0이 되는 값만 구해주면 되는데요. 지금 카테고리컬 분포의 경우에는 PK를 다 더해줬을 때 1이 돼야 된다라는 제약식이 있기 때문에 그 제약식을 반영한 부분이 같이 포함돼서 목적식에 들어가야 됩니다. 따라서 이 목적식에 들어간 부분에 해당하는 승수 라그란운치 승수 람다에 대해서도 미분을 취해줘야 되는데 이 두 개를 결합해서 최적화를 수행해 주고 결론을 내면은 이 케 번째 인덱스 케이스를 발견하게 될 데이터의 숫자를 케 번째 케이스에 해당하는 보수 PK로 나눠준 거에다가 빼기 람다가 0이 만족해야 되는 것이고 그리고 이 람다에 대해서 미분해 취한 거를 한번 관찰해 보시면 1 빼기 케는 14부터 디까지 피케는 0이다라는 식이 나오게 되시죠. 자 근데 이 수식은 사실 가만히 살펴보시면 원래 이 제약식에서의 조건과 차이가 없다라는 거를 볼 수가 있게 됩니다. 자 그러면은 이 부분만 여러분이 풀어내시면 되는데요. 자 그러면은 람다 대신에 여러분이 피케 분의 엔케로 여러분이 정리해서 쓸 수가 있게 되는 것이죠. 네 그래서 이렇게 정리해 주게 되면은 라그란지 승수법으로 얻게 되는 결론이 이렇게 두 가지 수식으로 얻게 됩니다. 자 이거를 우리 여러분들이 활용하실 수가 있는데요. 자 그래서 우리가 이 두 개의 수식을 활용해서 여러분이 얻게 되는 수식이 바로 뭐가 되냐면은 이 피케는 엔케 케이스에 대해서 키는 14부터 디까지 모두 다 더해줬을 때 이 전체 케이스에 대해서 우리는 전체 데이터의 숫자가 되는 것이고 그리고 전체 데이터 중에서 케 번째의 경우가 발생했을 때의 데이터 즉 케 번째 경우에 발생한 데이터의 숫자 NK로 나눠주는 게 바로 이 PK에 대한 최적 페라미터의 모수 값이 되는 것이죠. 즉 카테고리컬 분포에서 주어진 데이터에 대해 가장 최대 가능도를 만족하는 모수 추정 그건 바로 뭐냐면은 주어진 데이터 전체 n개 중에서 각각 케이스가 관찰되는 경우가 얼마나 되는지 네 생각해 보면 되게 직관적으로는 당연한 해석이죠. 해당 케이스가 관찰된 빈도만큼 그 빈도에 해당하는 확률을 모수로 쓰면은 가장 적절한 최대 가능도 추정이 된다. 즉 다시 말해 카테고리 분포의 최대 가능도 추정은 경우의 수를 세워서 그 비율을 쓰면 된다라는 결론을 얻게 됩니다. 사실 직관적으로는 굉장히 자명한 결론은 같지만 우리가 이렇게 통계학적 원리로 살펴봤을 때 이렇게 추정하는 것이 데이터에 가장 적합한 추정이구나라는 어떤 근거를 얻게 되는 것이죠. 이와 같이 정규 분포 카테고리 분포 이렇게 어떤 분포를 우리가 미리 상정했을 때 최대 가온도 추정법에 근거한 모수를 추정하는 방법을 우리가 배우게 되었습니다."
}