{
  "lecture_name": "NLP Recent Trends Part 5_LLM Trends",
  "source_file": "NLP Recent Trends Part 5_LLM Trends_82.mp4_2025-12-04-104107885.json",
  "text": "앞선 강의에서 LLM들의 응용하고 활용법 그리고 한계에 대해서 배워보았는데요. 이제 이번 강의에서는 LLM의 현재 시장 현황하고 그리고 상용 프로그램을 그리고 그런 것들이 어떻게 세상에서 나오고 있는지에 대해서 좀 공유하는 시간에 대해서 얘기할까 생각이 듭니다. 우선 그래서 이 강의는 세 가지로 구성돼 있는데요. 첫 번째는 LLM 시장 자체의 현황하고 특성에 대해서 좀 얘기를 한 다음에 그리고 크게 두 가지로 나눠 가지고 이 시장의 현황을 좀 확인해 보려고 합니다. 첫 번째는 엘엠 프로바이더 측 그리고 두 번째는 엘엠 애플리케이션 측 해가지고 어 뭔가 LLM을 제공하는 자하고 그리고 우리가 활용하는 자 가지고 그 두 개로 나눠 가지고 이 두 개가 어떤 애플리케이션들이 존재하고 어떻게 상용화되었는지 공유하는 시간을 가지겠습니다. 그래서 뒤에 있는 얘기를 먼저 하기 전에 우선 이 현재 LLM 시장 자체를 분석하고 그 특성을 확인해 보도록 해봅시다. 우선 AI 시장 규모 자체는 지속적으로 성장할 것으로 예상이 됩니다. 특히 지금 보시면 알겠지만 AI 시장은 이렇게 지금 달러 자체가 빌리언 달러 해가지고 10억 달러인데 2천억 달러에서 지금 한 7천억 달러까 2030년까지 계속 증가할 것으로 예상되면서 그리고 또 최근에는 핫한 생성형 AI 그러니까 뭔가 이미지 생성형이든 텍스트 생성형이든 이러한 어 이러한 생성 AI 시장 역시 AI 시장이 발전함에 따라 같이 지속적으로 증가할 것으로 예상이 되고 있는 상태입니다. 현재 현황 자체를 확인해 보자면 사실 이 텍스트 생성형 AI 중에서 금액적인 비율이 아니고 그냥 사용자 비율만 따져봤을 때 거의 한 73%가 이 챗챗피티를 활용하고 있습니다. 정확히 말하면 그러니까 100% 이 AI 시장 텍스트 산성용 AI 시장을 활용한 유저 수가 한 100명 정도 있다면 그중 한 73명 정도는 TPPT를 쓰고 있고 16%는 디플 가지고 아마 이거 아시는 분들도 있을 건데 번역기 관련 그런 생성형 AI죠 번역기 아니면 구글 바드 해가지고 이러한 뭔가 유저들의 측면으로 봤을 때는 지금은 아직 챗cpt가 즉 독점 시장이다라는 것을 좀 분석을 해 볼 수가 있습니다. 이 LLM 시장 같은 경우에는 어 특히 이 엘엠의 사전 학습의 특성 때문에 몇 가지 다른 시장에서 보이지 않는 특징들이 몇 가지가 있는데요. 첫 번째는 이 엘엘램 사전 학습 자체는 되게 많은 비용을 요구를 합니다. 어 뒤에 나오는 예시 같은 경우에는 우선 라마 65b 기준이고요. 그래서 실제로 라마 테이블 라마 원의 테이블 페이퍼를 확인해 보시면 이런 테이블이 존재하는데 라마 65b 같은 경우에는 지금 보시면 알겠지만 GPU a100이라는 GPU로 학습이 되었고 그리고 학습 시간 자체가 거의 한 100만 시간 학습이 되었다는 것을 확인해 볼 수 있습니다. 정확히는 abc GPU 80gb짜리 abc GPU 2048대 가지고 이 라마 65비가 학습되었는데요. 이 에이백 지피 같은 경우에는 이 80기가바이트의 정가 자체는 이거 한 대당 거의 한 1만 달러 그러니까 한 1300만 원 정도 합니다. 그러나 이 생성 AI가 지금 세계 핫하고 이 GPU 가격이 계속 되게 높게 뛰면서 사실상 이 a10 GPU는 구하기도 어렵고요. 가득 기업이 되게 폭등하는 중이라서 되게 비싸고요. 어 그렇다면 이것을 에백을 직접 사지 않고 뭔가 대여하는 뭔가 클라우드 시스템을 생각해 볼 수 있는데 AWS BWB 이거 같은 경우에는 에백 지pu가 8개 달린 클라우드 인스턴스거든요. 시간당 요금이 온디맨드가 32달러 그러니까 우리나라 돈으로 대략 한 5만 원 4 5만 원 정도 하고 스팟이 10달러 해서 우리나라 돈으로 한 만 4천 원 정도 가격을 합니다. 어 그러다 보니까 이것 자체가 지금 보시면 온디맨드하고 스팟이 있는데 참고로 온디맨드 같은 경우에는 좀 더 안정적인 뭔가 여러분들이 대여했을 때 따로 뭔가 AWS 안에서 GPU 자체가 부족하더라도 뭔가 꺼지지 않는 안정적인 서버라고 생각하시면 좋을 것 같고 스폿 같은 경우에는 여러분들이 단기 대여하는 금액이라고 생각하시면 좋을 것 같아요. 여하튼 어느 걸 쓰든 간에 이게 시간당 10달러라는 것은 꽤 비쌉니다. 어 그리고 그 라마 65비 같은 경우에 결국에는 이 표에 보시는 것처럼 거의 한 100만 시간의 지pu를 활용하였는데요. 이거 같은 경우에는 결국에는 21일 동안 1.4 트릴리온 토큰을 처리하였습니다. 그러다 보니까 저희가 만약 이것은 이러한 뭔가 이 논문에 주어진 수치 가지고 저희가 학습 비용 자체를 계산해 볼 수가 있는데요. 학습 비용 같은 경우에는 AWS p4d 스팟 요금에 가서 80대 100만 시간이니까 한 대 했을 때 100만 시간 걸린다는 소리거든요. 그러니까 8대 하게 된다면 이번에 8분의 1 시간 그러니까 한 12.5시간 2만 시간 정도가 걸리겠죠. 그래서 이 12.5만 시간만큼 만약 스팟으로 불린다 할 경우에는 거의 가격이 한 18억 원 136만 달러가 나오게 됩니다. 되게 아주 비싼 금액이죠. 우리가 a1 GPU를 직접 구입 직접 구입해 가지고 이것을 그냥 단순히 돌린다 하더라도 저희가 그 한국전력에 산업용 전기 요금을 합해 가지고 측정해 보면 저희가 실제로 이거 파워 지피유 파워 컨셉션하고 지피 아웃을 계산하게 된다면 이 토탈 파워 컨셉션 기반으로 저희가 실제로 전기 요금을 계산할 수가 있거든요. 이거 계산해 봐도 거의 한 1억 원 가까이 금액이 나옵니다. 물론 이 1억 원 금액은 이 지피유 비용하고 서버 유지비가 별도죠. 즉 이러한 LLM 자체를 학습시키는 것은 되게 비용이 큰 일이고 이런 뭔가 소규모 기업이라든가 벤처에서는 되게 하기 어렵습니다. 그럼에도 불구하고 이 LLM의 계산량 자체는 계속 지속적으로 증가해 왔어요. 그래서 지금 보시면 이쪽이 거의 어 와 축이 커지면 커질수록 지금 익스포넨셜 로그 스케일로 지금 한 칸 더 올라갈 때마다 10배씩 커진다는 걸 확인해 볼 수 있는데 이렇게 시간에 따라 이것이 익스포넨셜하게 계속 증가해 왔다는 것을 확인해 볼 수 있습니다. 그래서 최근에 예전에 나왔던 트랜스포머 같은 경우에는 이 플러스 계산량 자체가 그렇게 크지 않았는데 이것이 익스포넨셜리하게 증가하다 보니까 gpt4에 와서는 거의 한 이제는 일반 기업에서는 뭔가 도전해 볼 수 없을 정도의 그 정도의 계산량을 요구한다는 것을 확인해 볼 수가 있죠. 이런 LLM이 단순히 뭔가 금액의 문제가 아니고 단순히 이것을 학습시키는 것 자체가 되게 노하우가 많이 요구하는 일입니다. 그래서 뭔가 챗gpt 같은 고성능 LLM의 학습은 대기업도 어렵습니다. 그래서 사실상 제가 보기에는 오픈ai의 챗gpt의 성능에 도달한 기업은 아직 없다라고 생각해요. 유일하게 구글에 있는 제미나잇 울트라가 뭐 지피티4하고 유사한 성능을 가진다고 알려져 있긴 한데 이것 자체가 공개적으로 검증되어 있진 않고요. 뭐 실제로 제미니아의 프로 같은 경우에는 챗지피티 3.59보다 뭔가 성능이 좀 떨어진다 그런 얘기도 있기는 합니다. 그리고 무엇보다도 뭔가 이러한 추론 가성비 측면 그러니까 내가 이 토큰을 넣었을 때 어 뭔가 생성할 때 이것의 가성비 측면에서는 사실상 아직 챗지피티가 비용 대비 성능이 매우 압도적이라고 저는 개인적으로 생각합니다. 그리고 이 LLM과 같은 거대 모델은 학습할 때 노하우가 되게 중요하다고 내가 앞서 말씀드렸었는데 노하우라는 게 무엇이냐 한다면 뭔가 하드웨어 혹은 소프트웨어 오머 테크닉 예를 들어 뭔가 GPU 클러스터 제가 말씀드렸었죠 앞쪽에서 나왔던 라마 65b 같은 경우에는 a b GPU가 a1 GPU 2048대 이렇게 학습시켰는데 이거 같은 경우에는 당연히 GPU가 한 컴퓨터에는 GPU가 8대밖에 안 뽑히기 때문에 멀티 노드 트레이닝이 필요하고 이러한 지표유 클러스트 혹은 멀티 노드 학습 방법 자체가 되게 그 이것을 뭔가 서버가 꺼졌을 때 이것을 어떻게 대처할까 이런 것들을 어떻게 다룰까도 되게 중요한 문제이기 때문에 이런 것은 뭐 노하우가 없다면 되게 다가가기 어려운 그런 문제라고 생각하실 수가 있습니다. 뭐 단순히 이러한 소프트웨어적인 하드웨어적인 문제 외에도 머신러닝적인 뭔가 테크닉도 존재하는데요. 예를 들어 사전 학습 데이터를 어떻게 필터링 할까 실제로 여러분들이 단순히 크롤링 인터넷에서 데이터를 잔뜩 크롤링해서 학습시키면 LLM 성능이 그렇게 좋지가 않습니다. 실제로는 이 잔뜩 크롤링 된 클레인 텍스트에서 좋은 데이터를 줄여가지고 물론 그 좋은 데이터도 많아야 되지만 줄여가지고 학습시키는 것이 일반적으로 LLM 성능이 더 높아요. 하지만 이러한 필터링을 어떻게 할까도 되게 중요한 노하우 중 하나이죠. 그리고 커리큘럼 러닝 해가지고 결국에는 이 LLM 자체가 사실상 거의 1회 폭 수준에서 그냥 데이터가 있을 때 1회 단 한 번만 보고 학습시키는데 그렇다면 이것을 1회폭 돌리는 동안 어떤 데이터 셋을 어떤 텍스트를 먼저 학습시킬까 이런 것을 뭔가 커리큘럼 러닝이라고 합니다. 예를 들어 뭔가 쉬운 데이터에서 쉬운 텍스트에서 어려운 텍스트 순으로 학습시킨다든가 뭔가 좀 더 플레이한 텍스트에서 뭔가 논문스러운 그런 텍스트로 학습시킨다든가 뭐 그런 것도 일종의 커리큘럼 러닝이라고 생각할 수 있겠죠. 그리고 이러한 방법론들 뭔가 이런 필터링을 어떻게 할까 커리큘럼은 어떻게 할까 이런 것들도 다 노하우인 부분이 있고 사실상 이것은 좀 기밀의 영역에 가깝습니다. 사실상 이거 저희가 이것을 그래서 뭔가 프라이빗 레시피 해가지고 뭔가 개인 레시피 같은 형태로 기업마다 가지고 있는 레시피가 있고 그리고 이것들 자체가 사실 공유가 되고 있지 않아가지고 이것들이 단순히 챗gpd가 그렇게 데이터를 많이 넣고 모델이 커져서 잘 됐다라고 치부하기에는 오픈 AI 내부에 쌓인 노하우라든가 그런 것 때문에 우리가 사실 대기업이라든가 미국에 있는 거대 기업인 아마존이라든가 구글 이런 기업에서도 되게 따라가기 힘든 실정입니다. 그러다 보니까 되게 여러 대기업에서 돈과 GPU를 되게 어마어마하게 투입했지만 사실 좋은 결과물로 이어진 것은 그렇게 많지가 않아요. 물론 구글은 거의 거의 비슷하다라고 알려져 있긴 한데 그거 외에 기업 중에서는 사실 AI 중에서 유명한 기업이라고 생각한다면 사실 여러분들은 미국에서 있는 GPT가 대단하니까 미국에 있는 대 기업들은 아 이런 것을 다 하겠구나 생각하지만 사실은 챗gpt만큼 오픈 AI만큼의 뭔가 성능을 따라가고 있는 기업은 사실 미국 내에서도 거의 한정적이라고 저는 개인적으로 생각합니다. 제 개인적인 사견으로는 사실 지금 현재 대기업들 그러니까 한국 기업이든 혹은 현재 기업들 대기업뿐만 아니고 뭔가 소규모 기업 벤처 기업들은 지금 사실 LLM 자체 개발의 늪에 빠져 있다라는 개인적인 생각이 좀 드는 편입니다. 이게 그럴 수밖에 없는 것이 사실 챗지피티를 활용해가지고 애플리케이션 개발한다는 것 자체가 되게 이것이 벽이 되게 높아요. 심리적 벽이 되게 높습니다. 이것이 우선 첫 번째로 고객 정보 보안 문제가 존재하고요. 만약 저희가 챗tpt로 뭔가 애플리케이션을 제작한다 하게 된다면 저희가 결국에는 유저의 사용자의 정보라든가 그런 것을 오픈 AI 쪽에 건네주게 되니까 이것이 문제가 생길 수가 있고 또한 국내 GPT가 아니죠 국내 모델이 아니죠. 결국에는 외산 LLM 자체를 활용해 가지고 뭔가 이것을 응용 프로그램 만든다는 것 자체 때문에 사실 자체 LLM 개발하는 경우가 되게 많아요. 하지만 이렇게 자체 LLM을 개발하더라도 사실 돈은 돈대로 나가면서 실제로는 LLM 성능이 앞에 나왔던 뭔가 노하우 문제라든가 이런 GPU 클러스터 문제 때문에 잘 안 나오는 경우가 되게 많기 때문에 되게 어려움에 부닥치는 경우가 많습니다. 물론 제2 견해는 그럼에도 물론 도전해야 합니다. 물론 대기업이라든가 자체 LLM 자체는 되게 아주 중요한 LLM 자체에 뭔가 기반 기술이고 그런 것을 우리나라가 분명 확보해야 된다는 것은 저도 매우 동의하는 바입니다. 하지만 그러다 보니까 이런 자체 LNG의 개발 자체가 매우 미뤄지게 되고 이거에 기반한 뭔가 응용 프로그램 개발 자체가 늦어지다 보니까 뭔가 이거 기존 다른 업체 가 외국 업체에서는 이미 챗gpt 기반으로 애플리케이션 만들고 그런 모델에 비하면 응용 프로그램적인 성능이 되게 낮은 문제가 발생하고 자체 LLM 기반으로 뭔가 하려다 보니까 되게 애플리케이션 응용 분야 자체가 조금 발전이 늦어지는 게 아닌가에 대한 개인적인 우려가 존재합니다. 그래서 저는 개인적으로 이 엘레램 애플리케이션 개발 결국 챗gpt 기반이라든가 뭔가 기존에 있는 뭔가 성능 좋은 것들 써가지고 기본적으로 나가면서 또 투트랙 전략으로 뭔가 LLM을 따로 학습시키는 기반 기술을 확보하는 그런 전략을 취하지 않을까 생각이 듭니다. 여하튼 이것은 뭔가 대기업이라든가 뭔가 국가 과제로서 뭔가 그렇게 해보면 좋을 것 같다는 의견이고 하지만 스타트업 같은 경우에는 사실 자체 LLN 자체를 뭔가 하는 만드는 것 자체가 되게 쉬운 일은 아니겠죠. 하지만 현재 대기업들은 이러한 자체 기업 자체 엘램 이런 것들에 뭔가 투자를 많이 하고 있고 그러다 보니까 오히려 엘엠 응용 애플리케이션 같은 경우에는 오히려 저의 의견으로는 스타트업에게 좀 더 기회가 되지 않을까라는 생각이 듭니다. 제가 의견 나누자면 결국에는 이 LLM을 만드는 사람하고 이 LLM을 응용하는 사람 해가지고 이렇게 두 분야로 나눠질 거라고 저는 의견이 드는데요. 그래서 결국에는 LLM 프로바이더 혹은 제공하는 사람 벤더스 같은 경우에는 LLM을 직접 제공하는 업체를 저희가 이렇게 부를 수가 있겠죠. 예를 들어 오픈 AI GPT라든가 아니면 구글의 제미나이 아니면 우리나라 같은 경우에는 네이버의 하이퍼 클로바 이런 기업들이 LLM 프로바이더로서 기업이 돌아가지 않을까 생각이 듭니다. 그래서 이거 같은 경우에는 일종의 인프라 기업이자 뭔가 플랫폼 기업이 되지 않을까 생각이 들고요. 결국에는 그 LNN 프로바이더들이 직접 LNN 애플리케이션을 제작할 수는 있겠지만 어 그러지 않고 오픈 오픈 에에서 직접 응용 프로그램을 만들기보다는 뭔가 단순히 인터넷 선을 깔듯이 그냥 엘렌만 제공해 주고 그리고 그거에 대한 사용 비만 걷는 형태로 아마 사업이 이루어지지 않을까 생각이 듭니다. 그래서 뭔가 저희가 굳이 인터넷 서비스할 때 네이버 같은 것을 직접 통신사에서 운영할 필요가 없고 네이버가 운영하는 뭔가 통신비 같은 거를 그러니까 통신사가 선만 깔아주고 그거에 대한 비용을 챙기는 것과 유사하다라고 생각할 수가 있죠. 그에 비해서 LLM의 애플리케이션 그러니까 응용 뭔가 프로그램을 만드는 업체 같은 경우에는 LLM을 직접 만들 필요는 없다라고 개인적으로 생각합니다. 실제로 챗지피티라든가 제미나이 같은 것이 API 형태로 잘 제공이 되고 있고요. 그리고 그러한 응용 프로그램 만드는 거는 단순히 생성만으로도 충분히 만들 수가 있기 때문에 저희가 굳이 LLM 자체를 자체 LLM을 확보하지 않고도 충분히 응용 프로그램을 만들 수 있다라고 저는 개인적으로 생각합니다. 네이버와 같은 포털 사이트를 운영하는데 내가 직접 인터넷 선을 깔 필요는 없다는 것이죠. 그러다 보니까 이 애플리케이션 만드는 쪽에서는 당연히 여러 LLM 프로바이더 구글 챗 PPT라든가 혹은 제미나이라든가 그런 LLM 중에서 더 좋은 성능 더 좋은 정확도 더 좋은 가격 해가지고 그러한 고품질 가성비의 LNM을 제공하는 업체를 선택하는 형태로 갈 거라고 저는 생각을 하고요. 반대로 LNM 프로바이더 같은 경우에는 LNM 프로바이더끼리 경쟁을 해가지고 뭔가 이것을 제공할 수 있는 그러한 서비스 형태가 제공되지 않을까 생각을 하고 있습니다. 다만 지금은 어마어마한 학습 비용과 앞서 말했던 뭔가 노하우 그런 문제들 때문에 지금 한동안은 독과점 시장이 되지 않을까 생각 들어요. 사실 요즘에 나오는 LLM 애플리케이션 응용 프로그램들은 다 대부분 다 채집피티 기반이라고 생각합니다. 하지만 이 LLM 학습 방법의 발전으로 저는 이 시장 자체가 되게 자유 시장으로 점차 변환이 될 거라고 기대를 하고요. 실제로 나중에 소개드릴 뭔가 랭체인이라든가 그런 거 같은 경우에는 이 LLM 자체는 그냥 아주 플렉시블하게 되게 원하는 LLM을 그냥 한 번만 그냥 코드만 한 줄 바꾸면 바꿀 수 있게끔 설계되어 있어 가지고 뭔가 이 선택 자체가 되게 쉬워질 것이고 성능 자체도 결국에는 이게 LLM 학습 방법이 발전하다 보면 어느 정도 비교가 가능한 그러한 모델도 나타날 것이고 저희 같은 뭔가 응용 프로그램을 만드는 입장에서는 저희가 그러한 선택권이 넓어지는 효과가 나지 않을까 생각이 듭니다. 그리고 결국에는 이렇게 재gpt라든가 뭔가 재미나 같은 것이 발전해서 저희가 그런 엘엘엠을 활용하고 그리고 그 엘엘엠 기반으로 애플리케이션을 만들어서 저희가 사용자에게 엘엘엠을 이용한 서비스를 제공해서 저희가 사용자에게 비투시를 하는 거라고 생각하시면 좋을 것 같네요. 이 경우에는 결국에는 이 LLM 애플리케이션 같은 경우에는 당연히 LLM을 직접 만드는 건 아니기 때문에 스타트업의 접근성이 되게 좋습니다. 이게 그럴 수밖에 없는 것이 LLM의 응용 방안 자체는 되게 무궁무진합니다. 앞서 나왔던 리즈닝과 뭔가 플래닝 이런 것이라든가 뭔가 에이전트 이런 걸 활용하게 된다면 로봇이라든가 뭔가 되게 다양한 작업들을 되게 자동화를 할 수 있고 하지만 이런 수요 같은 경우에 되게 다양하고 되게 곳곳에서 발생하기 때문에 각 분야에 빠르게 대응하고 적용하기에는 오히려 대기업보다는 스타트업이 좀 더 최적화가 되지 않을까 생각이 듭니다. 어 물론 엘레엠 스타트업에서 엘레엠을 안 만드는 건 아닙니다. 하지만 일반적으로 스타트업이 엘엘램 제작에 뛰어드는 경우에는 다소 작은 LLM 해서 이것을 에엘엠이라고 부르는데 어 스몰 LLM 아마 들어보신 분이 있을지도 모르겠네요. 그래서 이 SLM 제작을 많이 만듭니다. 그래서 SLM 같은 경우에는 일반적으로 매개 변수 수가 7빌리언에서 70빌리언 사이 모델을 지칭하는데요. 7빌리언 같은 경우에는 일반적으로 LLM이 이멀전트 어빌리티가 발생하는 지점이라고 알려지고 있고 그리고 70 빌리언 같은 경우에는 뭔가 틴틸라라든가 라마 그런 모델에서 이 정도 70b 모델이 가장 큰 모델이고 일반적으로 이 정도 모델이 GPT 기존에 있던 gpt3 하고 유사한 성능을 낸다라고 알려져 있습니다. 하지만 이 SLM 같은 경우에는 아 물론 이것 자체로 뭔가 다양한 애플리케이션을 만들려는 뭔가 노력들이 있긴 하지만 제가 에스엘엠을 좀 써봤을 때는 사실 이것 자체는 뭔가 엘엘엠을 대체하는 용도라고 생각이 들진 않고요. 뭔가 LLM의 보완재의 역할을 하지 않을까 생각이 듭니다. 왜냐하면 LLM 같은 경우에는 앞서 나왔던 것처럼 앞서 뭔가 리즈닝과 플래닝 에이전트에서 나왔던 것처럼 되게 제너럴한 테스크 그러니까 일반적인 테스크라도 되게 잘 풀 수 있지만 sLLM 같은 경우에는 이런 제너럴 테스크를 풀기 위해서는 그러한 성능이 좀 많이 낮은 편입니다. 그렇기 때문에 일반적으로 특정한 테스크 특정한 뭔가 스페스픽한 테스크를 뭔가 학습시켜가지고 그 타겟팅된 텍스트만 푼다든가 그런 식으로 많이 활용하기 때문에 뭔가 제너럴한 목적으로서 뭔가 엘엠을 대처할 수는 없다라고 저는 개인적으로 생각하고 LM 자체가 뭔가 대게 특정한 테스크를 풀기 위해서 엘엠을 쓰기보다는 좀 더 가성비 있게 활용하기 위해서 뭔가 파인튜닝해서 쓰는 용도로서 스엘엠을 많이 활용한다 정도로 이해하시면 좋을 것 같습니다. 네 그래서 시장 자체가 이제 엘앤 프로바이더하고 엘엔 애플리케이션으로 나눌 것이라고 제가 예측을 해보았는데 이제 그 엘앤 프로바이더 측에 대해서 좀 알아보도록 하겠습니다. 여러분들이 만약 LLM을 활용한다면 사실 활용할 수 있는 옵션이 두 가지가 있을 겁니다. 첫 번째는 사유 LLM 영어로 하면 propria tere LLM이라고 부르는데 어 이거 같은 경우에는 여러분들이 알고 있는 그냥 챗지피티 같은 거 생각하면 어떨까 싶네요. 그래서 가중치가 공개되어 있지 않고 뭔가 API 형태로 사용하는 서비스라고 생각해 보시면 좋을 것 같습니다. 그래서 이 경우에는 그 사용량에 따라 요금이 부과되고 실제로 여러분들이 어 챗지피티 API 같은 걸 써가지고 하게 된다면 이거 토큰 단위로 가격이 나오죠. 그래서 사용량에 따라서 금액이 나오는 것을 확인해 볼 수가 있을 겁니다. 어 다만 이거는 뭔가 정해진 형태로만 사용이 가능하고 그래서 뭔가 채팅 형태라든가 아니면 뭔가 컴플리션 형태로 뭔가 사용이 가능하고 대표적인 서비스로는 뭔가 오픈 AI의 챗gpt 구글 제미나이 이런 것이 있을 겁니다. 어 이거에 뭔가 대척점으로서 그런 뭔가 사유 엘엠 그러니까 공개되지 않은 모델 외에서 저희 여러분들이 만약 모델 자체를 뭔가 활용하고 다운 받아 가지고 실제로 엘엘램을 돌리고 싶다고 한다면 그 옵션이 바로 이 공유 엘램 오픈 웨이트 엘엠이 될 겁니다. 그래서 이거 같은 경우에는 가중치가 공개되어 있는 모델이라고 생각하시면 좋을 것 같아요. 다만 이게 오픈 소스 엘엠이랑은 조금 다를 수가 있는데 이 가중치 공개와 별도로 라이선스에 따라 뭔가 상용으로 사용이 불가능하다든가 그런 옵션이 달려 있을 수가 있습니다. 이것을 완전한 오픈 소스라고 부르기에는 좀 제약이 있고 정확하게 부른다면 오픈 웨이트 아니면 그냥 공유 LLM 정도로 부르면 어떨까 싶습니다. 이거 같은 경우에는 결국에는 가중치가 공개되어 있기 때문에 가중치하고 이걸 돌리는 코드 같은 것도 다 공개가 돼 있어 가지고 여러분들이 원하는 대로 커스터마이징을 할 수가 있습니다. 뭔가 파인튜닝 한다든가 뭔가 어댑터를 단다든가 뭔가 로라를 단다든가 그런 식으로 활용할 수가 있는 것이죠. 그에 비해서 4ullm 같은 경우에는 파인 튜닝 하려면 정해진 형태로 파인 튜닝 해야 된다든가 파인 튜닝이 불가능하진 않습니다. 실제로 챗치피티 오픈 API 적으로 파인튜닝을 지원합니다. 하지만 거기다 뭔가 어댑터를 딴다든가 뭔가 특정한 파인튜닝 방법론을 적용한다든가에서는 뭔가 한계가 존재하죠. 그래서 이 공유 LM 같은 경우에는 여러 가지 되게 오픈된 모델들이 존재하는데 대표적으로 지적하자면 뭔가 메탈라마 이스트라 아니면 업스테이지 솔라 이런 것들을 이제 생각할 수 있지 않을까 생각이 듭니다. 그래서 하나하나 좀 얘기하기 전에 이것을 다시 표로 정리해 보자면 결국에는 저희가 사용 LLM을 활용할 수 있는 옵션이 몇 가지가 있을 건데 첫 번째는 사유 LLM 그리고 공유 LLM이 존재할 겁니다. 다만 공유 LLM 같은 경우에는 돌리는 방법이 두 가지가 있을 거예요. 첫 번째는 GPU를 직접 구축하는 방법 그러니까 서버를 직접 사는 방법이죠. 이러한 방법론이 하나 있을 거고요. 두 번째는 AWS 같은 걸 뭔가 서버를 대여해 가지고 거기서 LLM을 돌리는 방법이 하나가 될 겁니다. 그리고 사유 LLM 같은 경우에는 애초에 오픈 AI 서버에서 돌리는 거기 때문에 그것도 일종의 클라우드 서비스라고 생각해 볼 수가 있겠죠. 이 경우에는 설비를 생각해 본다면 여러분들이 GPU 구축하고 GPU를 금액을 금액을 사야 된다면 당연히 이것이 자체 설비 구축하고 관리가 필요하기 때문에 되게 설비 금액이 되게 커지게 됩니다. 그에 비해서 저희가 클라우드 서비스를 이용하게 된다면 이 설비 구축 및 관리가 최소화 될 겁니다. 따로 서버에 대한 걱정을 안 해도 되는 것이죠. 그러다 보니까 초기 비용 자체는 저희가 공유 엘램을 뭔가 자체 구축 그러니까 지표 구축을 하겠다 할 경우에는 지표 서버를 많이 구매해야 되기 때문에 대개 초기 비용은 이 공유 엘램을 쓰는 것이 비쌉니다. 그리고 초기 비용은 당연히 클라우드 서비스로 해가지고 필요한 만큼 대여해서 쓰는 것이 당연히 적겠죠. 하지만 이 초기 비용과 생각해봐야 될 점이 바로 이 유지 비용 부분인데 어 가만히 생각하면 당연히 이 공유 LLM이 당연히 유지 비용도 싸겠다라고 생각이 들 수 있겠지만 사실 또 그렇지도 않습니다. 가장 큰 이유는 우선 여러분들이 직접 GPU를 관리하고 구매하기 때문에 그 결과 이 설비 관리 인력의 급여가 좀 필요하다라는 점이 존재하고요. 물론 이걸 돌리는 데에 대해서는 되게 금액이 쌀 겁니다. 그리고 만약 사유에 같은 경우 그러니까 챗지피티 같은 경우에는 아 이것을 유지하는 데는 되게 비싸지 않을까라는 의문이 들 수가 있는데요. 이거 직접 돌려보면 알겠지만 이게 그렇게 비싸지는 않아요. 그리고 무엇보다도 이것이 요청하는 만큼 금액이 청구되다 보니까 만약 사용량이 적다 사용량이 그렇게 많지 않다. 하루에 한 1k 리퀘스트 그러니까 리퀘스트 요청 자체가 한 천 개 미만이다 할 경우에는 오히려 GPU를 직접 사서 돌리는 것보다 그냥 요청하는 만큼 금액을 청구해서 이게 그렇게 비싸지 않다 보니까 거의 금액 자체가 1k 토큰 생성하는데 거의 한 1원 그러니까 0.001달러 그 정도 들거든요. 그러다 보니까 금액 자체가 그렇게 비싸진 않습니다. 물론 이걸로 한 밀리언 토큰을 뽑아낸다든가 아주 많은 토큰을 뽑아내게 된다면 금액 자체가 당연히 공유 ln보다 비싸겠지만 우선 기본적으로 뭔가 소량으로 뭔가 생성한다든가 그렇게 뭔가 많이 생성하지 않을 경우에는 어 사유에 이 사유 엘램 써가지고 그냥 챗치피티에 쓰는 것이 더 쌀 수가 있다라는 것을 좀 생각해 보면 좋을 것 같아요. 자 그렇다면 그 사이에 있는 클라우드를 써 가지고 저희가 공유 엘레램을 구축하는 것도 생각해 볼 수가 있을 건데요. 이 경우에는 저희가 서버를 대여할 때 이 대여 시간만큼 만큼 금액을 지불하죠. 실제로 저희가 에 백 지피가 달린 8대 달린 그러한 서버를 빌리면 1시간에 한 시간에 한 만 4천 원 그러니까 10달러 정도 나왔었죠 스팟으로 물론 저희가 이제 실제 서비스를 한다면 스팟보다는 온디맨드가 좀 더 알맞은 서비스 형태가 되겠지만 여태 한 10달러에서 한 30달러 정도가 될 겁니다. 이렇게 빌렸을 때 이 GPU가 자체가 저희가 만약 활용하지 않고 만약 활용률이 낮다 그러니까 지피 유틸리제이션이 낮을 경우에는 이게 되게 비싼 편에 속합니다. 저희가 만약 이것을 유틸리제이션을 최대한 뽑아내가지고 되게 요청들을 잘 효율적으로 처리해가지고 GPU를 모두 다 처리할 수가 있다면 이 경우에는 공유의 사유 ll는 뭐다 좀 더 쌀 수가 있겠지만 만약 뉴틸리제이션이 만약 50 미만으로 나온다 할 경우에는 사실상 금액이 2배 비싸지는 거랑 동일합니다. 그렇기 때문에 이 유틸리제이션을 최대한 높이는 것이 중요하고 이 GPU 자체를 최대한 활용하지 않을 경우에는 되게 비싸다 그러니까 한 10% 미만으로 활용한다. 그럴 경우에는 사실 지피유 한대만 빌리는 것이 더 유용할 것이기 때문에 이 지표 활용성 자체에 대해서 조금 더 고민해 봐야 될 것입니다. 그렇다면 이 사유 에덴 같은 경우에는 단점이 아예 없느냐라고 생각이 들 수 있는데 단점이 몇 가지가 있습니다. 첫 번째로는 파인튜닝 했을 때의 문제가 조금 있어요. 우선 만약 사유 LNM 자체 그러니까 챗치pt 자체에서도 파인튜닝을 하는 기능을 제공하긴 하거든요 근데 그거 매우 비쌉니다. 특히 이것을 활용하시라고 적혀 있는데 이거 학습하는 것도 비싸고요. 학습된 모델을 인퍼런스 하는 것도 한 10배 비싼 걸로 알고 있어요. 그래서 만약 파인트린된 모델을 활용하겠다라고 한다면 사실 4ulmm에서 뭔가 이런 것을 대용량으로 뭔가 학습시키고 뭔가 이것을 인퍼런스 하는 것을 내게 추천하지 않습니다. 그에 비해서 뭔가 공유 LLM을 쓸 경우에는 저희가 이미 그냥 파인튜닝해서 그냥 그냥 내부적으로 쓰면 되는 거기 때문에 이게 파인 튜닝 여부랑 관계없이 사실상 동등한 유지 비용이 나옵니다. 그러다 보니까 만약 여러분들이 이 모델을 뭔가 특정 테스크에 대해서 스페스픽하게 뭔가 튜닝을 해가지고 활용하겠다 할 경우에는 공유 LM을 쓰는 것이 더 금액이 적절할 수 있다라고 생각해 보시면 어떨까 싶습니다. 그리고 또 하나의 단점이 보안 문제가 존재하죠. 그래서 어 물론 그 클라우드 서비스 특성상 단순히 에블스 빌려 가지고 하는 거 그리고 오픈 AI에 쿼리를 주는 거 둘 다 우선 데이터 유출 우려가 존재하고요. 결국엔 쿼리가 존재하지 쿼리가 외부 회사로 전달되기 때문에 하지만 저희가 자체 구축한다든가 그런 경우에는 데이터 유출 우려가 좀 없는 편이기 때문에 이러한 보안 관련 우려는 좀 적은 편입니다. 다만 챗gpt 혹은 오픈 AI 쪽에서도 그걸 알고 있기 때문에 이것을 뭔가 이런 데이터 보안을 좀 더 여러분들이 기업들이 우려하는 그런 보안들을 지키기 위해서 뭔가 에이저 기반 해 가지고 기업 전용의 뭔가 챗지피티를 운영한다든가 그러한 것도 서비스를 운영하고 있으니까 그런 것들도 관심 있으시면 찾아보시면 좋을 것 같습니다. 그래서 이제 이 4ulm들의 대표적인 예시들을 확인해 봅시다. 그래서 역시 대표적인 건 첫 번째로 대라고 한다면 챗챗 PT가 되겠죠 이 gpt3 같은 경우에는 나온 지가 꽤 오래됐습니다. 거의 한 2020년 이때 나왔었는데 이 이후에 좀 더 세상에 화제가 된 것이 시작한 것이 2022년 12월 30일쯤 그 챗gpt 3.5를 발표하면서 이것을 텍스트 채팅 기반 서비스를 발표하고 이것이 되게 화제가 되면서 지금 이렇게 생성형 AI 시장이 열리게 된 것이죠. 그리고 그 뒤로 한 6개월 뒤에 이 gpt4 텍스트 기반 gpt4를 개발 발표하였고 그리고 텍스트하고 이미지를 동시에 다룰 수 있는 그러한 gpt4 터보를 2023년 11월 6일에 발표하였습니다. 그리고 이 오픈 AI 같은 경우에는 지금 협업 자체를 이 마이크로소프트랑 같이 협업을 하고 있기 때문에 이것을 애저에서 뭔가 기업용 솔루션 방금 전에 말했던 그런 보안 관련 이슈 그런 것들을 해결하기 위한 기업용 솔루션을 제공 중에 있습니다. 그래서 이 gpt4 터볼 같은 경우에는 지금 2023년 4월까지의 데이터로 학습되었고 이걸 비교해 보면 그냥 dpt4는 2021년 기준이거든요. 그래서 좀 더 최신 데이터를 학습시켰고 그리고 최대 토큰이 되게 깁니다. 이게 12만 토큰 해가지고 일반적으로 뭔가 볼트라든가 그런 것들이 한 2048 그러니까 이케까지밖에 못 들어오는데 이것은 되게 아주 긴 텍스트도 볼 수 있다라는 것을 확인해 볼 수가 있고요. 그리고 이것이 달리 3하고 연동이 돼 가지고 이거 지피티4랑 대화를 하면서 뭔가 이미지를 생성하는 것도 가능합니다. 아마도 이게 지피티4가 직접 생성하는 건 아닐 거예요. 지프티4가 현재 예전에 엘은 애플리케이션에서 설명드렸듯이 현재 상황에서 지금 달리를 호출해야 되는가를 판별하고 이제 그 상황에서 달리를 호출하면서 프롬프트를 주면 달리가 여기서 이미지를 생성해 가지고 이미지를 출력하는 그런 형태일 겁니다. 그리고 그거 외에도 뭔가 이미지 데이터를 분석한다든가 뭔가 문서 업로드 혹은 PDF 검색기라든가 다양한 기능들을 지원하고 있고요. 그래서 금액 같은 걸 보시면 알겠지만 금액이 gpt4 같은 경우에는 3.5 같은 경우에 금액이 되게 싸요. 그래서 1k 도크만 했을 때 거의 한 0.001달러 해가지고 거의 한 1원 천토큰 뽑아내는데 거의 1원 나온다는 걸 확인해 볼 수가 있고 다만 gpt4 같은 경우에는 조금 비쌉니다. 터보 기준으로 하더라도 거의 한 10배 가격이기 때문에 에 내가 현재 활용하고자 하는 서비스가 무엇인지 그리고 지금 과연 이 서비스에서 지피티4가 과연 필요한 것인가 같은 것에 대해서 좀 고민해 보시면 어떨까 생각이 듭니다. 물론 성능 자체는 해보시면 알겠지만 지피티4가 훨씬 좋습니다. 그래서 특히 gpt4 같은 경우에는 되게 거의 사람과 유사한 성능 사람과 유사한 정도의 뭔가 그런 시험 테스트 성적이 나온다는 거로 알려져 있고요. 그래서 3.5에 비해서 되게 다양한 테스크에 대해서 성능이 많이 올랐다. 다양한 데이터 챗에서 그런 것을 확인해 볼 수가 있고 그리고 gpt4가 제가 앞서 말했던 것처럼 뭔가 이미지를 이해할 수 있다고 말씀드렸었죠. 그래서 저희가 이렇게 이미지를 주고 어 이 이미지에서 무엇이 웃긴 포인트니 이제 패널 바이 패널로 설명해 줘 이렇게 들었을 때 내가 지피티 폴리 생성물을 보게 된다면 실제로 이 이미지가 아 라이트닝 케이블이 이 디서브 포트에 박혀 있는 것이 되게 웃기다 이런 포인트를 다 집어낼 수 있다는 것을 확인할 수가 있어 가지고 확실하게 이 지피티4 자체가 이미지를 그 뭔가 해석하고 그리고 뭔가 인간만이 그 이해할 것 같은 뭔가 웃음 포인트들을 잘 잡아낸다는 것을 확인해 볼 수가 있습니다. 이 오픈 AI의 경쟁자라고 한다면 역시 구글이겠죠 구글 같은 경우에는 예전에 뭔가 알파고 같은 걸로 해서 딥마인드 이러한 회사들을 합병해가지고 되게 이러한 LLM 관련 연구들도 많이 진행하고 있었는데요. 어 최근에는 좀 새지피티에 좀 많이 밀리는 것 같은 성능을 보여 주였지만요. 어 최근에 발표한 제미나이 같은 경우에는 그 구글 내부 문서에 따르면 내부 문서 혹은 뭐 발표 자료에 따르면 아 이것이 충분히 지피티 4를 이긴다라고 공개를 하였습니다. 그래서 이 제미나이 같은 경우에는 2023년 11월 7일에 공개되었고 우선 이건 앞쪽에 있던 챗gpt랑 다르게 뭔가 되게 다양한 모달리티를 지원합니다. 그래서 생성 AI 애플리케이션에 뭔가 유연성을 두고 설계했다라고 얘기를 합니다. 그래서 입력을 챗gpt 같은 경우에는 텍스트하고 이미지만 입력이 가능했었는데 그거 외에도 텍스트 이미지 비디오 그리고 오디오 같은 이러한 멀티모달 데이터를 한 번에 다룰 수 있도록 그리고 그런 것을 한 번에 생성하고 한 번에 뭔가 처리할 수 있도록 만들었다라고 생각하시면 좋을 것 같습니다. 어 이 제미나이 같은 경우에는 세 가지 종류의 크기를 제공하는데요. 첫 번째가 울트라 두 번째가 프로 세 번째가 나노입니다. 그래서 프로 같은 경우에는 비교 대상이 거의 챗gpt 챗gpt 3.5 정도라고 생각하면 좋을 것 같고요. 나노 같은 경우에는 일종의 sLLM이 비교 대상이라고 생각하시면 좋을 것 같아요. 모델 크기가 3빌리언을 넘지 않아요. 그래서 이런 이러한 SLM 같은 경우에는 뭔가 온디바이스 작업을 위한 뭔가 그러한 모델로서 제한되었다라고 생각하시면 좋을 것 같습니다. 그러니까 뭔가 휴대폰에 돌아간다든가 그런 용도라고 생각하시면 좋을 것 같네요. 그리고 울트라 같은 경우에는 지금 32개의 벤치마크 중에서 30개에서 소타를 찍었다. 즉 gpt4보다 더 좋은 성능을 보였다라고 우선은 논문에서 밝히고 있지만 지금 이게 공개되어 있지 않습니다. 그래서 현재 밝혀 현재 공개된 것은 프로만 공개돼 있는 상태고요. 그래도 울트라 울트라의 성능을 공개된 표로 확인해 본다면 확실하게 지피티4에 비해서 다양한 테스크에서 더 좋은 성능을 쟁취하였다라고 얘기를 하고 있습니다. 특히 이 구글 제미나이 같은 경우에는 멀티모달 데이터에 대해서 조금 더 집중적으로 학습되었기 때문에 플레인 텍스트 기반 데이터셋보다 벤치마크보다 이 멀티모달 벤치마크에서 이 지피디4보다 더 좋은 성능을 보인다라고 알려져 있습니다. 그래서 실제로 다양한 데이터셋에서 더 좋은 더 우월한 성능을 보였다라고 리포팅하고 있는데요. 다만 이 리포팅된 성적 같은 경우에는 지금 공개적으로 검증되지 않았고 구글에서 나온 보고서의 형태로만 나왔기 때문에 이거 같은 경우에는 어 좀 더 검증할 필요가 있다라고 얘기를 드리고 싶습니다. 그렇다면 한국에서 나오는 뭔가 이러한 사유 엘엘엠은 없는가라는 생각이 들 수 있는데요. 역시 대표적인 뭔가 이런 LLM을 서비싱하는 기업이라고 한다면 역시 네이버가 존재하겠죠 그래서 이 네이버의 하이퍼 클로바 스 같은 경우가 이 대표적 예시라고 생각이 듭니다. 그래서 이거는 2023년 9월 24일 날 공개되었고요. 우선은 한국어 그리고 우선은 당연히 한국 기업이기 때문에 좀 더 강점으로 삼는 부분은 이 챗치pt 대비 한국어를 더 많이 학습시켰기 때문에 한국어를 좀 더 이해할 수 있다라고 얘기를 하고 있습니다. 그래서 한국어를 거의 한 6만6500배 더 많이 학습하였고 어 그리고 한국 기업이다 보니까 좀 더 해 외산 엘엠에 대해서 뭔가 거부감이 있다든가 한국 기업끼리 뭔가 제휴를 맺는 경우가 되게 많기 때문에 뭔가 쏘카라든가 스마일 게이트라든가 뭔가 이러한 다수의 스타트업과 제휴를 맺은 상태입니다. 그래서 이러한 하이퍼 클로바 기반 서비스들이 몇 가지 존재하는데요. 뭔가 클로바 x 같은 뭔가 대화형 인공지능 서비스라든가 혹은 q q 콜론 콜론까지가 이름입니다. 큐 콜론과 같은 뭔가 생성형 AI 검색 서비스 그리고 앞서 나왔던 챗지피트에 뭔가 애저 기반의 뭔가 기업용 엘엠 서비스와 같이 클로바 스튜디오 뉴로 클라우드 이런 기업용 솔루션들을 네이버에서 하이퍼 클로바 스 가지고 제공을 하고 있습니다. 네 그럼 이제 이러한 4u LLM 말고 이제 가중치가 공개되어 있는 오픈 웨이트 모델로는 어떤 것이 있냐라고 한다면 역시 대표적인 모델이라고 한다면 이 메타라마가 존재할 것 같습니다. 일반적으로 이 라마 2 라마 이런 형태로 많이 부르는데 이거 같은 경우에는 그 메타에서 만든 LLM으로 메타는 그 페이스북으로 유명한 회사이죠. 그래서 2023년 7월 18일에 공개된 모델입니다. 그래서 이 라마 2 같은 경우에는 기존에 라마 1이 존재했었고 이제 이것이 뭔가 이 rnatf라든가 그런 것을 통해 가지고 이 프리퍼런스가 튜닝 된 모델들을 같이 공개 한 그런 모델이라고 생각하면 좋을 것 같은데 물론 이거 라마 2 같은 경우에는 라마 1과 사전 학습 데이터도 다 다릅니다. 다만 기본적으로 라마 2 같은 경우에는 이 RH 파인튜닝 된 채팅 형태로 파인 튜닝 된 모델도 같이 제공된다라고 알고 있으면 좋을 것 같아요. 그래서 세 가지 형태의 세 가지 크기의 모델을 제공하는데 7b 13b 70b 모델을 제공합니다. 이거는 이 lama to 같은 경우에는 기본적으로 무료로 상업적으로 이용이 가능하지만 다만 라이선스 조항이 좀 걸려 있어요. 첫 번째로 월간 사용 활성 사용자 그러니까 먼슬리 액티브 유저 해가지고 MAU가 7억 명 이상일 경우에는 메타하고 별도 계약이 필요하고요. 그리고 디스틸레이션 등을 통해가지고 뭔가 다른 LLM을 개선하는 것이 불가능합니다. 그러니까 남아가지고 다른 LLM을 개선하지 말라는 것이죠. 그리고 뭔가 라이센스 자체를 파일을 뭔가 공유 남아가지고 만든 애플리케이션에 포함을 시켜라 이런 제약 조건이 있긴 하지만 기본적으로는 이러한 제약 조건만 잘 지키면 상업적으로 이용하는 데 문제는 없습니다. 그래서 앞서 얘기했던 것처럼 단순히 카우s 랭귀지 모델 그러니까 다음 단어를 예측하는 모델 외에도 LLHF로 파인튜닝 된 챗 모델 그러니까 채팅 형태로 대화 가능한 모델도 공개하였고요. 사실 라마 2 같은 경우에는 뭔가 기업에서 뭔가 상용으로 많이 쓰기보다는 현재 연구 표준 그러니까 연구 논문을 쓴다든가 그런 용도에서 되게 나마 2를 표준으로 활용하고 있는 추세이긴 합니다. 그래서 특히 연구용으로 무제한 활용이 가능하고 그래서 엘엠 관련 논문을 쓸 때 일반적으로 많이 활용되는 편이라고 생각하시면 좋을 것 같습니다. 그래서 여러분들도 이 허깅 페이스 허브 들어가 가지고 라마투를 검색하게 된다면 이 라마 투 라이센스 조항 확인해가지고 체크 몇 번 누르는 것만으로도 쉽게 다운받을 수가 있습니다. 그렇다면 라마 외에는 어떠한 모델들이 있냐 좀 공유를 드리자면 사실 빅 모델은 좀 라마가 대표적이긴 한데 좀 작은 모델들 sLLM 관련 모델들이 제가 앞서 말했던 것처럼 뭔가 스타트업에서 되게 많이 발표가 되었습니다. 그래서 이런 SLM들 조금 소개해 드리자면 대표적으로 이 스테빌리티 AI에서 만든 스테이블 LM이 있을 것 같습니다. 그래서 아마 스테이블 디퓨전은 들어보신 분이 있을 것 같아요. 그래서 스테이블 디퓨전이 그 모델에서 이미지 만들어 주는 모델이죠. 생성용 이미지 이미지 생성용 모델인데 그걸 만들었던 그 스테이블 스테이빌리티 AI에서 만든 언어 모델이라고 생각하시면 좋을 것 같아요. 그래서 2023년 4월 20일에 공개되었고 대표적으로 모델 크기는 3비 7비 해가지고 SLM을 공개하였습니다. 특히 어 3비 421기 이거 같은 경우에는 3빌리언 모델인데 1 트릴리언 토큰으로 4회 폭 학습시켰다. 최근 나온 논문 연구 결과에 따르면 굳이 14가 아니고 몇 회폭 정도는 반복 학습시켜도 성능이 오른다라는 연구 결과가 있는데 그거 기반으로 학습된 모델 같은 경우에는 거의 7b 모델보다 실비 모델에 지치지 않는 그런 성능이 나온다라는 것을 보여주고 있습니다. 그래서 실제로 오른쪽에 나와 있는 것이 여러 가지 테스크에 대해서 평가표인데 스테빌리티ai에서 만든 이 모델 자체가 뭔가 라마at라든가 개 라마 2 13빌리언 뭔가 7빌리언 이런 모델들에 비해서 3빌리언 거의 한 2분의 1 크기인데도 불구하고 거의 7빌리언 모델에 근접하는 성능을 보여준다는 것을 확인해 볼 수가 있습니다. 이거 역시 상업적인 목적이든 연구적 목적이든 자유롭게 활용이 가능하고요. 라이센스는 정확히는 크리에이티드 커먼스 by SA 4.0 라이센스로 포함돼 있어 가지고 여러분들이 이런 라이센스만 지키게 된다면 이 스테이블 LM을 활용할 수가 있습니다. 그리고 또 유명한 오픈 웨이트 LM을 소개한다면 이 미스트랄이 있을 것 같습니다. 이 미스트라 같은 경우에는 이 프랑스 회사 정확히 유럽 회사인 미스트라 AI에서 만들었는데요. 제가 알기로는 이거 오프네이아 퇴사자들이 만든 그 AI 기업으로 알고 있어요. 그래서 이 경우에는 미스트랄 7비 같은 경우에는 2023년 9월 27일에 공개되었는데 모든 벤치마크에서 라마 2 13p보다 우수하다라고 알려져 있습니다. 그래서 이 모델 역시 아파치 2.0 라이센스로 뭔가 배포가 되어 있기 때문에 충분히 여러분들이 활용할 수가 있고요. 그리고 이걸 기반해 가지고 또 나온 모델이 이 믹스트랄 s가 x로 바뀝니다. 그래서 믹스트랄 8x 7비인데 미스트랄 7b가 나온 지 거의 두 달 만에 나온 모델입니다. 이거 같은 경우에는 7b 모델을 믹스트 오브 익스포트 해가지고 moe 기반으로 저희가 모델을 만든 것인데요. 즉 moe 같은 경우에는 여러분들이 전문가 혼합이라고 해서 아시는 분들도 아시겠지만 뭔가 기계 학습 쪽에서 많이 쓰였던 개념이죠. 뭔가 여러 가지 모델이 있고 이것들을 뭔가 게이팅을 통해 가지고 모델이 결괏값을 합친다든가 그런 결과였는데 이것도 비슷하게 7비 모델들이 여러 개가 있고 세로로 뭔가 이렇게 이것이 일렬로 있지 않고 이것은 병렬적으로 있고 이것에 나온 결과값들을 뭔가 MOU를 통해 가지고 합치는 그런 형태라고 기본적으로 알면 좋을 것 같습니다. 물론 세부적으로는 조금 다르긴 한데 기본적으로 이렇게 7비 모델들을 여러 개 엮어가지고 학습시키는 방법론이라고 생각하시면 좋을 것 같네요. 특히 이 moe 방법론 같은 경우에는 gpt4로 어 지피티4에서 이 엠오가 쓰였다라는 이야기가 널리 알려져 있어 가지고 어 이러한 최근에는 뭔가 이런 라지 모델을 만들 때 이 엠오이를 많이 활용하는 편이기도 합니다. 우선은 기본적으로 이 미스트랄과 동일한 라이선스고요. 이거 같은 경우에는 8 곱하기 7비기 때문에 사실상 모델 크기 자체는 56b 정도라고 생각하시면 좋을 것 같은데 실제로는 이 56비가 뭔가 이렇게 세로로 연결돼 있는 게 아니고 가로로 연결돼 있기 때문에 이것이 인퍼런스 속도도 되게 빠르고 그리고 그 70p보다도 라마 2 70p보다도 더 좋은 성능 벤치마크에서 더 좋은 성능을 보인다라고 알려져 있습니다. 그리고 이 미스트랄 같은 경우에는 이것을 dpo라고 해가지고 RHF랑 f랑 뭔가 수학적으로 동일한데 뭔가 좀 더 학습이 안정적인 그러한 방법론으로 학습시켰다 정도로 그러한 방법론 가지고 이제 얼라이먼트 튜닝 그러니까 정확히는 채팅 형태 혹은 인스트럭션 튜닝 형태로 인스트럭션 형태로 패션앤 엔서로 대답할 수 있는 그러한 프리퍼런스가 맞춰진 모델도 추가로 공개돼 있어 가지고 여러분들이 따로 뭔가 이것을 그 얼라인먼트를 맞추지 않고도 충분히 활용이 가능합니다. 그래서 오른쪽 그래프를 보시면 이것이 실제로 그러한 라마 2하고 이제 믹스트라의 비교를 볼 수가 있는데 라마 2의 모델이 이렇게 이렇게 증가할 때 지금 보시면 인퍼런스 버킷 같은 경우에는 이것이 믹스트라 같은 경우에는 실제로는 이게 실비 모델이 그냥 병렬적으로만 있는 게 아니고 맨 처음에 실비를 한 번 통과하고 나서 여러 개의 칠비를 다시 통과하는 그런 형태로 만들어져 있거든요. 그러다 보니까 인플런스 버겟 자체는 거의 한 2배 정도밖에 안 늘어납니다. 그에 비해서 성능 자체는 거의 한 70p보다 더 좋은 그런 성능이 나온다라는 것을 이 논문을 통해서 밝히고 있습니다."
}