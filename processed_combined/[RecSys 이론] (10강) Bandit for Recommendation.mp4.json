{
  "lecture_name": "[RecSys 이론] (10강) Bandit for Recommendation.mp4",
  "source_file": "[RecSys 이론] (10강) Bandit for Recommendation.mp4_129.mp4_2025-12-04-110025768.json",
  "text": "안녕하세요. 추천 시스템 강의를 맡은 강사 이준원입니다. 이번 시간은 추천 시스템 이론 강의의 마지막 열 번째 시간입니다. 추천 시스템의 연구 분야에서는 아주 많이 다루진 않지만 실제 현업에서 많이 사용하는 밴딧 알고리즘에 대해서 이번 시간에 학습하겠습니다. 밴디 문제는 추천 시스템보다는 강화 학습을 배울 때 처음 배우는 개념입니다. 그동안 배웠던 추천 모델과는 다소 거리가 있지만 실제로 구현 방법이 간단하면서 좋은 성능을 보이기 때문에 현업에서는 추천 기법으로 종종 사용되기도 합니다. 목차는 다음과 같습니다. 먼저 멀티 암드 밴딧 문제에 대한 개요와 정의를 배우고 이 mab가 추천 시스템에서 어떻게 사용되는지 그 예시를 살펴봅니다. 그 이후에 세 가지 mab 기본 알고리즘인 그리디 알고리즘, 입실론 그리디 알고리즘 그리고 어퍼 컨피던스 바운드라는 뉴c비 알고리즘에 대해서 각각 이해합니다. 그 이후에 좀 더 심화된 알고리즘인 톰슨 샘플링과 리니얼 유시비에 대해서 다뤄보도록 하겠습니다. 먼저 mab라고 불리는 멀티 암드 밴딧이 어떤 문제인지 알아보고 이것이 추천 시스템에 어떻게 활용될 수 있는지를 살펴봅시다. 먼저 mab 문제가 무엇인지 이해하기 위해서는 하나의 r이 달려 있는 원남 밴딧이 무엇인지 봐야 합니다. 원암 밴딧은 카지노에 있는 슬롯 머신을 생각하면 되는데요. 한 번에 한 개의 알을 당길 수 있고 그에 따라 리워드를 얻게 됩니다. 이제 여기서 카지노에는 슬롯 머신이 한 개만 있는 것이 아니라 케이에 즉 여러 개의 슬롯머신이 있을 수 있고 우리도 한 번 플레이하는 것이 아니라 여러 번 플레이할 수 있겠죠. 이제 이렇게 플레이할 수 있을 때 우리가 어떻게 하면은 가장 많은 리워드를 얻을 수 있을까 그것이 바로 멀티 암드 밴딧 문제입니다. 그래서 이 멀티 암드 밴딧에서는 케이의 슬롯머신에서 얻을 수 있는 리워드의 확률이 모두 다르고 어떤 슬롯 머신이 가장 많은 리워드를 줄지를 추정하면서 가장 많은 리워드를 얻어내는 문제입니다. 그래서 수익을 최대화하기 위해서 즉 리워드를 최대화하기 위해서 다양한 케이크의 슬롯머신 가운데 어떤 앎을 어떤 순서대로 혹은 어떤 정책에 의해서 당겨야 하는가를 학습하는 알고리즘입니다. 그래서 이 mab를 통해서 얻어지는 것은 우리가 어떤 정책을 통해서 리워드를 얻을 것이다라는 결과입니다. 즉 polic를 얻는 것이죠. 이 mab 문제에서 어려운 점은 바로 우리가 아직 슬롯 머신의 각각의 리워드의 확률, 즉 기대 리워드를 정확하게 알 수 없다는 것입니다. 그래서 이 mab 정책의 예시를 한번 두 가지로 적어봤는데요. 먼저 가장 단순한 방법, 그냥 주어진 모든 슬롯 머신을 다 동일하게 다섯 번씩 혹은 세 번씩 당기는 정책이 있을 수 있겠죠. 이제 이럴 경우 가장 리워드가 높은 슬롯 머신을 찾아야 하는데 그렇지 않기 때문에 높은 리워드를 얻을 수 없게 되겠죠. 반대로 일정 횟수만큼 다 슬롯 머신을 당겨보고 그 평균 리워드가 가장 높은 리워드 하나만을 계속해서 당기는 방법도 있겠습니다. 이제 이렇게 된 경우에 동일한 슬롯만 계속 당기게 될 수 있는데요. 이제 여기서 다양한 슬롯 머신을 당기는 행위와 그중에 가장 리워드가 높은 슬롯 머신을 하나만 계속 당기는 행위가 바로 익스플로레이션과 익스플로이테이션이라고 표현하고 있습니다. 그래서 익스플로레이션 다양한 슬롯머신을 당겨보면서 정확한 예측값을 얻는 행위고요. 이제 그렇게 하다 보면은 실제 우리가 리워드를 얻어야 하는데 리워드를 많이 얻지 못하게 되겠죠. 그래서 리워드가 많이 얻는 리워드를 많이 주는 슬롯머신을 찾았을 때 그것만 당기는 걸 익스플로이테이션이라고 합니다. 그러나 우리가 충분히 어떤 슬롯 머신이 가장 좋은 슬롯 머신인지 탐색을 해보지 않고 잘못해서 익스플로이테이션만 하게 되면은 결과적으로 장기적으로 봤을 때 그 리워드는 우리가 얻는 리워드는 적게 얻을 수밖에 없겠죠. 그래서 이 둘 사이에 트레이드 오프가 항상 존재한다고 이야기합니다. 그래서 익스플로레이션과 익스플로이테이션에 대해서 각각 자세히 설명해 보도록 하겠습니다. 익스플로레이션과 익스플로이테이션 사이에 트레이드 오프가 있다고 했는데요. 먼저 익스플로레이션은 더 많은 정보 더 정확한 정보를 얻기 위해서 새로운 앎을 선택하는 것입니다. 새로운 슬로머신을 계속 선택해서 거기서 혹시 리워드가 많이 나오는지 적게 나오는지에 대한 정보를 얻는 것이죠. 그리고 익스플로이테이션은 기존의 경험이나 기존의 데이터 관측된 데이터를 가지고 가장 좋은 슬롯머신 하나를 선택해서 그것을 계속 당기는 것입니다. 만약에 익스플로레이션 탐색을 너무 많이 하는 경우 는 즉 익스플로이테이션을 너무 적게 하는 것이기 때문에 탐색에 지나치게 비용을 낭비하여서 높은 리워드를 얻을 수가 없게 되겠죠. 반대로 탐색이 너무 적은 경우에는 충분한 데이터를 확보하지 못하고 즉 잘못된 정보를 가지고 익스플로이테이션을 할 수 있습니다. 그럴 경우 높은 리워드를 우리가 보장할 수 없게 되겠죠. 그래서 이 두 가지의 트레이드 오프를 적절히 고려한 정책 적절한 폴리스를 수립하여서 최대의 리워드를 얻는 것이 바로 이 mab 알고리즘의 핵심입니다. 자 그러면 수식을 통해 mab 문제를 정의해 보도록 하겠습니다. 여기서 t라는 것은 타임 스텝을 의미합니다. 혹은 플레이 넘버 즉 첫 번째 두 번째 세 번째 당기는 그 시간을 의미하고요. 이 액션 t는 그 t 스텝에 내가 선택한 슬롯 머신 즉 내가 선택한 액션을 의미합니다. 그리고 그 액션이 a일 때 얻는 리워드가 RT가 되겠죠 그럼 우리가 최종적으로 알고 싶은 것은 각각의 액션에 대해서 실제 리워드의 기댓값인 q스타a 이것을 정확하게 추정하는 것이 중요한 문제입니다. 이제 이 q스타a를 정확하게 알고 있다면 우리가 그 큐스타 a가 가장 높은 액션만 계속 취해서 아주 많은 리워드를 얻을 수 있을 것입니다. 하지만 문제는 우리는 실제 리워드의 트 디스트리뷰션을 모르기 때문에 즉 얻어진 데이터가 아무것도 없기 때문에 우리는 q스타a를 바로 구할 수 없고 그래서 시간 t에서의 추정치인 라즈 qta를 정의해서 이 라지 qta를 최대한 q스타a와 비슷하게 구하는 것이 이 mab의 목표가 되겠습니다. 그래서 매번 타임 스텝마다 이 추정치를 계산하고 이 추정 가치가 최대인 액션을 선택하는 것을 그리디언 액션이라고 합니다. 지금까지 얻어진 데이터를 얻어진 관측 값을 토대로 각각의 액션에 대한 추정치를 계산한 다음에 그중에 제일 추정치가 높아 보이는 액션을 통해서 우리가 더 많은 리워드를 얻길 기대하는 것이죠. 그래서 이 그리디안 액션을 선택하는 것이 익스플로이테이션이고 그리디안 액션이 아니라 다른 액션을 통해서 혹시 그 액션이 더 좋은 액션인지 정보를 얻는 행위를 익스플로레이션이라고 분류할 수 있습니다. 네 그래서 이번 파트에서는 가장 기초적인 세 가지 mab 알고리즘을 배워보도록 하겠습니다. 그리디 알고리즘과 입실론, 그리디 알고리즘 그리고 ucb에 대해서 학습합니다. 가장 기초적인 mab 알고리즘은 그리디 알고리즘이다. 다른 말로는 심플 에버리지 메소드 단순하게 평균을 사용한다는 방식입니다. 실제 기댓값인 큐스타 a는 가장 간단하게 추정했을 때는 현재까지 얻어진 관측 값으로 표본 평균을 내용해서 구해서 사용하는 것입니다. 즉 지금까지 각각의 액션에 대해서 관측된 리워드의 평균값을 사용하는 것이죠. 그래서 그 액션을 수행한 횟수를 분모로 나누고 그 액션을 수행한 전체 횟수 가운데 우리가 얻은 리워드를 분자로 둬서 평균 리워드를 다음과 같은 라지 qta로 구하는 것입니다. 그래서 이 그리드 알고리즘은 이 qta를 구한 이후에 이 평균 리워드가 최대인 액션을 계속해서 선택하는 것을 의미합니다. 그래서 매번 타임 스텝마다 이 qta는 계속해서 리워드가 수집됨에 따라서 업데이트가 될 거고 그래서 그 타임 스텝마다 계속해서 평균 표본 평균이 제일 높은 액션을 선택하는 폴리스가 바로 그리디 알고리즘입니다. 이 문제는 이 폴리시가 처음 선택될 때 액션과 리워드에 굉장한 큰 영향을 받는다는 것인데요. 만약에 실제로는 리워드가 굉장히 높은 액션이지만 그 액션의 초반 얻었던 데이터가 운이 굉장히 나빠서 아주 낮은 리워드가 구해졌을 때 이제 그 액션의 리워드 평균은 굉장히 낮기 때문에 다시는 그 액션이 선택될 기회조차 없어집니다. 즉 충분한 탐색을 하지 못한 상태로 그 액션은 더 이상 선택될 가능성이 없어지는 것이죠. 그래서 이 익스플로레이션이 부족한 그리디 알고리즘을 일부 수정한 방식이 입실론 그리디 알고리즘입니다. 익스플로레이션이 부족한 그리디 알고리즘에 추가적으로 어떤 랜덤한 익스플로레이션을 추가하는 것인데요. 이 아래 설명처럼 일정한 확률로 랜덤으로 슬롯 머신을 선택하게 강제하는 것입니다. 예를 들면 입실론이 0.5라고 했을 때 동전을 안면으로 던져서 앞면이 나오면은 현재까지 계산된 표본 평균 즉 이 추정 리워드가 가장 높은 액션을 선택하는 것이고요. 뒷면이 나오면은 이 평균은 전혀 고려하지 않고 그냥 랜덤하게 주어진 액션 가운데 하나를 선택하는 것이죠. 그래서 이 입실론 gd 알고리즘은 심플하면서도 강력한 익스플로레이션과 익스플로이테이션을 항상 어느 정도는 보장하는 알고리즘. 그래서 보통 mab 기법을 비교할 때 베이스 라인으로 입실론 그리디를 많이 사용하는데 가장 간단하면서 안정적인 성능을 내기 때문입니다. 다만 타임 스텝이 많이 지나고 데이터가 충분히 쌓여서 각각의 액션에 트루 디스트리뷰션을 충분히 추정했음에도 불구하고 이 입실론 그리디는 항상 입실론의 확률로 랜덤한 액션을 선택하는 익스플로레이션을 하기 때문에 후반 가서는 이 입실론 그리디 알고리즘이 손해를 비교적 많이 봅니다. 다음은 유시비 알고리즘입니다. 이 수식을 보면은 이 앞에 부분은 그리디 알고리즘에서 봤던 심플 에버리지 표본 평균인데요. 이제 이 유씨비는 거기에 한 가지의 텀을 추가하여서 익스플로레이션이 잘될 수 있도록 하였습니다. 이 새로 추가된 텀이 해당 액션이 최적의 액션이 될 수도 있을 불확실성을 표현하는 텀인데요. 이제 이 텀에 추가되어 있는 어노테이션을 살펴보면 이 nta란 액션이 선택된 횟수, 지금까지 관측값에서 그 액션이 선택된 횟수를 의미합니다. 그리고 위에 있는 값은 현재 타임 스텝의 로그를 취해준 것인데요. 현재까지 이 액션 특정 액션에 대한 데이터가 많이 부족한 경우에는 이 분모의 값이 낮겠고 그러면은 상대적으로 이 불확실성 텀이 증가하기 때문에 이 전체 값이 커져서 현재 상황에서 이 액션이 선택될 확률이 높아지게 되겠죠. 이 값이 커지기 때문에 그리고 이 불확실성 한 텀을 얼마나 더 크게 할 거냐 작게 할 거냐는 바로 이 씨라는 하이퍼 파라미터를 통해 조절합니다. 그래서 나중에 모든 액션이 충분히 많이 탐색된 이후에는 이 nta 값은 모두 전반적인 액션 모든 액션에 대해서 더 높을 것이고 이 티는 시간에 따라서 로그 함수로 점점 증가 폭이 줄어들기 때문에 결국에는 이 뒤에 있는 텀은 0으로 수렴하게 되고 최종적으로는 데이터가 충분히 모인 상태에서 지금까지 구해진 표본 평균 평균 리워드로 최종 익스플로이테이션을 수행하게 되는 것입니다. 그래서 자연스럽게 익스플로레이션에서 익스플로이테이션으로 폴리스가 넘어가게 됩니다. 네 그래서 다음은 각각의 벤딧 알고리즘에 대한 성능이 기록된 그림인데요. 여기서 제일 중요한 점은 이 ucb가 제일 좋은 성능을 나타낸다는 것이 아니라 물론 일반적인 mab 상황에서 유시비 알고리즘이 보통 리워드를 기대 리워드가 높은 폴리스이긴 하지만 여기서 중요한 것은 벤딧 알고리즘에 튜닝해야 될 파라미터가 있다는 것입니다. 입실론 그리디 같은 경우에는 그 입실론 값 즉 얼마나 얼마의 확률로 랜덤하게 학습할 것인 ucb는 그 불확실성한 텀 하이퍼 파라미터를 얼마로 조정할 것인가인데요. 이 그림들을 다 보시면은 다 유자형 거꾸로 된 유자형의 모양을 가지고 있음을 알 수 있습니다. 즉 mab 문제를 풀기 위해서 최적의 폴리시를 찾기 위해서는 적절한 하이퍼 파라미터를 찾아서 익스플로레이션과 익스플로이테이션이 적절하게 트레이드 오프가 고려된 이 옵티멀한 포인트를 찾아야 한다는 것입니다. 그래서 각각의 알고리즘을 사용할 때 이 하이퍼 파라미터를 적절하게 조절함 안에서 적절한 익스플로레이션과 적절한 익스플로이테이션이 이루어질 수 있도록 결정해야 합니다. 자 그렇다면 지금까지 배웠던 이 기본적인 mab 알고리즘이 추천 시스템에 어떻게 활용되는지를 살펴봅시다. 먼저 추천 시스템의 두 가지 문제가 있는데요. 유저에게 아이템별 선호도를 계산해서 그걸 가지고 탑 n개의 아이템을 추천하는 문제가 있고요. 두 번째는 주어진 아이템과 가장 유사한 아이템을 추천하는 일반적인 두 가지의 문제가 있죠. 자 그러나 밴딧 문제는 기존의 추천 시스템과는 완전 다른 접근을 합니다. 이 그동안 추천 시스템 모델링을 할 때 사용했던 아이템에 대한 평점이나 유사도 같은 것을 사용하지 않고 대신 추천을 통해서 최종적으로 얻어지는 클릭이나 구매를 mab 문제의 리워드로 가정합니다. 그리고 이 리워드가 최대화되는 방향으로 이 mab 알고리즘이 학습이 되고 그에 따라 추천이 수행되게 되는 것입니다. 그래서 무거운 추천 모델을 사용하지 않고 간단한 배닛 기법을 적용했을 때에도 우리가 원하는 것은 오프라인 성능이 아니라 결국에는 온라인 지표 즉 CTR 같은 것들인데요. 무거운 추천 모델을 사용하지 않고도 간단한 밴드 기법을 사용하여서도 그 리워드인 클릭이 바로 높아지는 방향으로 최적화가 되기 때문에 곧바로 온라인 지표가 좋아지는 장점이 있습니다. 자 그럼 추천을 수행하는 것과 mab가 어떻게 연결이 될까요? 우리가 추천하는 것은 개별 아이템인데요. 이 아이템을 선택하는 것 하나하나가 개별 액션에 해당하는 것입니다. 그리고 우리가 유저에게 아이템을 추천하는 방식이 곧 mab 알고리즘의 폴리시에 해당합니다. 그리고 아이템을 추천했을 때 사용자가 클릭했다면 리워드를 일로 두고 클릭하지 않았다면 0으로 둬서 최종적으로 이 클릭이 최대화되는 쪽으로 엠에이비 알고리즘이 학습됩니다. 따라서 meab의 경우 그동안 배웠던 추천 모델에 비해서 훨씬 구현이 간단하고 이해하기도 쉽습니다. 그러면서 바로바로 서비스나 비즈니스 애플리케이션에서도 원하는 클릭을 직접적으로 향상시키는 쪽으로 이 알고리즘이 학습되기 때문에 실제 애플리케이션에 굉장히 유용합니다. 이 mab에서 두 가지 중요한 개념 익스플로레이션과 익스플로이테이션을 추천 시스템과 연결 짓자면 익스플로레이션은 지속적으로 변화하는 유저의 취향이나 취향을 탐색하고 새로운 추천 아이템이 들어왔을 때 그 아이템에 대한 정보를 얻는 그것이 익스플로레이션이고요. 익스플로테이션은 그러한 탐색이 다 끝난 이후 유저의 취향에 가장 적합한 아이템을 계속해서 추천해 주는 그래서 클릭률을 결국에는 높이는 이 두 가지 원리를 추천 시스템에 연결 지을 수 있습니다. 자 그럼 mab를 이용하여서 실제로 유저에게 아이템을 추천한다고 했을 때 어떤 프로세스로 진행되는지 살펴봅시다. 첫 번째 예시는 유저 추천인데요. 근데 문제는 mab를 이용하여서 실제 유저에게 아이템을 추천한다고 했을 때 어떤 프로세스로 진행되는지 살펴봅시다. 먼저 유저에게 아이템을 추천하는 예시입니다. 유저에게 아이템을 추천한다고 했을 때 개별 유저 한 명 한 명에 대해서 모든 아이템의 밴딧을 구하는 것은 불가능합니다. 왜냐하면 개인별로 수집되는 데이터는 한계가 있기 때문에 개인마다 하나하나의 밴딧을 구성하기 위해서는 굉장히 많은 밴딧도 필요하게 되고 그 밴딧 알고리즘이 수렴하지 않게 됩니다. 따라서 클러스터링을 통해서 유저 한 명 한 명이 아니라 유저를 그룹화해서 그 유저 그룹별로 밴딧을 구축하는 방법을 종종 사용합니다. 그리고 이 유저 클러스터별로 아이템 후보 리스트를 생성하는 생성하여서 최종적으로 유저 클러스터의 개수 곱하기 이 아이템 후보 리스트에서 생성한 후보 아이템의 개수만큼의 밴딧이 생성됩니다. 그래서 그림을 통해 다양한 아이템들이 있고 이 아이템들을 각각의 유저 클러스터에 할당하게 된 이후에 어떤 특정 유저가 들어왔을 때 이 유저의 클러스터가 무엇인지 찾고 그 클러스터에 있는 후보 아이템들이 3개가 있다고 했을 때 이 3개에 대해서 mab 알고리즘을 태워서 이 중에 어떤 액션 즉 어떤 아이템이 가장 리워드를 높게 얻게 할지 즉 클릭을 가장 많이 얻게 할지 찾아서 최종적으로 이 치마라는 아이템을 노출하는 방식입니다. 두 번째는 유사한 아이템을 추천할 때 mab를 어떻게 활용할 것인가인데요. 비슷하게 주어진 아이템과 유사한 아이템을 찾기 위해서는 먼저 유사한 후보 아이템 리스트를 어느 정도 만들어 주고 그 안에서 밴딧을 적용하는 방식입니다. 여기서 유사한 아이템을 추출하기 위해서는 우리가 그동안 배웠던 추천 모델인 매트리스 팩토라이제이션이나 아이템 투 백 같은 유사도를 사용할 수도 있고요. 혹은 아이템의 콘텐츠 기반 유사도를 사용하여서 그 아이템과 비슷한 후보 아이템 리스트를 찾을 수 있겠죠 이렇게 했을 때 주어진 아이템 곱하기 그 아이템과 비슷한 후보 아이템 개수만큼 밴딧이 생성됩니다. 그래서 우리가 이 신발이라는 아이템이 소개된 페이지에 방문했다고 했을 때 이 신발 아이템과 유사한 아이템을 추천해 주어야 하는데요. 신발과 유사도가 비슷한 즉 MF 기반의 유사도를 사용한 후보 아이템 리스트 그리고 또 다른 유사도 기준 예를 들면 c2 콘텐트 베이스 레코멘데이션을 사용한 유사도를 활용해서 추출한 후보 아이템 리스트들이 있을 것이고요. 전체 아이템 리스트 중에서 훨씬 작은 리스트가 있을 것입니다. 이제 거기에 대해서 우리가 밴딧 알고리즘을 적용해서 클릭을 가장 많이 얻어낼 것 같은 이 게임기라는 아이템을 추천해 주게 되는 것입니다. 방금 설명했던 유저에게 아이템을 추천해 주는 케이스나 유사 아이템을 추천해 주는 케이스는 어떤 정해진 방식이 있는 것이 아니라 하나의 예시일 뿐이고요. 서비스에 맞게 그 적용 방식에 맞게 후보 아이템 리스트를 적용하고 그 후 그 후보 아이템 리스트에 밴딧을 적용해서 추천하는 애플리케이션은 굉장히 다양하게 변화할 수 있습니다. 네 다음은 mab 알고리즘 심화 부분입니다. 입실론 그리기와 유시비보다 좀 더 발전된 기법이며 현업에서 많이 사용되고 있는 톰슨 샘플링과 린 유시비에 대해서 다뤄보겠습니다. 먼저 톰슨 샘플링 부분입니다. 톰슨 샘플링과 다른 mab 알고리즘의 가장 큰 차이점은 각각의 액션에 이 리워드를 추정하는 이 라지 큐티에 를 계산할 때 확률 분포를 사용한다는 것입니다. 주어진 k 액션 k개의 액션 각각이 어떤 확률 분포를 따른다고 가정하고 그 추정치를 업데이트합니다. 즉 확률 분포를 계속해서 업데이트하는 것이죠. 이때 많이 사용하는 확률 분포가 바로 베타 분포인데요. 베타 분포란 두 개의 양의 변수로 표현할 수 있는 확률 분포이고 이 확률 분포에서 샘플링한 값은 0과 1의 사이의 값을 갖습니다. 그리고 여기에 있는 이 알파와 베타가 이 확률 분포를 설명하는 두 개의 파라미터입니다. 그래서 이 알파와 베타가 어떠냐에 따라서 확률 분포의 모양 그리고 샘플링 되는 값이 달라지게 됩니다. 자 이 예시를 통해서 각각의 액션 즉 각각의 아이템별 추정치를 계산할 때 어떻게 확률 분포를 사용하는지 즉 베타 분포를 사용하는지를 살펴봅시다. 먼저 주어진 데이터와 상황은 다음과 같습니다. 현재 사용자에게 광고 배너를 추천해야 하는 상황인데요. 최종적으로 이 세 가지 배너 중에 어떤 배너를 노출했을 때 가장 많은 클릭을 얻어내는지를 알아서 그 아이템을 추천해 주는 것이 이 시스템의 목표입니다. 그래서 현재까지 주어진 데이터는 다음과 같고요. 우리는 이 상황에서 가장 많은 클릭을 이끌어낼 즉 CTR이 가장 높을 것 같은 배너를 셋 중에 하나 선택해서 이를 추천해 주는 정책을 세워야 합니다. 그래서 이 액션의 추정치를 구하기 위해서는 베타 분포가 필요한데요. 방금 전에 베타 분포에서는 두 가지 알파와 베타라는 파라미터가 필요하다고 했는데요. 여기서는 이 알파와 베타를 배너를 노출시켰을 때 클릭한 횟수와 노출시켰을 때 클릭하지 않은 횟수로 두어 베타 분포를 만듭니다. 그래서 이 3개의 배너에 대해서 현재까지 얻어진 데이터는 다음과 같고요. 이를 사용하여서 각각의 배너에 대해서 베타 분포를 정의할 수 있습니다. 또한 이 베타 분포의 샘플링을 했을 때 수렴하게 되는 평균 값 즉 표본 평균은 알파 플러스 베타분의 알파인데요. 이 3과 10은 두 개 합쳤을 때 이 배너를 노출시킨 전체 노출 수가 되고 그중에 알파는 클릭을 한 횟수가 되죠. 그래서 이 표본 평균값은 결국에 각각의 배너의 CTR 노출 대비 클릭할 확률이 됩니다. 그래서 이 배너를 클릭할 확률 자체가 베타 분포를 따르기 때문에 베타 분포에서 샘플링한 그 값이 최종적으로는 그 배너를 클릭할 확률을 따라간다는 것을 사용합니다. 이제 여기서 그리드 알고리즘 같은 경우에는 이 셋 중에 가장 값이 높은 배너 2 를 선택해서 배너 투만 계속해서 노출하겠지만 이 톰슨 샘플링 같은 경우에는 각각의 배너에 가가 가지고 있는 베타 분포로부터 샘플링한 값이 가장 높은 배너를 선택해서 노출합니다. 자 다음 예시를 통해서 어떻게 샘플링이 이루어지고 어떻게 아이템이 선택돼서 노출되는지를 살펴봅시다. 자 톰슨 샘플링의 예시인데요. 일단 아무런 데이터가 없는 상황 즉 배너 1 2 3에 대해서 아무런 데이터가 없는 최초의 사전 확률 분포를 가정합시다. 각각 베타 1 콤마 원으로 모두 동일하게 이루어져 있습니다. 이제 여기서 어느 정도의 기간 동안은 랜덤하게 노출시키는 구간을 갖는데요. 어느 정도의 관측값을 수집해서 베타 분포를 적절한 수준까지 업데이트하기 위함이죠. 그래서 배너 원을 노출시켰더니 클릭해서 알파가 업데이트 되었고 배너 2는 클릭되지 않아서 베타가 업데이트 되었습니다. 그리고 노출되지 않은 아이템은 아무것도 업데이트가 되지 않았죠 이와 같이 랜덤하게 몇 번 더 노출을 시켜서 계속해서 베타 분포를 업데이트합니다. 이 오른쪽에 있는 그림은 각각의 베타 분포를 그림으로 나타낸 것입니다. 자 그래서 여기까지 랜덤하게 노출시켜서 각각의 배너가 다음과 같은 3개의 베타 분포를 갖게 되었습니다. 이제 여기서부터는 더 이상 랜덤하게 노출하지 않고 이제 톰슨 샘플링의 폴리시를 사용합니다. 이 베타 분포에서 샘플링을 해서 그 샘플링한 값을 사용한다고 했는데요. 이 각각의 배너들이 가지고 있는 베타 분포에서 샘플링을 했을 때 그 샘플링한 값을 값은 이렇게 구해지고요. 샘플링한 결과 이 파란색 즉 배너 3에서 샘플링 된 값이 가장 높아서 이 톰슨 샘플링의 폴리시에 따라서 배너 3을 추천하게 됩니다. 배너 3이 추천된 이후에 클릭이 일어났기 때문에 알파를 업데이트합니다. 그렇게 하고 나서 다시 새로운 베타 분포 새로운 베타 분포라고 하면 배너 3만 업데이트가 되었고 배너 1 2는 그대로 있겠죠 거기서 다시 샘플링을 했을 때 이번에는 배너 원 빨간색이 가장 높은 값을 가지고 그래서 배너 원을 추천해 주게 됩니다. 이렇게 해서 계속 샘플링 된 값이 가장 높은 배너를 추천하고 그 추천에 해당하는 리워드가 클릭이 됐는지 안 됐는지를 가지고 베타 분포 샘플 베타 분포 업데이트를 계속해서 수행해 나가는데요. 이렇게 몇 번의 샘플링 이후 노출을 겪게 되면은 최종적으로는 베타 분포의 모양이 점점 변하면서 트루 디스트리뷰션에 점점 가까워지게 됩니다. 보시면 점점 파란색 배너가 이쪽으로 이동함을 볼 수 있는데요. 최종적으로 이러한 톰슨 샘플링을 계속 반복적으로 수행했을 때 이렇게 수많은 노출을 거친 이후에는 각각의 베타 분포의 파라미터가 어느 정도 많이 업데이트 되었고 각각의 분포는 굉장히 뾰족해지게 됩니다. 그래서 이렇게 수많은 노출을 거친 이후에는 베타 분포가 굉장히 뾰족해져서 수렴했기 때문에 더 이상은 샘플링을 해도 평균 CTR이 제일 낮은 배너 원이 배너 3보다 높게 샘플 될 확률은 거의 없습니다. 따라서 이 정도의 수렴을 거친 이후에는 계속해서 배너 3만 추천이 되게 됩니다. 그래서 이러한 과정을 거쳐 통해서 익스플로레이션과 익스플로이테이션이 확률 분포를 따라서 적절한 트레이드 오프를 유지하면서 계속해서 다양한 아이템들이 노출되다가 결국에는 가장 리워드가 높은 아이템이 노출되는 쪽으로 수렴하는 결과를 얻게 됩니다. 네 지금까지 설명한 톰슨 샘플링의 업데이트 과정을 스톱 코드로 표현하면 다음과 같습니다. 처음에 알파 베타를 두고 베타 분포를 이니셜얼라이즈 해줍니다. 그 이후에 타임 스텝 1부터 t까지 매 타임 스텝마다 케이의 액션에 대해서 각각 베타 분포에서 샘플링을 하고 이 샘플링 된 세타 아이 중에 가장 큰 샘플링 값을 가진 액션을 선택하게 됩니다. 그리고 그 액션에 따라서 클릭이 일어났으면은 앞에 베타 분포 값을 업데이트해주고 클릭이 일어나지 않았다면 뒤에 베타 분포 값을 업데이트합니다. 이렇게 해서 액션을 수행해서 업데이트한 뒤 다시 두 번째 타임 스텝으로 돌아가서 똑같은 과정을 반복하는 그래서 여기까지가 톰슨 샘플링에 대한 내용이었고요. 자세한 내용은 부록에 있는 어 앰피리컬 이밸류에이션 오브 톰슨 샘플링이라는 페이퍼를 참고하시면 이 톰슨 샘플링을 공부하시는 데 도움이 많이 될 것입니다. 마지막으로 배울 mab 알고리즘은 린 유시비 알고리즘입니다. 먼저 그전에 컨텍스추얼 밴딧 문제가 무엇인지 살펴봅시다. 이 컨텍스트란 8강에서도 잠시 다루었는데요. 유저 아이디나 아이템 아이디 외에 유저나 아이템이 가지고 있는 다른 부가 정보 다른 특성 정보들을 의미합니다. 그래서 이 컨텍스트를 고려하냐 고려하지 않느냐에 따라서 컨텍스트 프리 bandi 컨텍스트얼 밴딧으로 구분할 수 있습니다. 먼저 컨텍스트 프리 밴딧 같은 경우에는 우리가 앞서 배운 입실론 그린이나 ucb 같은 mab 알고리즘인데요. 동일한 액션에 대해서 유저의 컨텍스트 정보와 관계없이 항상 동일한 리워드를 가진다고 가정하고 모델링을 하는 것입니다. 예를 들면 카지노의 슬롯머신 같은 경우에는 어떤 유저가 가서 암을 당긴다고 해도 사실 그 리워드의 분포가 사람마다 달라지지는 않겠죠. 따라서 이 컨텍스트 프리 밴딧으로 모델링하는 것이 더 적합하다고 볼 수 있습니다. 그러나 컨텍스트얼 밴딧은 유저의 컨텍스트 정보에 따라서 같은 액션을 수행했을지라도 서로 다른 리워드를 가질 수 있다고 가정하는 모델링입니다. 예를 들면 동일한 스포츠 기사를 보더라도 나이나 성별에 따라서 클릭 성향이 달라지는 경우 이제 이 상황을 모델링하기 위해서는 컨텍스트롤 벤딧이 더 적합합니다. 즉 이러한 상황은 개인화 추천과도 연관이 있습니다. 자 다음은 린 유시비 기법인데요. 액션을 선택하기 위한 폴리스는 다음 수식을 따릅니다. 이전까지 배웠던 컨텍스트 프리 밴딧과는 좀 많이 다른 형태인데요. 먼저 각각의 수식을 구성하는 어노테이션을 살펴보겠습니다. 먼저 이 xt 콤마 a 라는 디차원 컨텍스트 벡터는 이 컨텍스트 벡터 안에 유저와 컨텍스트에 대한 피쳐가 사용됩니다. 뭐 추첨 문제의 예시를 들자면 이 아이템을 노출시킬 유저의 성별이나 연령 혹은 노출되는 시간적 공간적 정보 등이 다 이 컨텍스트 벡터 안에 포함되야겠죠 또한 선택돼야 되는 그 액션 그 아이템의 부가적인 정보도 이 컨텍스트 벡터 안에 들어갈 수 있습니다. 그리고 이 sta는 액션별로 가지는 하나의 파라미터인데요. 이 xt 콤마 a가 d 차원으로 이루어져 있고 그 차원과 같은 d 차원을 갖는 학습 파라미터입니다. 그래서 이 학습 파라미터는 액션별로 데이터가 계속 수집되면서 업데이트가 되고 이 셋탑 에를 계속해서 업데이트해서 이 룬 뉴시비 알고리즘이 점점 더 정교해지는 것입니다. 그리고 이 da는 각 액션별로 이미 관측된 학습 데이터가 담겨 있는 데이터 매트릭스를 의미합니다. 그래서 현재 타임 스텝을 기준으로 총 m개의 액션 데이터가 수집되었다면 m 곱하기 디 행렬로 이루어진 데이터 매트릭스입니다. 자 다음에는 수식을 살펴봅시다. 먼저 이 앞의 부분은 익스펙티드 리워드인데요. 주어진 컨텍스트 상황에서 내가 액션 a를 선택했을 때 얻게 되는 리워드를 의미합니다. 그리고 이 뒤에 있는 부분은 그 액션에 대한 익스플로레이션을 추가해 주는 부분입니다. 시간이 지남에 따라서 학습 데이터가 많이 수집되면서 이 뒤에 있는 익스플로레이션의 텀은 점점 작아지게 되고 최종적으로 익스펙티드 리워드에 수렴하게 되면서 그 익스펙티드 리워드가 가장 높은 액션을 선택하는 쪽으로 익스플로이테이션이 진행됩니다. 자 그럼 다음 예시를 통해 컨텍스트 벡터와 각 액션별 파라미터가 어떻게 학습되고 어떤 의미를 가지는지 살펴봅시다. 먼저 다음과 같은 3명의 유저가 있고 이 유저는 각각 서로 다른 컨텍스트 벡터를 가지는데요. 여기서 컨텍스트 벡터는 총 4차원으로 이루어져 있고 각각의 차원은 다음과 같은 의미를 갖습니다. 여기에 있는 첫 번째 유저는 남자면서 올드하다는 피처를 가지고 있고요 두 번째 유저는 여자면서 영하다는 형식으로 컨텍스트 벡터가 구성되어 있죠. 그리고 세 번째 유저는 여자면서 올드하다는 피처를 가집니다. 이제 이 유저들이 어떤 아이템을 선택하느냐가 mab 문제에서 추천이 대상 되는 방식인데요. 추천이 되는 방식인데요. 그래서 우리는 이 각각의 아이템 즉 각각의 액션에 대해서 그 파라미터를 학습해야 합니다. 그래서 다음과 같이 첫 번째 유저는 빵이라는 아이템을 소비하였고 선호하였고 두 번째 유저는 스파게티와 맥주를 선호하였고 세 번째 유저는 스파게티와 빵을 선호했다고 봅시다. 그럼 이 첫 번째 액션에 해당하는 데이터는 총 2명에게 수집되었기 때문에 각각의 데이터 매트릭스가 업데이트 되고요. 두 번째 액션도 첫 번째 유저와 세 번째 유저에게 소비되었기 때문에 각각의 컨텍스트 벡터가 추가되어서 데이터 매트릭스를 이루게 됩니다. 그리고 마지막 세 번째 아이템은 두 번째 유저에게만 소비되었기 때문에 데이터 매트릭스는 하나만 업데이트 되게 됩니다. 이렇게 해서 학습 과정을 통하여서 최종적으로는 이렇게 컨텍스트 벡터와 같은 차원의 각각의 액션에 대한 학습 파라미터가 구해지게 되는데요. 이제 그 학습 파라미터를 구하는 방식을 자세히 설명하기보다는 그 결과를 보면서 어떤 의미를 가지는지 살펴봅시다. 이 첫 번째 파라미터는 두 번째 차원이 가장 웨이트가 높은데요. 이 두 번째 차원은 피메일이라는 의미를 가집니다. 즉 이 액션은 여자한테 더 높게 추천될 확률이 즉 여자가 더 좋아할 만한 액션이라는 것이고요. 이 두 번째 아이템은 올드하다는 차원의 값이 제일 높습니다. 즉 올드한 유저한테 더 높은 리워드를 줄 거라고 이 모델이 학습했다는 것이죠. 이제 마지막은 세 번째인 0 어리다는 차원이 가장 값이 높은데요. 즉 어린 사용자 에게 이 아이템이 줄 수 있는 리워드가 가장 크다고 학습을 한 것이죠. 그래서 다음과 같이 각각의 유저의 컨텍스트 벡터 즉 각각의 유저가 가진 특징에 따라서 어떤 아이템이 더 높은 리워드를 줄지 그 리워드는 곧 클릭, 어떤 아이템이 더 클릭률이 높을지 결국에는 CTR이 가장 높은 아이템이 각각의 유저에게 적합하게 추천되도록 1인 유시비가 학습됩니다. 마지막으로 이 ln 뉴시비의 학습이 이루어지는 수도 코드입니다. 전체를 다 살펴보지는 않고요. 중요한 부분만 살펴보면 이 매번 타임 스텝마다 아까 수식으로 봤던 이 PTA 즉 이 값이 가장 높은 아그맥스에 피티에 이 값이 가장 높은 액션이 선택돼서 그 액션으로 노출을 시키고 그로 인해 얻는 리워드 값을 가지고 이 에와 비를 업데이트해 줍니다. 그리고 이 a와 b는 결국에 우리가 학습해야 될 이 세타 를 업데이트하는 데 사용됩니다. 그래서 시간을 계속 지나면서 세터가 계속 업데이트가 되고 결국에는 데이터 매트릭스가 커지면서 이 값은 점점 0에 가까워지고 최종적으로 익스펙티드 리워드가 가장 높은 액션이 해당 아이템이 사용자에게 제공되는 것입니다. 마지막으로 컨텍스추얼 밴딧이 사용되는 예시를 살펴봅시다. 컨텍스추얼 밴딧을 추천 시스템에 사용한다고 했을 때 밴딧의 액션 하나하나가 아이템이 된다고 말씀을 드렸는데요. 실제 추천 시스템에서는 아이템이 너무 많은 경우 즉 후보가 너무 많은 경우에는 각각의 아이템별로 밴딧의 파라미터가 학습되기 어렵습니다. 뭐 아이템이 수십만 개 수백만 개 있을 때 그 아이템 하나하나 별로 파라미터를 학습해야 하는데 다소 무리가 있기 때문이죠. 따라서 이 네이버 에어스 추천 시스템에서는 먼저 인기도 기반 필터링을 통해서 탐색해야 되는 대상, 즉 추천해야 되는 후보 리스트를 수천 개로 줄인 다음에 이 수천 개의 아이템에 대해서만 컨텍스트 벤딧 알고리즘을 사용해서 최종적으로 유저의 취향에 맞는 추천을 제공했다고 합니다. 네 이상 mab에 대한 10강 강의를 모두 마쳤습니다. 10개 강의 모두 수강하시느라 정말 수고 많으셨습니다. 감사합니다."
}