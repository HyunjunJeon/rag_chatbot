{
  "lecture_name": "NLP Recent Trends Part 4_Knowledge Update",
  "source_file": "NLP Recent Trends Part 4_Knowledge Update_81.mp4_2025-12-04-104002933.json",
  "text": "이전 강의에서는 저희가 LNML 중요한 약점 중 하나였던 뭔가 할루시네이션이라든가 디바이어싱 혹은 톡시티 뭐 그런 얘기를 했었는데요. 이번 강의에서는 LLM의 또 다른 약점인 그 옛날 정보들을 어떻게 갱신할까 즉 논리적 업데이트에 대한 이야기를 좀 해보도록 하겠습니다. 그래서 우선 첫 번째로 이러한 논리적 업데이트가 없어 가지고 LLM 자체가 되게 그 옛날 사전 학습 데이터로 학습되어 있기 때문에 발생하는 원 템퍼런 미소라인먼트에 대해서 학습해 보고요. 그리고 그걸 완화하기 위한 두 가지 방법으로 컨티뉴얼 러닝과 레그 그러니까 리트리벌 어그맨티드 제너레이션에 대해서 좀 얘기해 보도록 하겠습니다. 자 첫 번째로 우리가 이 LLM이 새로운 지식을 배우지 못해서 발생하는 문제 중 하나인 이 템퍼럴 미스 얼라이먼트 자체를 좀 정의하고 이것이 어떻게 측정되는가에 대해서 얘기해 보도록 하겠습니다. 자 우선 LLM 자체는 여러분들이 챗지피티 같은 친구들과 한번 얘기해 보면 알겠지만 최신 정보를 모른다는 것을 아마도 여러분들도 느꼈을 겁니다. 그래서 이것 자체가 사실상 고정된 데이터에 대해서 학습된 모델이고요. 그래서 만약 여러분들이 챗gpt한테 어제 일어난 일에 대해서 알려줘라고 물어보게 된다면 뭐 이런 대답을 받을 거예요 s AR 랭귀지 모델 난 AI 랭귀지 모델로서 나 랭귀지 나 놀리지는 10월 21일에 멈춰져 있어 아마 지금 최신 모델은 아마 23년 어느 중반 정도로 지금 나온 걸로 기억하는데 뭐 여하튼 어제 일어난 일이라든가 뭐 최신 최근에 일어난 일 같은 경우에는 자기 자신이 모른다라고 고백하고 말을 시작하는 것을 확인해 볼 수가 있습니다. 그래서 결국에는 이것 자체가 뭔가 시간에 따라 변하는 그러한 지식을 학습하지 못한다는 그런 문제점이 존재하죠. 그래서 어 결국에는 시간에 따라 변하는 지식을 질문했을 때 틀린 대답을 할 가능성이 높습니다. 예를 들어 어 지금 그 코비드 19 그러니까 코로나 바이러스의 가장 도미넌트한 종이 무엇인가라고 물어보게 된다면 지피티 같은 경우에는 결국에는 자기 자신의 시간이 2021년 10월에 멈춰 있기 때문에 지금 델타 베리언트가 지금 가장 유효하다 가장 활성화돼 있다 뭐 그런 얘기를 합니다. 하지만 실제로 지금은 좀 다르죠 지금은 델타 말고 오메가라든가 어 그러한 더 다른 변종들이 존재하기 때문에 실제로 이 챗gpt 자체가 그러한 옛날 기억에 기반해서 대답을 한다는 것을 확인해 볼 수가 있습니다. 비슷하게 어 뭔가 저희가 컴플리션 모델로 여러분들이 플레이 오픈 AI 플레이 그라운드에서 실험을 좀 해보시면 코비드 배리언트를 물어보게 된다면 델타 베리언트가 나올 때도 있고 뭔가 알파 베리언트가 나올 때도 있고 지금 미국의 대통령이 누구시냐 물어보게 된다면 바라 오마바가 나온 것을 확인해 볼 수 있고 뭔가 현재 한국의 대통령을 물어본다면 문재인이 나온다는 것을 확인해 볼 수가 있습니다. 즉 랭귀지 모델 자체가 결국에는 자기 자신의 지식이 사전 그 옛날 지식에 머물러 있기 때문에 이렇게 뭔가 현재 대통령이라든가 아니면 현재 유행하고 있는 바이러스 증후 이런 걸 물어보게 된다면 매우 틀린 틀린 대답을 한다는 것을 확인해 볼 수가 있습니다. 이것이 실제로 그래서 과거 데이터로 학습된 모델이 이제 현재 정보를 모른다는 것을 알았는데 실제로 이것 자체가 정확도에 영향을 끼칠까에 대해서 얘기를 해볼까 합니다. 저희도 한번 실험을 정상적으로 확실하게 옛날 모델 지금 모델 자체가 옛날 지식에 기반하고 있으니까 틀린 대답을 하고 있다는 걸 확인을 해서 볼 수가 있는데 이것 자체를 정량적으로 측정하는 방법에 대해서 얘기를 하는 거예요. 어 이런 실험을 세팅을 해보도록 합시다. 저희가 어떤 랭귀지 모델이 있을 때 실험군으로 저희가 2017년 9월까지 학습한 학습 데이터 자료 그러니까 이러한 2017년 9월까지의 뭔가 랭귀지 텍스트로 뭔가 랭귀지 모델을 학습시키고 반대로 대조군으로 2019년까지 학습된 랭귀지 모델 그래가지고 뭔가 학습 데이터를 자료를 만들어 가지고 저희가 랭귀지 모델을 이제 트랜스포머 엑셀 모델에 대해서 2개를 학습시키는 겁니다. 그래서 이렇게 실험군하고 대조군 해가지고 랭귀지 모델을 하나는 2017년까지 하나는 2019년까지 한 다음에 아 평가 데이터로 2018년에서 2019년 다음 사이의 텍스트를 통해 가지고 저희가 이것을 저희가 퍼플렉시티를 재볼 수가 있겠죠. 그래서 이제 대조군 대비 실용분을 비교해 가지고 실제로 아 이러한 퍼플렉시티가 얼마나 떨어졌는지 비교를 하게 된다면 이렇게 시간 간격으로 인한 뭔가 이런 미s라먼트가 어떻게 얼마나 발생했는지를 정량적으로 측정하는 것이 가능할 거예요. 그래서 실제로 보시면 알겠지만 지금 테스트 먼스가 보시면 지금 다양한 데이터셋에 대해서 비교를 한 거거든요. 그래서 지금 보시면 확실하게 모든 데이터셋에서 확실한 시간이 지나면서 원래 우리가 비교를 했을 때 만약 2017년까지 학습된 모델로 저희가 퍼플렉시를 재게 된다면 릴레이티브하게 퍼플렉시가 증가되는 정도로 비교하면 확실하게 시간이 지날수록 테스트 한 번스가 더 과거로 가면 더 미래로 갈수록 그러니까 시간 간격이 크면 클수록 이러한 미스라이먼트가 증가한다는 것을 확인해 볼 수가 있습니다. 그래서 실제로 이제 실험군 모델 같은 경우에는 학습된 기간과 평가 기간이 멀어질수록 대조군 대비 퍼플렉시가 증가한다는 것을 확인해 볼 수가 있으며 따라서 저희가 이 템플롤 미스라이먼트 자체 그러니까 이런 시기에 따른 이런 그 문제 자체가 되게 이 기간이 멀어질수록 되게 틀어지고 그리고 이것 자체가 계속 문제가 계속 점점 커진다는 것을 이렇게 정량적으로 확인해 볼 수가 있습니다. 단순하게 생각하는 사람들도 있을 거예요. 아 그렇다면 그냥 최신 데이터를 새로 학습시키면 되는 거 아닌가 그러니까 아 2017년까지 데이터를 알고 있으니까 그냥 2018년 2019년 데이터를 그냥 추가적으로 그냥 랭귀지 모델에다가 이것만 그냥 따로 학습시키면 되는 거 아닌가라고 생각할 수가 있는데요. 이거 같은 경우에는 조금 치명적인 문제가 있습니다. 아마도 이건 딥러닝 심층 학습 쪽 연구 좀 공부를 해보신 분이라면 한 번쯤 들어봤을 문제일 것 같은데 이 카타스트로픽 볼게팅 문제가 발생합니다. 어 이게 무엇이냐면 단순하게 어 여러 테스크를 순차적으로 배울 경우에는 앞선 테스크에 학습된 정보를 심각하게 까먹는 문제가 이 딥러닝 문제에서 아주 큰 문제 중 하나입니다. 예를 들어 본다면 저희가 멀티모달 모델이 있다고 합시다. 여기서는 정확히 라바 모델인데요 이런 라바 멀티모델 모델이 시파텐이라는 데이터 셋의 이미지 분류해서 뭔가 학습한 다음에 실제로 보시면 이 프롬프트를 넣어 가지고 멀티 모달 프롬프트 모델이기 때문에 결국엔 이미지 텍스트를 넣고 이미지를 넣은 다음에 이거에 대해서 텍스트를 뽑아내는 거거든요. 그래서 어 이러한 텍스트를 이렇게 파인튜닝 한 거예요. 그래서 이것을 시파텐이라는 데이터에 대해서 뭔가 학습한 다음에 저희가 만약 엠리스트 데이터셋에 대해서 파인튜닝을 엠리스트 데이터 셋은 들어보신 분들 좀 많죠 이것을 저희가 실제로 숫자 분류하는 데이터 셋이죠 아마도 여러분들 다 아실 거라고 생각합니다. 그래서 이걸로 추가로 파인튜닝 하게 된다면 어 만약 이렇게 저희가 따로 조절하지 않고 그냥 시파텐 학습시키고 그다음에 앱리스트 학습시키게 된다면 시파텐 데이터 셋이 다 까먹고 더 에어플레이 스8 해가지고 완전 틀린 대답을 내뱉았다는 것을 확인해 볼 수가 있습니다. 즉 이렇게 과거의 데이터를 뭔가 뭔가 그거에 대해서 레귤라이저라든가 과거의 지식 정보 자체를 뭔가 지키는 그러한 조치를 취하지 않고 단순히 다음 데이터 혹은 뭐 다음 시기의 데이터 그런 것들을 추가적으로 학습시키게 된다면 이전에 기억했던 그러한 모델에 대해 정보들이 많이 지워져 가지고 이런 것들을 잘 활용하지 못하는 그런 문제가 발생하게 되고 이것이 카타스트로픽 포게팅 그러니까 한국어로는 파괴적 망각 현상이 발생하게 됩니다. 자 결국에는 저희가 랭귀지 모델로 뭔가 이렇게 계속 지속적으로 이어지는 그러한 데이터들이 이어질 때 결국에는 저희는 단순히 최신 지식만 학습시키는 게 중요게 아니고 결국에는 이 과거 정보를 기억하는 것도 되게 중요할 겁니다. 즉 유용하면서 변하지 않는 정보 자체는 기억해야 된다는 거죠. 예를 들어 뭔가 역사적 사실이라든가 과학적 사실들 뭐 1 더하기 1은 2다. 뭐 월식은 뭔가 달의 지구 달이 지구 그림자에 가려지는 과학 현상이다. 뭐 이런 과학적 사실 코로나는 2019년에 발생했다. 대한민국 제1 대통령은 이승명 이승만이다 해가지고 현재 대통령이 바뀌더라도 제1대 대통령 자체는 바뀌지 않죠. 그렇기 때문에 이러한 역사적 사실이나 과학적 사실들은 잊지 않고 새로 들어온 지식들에 대해서 갱신시킨 랭귀지 모델을 갱신시키는 방법론이 필요하게 됩니다. 이러한 카타스트로피 볼게팅을 해결할 수 있는 방법론이 크게 보면 매우 몇 가지가 존재하는데 대표적인 것 두 가지만 얘기한다면 어 컨티뉴얼 러닝과 이 레그 어트리벌 어그맨티드 제너레이션이 존재합니다. 그래서 컨티뉴얼 러닝 같은 경우에는 결국에는 과거 정보를 까먹지 않으면서 LLM 자체를 추가로 학습시키는 방법이라고 생각하면 좋을 것 같고요. 두 번째로 레그 리트리벌 어그멘티드 제너레이션 같은 경우에는 프롬프트로 엘엠에 추가 정보를 주자 그러니까 학습하지 않고 추가 정보를 줘 가지고 하는 방법론이다 생각하시면 좋을 것 같습니다. 그중 첫 번째 방법론이 이 컨티뉴얼 러닝에 대해서 조금 얘기해 보도록 하겠습니다. 우선 이 컨티뉴 러닝 방법론 같은 경우에 단순히 랭귀지 모델에만 한정된 이야기합니다. 되게 예전부터 많이 학습 연구되었던 방법론이거든요. 그래서 단순히 뭐 이미지 이미지 클래시피케이션이라든가 그러니까 분류라든가 혹은 뭐 다른 뭔가 비전 테스크뿐만 아니고 뭔가 타임 시리즈 이런 곳에서도 되게 컨티뉴얼 러닝 방법론이 되게 많이 나왔고 이런 이러한 방법론들을 랭귀지 모델 그러니까 자연어 처리에서도 작용하면 되는 거 아닌가라는 것이 기본적인 아이디어라고 생각하시면 좋을 것 같아요. 그래서 결국에는 이전 학습 데이터를 까먹지 않으면서 새로운 과제 새로운 데이터에 옵티미제이션 최적화 그러니까 학습을 해보자라는 것이 기본 아이디어라고 생각하면 좋을 것 같아요. 이 컨티뉴얼 러닝 같은 경우에는 크게 보면 두 가지 테스크가 있는데요. 첫 번째는 뭔가 도메인 인크리멘탈 해가지고 도메인 자체가 증가하는 방향 그러니까 첫 번째는 뭔가 바이오 메디컬라는 텍스트를 뭔가 학습시키고 그러니까 생의학적인 뭐 그러한 텍스트를 학습시키고 그다음에 컴퓨터 사이언스 학습시키고 머테리얼 사이언스 비디스 물리 이런 걸 학습시켰을 때 이것을 순차적으로 그러니까 한 번에 섞어서 학습시키지 않고 이러한 데이터들 다양한 도메인의 데이터를 순차적으로 학습시켰을 때 그 이후에 나온 모델들이 모든 도메인에 대해서 다 정보를 기억하고 있게 하고 싶다라는 것이 이 컨티뉴얼 러닝에 적용할 수 있는 한 가지 방법 중 하나예요. 결국에는 이 새로운 도메인에 대해서 새롭게 컨티뉴얼 러닝하는 방법 이러한 방향이 하나가 있고요. 두 번째 방향 같은 경우에는 뭔가 시간 저희가 지금 하고 싶은 것처럼 시간에 따라 변하는 데이터에 대해서 컨티널 러닝을 하는 방법론입니다. 그래서 이렇게 뭔가 2014년 2016년 2018년 2020년 데이터가 있을 때 순차적으로 이렇게 학습시켜 가지고 결국에는 2014년 데이터도 까먹지 않고 2020년 데이터도 동시에 하고 싶다 이런 것이 이 컨티널 러닝에서 해결하고자 하는 그러한 문제 중 하나입니다. 그래서 이거 두 개 모두 다 사실 컨티널 러닝으로 해결하고자 하는 그러한 테스크 2개라고 생각하시면 좋을 것 같고요. 방법론 자체는 두 개 다 좀 비슷비슷한 걸 씁니다. 그래서 이 컨티뉴얼 러닝의 대표적인 세부 방법론으로는 이렇게 세 가지가 있습니다. 첫 번째가 레귤러라이제이션 메소드, 두 번째가 파라미터 익스펜션 메소드, 세 번째가 리허설 메소드입니다. 그래서 간략하게 소개하자면 레귤러라이제이션 메서드 그니까 이건 규제 방법이라는 뜻이죠. 듀라이제이션이 규제라는 뜻이죠. 그래서 이거 같은 경우에 매개 변수를 덜 바꾸자 두 번째는 파라미터 익스펜션 메소드는 이 추가 매개 변수를 학습시키자 조금 다릅니다. 매개 변수를 덜 바꾸는 거하고 추가 매개 변수를 학습시키는 것은 나중에 설명드리겠지만 조금 미묘하게 달라 네 세 번째 리허설 메소드 같은 경우에는 과거 데이터를 그냥 까먹으니까 과거 데이터도 그냥 추가로 학습시키면 되는 거 아닌가 뭐 그런 방법론이라고 그냥 기초적으로 알고 있으면 좋을 것 같네요. 자 우선 그 첫 번째 방법인 레글라이제이션 메서드 그러니까 규제를 사용한 방법론을 얘기해 보도록 합시다. 어 한 줄로 요약하면 결국에는 4개의 변수를 덜 바꾸자라는 것이 기본 아이디어예요. 간단하죠. 그냥 모델 자체가 우리가 랭귀지 모델에서 아 그 랭귀지 모델에 있는 파라미터들 뭔가 MMP 레이어에 있는 뭔가 그러한 웨이트 매트릭스라든가 뭔가 어텐션 매트릭스에 뭔가 키 쿼리 밸류 있잖아요 키 쿼리 밸류 이런 거에 웨이트 매트릭스 이런 거 자체가 사실상 어느 정도 이 ln 자체의 정보를 나타낸다라고 생각해 볼 수가 있을 겁니다. 다르게 말하면 이 매개 변수 자체를 좀 덜 바꾸면 덜 까먹지 않을까가 기본적인 개념이라고 생각하면 어떨까 싶어요. 그래서 결국에는 우리가 새로운 데이터에 대해서 뭔가 모델을 학습시킬 때 이 기존 매개 변수 방금 전에 봤던 엠에피 레이어의 웨이트 매트릭스라든가 아니면 샘 키쿼리 밸류 밸류에 뭔가 그러한 웨이트 매트릭스라든가 그것을 크게 차이 나지 않도록 규제한다라고 생각하면 좋을 것 같습니다. 가장 쉬운 것은 뭔가 기존 매개 변수와 엘투 거리를 최소화하면서 학습시키는 거겠죠. 그러니까 이것 자체가 결국에는 웨이트 매트릭스 자체가 어떤 행렬이 나올 건데 이 행렬 자체가 그냥 어 우리가 그 모델에다가 새로운 텍스트를 넣어 가지고 학습시킨다. 커즈 랭귀지 모델링으로 학습시킨 다음에 이 웨이트 매트리스가 변할 거예요. 하지만 이것 자체가 기존에 있는 거에서 엘투 거리만큼 그러니까 단순히 그냥 엘투 해 가지고 기존 더블 기존 더블 제로 하고 그리고 학습되고 있는 더블하고 이거 2개 자체를 미니마이저 하는 방향으로 하게 된다면 더블 제로 더블 우리가 학습시키고 있는 이 웨이트 매트릭스 자체가 기존에 있는 이 더블제로하고 크게 차이 나지 않게 될 겁니다. 기본적으로는 기본 이런 아이디어라고 생각하면 좋을 것 같은데 사실 엘2가 그렇게 잘 작동되지 않아요 밑에 보시는 이러한 비교를 보시면 이게 왜 엘2가 좀 작동이 안 될 수 있는가에 대해서 좀 이해가 되지 않을까 싶습니다. 그래서 실제로 저희가 원래 우리가 테스크 이건 그 타이밍 인크리멘탈은 아니고 테스트 인크리멘탈 그러니까 도메인 크리멘탈 새로운 도메인들이 다 순차적으로 들어왔을 때 모든 걸 다 잘하고자 그러한 아이디어에서 나왔던 그쪽 계열의 논문이긴 한데 그림 자체가 되게 좋아 가지고 제가 들고 왔어요. 그래서 보시면 알겠지만 맨 처음에 우리가 어떤 테스크에 가지고 모델을 학습시켰다 하게 된다면 이렇게 이 테스크 a에 대한 분포 도대체 이 테스크 a에 대한 분포가 무엇이냐 하면 되게 애매한 부분이 있죠. 근데 뭐 간단하게 생각하면 되게 좋을 것 같아요. 그냥 세이트 테스크에 같은 경우엔 앞서 봤던 예시를 생각한다면 뭔가 시바텐이라든가 시바텐의 분포 테스크 비바은 같은 경우에는 뭐 앱리스트 이런 분포라고 생각하시면 좋을 것 같아요. 그리고 테스크의 분포라는 단어가 좀 그 개념적으로 감이 잘 안 잡힐 수 있는데 뭔가 수학적인 개념이라기보다는 그냥 우선 추상적인 개념이라고 생각하면 좋을 것 같아요. 이러한 분포가 테스크에 대한 분포가 이렇게 들어가 있고 그 테스크를 풀 수 있는 파라미터가 여기저기 주어져 있다라고 합시다. 그리고 테스크 b를 푸는 분포 테스크 b의 분포 같은 경우에는 이렇게 그래 그래프가 나왔다고 합시다 하게 된다면 사실은 우리가 만약 a에서 b로 갈 때 그냥 패널티 없이 그냥 a를 바로 b로 비에에서 학습된 모델을 저희가 비에 대해서 테스크 비에 대해서 그냥 학습을 시킨다. 추가 추가적으로 컨티널 러닝 쓰지 않고 바로 파인튜닝을 하게 된다면 그냥 이것이 여기로 넘어가게 되니까 원래 있던 것들을 까먹게 되는 그러한 문제가 발생할 겁니다. 네 그럼 단순히 그냥 l2 같은 걸로 해 가지고 덜 배하면 되는 거 아닌가 생각이 들 수 있는데요. 그거 같은 경우에는 결국에는 기존에 있는 가중치랑 덜 멀어지게 하는 것이 목적이었기 때문에 이것이 지금 보시면 여기서 덜 멀어지면서 비로 가까이 가려다 보니까 되게 이도 저도 아닌 그러한 곳에 빠질 수도 있다라고 얘기를 합니다. 이 논문에서 그렇다면 여기서 결국에 하고 싶은 것은 에 테스크하고 비 테스크하고 이것을 동시에 잘 할 수 있는 그런 메소드가 필요하겠죠. 실제로 필요한 위치는 이 부분 실제로 저희가 학습이 학습해 가지고 가고자 하는 위치는 아마도 둘 다 잘하는 위치는 이 부분일 겁니다. 그래서 이 부분으로 가기 위한 그러니까 에랑 비 테스크를 단순히 이것을 파라미터를 잘 조절해 가지고 그걸 동시에 잘할 수 있는 그 가중치를 찾는 것 연구가 이 레글라이저의 메소드 부분의 목적이라고 생각하시면 좋을 것 같네요. 그래서 이거의 반복 언론 같은 경우에는 이 논문 이 그림이 나온 WC 엘라스틱 웨이드 콘솔리데이션 해가지고 WC라는 논문이 존재하는데 이것 자체는 제가 좀 설명드리기에는 좀 양이 많은 것 같고 뭐 이런 것을 목적으로 했다 그렇게 뭔가 두 개의 테스크를 다 풀기 위해서 디자인했다라고 생각하시면 좋을 것 같고요. 또 다른 유명한 방법론으로는 레카d 해가지고 아담 자체는 여러분들이 다 아실 거예요. 그러니까 그 아담 오티마이저죠. 결국에는 더블 웨이트 매트릭스에서 웨이트 매트리스를 업데이트시키는 그런 방법론인데 원래 하다면 이렇게 학습됐는데 뭔가 되게 엄청 복잡한 수식이 적혀 있죠. 근데 이걸 다 설명드리진 않을 거고요. 결국에 하고 싶은 건 뭐냐 이 밑에 이 부분이 추가되는데 이 부분 자체가 사실상 어느 정도 뭔가 엘투걸이라든가 이러한 거리를 미니마이즈 하면서 즉 기존에 있는 가중치하고 지금 보시면 알겠지만 이것 자체가 원래 더블 섹타 스텍사스타가 기존에 있던 초기 가중치이고 그리고 그거와 지금 학습시키고 있는데 자체를 지금 이거의 차를 구해 가지고 이것을 뭔가 스케일링 한 다음에 이것을 뭔가 좀 미니마이즈 한 형태로 해가지고 아담 옵티마이징 과정에 이러한 좀 덜 변하도록 하는 그런 레클라이저가 들어간 레글라이저 규제가 들어간 그러한 옵티마이제이션 방법론이라고 생각하시면 좋을 것 같습니다. 그래서 이걸 뭐 복잡하게 얘기하는 것보다는 그러한 방법론 그러니까 옵티메이제이션 그러니까 최적화 과정에서 뭔가 이런 것을 제약을 들어 가지고 뭔가 덜 바꾸게 하자라는 방법론도 이러한 레귤라이제이션 메소드의 한 종류다 라고 생각하시면 좋을 것 같네요. 두 번째 방법은 파라미터 익스펜션 메소드인데 방금 전에 했던 그 방법론 같은 경우에는 원래 있던 웨이트 매트릭스 그러니까 그 랭귀지 모델에서 엠엘피 레이어라든가 혹은 뭐 키쿼리 밸류 이런 것을 좀 덜 바꾸자라는 기본 아이디였는데 그러지 말고 이것 자체를 그냥 고정시킨 다음에 이 파라미터 파라미터 익스펜션 메소드 그냥 기억하면 매개 변수 확장 방법이죠. 즉 그냥 여기다가 추가로 뭔가 매개 변수를 달아 가지고 이것을 이 추가 매개 변수만 학습시키다라는 방법론이라고 생각하면 좋을 것 같아요. 이 대표적인 방법론은 아마도 여러분들이 한 번쯤 들어봤을 메소드인 이 로라라는 게 존재할 겁니다. 준말이긴 하죠. 이게 로 랭크 어댑테이션 준말이죠. 그래서 이거 같은 경우에는 아마 여러분들이 생성 모델 쪽 강의에서 아마 들었을 겁니다. 그래서 뭔가 가중 사전 학습 가중치 그러니까 프리트레인드 웨이트가 있을 때 이것 자체를 풀 파인튜닝 하지 않고 랭크가 낮은 로랭크 백크라이제이션을 통해 가지고 이 로크 로랭크 자체를 학습시키자 그래서 에 매트릭스하고 비 매트릭스가 있고 이것이 랭크가 되게 작죠 이게 아마도 제 기억이 맞다면 4 정도 2에서 4 정도가 됐던 걸로 기억하는데 그래서 이것을 저희가 로우 랭크 이렇게 학습 이것을 두 개가 그 매트릭스 멀티플리케이션을 하게 된다면 웨이트랑 똑같은 뭔가 행렬이 나오게 되고 다만 랭크가 좀 낫겠죠 그래서 이것 자체를 학습시킴으로써 뭔가 모델 전체를 학습시키지 않고 일부분만 학습시키는 것이 가능하다라는 것을 아마도 여러분들도 배우셨을 겁니다. 근데 사실 이 파라미터 익스펜션 메소드는 로라만 있는 건 아니에요. 어 또 대표적으로는 어댑터 어댑터 방법론이 있고요. 어댑터 방법론 같은 경우에는 간단해요. 그냥 우리가 MLP 레이어 있잖아요 MLP 레이어라든가 트랜스포머 블록들이 있잖아요 실제로 여기가 셀프 어텐션 블록이 있고 그리고 그 위에는 MLP 레이어 멀티 헤드 어텐션 셀프 어텐션 부분 위에 LLP 레이어가 존재하는데 그냥 이 사이에다가 어댑터라는 모듈 그냥 조그마한 그 보틀랙 구조에 그냥 뭐 두 개를 이렇게 끼워 놔 가지고 레이어마다 끼워놔서 이것을 학습시키는 뭐 그러한 파라미터 익스펜션 방법론도 존재하고요. 또 유명한 파라미터 익스펜션 방법론이라고 한다면 뭔가 프레픽스튜닝 프레픽스 튜닝이라든가 혹은 뭐 프롬프트 튜닝 뭐 피 튜닝이라든가 이런 친구들 프롬프트 튜닝 그래서 어 이렇게 레이어의 사이에 끼워 넣는다든가 웨이트 자체를 직접 조절하지 않고 뭔가 이렇게 토큰들이 쭉 있을 때 뭐 히든 리프레젠테이션이라고 해야 될까요? 그런 히든 리프레젠테이션 토큰들이 아 러뷰 유 어쩌고저쩌고 텐센테스가 있을 때 이 뒤에다가 혹은 앞에다가 뭔가 학습 가능한 임베딩을 붙여서 뭔가 이렇게 학습 가능한 임베딩을 한 10개에서 30개 정도 붙여가지고 이것을 그 모델에다 태운 다음에 학습 자체를 모델을 건드리지 않고 이 토큰들만 학습시키는 방법론들도 존재합니다. 그래서 이런 방법론이 대표적으로 freafic x튜닝in fromp t튜닝 피튜닝 이런 방법론이 존재하고요. 그래서 어떤 걸 써도 상관없는데 결국에는 하고자 하는 바는 결국에는 이렇게 추가 매개 변수 원래 있던 모델 자체를 건들지 않고 이렇게 추가된 모델을 학습시키게 된다면 모델을 뭔가 모델이 가지고 있는 이러한 기업들을 좀 지킬 수 있지 않을까라는 얘기입니다. 다만 이것 자체에 대해서는 사실 좀 논란이 있긴 해요. 어 그렇게 모델에다가 결국에는 로우 링크를 하더라도 대개 이것이 모델의 가중치에 직접 영향을 끼치는 것인데 이것 자체가 실제로 뭔가 이 모델의 파괴적 망각에 영향을 안 끼치는 게 맞느냐라는 의문이 들 수도 있고요. 그렇기 때문에 어 이것 자체는 뭔가 이것이 가설적으로는 뭔가 여러 가지 비판들이 많긴 하나 실제 실험적으로는 정성적으로가 정량적으로는 되게 잘 나오는 방법론이기 때문에 되게 많이 쓰이는 컨티뉴얼 방법론 중 하나라고 생각하시면 좋을 것 같습니다. 이런 뭐 특히 방금 전에 말했던 뭔가 로라 외에도 이렇게 어댑터를 활용해 가지고 k 어댑터라는 방법론도 존재해 가지고 이거 같은 경우에는 어댑터 자체는 그냥 파라미터 익스펜션 메소드지만 이것을 어떻게 하면 컨티뉴얼 러닝에 적용할 수 있을까에 대해서 고민하는 뭐 그런 방법론들도 존재합니다. 그래서 이렇게 k 어댑터 해가지고 여러 가지 도메 텍스트가 있을 때 이것 자체를 그 도메인을 그 테스크를 2개로 나눠 가지고 각각의 테스크에 대해서 뭔가 이렇게 어댑터를 학습시키는 뭐 그러한 학습 그러한 방법론이 존재한다 정도만 하고 얘기하고 넘어가면 어떨까 싶습니다. 자 그래서 앞서 봤던 방법론 같은 경우에 결국에는 매개 변수를 좀 덜 바꾸던가 아니면 매개 변수에서 뭔가 추가 매개 변수를 더 해 가지고 그 추가 매개 변수만 학습시키는 그런 방법론이었는데요 그러한 방법론 말고 단순하게 그냥 이 사전 학습 데이터 자체를 그냥 다시 리마인드 시켜 줌으로써 뭔가 모델 자체가 옛날 기억을 까먹지 않게끔 하는 방법론들도 존재할 겁니다. 이러한 방법론을 리허설 메소드라고 얘기를 하는데요. 결국에는 과거 학습 데이터를 학계 학사파사라고 생각하시면 좋을 것 같아요. 그래서 결국에는 특정 과제를 파인튜닝 할 때 해당 테스크 스페스픽한 데이터셋만 학습시키지 말고 그러니까 결국에는 우리가 랭귀지 모델이 있을 때 어 2017년까지 학습된 랭귀지 모델이 있을 때 2019년 모델 데이터를 그냥 이것만 학습시키지 말고 그냥 2017년 데이터도 들고 와 가수 다만 2017년 데이터 전체를 들고 오지 않고 적당하게 섞어 가지고 아예 전체를 들고 오게 된다면 사실상 2017년 데이터하고 2019년 데이터를 같이 합쳐 가지고 풀 파인튜닝 하는 거잖아요 그러지 말고 그냥 2017년에서 샘플링을 잘해서 우리가 이것을 표본을 뽑아가지고 대표적인 표본을 뽑아서 2019년 데이터랑 섞어 가지고 학습시키겠다가 이 리허설 메소드라고 생각하시면 좋을 것 같습니다. 대표적으로는 믹스 리뷰라든가 딥 제너레이션 리플레이 같은 디지알 같은 방법론이 존재하는데요. 그중 믹스 리뷰를 확인해 본다면 되게 간단합니다. 결국에는 파인 튜닝 할 때 사전 학습 데이터를 같이 학습시키겠다는 거예요. 그래서 믹스 리뷰의 로스 자체는 결국에는 타겟 텍스트에 대해서 파인 튜닝 하면서 아 간단해요 프리 트레인 데이터에 대해서 사전 학습도 같이 해서 믹스 레이큐만큼 섞어 가지고 학습시키겠다라고 생각하시면 좋을 것 같아요. 그래서 이것이 되게 간단한 방법론인 것 같은데요. 이것이 단순히 웨이트 dk라고 적혀 있는데 이거 앞서 얘기했던 레귤라이제이션 그러니까 규제 방법이랑 똑같습니다. 그냥 사전 학습 가중치에 l2 규제에서 초기 가중치랑 덜 변하게 끔 했던 그 방법론이랑 비교하게 된다면요. 단순히 과거 데이터를 학습시키는 것이 다른 방법론과 비교를 좀 해봐가지고 비교하게 된다면 되게 이것 자체가 되게 간단하지만 잘 작동한다는 것을 확인해 볼 수가 있습니다. 비교 대상으로는 저희가 이 웨이트 dk를 확인해 볼 건데요. 단순하게 그냥 사전 학습 가중치로 l2 규제를 하는 건데 그냥 파인튜닝 할 때 프리 트레인 데이터하고 지금 학습시키고 있는 가중치하고 엘트로스를 걸겠다. 즉 이것 자체는 앞쪽에서 봤던 그 레귤러라이제이션 방법론과 동일한 것을 확인해 볼 수가 있습니다. 그래서 이 웨이트 디케이 방법론과 비교하게 된다면 오른쪽 비디오가 이렇게 이 믹스 리뷰하고 웨이트 디케이 하고 그리고 그냥 단순하게 어 기존에 있는 데이터를 사용하지 않고 그냥 바로 학습시켰을 때 그 파인튜닝 데이터를 바로 학습시켰을 때를 비교했을 때인데요. 우선 기본적으로 이 씨씨 뉴스로 사전 학습을 하고 그다음에 이 다이올로그라는 데이터 셋으로 뭔가 파인튜닝한 테스크라고 생각하시면 좋을 것 같습니다. 그래서 여기 보이시는 점선이 이것이 실제로 그냥 이 지시 뉴스를 학습시키고 따로 뭔가 컨티뉴얼 러닝 방법론을 적용하지 않고 그냥 다이얼로그 데이터셋을 파인튜닝 한 결과고요. 그 결과를 확인해 보시면 확실하게 이렇게 포게팅이 이 시시뉴스에 대한 포게팅이 심하게 일어난다는 것을 확인해 볼 수가 있습니다. 그렇다면 웨이트 디케이 같은 경우에는 저희가 보시면 씨씨 뉴스가 이 7선으로 나타나 있는데 이것 역시 어 학습이 그 에폭수가 늘어나면 늘어날수록 이것이 점점 까먹는다는 것을 확인해 볼 수가 있습니다. 그에 비해서 믹스 리뷰를 하게 믹스 리뷰를 사용하게 된다면 지금 이 어 씨씨뉴스에에 대한 이러한 애널레 자체는 유지가 되면서 또 다이올로그하고 다이올로그에 대한 이러한 학습 자체는 되게 안정적으로 잘 학습되었다는 것을 확인해 볼 수 있습니다. 참고로 밑에 있는 점선 같은 경우에는 점선 자체가 방금 전에 말씀드렸죠 그냥 어 2017년 학습시키고 2019년을 그냥 바로 컨티을 러닝 쓰지 않고 학습시켰던 방법론이었죠. 그때의 경우에는 역시 포게팅과 동시에 이 다이올로그 데이터셋에 대해서 오버피팅이 난다는 것을 확인해 볼 수 있어 가지고 확실하게 두 방법 모두 확실하게 기존에 있는 이 나이브하게 학습시키는 것보다 더 좋지만 이렇게 리허설 방법론이 되게 성능이 이 웨이트 dk보다 더 좋았다는 것을 확인해 볼 수가 있습니다. 그래서 웨이트 dk는 파인튜닝 할수록 사전 학습 데이터의 NLL이 좀 커지는 경향이 보이지만 믹스 리뷰 같은 경우에는 그 파인튜닝 해도 사전 학습 데이터의 앤엘알 그러니까 negative 로스 negative 라이클리후드 로스가 유지가 된다는 것을 확인해 볼 수가 있습니다. 하지만 이 방법론 특히 믹스 리뷰 같은 경우에는 문제점이 하나 있는데 결국에는 이 사전 학습 데이터 프리 트레인 데이터 자체를 저희가 어딘가에서 들고 와야 된다는 문제점이 있죠. 근데 문제는 저희가 gpt2라든가 그러한 것은 모델은 공개 이런 모델들 혹은 뭐 라마 이런 모델들은 어 사전 학습 데이터가 공개되어 있지 않습니다. 모델은 공개되어 있어 가지고 파라미터 자체는 공개가 돼 있는데 사전 학습 데이터가 무엇으로 학습돼있다면 무엇으로 학습되어 있다만 알려져 있을 뿐 이것이 어떠한 데이터를 썼는지는 알려져 있지만 이거 데이터 자체가 공개되어 있지 않습니다. 그렇다면 이러한 경우에는 믹스 디비를 쓸 수 없느냐 하면 그렇진 않은데요. 그러한 것을 해결하는 방법론이 이 디지알이라고 생각하면 좋을 것 같아요. 이거 같은 경우에는 따로 슬라이드가 없긴 한데 어 좀 설명을 해드리자면 결국에는 어 엘램 자체가 2017년 데이터를 학습시켰으니까 그냥 2017년 대 2017년에 그러니까 자기 자신이 알고 있는 데이터 자체는 이미 기억하고 있는 상태입니다. 그렇기 때문에 단순하게 우리가 사전 학습 데이터를 직접 들고 오지 말고 그냥 엘엠에서 그냥 제너레이션 시켜 가지고 이것 자체가 사실상 과거 데이터 2017년 데이터니까 모델 자체를 어 어떤 옛날 모델들을 모델을 하나 고정시켜 놓고 고정시켜 놓고 그리고 모델 새로 학습시킬 때는 옛날 모델에서 하나 생성해 가지고 센텐스를 하나 만들고 그리고 우리가 타겟팅하는 데이터에서 센텐스 하나 만들고 해서 이거 2개를 믹스 리뷰를 한다라고 기본적으로 생각하시면 좋을 것 같아요. 이 경우에는 결국에는 랭귀지 모델 기존에 있는 랭귀지 모델을 가지고 뭔가 사전 학습 데이터를 모사하기 때문에 따로 사전 학습 데이터가 필요하지 않다라는 장점이 존재하고 이러한 아이디어가 좀 더 구체화된 것이 이 딥 제너레이티브 리플레이라고 생각하시면 좋을 것 같습니다. 네 그렇다면 이때까지 해서 세 가지의 컨티뉴얼 러닝 방법론을 비교해 보았는데요. 그럼 이런 생각이 들 겁니다. 그래서 도대체 어떤 컨티뉴얼 방법론이 좋냐라고 이 문이 드실 건데요. 실제로 이것을 좀 랭귀지 모델에서 비교한 실험들이 존재합니다. 해서 시기에 따라서 변한 데이터셋으로 이 앞서 나왔던 세 가지 방법론 컨티널 러닝 메소드를 비교한 논문이 존재하는데요. 그래서 이거는 간단하게 위키 데이터에서 했습니다. 여러분들이 아시는 그 위키피디아 같은 경우에는 어 이게 2017년 위키 피디하고 그리고 2019년 위키 피디하고 이것 자체가 기록 로그가 다 남아 있기 때문에 이거 2개를 비교함으로써 변환 정보하고 안 변한 정보를 저희가 확인하는 것이 가능하거든요. 그래 가지고 이것을 이 위키피디아 데이터로 뭔가 시기에 따라 변하는 데이터셋과 변하지 않는 데이터셋으로 뭔가 퍼플렉스를 재고 학습시키고 하는 것이 가능합니다. 그래서 이것을 좀 비교를 좀 해보자면 어 변한 정보로만 학습한 디프 그러니까 여기에 몇 가지 베이스 라인이 존재하는데 이니셜이 최근 데이터를 배우지 않은 거 지금 보시면 데이터가 지금 0809 0910 1011112가 있죠 이거 달인데요. 그러니까 7월달까지 학습된 모델이라고 생각하시면 좋을 것 같고요. 기본적으로 그리고 풀 파인 트윈 같은 경우에는 이 전체를 학습시킨 모델 그리고 df는 차이점만 그러니까 이 0809 하고 07과의 차이점 이거 2개를 비교해서 학습시킨 모델 이런 것이라고 생각하시면 좋을 것 같아요. 그래서 이 경우에는 확실하게 보시면 알겠지만 디프 같은 경우에는 그 학습 데이터 자체가 많지 않다 보니까 디프가 그렇게 막 학습 시간이 오래 걸리지 않는다는 것을 확인해 볼 수가 있지만 변하지 않은 종목 같은 경우에는 실제로 이니셜에 비해서 이렇게 퍼플리시티가 증가했다는 것을 확인해 볼 수가 있습니다. 즉 확실하게 카타스트로픽 볼게팅 그러니까 변한 정보 자체에 대해서는 이렇게 컴플렉시티가 많이 떨어졌다는 것을 확인해 볼 수가 있지만 변하지 않는 정보에서 이것이 카타스트로픽 볼게팅이 발생했다는 것을 확인해 볼 수가 있습니다. 그리고 이거랑 비교했을 때 단순히 디플 그러니까 이 차이점을 학습시킨 모델에 비해서 이 컨티뉴얼 러닝 메소드에서 앞쪽에서 얘기했었던 레크아담이라든가 믹스 리뷰 로라 케어댑터 같은 걸 비교하면 확실하게 변하지 않은 정보에는 이전 정보를 더 기억하면서 이니셜 대비 새로운 정보도 잘 학습했다는 것을 확인해 볼 수가 있습니다. 실제로 보시면 알겠지만 지금 df 같은 경우에는 거의 한 400대를 찍고 있는데 그 아래에 있는 예시를 보시면 거의 4배에 넘어가는 건 없죠. 실제로 이 UN 체인지 DM dam 모델의 데이터에 대해서 확실하게 df보다 더 잘 기억하고 있는 것을 확인해 볼 수 있으면서 그리고 어 체인지 된 체인지 된 그러니까 바뀐 정보 역시 새로운 정보도 역시 어 그렇게 나쁘지 않은 정도의 성능을 보이고 있다는 것을 확인해 볼 수가 있습니다. 그래서 실제로 이것이 이러한 컨티뉴얼 방법 러닝 방법론 같은 경우에는 사실 데이터 바이 데이터로 어떤 메소드가 가장 좋은지가 좀 다르긴 해요. 하지만 기본적으로 확실하게 단순히 새로운 데이터만 학습시키는 것보다는 그 확실하게 이 컨티뉴얼 러닝 메소드를 쓰는 것이 좀 더 안정적이고 이전에 있던 정보들을 덜 까먹는다는 것을 이렇게 실험적으로 조금 확인해 볼 수가 있습니다. 하지만 이 컨티뉴얼 러닝 같은 경우에는 한계가 존재하는데요. 사실 사실 이 한계 때문에 랭귀지 모델에서는 컨티뉴얼 러닝 자체가 잘 적용되지 않아요. 이 컨티널 러닝은 사실 랭귀지 모델에서는 좀 비주류인 좀 그러한 방법론 중 하나입니다. 왜냐하면 사실 어 컨티널 러닝 자체가 사실 완벽한 방법론이 아니기 때문이에요. 이것 자체가 사실 그때그때마다 학습시키는 거다 보니까 어 뭔가 하루마다 발생하는 일들을 위해서 매일 엘램을 뭔가 컨티널 러닝 한다는 것은 것은 사실상 불가능합니다. 사실상 매일매일 이 컨티뉴얼 러닝을 통해 가지고 새로운 정보를 파인튜닝 하는 것 자체가 되게 많은 자원하고 시간을 소모하기 때문에 사실상 좀 인피시블한 그런 상황이라는 것이죠. 그리고 만약 우리가 실제로 하루마다 이것을 컨티뉴얼 러닝을 한다 하더라도 어 그렇게 짧은 시간 간격을 두더라도 결국에는 이러한 템포럴 미스 얼라이먼트 자체는 피할 수가 없습니다. 예를 들어 뭔가 하루만에 학습 하루마다 데이터를 모아서 학습시킨다 하더라도 결국에는 1시간 전에 끝난 야구 경기 결과 자체는 저희가 얻을 수가 없겠죠. 그리고 또 하나 중요한 점이 저희가 사실 이 랭귀지 모델 같은 경우에는 단순하게 앞쪽에서 방금 던 그러한 컨티뉴얼 방법론은 사실 다 카우션 랭귀지 모델링 그러니까 다음 단어를 예측하는 그러니까 일반적인 라이브한 텍스트들이 존재하고 그러니까 플레이한 텍스트들이 존재하고 그래서 이 플레이한 플레인 텍스트들을 결국에는 이 다음 단어를 예측하는 모델로서 그냥 학습시키는 거기 때문에 결국에는 다음 단어를 맞추는 테스크에 대해서 컨티뉴얼 러닝을 하는 겁니다. 근데 실제로는 저희가 일반적으로 활용하는 엘레엠 같은 경우에는 여러분들이 쓰는 챗지피티라든가 아니면 뭔가 챗봇 형태로 파인 튜닝한 모델들은 결국에는 알치프나 디피오 뭔가 인스트럭션 튜닝 이런 걸 통해가지고 이미 파인 튜닝 된 그러니까 이런 인스트럭션 퀘스천 엔서 혹은 뭔가 유저 어시스턴트 이런 식으로 이미 파인튜닝 된 모델인데 이렇게 파인 튜닝 된 모델을 다시 컨티뉴얼 러닝 하는 것은 아 이것은 좀 예상치 못한 결과물로 이어질 수가 있습니다. 결국에는 우리는 컨티뉴얼 러닝 자체는 카우지 랭귀지 모델링으로 컨티널 러닝을 하는데 이걸로 이로 인해서 이러한 LLM 얼라이먼트 그러니까 RLHF라든가 dapo로 뭔가 우리가 맞춰놓은 프리퍼런스 이런 것들이 어떠한 영향을 끼칠까 같은 것이 명확하지 않은 부분이 있기 때문에 어 사실 이 컨티 러닝 자체를 뭔가 우리가 쓰고 있는 방법론에서 우리가 쓰고 있는 엘엘엠에서 이것을 함부로 적용하기에는 좀 쉽지 않은 난점들이 존재합니다. 그렇다면 여기서 드는 생각이 결국에는 이 모델 자체는 건드리지 않고 왜냐하면 저희가 얼라인먼트가 이미 맞춰져 있고 프리퍼런스가 맞춰져 있고 또 어차피 학습시켜도 진짜 최신 정보는 못 다루니까 모델을 학습시키지 않고 당장 방금 일어난 일 그러니까 1시간 전에 일어난 일 뉴스에 10분 전에 일어난 뉴스 이런 것들을 활용할 수 있는 방법이 있지 않을까라는 고민이 들게 됩니다. 그래서 이것을 해결할 수 있는 방법이 이 이제 설명한 이 레그 리트리벌 어그멘티드 제너레이션입니다. 이 리트리벌 어그멘티드 제너레이션 준말로는 레그라고 많이 이야기하죠. 라그라고도 하기도 하고 저는 레그라고 부르는데 이것 자체는 아마도 들어보신 분들이 있을지도 모르겠습니다. 그래서 엘엠 자체를 매번 학습시키지 않고 결국에는 외부 데이터를 활용해서 뭔가 모델 자체의 어 그 정보를 갱신해 보자라고 생각해 보시면 좋을 것 같습니다. 그래서 결국에는 간단합니다. 아이디어는 되게 간단해요. 어 텍스트를 생성할 때 그냥 외부 정보를 프롬프트로 제공하면 어떨까라는 것이 기본적인 아이디어라고 생각하시면 좋을 것 같아요. 결국에는 앞쪽에서 봤던 방법론들은 결국엔 파인 튜닝하는 일종의 파인 튜닝 방법론이었고 어 이 레그 같은 경우에는 결국에는 외부 정보를 프롬프트로 주는 거니까 인컨텍스트 러닝과 유사하다고 생각하시면 좋을 것 같습니다. 네 그리고 이게 만약 저희가 최신 뉴스라든가 시간에 따라 변하는 정보를 제공하게 된다면 이것이 타임 템퍼럴 이서라이먼트도 계산할 수가 있을 거고요. 또 제가 이것이 이전 강의에서 할루시네이션 얘기할 때도 한번 얘기를 했었는데 뭔가 주어진 질문과 관련된 정확한 정보를 제공하게 된다면 이 할루시네이션도 개선을 할 수가 있을 겁니다. 방법론은 간단해요. 결국에는 그러한 콘텍스트가 존재한다라고 한다면 그냥 콘텍스트를 주고요 콘텍스트를 주고 퀘션을 주는 거예요. 위에 위의 문맥 문맥에 따르면 넥에 따르면 어쩌고 저쩌고는 맞니 그래서 원래는 이 문맥을 안 줬었는데 이제 문맥을 적절한 이 퀘스천에 맞는 질의에 맞는 적절한 문맥을 들고 와서 같이 주게 된다면 엔서로서 저희가 뭔가 답변을 하는 것이 좀 더 할루시네이션을 즐기는 것도 가능할 거고요. 아니면 퀘스전에 뭔가 시기 최근 뉴스랑 관련된 질이었다 한다면 이러한 최근 뉴스를 들고 와가지고 콘텍스트에다 밀어 넣는 것도 가능하겠죠. 그래서 이 콘텍스트를 어떻게 찾을까가 조금 문제입니다. 그래서 결국에는 현재 문맥이라든가 상황 그러니까 대화 상황이라든가 지리 이런 거에 따라 가지고 알맞은 정보를 찾아 가지고 제공할 필요가 있습니다. 이거는 아마 여러분들도 이 앞선에 MRC 과제에서 아마 수업을 들었을 거예요. 이것을 리트리발로 해결할 수가 있어요. 그래서 리트리발 같은 경우에는 뭐 여러 가지 방법론이 있는데 인터넷 검색을 하는 것도 하나의 방법일 거고요. 그냥 단순히 쿼리를 아 구글이라든가 뭐 구글 검색창에 검색해 가지고 나온 결과물을 그냥 그대로 준다던가 아니면 뭔가 클로즈 디비 해가지고 뭔가 메일함이라든가 회사 내부 서류 같은 것을 뭔가 리트리발 해서 그것을 뽑아 가지고 문맥을 준다든가 아니면 뭔가 단순히 과거 생성 기록 이게 뭔 뜻이냐면 랭귀지 모델 같은 경우에는 결국에는 이 콘텍스트의 길이 제한이 있잖아요. 16회 길이가 있는데 이것이 아주 옛날에 생성했던 기록들이라든가 아니면 이 콘텍스트에 들어가지 않고 이것이 생성해서 그냥 저장해 놨던 유저랑 대화했던 과거 저장 기록 같은 것을 현재 지리와 유사한 걸 뽑아내 가지고 들고 오는 등 이런 것도 다 리트리벌의 일종이라고 생각하시면 좋을 것 같습니다. 결국에는 리트리버를 통해 가지고 지금 질이 문맥과 가장 유사한 문서 그것은 뭔가 검색을 통해서든 아니면 뭔가 이렇게 그 뭔가 매칭을 통해서든 어떻게든 뽑아온 다음에 그것을 콘텍스트로 준다라는 것이 레그의 기본적인 아이디어라고 생각하시면 좋을 것 같습니다. 그래서 실제로 이렇게 어 지리가 주어져 있을 때 위키피디아에서 도큐먼트 리트리벌 한번 검색을 해서 들고 온 다음에 리더로 들고 와서 이제 문서가 나왔으면 이제 문서를 여기다 집어넣고 콘텍스트에다 밀어놓고 답을 뽑아내 된다면 83만 3500 이렇게 나오기를 원한다는 것이죠. 리트리발 같은 경우에는 앞서 얘기했던 것처럼 다양한 방법론이 있긴 한데 그래도 아마 여러분들도 이런 DPR이라든가 이런 것들은 다 한 번쯤 배워보셨을 겁니다. 그래도 지금 기억나지 않는 사람들을 위해서 한 번 더 요약해서 한번 넘어가 보자 합니다. 리트리벌 방법론 중 하나가 결국에는 뭔가 픽스된 벡터를 뽑아내 가지고 그거와 가장 유사한 벡터 서치를 통해 가지고 아마 가장 유사한 문서를 뽑아내는 것이 하나의 방법론일 겁니다. 그래서 아마 여러분들도 다 배웠을 겁니다. DPR 같은 것은 여러분들이 이미 기계학 기계 독해 수업 시간에서 아마 배웠을 겁니다. 그래서 그거 외에는 뭔가 대표적으로는 tfidf라든가 bm25 같은 뭔가 단어의 빈도를 이용해서 뭔가 텍스트 유사도를 측정한다든가 아니면 뭔가 센텐스 볼트 같은 걸 활용해 가지고 볼트 기반의 인코더를 학습시켜서 문장의 유사도를 학습시킨다든가 아니면 뭔가 DPR 같은 걸 써가지고 주어진 커리와 패시지의 유사도를 측정한다든가 그러니까 결국에는 이것을 볼트를 2개를 준비해 가지고 여기다가 gd 퀘스천을 놓고 그리고 패시지 그러니까 뭐 문서 같은 거 넣은 다음에 그리고 이것에서 일반적으로 볼트를 디피알에서는 문서에서 시레스 토큰 위치하는 것들에 위치해 있는 그러한 히든 리프레젠테이션을 뽑아내서 이거 같은 경우에 고정된 크기의 벡터죠. 고정된 크기의 벡터를 뽑아낸 다음에 관련이 있는 문서 같은 경우에는 이 두 개의 문서 간의 시뮬러리티를 맥시마이즈 하는 방향으로 해가지고 저희가 이 DPR 혹은 뭐 센텐스 볼트 같은 것을 학습시킬 수가 있습니다. 그래서 결국에는 커리와의 패시지를 처리한 2개의 코드를 두고 커리와 관련이 있는 패시지라면 유사도를 올리고 커리어와 관련 없는 패시지나 유사도를 낮추는 식의 뭔가 이런 콘스트라티브 러닝을 통해 가지고 뭔가 이러한 히든 리프레젠테이션을 학습시킬 수가 있겠죠. 결국에 하고 싶은 것은 이 tfidf bm25 ST스 볼트 DPR 이런 것들이 결국에 하고 싶은 것은 결국에는 이 주어진 텍스트에 대해서 고정된 크기의 벡터를 뽑아내고 싶다는 것이 기본적인 아이디어라고 생각하시면 좋을 것 같아요. 우리가 이렇게 텍스트 인베딩을 뽑아낼 수 있는 뭔가 모델을 학습시켰다 하게 된다면 제가 이제 어 뭔가 활용하고자 하는 문서들이 잔뜩 있을 겁니다. 문서들이 이렇게 쭉 있을 때 이것들을 저희가 이 벌트 인코더에 태워 가지고 이렇게 인베딩을 고정된 크기의 벡터를 만들 수가 있겠죠. 그래서 이것들을 그냥 사전에 저장해 두는 겁니다. 사전에 이것들을 쫙 모아가지고 디비 벡터 디비 안에다가 저장해 놓는 거죠. 그래서 이거 같은 경우에 벡터 디비로는 대표적으로 뭔가 파이스라든가 뭐 파인콘이라든가 이런 것들이 있는데 이 파이스 같은 경우에는 아마 기계 독해 수업 시간에서 아마 활용해 보았을 거므로 아 이거에 대해서 자세하게 다루지 않도록 하겠습니다. 그래서 결국에는 현재의 콘텍스트를 쿼리로 이행하고요. 그 쿼리라는 게 뭔가 질이 그러니까 사용자가 지리에 했던 거 뭐 퀘스천이라든가 아니면 이 지리가 나오게 된 맥락 그러니까 뭐 생성 대화 기록 이런 것들이 뭔가 콘텍스트로 쓸 수가 있을 거고요. 그래서 이런 것을 쿼리로 두고 그리고 그 벡터 DB에다가 저장해 두었으므로 이것을 이 콘텍스트를 저희가 인코더에 태워 가지고 벡터를 뽑아낸 다음에 이 벡터하고 가장 유사한 벡터를 찾아 가지고 하나를 뽑아내게 된다면 이 도큐먼트를 찾을 수가 있겠죠. 다만 가장 유사한 내용으로 정확하게 찾는 것은 그래도 되게 시간이 오래 걸립니다. 정확하게 가장 가까운 걸 찾는 것은 오래 걸려 가지고 일반적으로는 어프록시메이트 리얼리스트 네이버 해가지고 그러니까 어프록시메이션이어서 완전 가깝진 않더라도 그래도 꽤 가까운 문서를 찾아가지고 활용하는 방법들이 되게 많습니다. 그래서 이런 것들을 좀 더 간편하게 할 수 있는 것이 방금 말씀드렸던 뭔가 파이스라든가 파인콘이 될 것 같네요. 근데 이러한 뭔가 벡터 벡터 디비를 활용하는 것이 뭔가 하나의 방법론이 될 수 있는데 어 그거 외에도 사실 리트리벌 방어는 되게 많습니다. 앞서 얘기했던 것처럼 뭔가 구글 같은 뭔가 상용 검색 엔진을 활용하는 것도 하나의 방법일 거고 아니면 자연어에서 뭔가 디비가 있어 가지고 정형 데이터가 있다 할 경우에는 굳이 벡터 서치를 하지 않고 자연어에서 에큐엘을 직접 생성해서 그러니까 쿼리가 있을 때 뭔가 실제로 여러분들이 챗치피티라든가 그런 걸 활용해 보시면 에큐엘 생성 되게 잘하거든요. 아니면 아니면 뭔가 챗치피티를 활용하지 않고도 뭔가 에스큐엘 자체를 생성하는 방법들은 뭔가 모델들이 되게 많이 나와 있습니다. 그래서 이 에스큐엘들을 직접 생성 SQL을 직접 생성해 가지고 직접 디비를 조회해 가지고 그거에 대한 인덱스를 뽑아온다든가 혹은 뭔가 해당 사용자와의 대화 기록을 특정 대화 기록을 뽑아온다든가 이런 식으로 리트리벌 뭔가 도큐먼트를 뽑아와가지고 이것을 저희가 프롬프터에 밀어 넣은 다음에 저희가 생성을 하게 된다면 저희가 질의할 때 좀 더 정확한 정보하고 아니면 최신 정보 기반으로 생성을 할 수가 있게 되는 것이죠. 그래서 결국에는 리트리벌 어그맨티 제너레이션 같은 경우에는 여러분들이 이미 리트리벌을 알고 있다면 사실상 그렇게 많이 어려운 개념은 아니게 됩니다. 결국에는 저희가 이렇게 인덱싱 도큐먼트가 있어 가지고 이것을 뭔가 벡터 디비 같은 데에다 인덱싱을 사전에 해둔 다음에 그리고 항상 하는 것은 결국에는 임베딩을 추출하고자 하는 것이죠. 그리고 우리가 하는 것은 결국에는 유저가 인풋으로 쿼리를 날렸다 지디를 날려 가지고 쿼리를 날렸다면 여기서 가장 유사한 도큐먼트를 뽑아 가지고 이 쿼리를 쿼리 인코더에 넣어 가지고 벡터를 뽑아낸 다음에 그리고 이 벡터 디비 안에 있는 도큐먼트 중에서 가장 유사한 걸 뽑아 가지고 그거에 대한 청크들 어 청크들이라는 게 도큐먼트가 되게 기니까 사실 도큐먼트도 잘라 가지고 활용하거든요. 그래서 관련 있는 청크들을 뽑아 와 가지고 이것들을 한 번에 묶어 가지고 이렇게 퀘스천 주고 그리고 플리스 앤설 더 어버브 캐스션 베이스 온 팔로잉 인포메이션 그러니까 다음 인포메이션을 기반으로 대답을 해라 같은 식으로 LLM에다가 넣어주는 것이죠. 그래서 청크1 천 2 청크 3를 넣은 다음에 이 다음에 ll엠으로 저희가 생성을 하게 된다면 우리가 이 엘램 자체는 뭔가 학습시키지 않고 고정시켜 놓고 단순히 이 정크를 바꾸는 것만으로도 모델에 대해서 뭔가 이 최신 정보가 어떠하며 그리고 정확한 정보가 어떤지 이것을 저희가 제공하는 것이 가능하다라는 이야기를 할 수가 있습니다. 그래서 레그를 하게 된다면 좀 더 정확한 답을 뽑아낼 수가 있게 되는 것이죠. 결국에는 이를 통해서 엘엠의 매개 변수는 거의 건드리지 않고 인컨텍스 러닝으로 생성 결과를 항상 향상시킬 수가 있다라고 생각하실 수가 있을 것 같습니다. 이 레그 같은 경우에는 뭔가 듣기만 하면 되게 완벽한 방법론인 것 같지만 사실 몇 가지 한계가 존재합니다. 첫 번째 한계는 이 레그 자체가 항상 정확도를 향상시키는 것은 아닙니다. 당연히 그럴 수밖에 없는 것이 이 리트리벌 자체가 사실 완벽하진 않잖아요. 그렇기 때문에 되게 적절치 않은 문서를 참고할 수도 있고 뭔가 관련 관련이 없는 그러한 문서를 볼 수도 있겠죠. 그리고 또한 리트리벌 된 문서 자체가 되게 길다 보니까 이것 자체에 내용이 다양한 내용들이 포함되어 있을 수도 있고 이것 자체가 뭔가 노이즈로 작용할 수가 있을 겁니다. 즉 리트리벌 된 문서 자체가 너무 길다 보면 결국에는 어 관련 있는 문서는 딱 한 줄만 있고 다른 곳에서 다 관련 없는 내용들만 가득 차 있어 가지고 이것 자체가 되게 엘엘엠 입장에서는 노이즈로 작용해 가지고 더 올바르지 않은 대답을 할 수도 있을 겁니다. 그리고 무엇보다도 여무 유명한 사실은 사실 엘엘엠 자체가 더 잘 압니다. 이게 그 엘램 자체가 사실 모델이 크다 보니까 어 이 프라이어놀리지라고 할까요? 사전 지식 자체가 되게 강력합니다. 즉 이미 알고 있는 사실이 많다는 것이죠. 그리고 모델 크기가 커지면 커질수록 되게 유명한 객체에 대한 정확도는 되게 올라가게 돼요. 그래서 비교를 해 보게 된다면 저희가 뭔가 루이지애나의 수도가 어디냐 그래서 물어보게 된다면 사실 이러한 너무 유명한 사실 같은 경우에는 오히려 뭔가 리트리바에서 줬을 때보다 그냥 모델 자체가 대답하는 것이 더 성능이 좋다는 것을 확인해 볼 수가 있습니다. 지금 파란색이 지금 이것이 레그를 안 쓴 모델이고요. 그리고 주황색이 레그를 쓴 모델인데 유명한 뭔가 객체 같은 경우에는 파퓰러리티가 높은 객체일수록 사실 리트러블 이트리바를 안 쓰는 것이 오히려 더 정확하다는 것을 확인해 볼 수가 있고요. 다만 좀 유명하지 않은 사실 그러니까 모델 입장에서는 헷갈리기 좋고 할루시네이션을 일으키기 좋은 사실 같은 경우에는 당연히 리트리벌 했을 때 정확도가 상승합니다. 그래서 덜 유명한 객체의 정확도 같은 경우에 여전히 낮기 때문에 이 경우에는 l의 활용도가 높고요. 아 하지만 뭔가 정말 유명한 객체 같은 경우에 오히려 레그가 방해 요소로 작용할 수 있다 정도만 알고 있으면 어떨까 싶습니다. 그러나 사실 앞쪽에 나왔던 뭔가 유명한 객체를 더 잘 안다 같은 경우에는 사실 큰 한계가 되지 않지만 이 두 번째 문제 같은 경우에는 좀 문제가 될 수가 있는데요. 두 번째 문제가 뭐냐 하면 사실 모델 자체가 프라이어 놀리지가 되게 강력하다 보니까 학습돼 본 지식으로 인해서 LLM 자체가 문서를 아예 안 볼 수가 있습니다. 즉 LLM의 프라이어 놀리지가 너무 강력하다 보니 콘텍스트하고 프라이어가 상충될 수 있어. 그러니까 내가 알고 있는 사전 지식하고 그리고 주어진 콘텍스트가 너무 내용이 다를 경우에는 이 컨텍스트 자체를 무시할 수가 있습니다. 그래서 실제로 위에 있는 예시를 좀 보면 좀 이해가 되지 않을까 싶은데요. 이거 같은 경우에는 모델 자체를 뭔가 뉴스qa라는 데이터로 뭔가 MRC 머신 리딩 컴플레이션을 학습시킨 다음에 뭔가 NTT를 교체해서 평가한 결과라고 생각하시면 좋을 것 같습니다. 그래서 뉴스 QA에서 NTT를 교체했다는 뜻이 무엇이냐 그래서 이러한 뉴스 큐에 데이터셋은 이렇게 퀘스천이 있고 오리지널 콘텍스트가 있고 그리고 앤서가 있는 이러한 뭔가 데이터셋이라고 생각하면 좋을 것 같은데요. 여기서 NDT를 교체해 가지고 원래 이 데이터스에서 이것을 절머니를 타이완으로만 교체했을 때 라고 넣게 된다면 모델 자체가 이 구조 자체가 너무 익숙하다 보니까 이것을 콘텍스트 자체를 잃지 않고 바로 절머니라고 독일이라고 뱉을 수 있는 문제점이 존재한다는 것이 이 문제입니다. 그래서 실제로 보시면 알겠지만 지금 어 이 뉴스큐에이로 트레인 한 다음에 인퍼런스로 저희가 데브라든가 이런 것을 그에 우리가 엔티티를 교체해서 아웃풋을 뽑게 된다면 그냥 오리지널 이 프레딕션이 나오게 되는 경우가 꽤 많다라는 것을 확인해 볼 수가 있습니다. 그렇기 때문에 뭔가 이것이 저희가 단순히 인컨텍스 러닝으로 정보를 제공하게 된다면 사실 어 이것 자체가 뭔가 아키텍처적으로 아 너 이 콘텍스트를 따라 가지고 생성하렴이라고 뭔가 제한을 둔 것이 아니기 때문에 뭔가 이 콘텍스트를 따르게 하는 것을 강제하는 것이 되게 어렵습니다. 그래서 뭔가 저희가 만약 엘엘엠 가지고 가상 역사를 창작한다 이런 경우에는 결국에는 배경을 콘텍스트로 주더라도 뭔가 해당 배경을 정확하게 따르는 것은 어려울 수도 있는 문제가 발생할 수도 있는 것이죠. 그렇기 때문에 이 문제는 사실 뭔가 이것이 레그의 한계라기보다는 이건 사실 LLM이 이러한 인스트럭션을 잘 따르지 못하는 문제라고도 볼 수가 있어요. 결국에는 저희는 이 콘텍스트를 보고 생성하라고 했었지만 이 콘텍스트를 어 뭔가 읽지 않고 그냥 바로 인스트럭션을 따른 거기 때문에 이러한 인스트럭션 팔로잉에 대한 문제라고도 해석할 수가 있을 것 같습니다. 그래서 이러한 한계를 뭔가 극복하고 그래서 이러한 리트리벌 자체를 좀 더 잘 활용하기 위해서 몇 가지 방법론이 제안되었는데요. 그중 두 가지 좀 옛날 논문 하나하고 좀 그나마 최신 논문 해가지고 하나 한 가지 해가지고 총 두 가지를 좀 소개해 드릴까 생각이 듭니다. 그래서 첫 번째 논문 이름이 어 이거 리트리벌 어그맨티 제너레이션이라고 적혀 있는데 이것 자체가 사실 이 모델 이름이 레그로 여기서 리트리벌 어그멘티드 제너레이션이라는 단어가 처음으로 제한됐어요. 그러니까 이것 자체가 레그 자체가 사실 여기 이 모델 이름에서 나온 겁니다. 그래서 현재는 물론 레그라는 단어 의미가 확장돼 가지고 뭔가 리트리벌을 활용하는 생성 자체를 통칭하게 되지만 이 리트리벌 어그맨티지 제너레이션 이 레그라 모델 처음 나온 그러한 모델이라고 생각하시면 어떨까 생각이 듭니다. 그래서 이 레그 모델 같은 경우에는 사실상 리트리벌로 한 지리에 대해서 여러 문서를 들고 온 다음에 각 문서 기반으로 계산된 토큰 확률 값을 마지널라이즈 해서 하나의 텍스트를 생성합니다. 그래서 지금 말이 조금 어려운데요. 실제로 저희가 리트리벌 어그멘티드 제너레이션 할 때는 도큐먼트를 이렇게 그냥 프롬프트를 콘텍스트로 합쳐 가지고 하나의 도큐먼트 혹은 뭔가 이런 프롬프트로 만든 다음에 퀘스천을 주었지만 여기 같은 경우에는 한 퀘스천에 대해서 다른 도큐먼트를 붙여가지고 이렇게 하게 된다면 사실상 어 퀘스천 똑같은 퀘스천에 대해서 도큐먼트 1번 그리고 도큐먼트 2번 하게 된다면 아마도 이거 기반으로 생성하는 그 도캡의 디스트뷰션이라 할까요? 그런 것이 좀 다를 겁니다. 왜냐하면 하나는 도큐먼트 1번을 보고 있고 다른 건 도큐먼트 2번을 보고 있으니까요. 그래서 이거 두 개가 이렇게 다른 도캡의 디스트뷰션을 가지게 될 건데 이거 두 개를 마지널라이즈 해서 결국에는 이것 자체가 첫 번째 같은 경우에는 도큐먼트 1번이 주어졌을 때 도큐먼트 1번과 퀘스천이 주었을 때 그리고 이 답인 와를 생성할 확률 라고 생각하시면 좋을 것 같고요. 두 번째 같은 경우에는 도큐먼트 2번과 퀘스천이 주었을 때 y를 생산할 확률 나올 확률 해가지고 이렇게 확률이 나올 건데 이거 2개 자체를 마지널라이즈를 해 가지고 단 하나의 텍스트를 생성하는 그러한 방법론이라고 생각하시면 좋을 것 같습니다. 그래가지고 이렇게 하게 된다면 여러 도큐먼트를 또 공평하게 보는 것이 가능하게 되겠죠. 앞선 문제에서는 이렇게 도큐먼트를 프롬프트에 여러 개 주더라도 결국에는 이 프롬프트를 다 볼 수 있느냐는 또 별개의 문제였는데 실제로는 여기서는 이것이 확률들이 다 마지널라이저가 되기 때문에 공평하게 보는 것이 가능하고요. 그리고 사실 여기서 또 하나 중요한 점이 여기서는 이 리트리벌과 이 제너레이터를 엔드 투 엔드로 학습시킵니다. 즉 여기서 똑같이 씁니다. 결국에는 이 쿼리 인코더를 통해 가지고 디피알과 유사한 방법으로 이 도큐먼트를 뽑아서 들고 오는데 어 우리가 앞쪽에서 봤던 방법 같은 경우에는 따로 이 도큐먼트 리트리버를 학습시키지 않았지만 여기서는 좀 더 레그에 알맞은 도큐먼트를 들고 오기 위해서 이 도큐먼트 리트리벌 자체를 뭔가 리트리버 자체를 학습시켜 가지고 뭔가 엔드 투 엔드로 학습시키는 방법론을 제안을 합니다. 하지만 이 방법론 같은 경우에는 엔드 투 엔드 학습도 필요하고요. 그리고 리트리벌 학습도 필요하고 좀 방법론 자체가 좀 오래되었는데 최근에는 사실 이쪽에서 얘기했던 것처럼 사실은 이렇게 도큐먼트를 안 보는 것 자체가 뭔가 이 인스트럭션을 잘 못 따르는 문제다라는 것에 초점을 맞춰 가지고 이런 인스트럭션을 좀 잘 따르게 하자. 따라서 그냥 rlrlahf라든가 이런 뭔가 프리퍼런스 튜닝 뭔가 dpo 이런 거 가지고 뭔가 그냥 모델 자체가 좀 더 콘텍스트를 잘 활용할 수 있겠다라는 뭔가 그런 방법론들이 되게 많이 나와 있습니다. 그래서 대표적인 방법론이라고 한다면 이 고퍼 사이트가 될 것 같은데요. 이 고퍼 사이트 같은 경우에는 그 고포가 뭔지 우선 소개하자면 고퍼는 그냥 구글에 한 280빌리언 되는 엘엠이라고 생각하면 좋을 것 같고요. 어 고퍼 사이트는 그 구글 검색을 통해 가지고 구글 검색 자체를 리트리버로 쏴 가지고 뭔가 모델을 그 이제 사이테이션 하면서 뭔가 이 레그를 하는 그런 모델이라고 생각하면 좋을 것 같습니다. 그래서 여러분들이 단순히 그냥 프롬프트로 집어넣는 게 아니고 애초에 우리가 rnahf라든가 그런 과정 속에서 저희가 해당 근거가 해당 답안의 생성을 뒷받침하도록 학습합니다. 이 밑에 있는 예시를 보면 좀 이해가 되실 것 같은데요. 저희가 어 굿 플레이스 레이처 인 프렌드 했을 때 한다면 레이처리 프렌드에서 레이 레이처리라는 것을 누가 플레이했는가 그러니까 누가 역할을 맡았는가에 대한 이야기인데 이거에 대한 위키피디아가 주어져 있고 그리고 여기에 대해서 답안을 주어져 있을 때 이것 자체가 plsi블한 그러니까 그럴듯한 답안인가도 체크를 하고요. 이것 자체는 사실 리워드 평가할 때 되게 중요한 요소겠죠. 그거 외에도 아 이 요소 이 에비던스 자체가 이것을 서포트하는가를 평가하게 해 가지고 이러한 에비던스 혹은 이런 근거를 잘 맞추고 있는가 역시도 또 다른 리워드를 도와서 이거 자체를 알레치프를 통해 가지고 모델이 좀 더 근거를 더 잘 활용할 수 있게끔 뭔가 이런 튜닝을 해보자는 것이 이 고퍼 사이트의 아이디어라고 생각하시면 좋을 것 같습니다. 그래서 이렇게 하게 된다면 사실상 구글 검색을 통해서 질문과 관련된 몇 페이지를 찾고 근거가 되는 문단을 함께 인용하는 식으로 즉 오른쪽에 있는 그림과 같이 이러한 질의가 들어왔을 때 실제로 이거에 관련된 페이지하고 그리고 그거에 대한 답변을 준 것을 확인해 볼 수가 있습니다. 즉 이렇게 단순히 모델 자체가 프롬프트만으로 주는 것이 아니고 추가적으로 이렇게 프롬프트를 더 잘 할 수 이해할 수 있도록 즉 프롬프트에 주어진 콘텍스트를 더 잘 이용할 수 있도록 그러한 저희가 인스트럭션을 주고 프리퍼런스를 튜닝함으로써 좀 더 모델 자체가 잘 이러한 그 근거에 뒷받침된 생성을 더 잘 할 수 있도록 저희가 튜닝을 할 수가 있게 되는 겁니다. 하지만 이런 생각이 드실 겁니다. 어 제가 챗gpt라든가 제가 챗gpt 를 이용했을 때 어 저는 그냥 이 프롬프트에다가 콘텍스트를 집어넣는 것만으로도 잘 작동했었는데요라는 의견이 생각이 들 수도 있을 겁니다. 왜 앞서 앞서 글에 나왔던 뭔가 레그라든가 구포 사이트처럼 따로 튜닝하지 않고도 잘 작동하는 것 같은데 어 이런 걸 그 이런 것을 어디에 활용할 수 있나요? 라고 생각이 들 수 있겠지만 사실 챗지피티라든가 그런 상용 프로그램 자체가 잘 작동되는 이유 자체가 사실 이렇게 고포 사이트처럼 이미 사전에 어 콘텍스트를 더 잘 이해할 수 있도록 이렇게 튜닝이 되어 있기 때문에 어 뭔가 저희가 따로 뭔가 튜닝하지 않고 뭔가 따로 모델을 아키텍처를 수정하지 않고도 그냥 프롬프트를 넣는 것만으로도 충분히 잘 작동돼 있는 것이다라고 이해하고 넘어가시면 어떨까 싶습니다. 네 이상으로 저희가 LLM의 큰 단점 중 하나였던 옛날 정보들을 어떻게 하면 갱신할 수 있을까에 대해서 좀 알아보았고요. 다음 시간에서는 저희가 이제 LLM에 대해서는 현황이라든가 시장 현황이라든가 뭔가 이것들을 실제로 어떻게 활용되고 있는가에 대해서 좀 알아보는 시간을 가지도록 하겠습니다. 여기까지 수업을 들어주셔서 감사합니다."
}