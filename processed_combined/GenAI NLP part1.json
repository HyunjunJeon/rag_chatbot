{
  "lecture_name": "GenAI NLP part1",
  "source_file": "GenAI NLP part1_76.mp4_2025-12-04-104006443.json",
  "text": "저는 고려대학교 산업경영공학부 강필성 교수님 연구실에서 자연어 처리를 연구하고 있는 김재희라고 합니다. 우선 이렇게 만나 뵙게 되어 정말 반갑습니다. 오늘은 저와 함께 최근 주목받고 있는 러쉬 랭귀지 모델의 학습 및 평가 방식에 대해 살펴보도록 하겠습니다. 특히 교수님께서 강의에서 다뤄주셨던 많은 내용들 중에서도 여러분들이 앞으로 실제 부스트 캠프에서 제공되는 서버와 같은 환경에서 사용하실 수 있는 학습 방법인 로라를 이용해 보도록 하겠습니다. 로라는 결국에 강의에서도 다루었겠지만 라즈 랭귀지 모델의 모든 파라미터를 학습할 수 없는 상황에서 로우 랭크 어댑테이션이라는 기술을 통해 극소수의 파라미터만 학습하는 방법론입니다. 이를 통해서 LLM을 인스트럭션 튜닝을 시켜보고 이렇게 인스트럭션 튜닝 된 모델과 튜닝하기 전에 모델에 대해서 여러 가지 평가 데이터를 통해서 직접 평가하는 과정도 살펴보고 평가된 점수를 살펴보면서 여러분들이 현업이나 학업에서 LLM을 학습하고 평가할 때 살펴봐야 할 여러 가지 지점들에 대해서 얘기해 보도록 하겠습니다. 우선은 저희가 사전에 업로드해 드린 제너레이티브 AI 강의의 이미지가 2개가 있을 텐데요. 두 이미지 중에서 엔엘피용 이미지를 이용해서 컨테이너를 생성해 주셔야 합니다. 컴퓨터 비전 실습과 다른 환경에서 진행이 될 예정이므로 엔엘피 이미지를 이용해 주셔야 된다는 점을 꼭 주의해 주세요. 저 역시도 여러분과 마찬가지로 엔알피 이미지를 통해서 컴테이너를 생성을 하고 이를 브스 코드 리모트를 통해서 접속할 예정입니다. 뭐 여러분들이 원하시는 대로 저와 똑같이 브스 코드 스치 리모트 기능을 이용을 하셔도 되고 그게 아니라 그냥 어 컨테이너 내부에서 주피터 노트북을 켜고 그를 원격 접속해서 이용하셔도 상관은 없습니다. 우선 저 같은 경우에는 여러분과 똑같이 어 컨테이너를 생성하고 컨테이너에 대해서 팸 키를 등록을 해 놓고 현재 접속을 해보도록 하겠습니다. 현재 이 이미지의 경우 여러분들도 똑같은 이미지를 열어보실 수 있을 텐데요. 아무것도 없는 빈 이미지라고 할 수 있습니다. 그러면 우선 루트 디렉토리로 현재 설정이 되어 있긴 하지만 이를 워크 스페이스로 이동을 해서 작업을 해보도록 하겠습니다. 그리고 현재 작업을 위해서는 이 이미지 내에는 파일이 아무것도 없기 때문에 파일을 다운로드 받아야 되는데요. 저희가 구글 드라이브에 공유 링크를 드리고 이 공유 링크로부터 실습 파일을 다운받을 수 있도록 되어 있을 겁니다. 이를 위해서 구글 드라이브로부터 직접적으로 다운을 받을 수 있는 라이브러리인 쥐다운을 설치해 보도록 하겠습니다. 지다운이 설치되었다면 이제는 지다운을 이용을 해서 폴더 옵션을 켜고 링크를 가지고 오게 되면 이런 식으로 파일들이 다운로드 되었을 겁니다. 그럼 이제 실제로 편집기를 통해서 확인을 해보면 이렇게 화면을 통해서 보이다시피 실제 주피터 노트북 실습 파일과 실습 파일을 통해서 생성된 데이터들이 담길 데이터 폴더가 생성된 것을 보실 수 있을 겁니다. 그러면 본 실습 같은 경우에는 라지 랭귀지 모델을 학습하기 위해서 뭐 여러 가지 구현을 할 수도 있겠지만 허깅 페이스에서 구현되어 있는 파라미터 이피전트 러닝 파인 튜닝 라이브러리를 이용을 하게 될 겁니다. 그리고 그 외에도 여러 가지 학습용 프레임워크나 코드 역시도 최대한 단순화해서 허깅페이스의 것들을 많이 사용할 예정입니다. 그리고 이 자료 외에도 사실 최근 온라인상에 공개되고 있는 다양한 엘르램 튜닝용 코드들이 많이 공개되어 있습니다. 저희가 코드를 만들 때도 여러분들이 함께 참고하실 수 있도록 어 자료들을 링크를 담아 놓았으니 나중에 관심이 있으시다면 살펴보셔도 좋을 것 같습니다. 우선 그러면은 리콰이먼츠부터 설치를 해보도록 하겠습니다. 사실 깃이 필요하긴 한데요. 깃이 없어도 동작하는 코드들이기 때문에 깃 없이 돌려보도록 하겠습니다. 네 이거는 오류 메시지가 아니니까 신경 쓰지 않도록 하고 필요한 파일들을 설치해 보도록 하겠습니다. 라이브러리를 설치하도록 할 텐데요. 이 비치 앤 바이츠라고 하는 거 트랜스포머스라고 하는 거 그리고 허깅페이스에서 제공하는 페프트와 엑셀러레 데이터 세트 모두 어 허깅페이스사에서 제공을 하는 여러 가지 라이브러리들입니다. 트랜스포머스 같은 경우에는 LLM 외에도 NLP나 CV에서 사용되는 여러 가지 모델과 데이터셋들에 대한 라이브러리라고 할 수 있고 데이터 셋츠는 이를 위해 필요한 여러 가지 공개용 데이터들을 다운로드 받고 관리할 수 있는 라이브러리, FFT 같은 경우에는 로라와 같은 여러 가지 파라미터 이피전트 파인튜닝 방법론들을 간편하게 사용할 수 있는 라이브러리, 엑셀러레잇은 분산 학습과 같은 매우 복잡한 학습 환경들에 대해서 간단하게 어 설정하고 학습할 수 있는 환경이라고 할 수 있습니다. 비치 앤 바이치 같은 경우에는 나중에 더 자세히 다루실 일이 있을지 모르겠지만 뭐 프리시전을 왔다 갔다 하면서 뭐 스케일을 조절해야 되는 경우에 많이 사용하게 됩니다. 그러면 하나씩 로드부터 해보겠습니다. 시드를 고정을 하고 데이터 셋 라이브러리에서 로드 데이터 셋을 임포트하도록 하겠습니다. 우선 첫 번째로는 학습 데이터가 필요합니다. 근데 학습 데이터가 여러분들이 기존에 배웠던 여러 가지 다음 스트림 테스크와 다른 점이라고 한다면 라지 랭귀지 모델은 기본적으로 특정 테스크에 대한 파인 튜닝보다는 범용적인 테스크 혹은 테스크를 정의하지 않은 상태에서 파인 튜닝이 되는 것이 일반적입니다. 즉 인스트럭션 튜닝이라고 하는 것 자체가 테스크를 정하지 않은 상태에서 사용자의 여러 가지 요청에 대응할 수 있어야 되기 때문에 인풋으로는 자연어로 구성된 매우 오픈 도메인의 질문과 요청 사항이라고 할 수 있고 그럼 모델은 이 인풋을 입력으로 받아서 출력 값이나 아웃풋으로는 이나 자연어로 구성된 답변을 출력해야 됩니다. 예를 들어서 인풋이 하늘의 색이 뭐야라고라는 하는 인풋이 들어간다면 모델은 하늘의 색은 파란색이야 와 같은 아웃풋을 낼 수 있어야 될 것이고 하루는 몇 시간으로 구성되어 있니라는 질문이 들어왔다면 아웃풋으로는 하루는 24시간으로 이루어져 있어 와 같이 출력할 수 있어야 될 겁니다. 이러한 접근 방식은 사실 기존의 다운스트림 테스크 같은 경우에는 인풋은 이런 쉐입일 거고요. 인풋 길이는 이 정도일 거고요. 인풋은 뭐 어느 도메인에서부터 온 글일 겁니다. 와 같은 도메인 그리고 아웃풋은 스칼라 값일 겁니다. 벡터 값일 겁니다. 혹은 클래시피케이션 테스크 같은 경우에는 프로버빌리티 디스트리뷰션일 겁니다와 같은 것들이 정해져 있었다면 이제 저희는 모두 다 텍스트 텍스트 프레임워크로 변환을 해서 사용을 하게 될 거고요. 이를 이용해서 모든 데이터셋들을 인풋 아웃풋 포맷에 맞추고 이를 제이슨 앤 형태로 가공하여 사용하는 것이 일반적입니다. 그러면 LLM 학습을 위한 한국어 데이터셋들은 사실 정말 많이 있긴 하지만 이를 가장 쉽게 그리고 여러 가지 데이터셋을 비교하면서 살펴볼 수 있는 기 탑 레플 스토리 같은 경우에는 여기 희규 님이라고 하시는 분이 정리해 놓은 레플 스토리가 있으므로 한번 참고해 보시면 좋을 것 같고요. 저희는 이번에 코 리마 비큐나 그리고 그룹 버전 2를 사용할 거고 사실 코알파카 데이터셋 역시 활용은 할 수 있어서 코드상으로 구현을 해놨는데 여러 가지 실습에 있어서의 시간 제약 그리고 여러분들이 실제로 돌려보면서 여러 가지 실험을 돌리기 위한 시간 제약 때문에 알파카 데이터는 현재 실제로 사용하지 않겠습니다. 우선 데이터 같은 경우에는 로드 데이터셋 라이브러리를 이용하면 이와 같이 쉽게 로드를 할 수 있고요. 네 이렇게 데이터셋을 다 다운 받았다면 이제 데이터셋을 한번 살펴보도록 하겠습니다. 리마 데이터셋을 살펴보도록 할 텐데요. 리마 데이터 셋을 살펴보게 되면 from이라는 값 그리고 밸류라는 값으로 되어 있는 것을 볼 수가 있을 겁니다. 그리고 이게 또 리스트로 묶여 있는 걸 볼 수가 있는데요. 사실 이 챗봇을 학습을 가정하고 있기 때문에 멀티턴과 같은 상황 때문에 이렇게 리스트로 묶여 있고 우선 사람이 이렇게 말했고 그에 대해서 GPT 혹은 챗봇이 이렇게 대답했다 라고 볼 수 있을 겁니다. 그러면은 이 아래 예시로 보면 좀 더 쉬울 것 같은데요. 사람이 비디오 게임에서 엠피씨랑 봇의 차이가 뭐야라고 물어봤다면 지피티는 이런 식으로 대답을 해야 된다라는 데이터셋이라고 할 수 있습니다. 구름 같은 경우에는 좀 더 단순한 데이터 혹은 조금 더 다른 데이터라고 할 수 있는데요. 그룹의 학습 데이터 셋 중에서도 인스트럭션 데이터만 살펴보게 되면 세 가지 인스트럭션이 이렇게 있는 걸 볼 수가 있습니다. 그룹 데이터 같은 경우에는 이렇게 인스트럭션 인풋 아웃풋 세 가지 값으로 되어 있는데요. 사실 아웃풋은 모델이 뱉어야 되는 출력 값에 해당될 거고 인스트럭션과 인풋이 모델의 입력에 해당되는 값이 될 겁니다. 밑에서 전처리 과정에서 살펴보도록 하겠지만 인스트럭션과 인풋이 항상 존재한다는 보장이 없기 때문에 이건 나중에 데이터를 조금 살펴보시면 알 수 있을 텐데 그렇기 때문에 이 두 가지 중에 하나라도 있으면 사용하도록 해야 합니다. 인스트럭션과 인풋이 다른 점이라고 한다면 인스트럭션은 모델에게 직접적으로 명령을 내리는 거 과정 혹은 어떤 테스크를 수행해 달라고 요청하는 과정이라고 할 수 있고 인풋은 그 테스크의 실제 입력 값이다라고 할 수 있는데 사실 그 구분이 명확히 되어 있지 않기 때문에 개념적으로만 그렇다라고 생각해 주시면 될 것 같습니다. 다음으로는 여러 가지 데이터를 한꺼번에 이용을 하다 보니까 어 중복된 데이터가 존재할 수 있습니다. 그래서 어 중복된 데이터를 제거하기 위해서 리무브 디플리 캐츠 리무브 디플리 캐치라는 함수를 이용하도록 하겠습니다. 이 함수는 사실 자세한 내용이 복잡하지는 않는데요. 어 데이터셋이 들어오게 되면 우리는 유니크 데이터라는 셋과 유니크 데이터셋이라는 리스트를 우선 만들어 주게 됩니다. 그리고 데이터 셋에서 그러니까 여러 가지 인풋 아웃풋 페어가 있는 데이터에 대해서 하나씩 인스턴스를 받아와서 이를 제이슨 형태로 변환을 시켜 준 아이덴티파이어를 만들게 됩니다. 그리고 아이덴티 파이어가 유니크 데이터 안에 없다면 데이터를 새로 추가해 주는 과정으로 되어 있습니다. 왜 제이son으로 비교해요? 그리고 왜 솔트 키스를 해요라고 여쭤보신다면 제이슨으로 하면은 조금 더 빠르게 비교를 할 수 있고 솔트 키스를 하면은 문자열의 순서가 다르더라도 똑같은 문자열을 사용했을 때 사실 동일한 데이터라고 볼 수 있잖아요. 그러니까 나는 밥을 먹는다와 나는 먹는다 밥을이 세 단어가 순서가 다르더라도 사실 똑같은 의미를 가지고 있기 때문에 이를 거르기 위해서 솔트 키스 옵션을 트르라고 하고 있습니다. 그러면은 결국에 리무브 듀필리캐치 함수를 이용을 하게 되면은 우리는 여러 가지 데이터셋들로부터 어 중복된 데이터가 존재하지 않는 데이터를 생성할 수 있다라는 점을 알 수 있습니다. 그리고 이걸 사용해야 되는 이유는 사실 지금 존재하는 한국어 엘엘엠 튜닝용 데이터 셋들이 여러 가지 영어 데이터셋들을 번역하다 보니까 동일한 데이터가 매우 중복된 경우가 많이 존재할 수 있습니다. 그리고 이를 걸러주는 것이 모델 성능에 꽤 중요한 영향을 끼칠 수 있기 때문입니다. 그러면 이제 멀지 데이터 셋을 만들 겁니다. 즉 리마 데이터와 그룹 데이터를 합쳐 주게 될 텐데요. 멀지 데이터 셋은 빈 리스트로부터 시작해서 리마 데이터를 더해주고 구름 데이터를 더해 줄 건데 모두 인풋과 아웃풋 이렇게 두 가지 키 값으로 되어 있는 딕셔너리로 될 겁니다. 이때 리마 데이터는 멀티턴일 수 있었기 때문에 턴 수가 2개인 즉 사람이 물어보고 로봇이 대답했던 딱 두 가지 대화로만 되어 있는 데이터만 가지고 올 것이고 그룹 데이터 같은 경우에는 아까도 말씀드렸다시피 인풋과 인스트럭션이 있을 텐데 인풋이 없다면 인스트럭션이 있는 경우에만 데이터를 추가를 해줄 겁니다. 그리고 만약에 인풋과 인스트럭션이 모두 있는 경우에는 이런 식으로 인스트럭션 개인 기호 그다음에 인풋 이런 식으로 해서 실제 인풋의 인스트럭션과 인풋 모두 담길 수 있도록 만들어 줄 겁니다. 그리고 마지막으로 앞서 선언했던 중복을 제거하는 리무브 디플리 캐치 함수를 이용을 해서 중복까지 제거를 합니다. 그렇게 되면 실제로 원래 이 두 가지 데이터를 단순 벌지를 하게 되면 17만 건 정도인데 이 중복을 제거했더니 14만 건 정도로 상당히 많은 데이터가 제거된 것을 알 수가 있습니다. 네 데이터를 합치고 병합하는 동안 저희가 밑에 코드를 마저 살펴보자면 이제 트레인하고 밸리데이션 데이터를 나눠야 될 텐데요. 저희가 그러면 왜 트레인 밸리데이션 테스트가 아니라 트레인 밸리데이션만 나누느냐 테스트 데이터는 사실 여기서 가지고 오는 게 크게 의미가 없습니다. 결국 테스트 데이터를 가지고 우리가 할 수 있는 건 정량적인 지표를 뽑을 수 있어야 될 텐데 인스트럭션 팔로잉 능력을 보기 위해서는 이걸 물어봤을 때 이거라고 답해줘를 사람이 직접 눈으로 보고 판단해야 되잖아요. 그래서 이와 같이 오픈된 상황에 대한 대화 데이터로 평가를 하는 게 아니라 실제 평가 같은 경우에는 향후 살펴보게 될 여러 가지 다운스트림 테스크들, 라즈 랭귀지 모델 평가를 위해 설계된 여러 가지 벤치마크 데이터셋들을 살펴보겠습니다. 그럼 밸리데이션 데이터를 천 개만 샘플링해서 사용할 거기 때문에 전체 데이터의 인덱스를 먼저 생성해 주고 이 인덱스 중에 랜덤 샘플을 통해서 천 개만 우선 걸러주게 되겠습니다. 그럼 이벨 데이터셋은 이 이벨 인덱스로부터 가지고 오게 될 거고 트레인 데이터 셋은 이벨 인덱스가 아닌 나머지 인덱스로부터 가지고 올 겁니다. 네 여기까지 했으면 트레인하고 이벨 데이터셋을 만들었으니까 이 두 가지 데이터셋을 제이스 엘 포맷으로 저장을 해주도록 하겠습니다. 그리고 이 데이터는 데이터 안에 저장이 되는 걸 알 수가 있죠. 그래서 데이터 안에 보면 이렇게 저장이 되어 있는데요. 한번 살펴보면 이런 식으로 제이스 l 형태이기 때문에 하나의 라인이 하나의 제이슨 파일이다 혹은 딕셔널이다라고 생각을 할 수 있습니다. 그래서 모든 데이터가 인풋 그리고 아웃풋으로 구성이 되어 있는 걸 볼 수가 있고요. 다음 문장을 리프레이징 해 주세요. 혹은 똑같은 의미를 가지도록 다시 써주세요 하고 그는 너무 피곤해서 잠들었습니다라고 한다면 극도로 지친 그는 잠에 빠져들었습니다와 같은 문장을 만들어 주도록 우리는 모델을 학습시킬 겁니다. 이 데이터가 상당히 많기 때문에 우선 이 정도만 살펴보도록 하고 나중에 실제로 대회에 나가시든 아니면 실습을 하시든 프로젝트를 하시든 데이터를 조금 더 자세히 살펴보시면서 품질이 어떤지 어떤 식으로 모델이 학습될지에 대해서도 한번 개념적으로 살펴보시면 좋을 것 같습니다. 그러면 저장된 데이터를 다시 로드를 해올 텐데요. 저장된 데이터는 제이스 포맷으로 되어 있고 데이터 파일은 이거고 스플릿을 저희가 따로 안 했기 때문에 트레인입니다. 그러면 저장된 데이터를 다시 불러오게 되고 이제 실제로 학습을 시켜보도록 하겠습니다. 딥러닝 학습 같은 경우에는 여러분들이 지금까지 진행한 거 그리고 버트나 지피티 2 사이즈 정도의 모델들은 하나의 GPU로 학습이 될 수 있지만 이제 라즈 랭귀지 모델로 접어들면서 결국에는 여러 개의 GPU를 기반으로 학습을 하는 게 일반적인 상황입니다. 그러면 여러 개의 GPU를 사용한다는 건 하나의 모델을 가지고 여러 개의 GPU에 쪼개서 올린다든지 데이터를 쪼개서 올린다든지 해서 학습을 시키게 될 텐데요. 이런 식으로 모델과 데이터를 쪼개는 분산 학습 상황을 효과적으로 사용하기 위해서는 엑셀러레잇 라이브러리를 많이 사용하게 됩니다. 그리고 구체적으로 가장 대표적인 건 풀리샤리드 데이터 패럴이라는 기술이 제공되고 있고 이 기술이 정말 많이 사용되긴 하는데요. 이거는 모델이 파라미터가 만약에 70 빌리언이다라고 하면은 대략 140기가 정도 되거든요. 140기가 될 정도 되는 파라미터는 거의 현재 존재하는 에이백이 최대가 80기가니까 에이백 한 장의 모든 모델이 올라가지 않습니다. 이와 같을 때 하나의 모델을 여러 개의 지피유에 나눠서 올리는 샤링 할 수 있는 기술이 됩니다. 그래서 일반적인 상황이라고 한다면 FSDP가 제공하는 기능이라고 하는 건 결국에 모델의 파라미터를 분할해서 각각의 GPU에 올려주고 이 올린 와중에도 메모리를 더 아껴야 된다면 메모리 사용량을 줄일 수 있는 여러 가지 트랙들을 사용하게 됩니다. 현재 본 실습 자체가 사실 분산 학습에 대해서 구체적으로 다루는 실습은 아니기 때문에 여기까지만 설명하도록 하겠습니다. 그런데 사실 하나의 지표만 사용해도 프스티피를 사용할 수 있거든요. 그 이유에 대해서 얘기를 해드리자면 프스티피가 메모리 최적화 관점에서 매우 강한 트레이드를 사용하고 있습니다. 그렇기 때문에 메모리가 부족한 상황이다라고 한다면 현재 학습에 쓰이지 않는 파라미터는 지표에서 내리고 나중에 필요해지면 다시 시피유로부터 지표로 올리는 방식으로 사용이 되기 때문에 쓸 수 있고요. 그 외에도 그레디언트 체크 포인팅이라고 해서 그레디언트 계산을 위한 중간 결과물만 저장하고 이를 조금 더 효율화적으로 사용해 줄 수 있는 트릭들이 있습니다. 그러면 그레디언트 체크 포인팅도 사용할 수 있기 때문에 엑셀러레잇이 사실 상당히 많이 사용됩니다. 그래서 분산 학습이 아니더라도 라주 랭귀지 모델이나 아니면 여러 가지 뭐 믹스트 프리시전 같은 기술을 쓰실 일이 있다면 엑셀러레이터를 쓰시는 것을 추천드리겠습니다. 그러면은 FSTP를 사용을 한다면 여기 주석 처리가 되어 있는 것들을 주석 처리를 해제하고 사용하시면 되고요. 프스tp를 사용하지 않는다면 단순히 프롬 엑셀러레이으로부터 액셀러레이터를 인포트 해 오고 엑셀러레이터를 불러와서 사용을 하시면 됩니다. 네 그러면 이제 학습을 해보도록 할 텐데요. 사실 LM을 학습할 때는 적절한 포맷 프롬프팅이 중요한 전략 중에 하나입니다. 뭐 여러 가지 뭐 코드를 생성해 주고 싶을 때는 이런 식으로 포맷을 맞춰준다든가 아니면은 테이블이나 긴 문서를 같이 입력을 넣을 때는 어떤 포맷으로 넣어줄지와 같은 것들을 고민을 해야 될 텐데 현재 실습은 가장 단순한 방식으로 그냥 질문이 인풋이고 답변이 아웃풋인 방식으로 포맷팅을 해주도록 하겠습니다. 이는 단순하게 하나의 이그젬플 즉 인풋이랑 아웃풋으로 되어 있는 딕셔너리 값이 들어오면 이 레프 스트링을 통해서 하나의 스트링 값으로 변환해 주는 함수를 사용하도록 하겠습니다. 이번 실습에서는 앞서 말씀드렸다시피 세븐빌리언의 모델을 로라 튜닝 하는 방법을 살펴보도록 할 텐데요. 저희가 다루고 있는 모델 같은 경우에는 여기 보이시다. 다시피 범인 님의 코라마 세븐빌리언 모델을 이용을 하겠지만 그 외에도 뭐 만약에 여러분들이 최근 많이 회자되고 있는 미스트라 모델을 쓰고 싶다라고 하시면 사용하셔도 되지만 사실 사용을 현재 상황에서 할 수가 없습니다. 왜냐하면 하나 보여드리면서 말씀을 드리면 더 좋을 것 같은데 네 허깅페이스 모델 스에서 저희가 사용할 버인 님의 코라마 세븐 비 모델에 대한 페이지인데요. 모델 페이지를 살펴보시면 여기 오른쪽과 같이 모델 사이즈는 6.9빌리언의 파라미터고 텐서 타입이 fp 16과 f30이다라고 나와 있습니다. 이게 의미하는 게 뭐냐면 여기 오른쪽에 화살표를 클릭해 보면 조금 더 알 수 있는데 모델이 여러 레이어에 걸쳐서 파라미터들을 가지고 있을 텐데 그 파라미터가 가지는 프리시전이 프피 16이거나 프피 30이다라는 얘기입니다. 즉 각각의 모델 웨이트가 가지는 프리시전을 의미하는 게 이거라고 할 수 있는데요. 여러분들이 지금 부스트 캠프에서 제공해 주는 GPU인 vba 같은 경우에는 프피 16 프피 32 밖에 사용할 수 없습니다. 하지만 미스트라 같은 경우에는 비프 16으로 되어 있고 해당 프리시전은 브이백에서 사용이 불가능하기 때문에 이를 고려하셔서 모델을 선택해 주시면 좋을 것 같습니다. 우선 모델은 오토 모델 폴 커주얼 LM 클래스를 이용을 해서 단순하게 불러오도록 할 테고 모델의 프리시전이 아까 보셨다시피 플로 16이었기 때문에 fp 16으로 불러오도록 해서 지피유에 올리도록 하겠습니다. 이 과정을 할 때 살펴보면 처음 모델을 다운로드 할 때부터 모델 웨이트가 0001 오브 00 15라고 되어 있잖아요 이게 무슨 의미냐면 모델 웨이트가 너무 크다 보니까 이게 세븐빌리온이니까 15기가 정도 되거든요 15기가짜리를 하나의 파일로 저장하는 게 아니라 15개의 파일로 쪼개서 저장해 놓고 그 15개 파일을 순서대로 다운 받고 있는 과정이다라고 할 수 있습니다. 그래서 15개 파일 중에 첫 번째 파일 다운 받을 때도 보면 965메가바이트 거의 1기가에 가까운 파일을 다운 받고 있는 걸 알 수가 있습니다. 모델 다운로드 자체를 받으면서 조금만 더 말씀을 드리자면 이 모델 파라미터 수랑 모델의 실제 기가바이트로 크기가 얼마인지를 저희가 환산해서 생각을 해 볼 수가 있는데요. fp16이나 bf16 같은 경우에는 결국에 하나의 파라미터가 두 배 정도를 차지하게 되니까 어 세븐빌리언이다라고 하면은 2배 시키면은 15기가에서 14기가 정도 차지한다라고 보시면 될 것 같습니다. 그래서 이거는 프리시전이 fp 16이나 비프 16일 때 얘기고요. 또 달라지면 얘기가 달라지게 됩니다. 열다섯 번째까지 다 다운로드를 받고 나서 로딩하는 과정에서도 결국 15개의 파일을 다 로딩해 놓고 이거를 한 번에 합치면서 모델을 구성을 해주고 있는 과정을 보실 수 있습니다. 네 모델 다운로드가 모두 끝났고 모델이 올라갔으니까 모델이 잘 올라갔는지 살펴보자면 보시는 것처럼 모델 크기만 봐도 벌써 14기가 정도의 GPU 메모리를 사용하고 있는 걸 볼 수가 있습니다. 토크나이저를 불러와서 학습에 사용할 데이터들을 전처리하도록 하겠습니다. 이때 모델과 동일한 토크 아이디를 가져와야 되니까 베이스 모델 아이디를 입력해 주게 되고 어 디코더 온리 모델이기 때문에 패딩이 왼쪽에 되어야 됩니다. 그래서 패딩 사이드 레프트 선언해 주고 이오스 토큰과 비오에스 토큰을 선언해 주도록 하겠습니다. 특히나 LLM들 디코더 온리 모델들 같은 경우에는 패딩 토큰하고 이오스 토큰 둘 중에 하나가 선원이 안 되어 있는 경우가 왕왕 있습니다. 그래서 이오스 토큰을 따로 설정 안 해주거나 패딩 토큰을 따로 설정을 안 해주게 되면 모델이 생성을 멈추지 않고 끊임없이 하는 경우가 있기 때문에 이 코드를 통해서 자 패드 토큰하고 이오스 토큰이 같아를 꼭 해 주시길 바라겠습니다. 그러면은 이제 포맷팅을 하고 이걸 토크나이저에 통과시켜서 모델 인풋을 구축하는 함수 제너레이드 앤 토크나이즈 프롬프트 함수까지 선언을 해 주게 되고 이 함수를 맵을 통해서 실제 학습 데이터와 평가 데이터셋에 대해서 전처리를 적용한 토크나이즈드 트레인 데이터 셋 그리고 토크나이즈드 벨 데이터셋을 만들어 주도록 하겠습니다. 네 이렇게 전처리된 데이터를 모두 저장을 했고요. 토크나이즈드 트레인 데이터 토크나이즈드 벨 데이터 셋이 완성됐습니다. 근데 사실 이걸 만들 때는 맥스 랭스를 고려하지 않았습니다. 즉 데이터가 길이가 얼마가 되는지 모든 데이터를 다 그냥 토크나이즈 시켰었는데요. 이 데이터를 시각화해서 살펴보게 되면 시각화해서 살펴보게 되면 이런 식으로 길이 분포를 가지게 됩니다. 즉 스축이 각각의 데이터들의 길이에 해당되고요. y축이 그 길이들의 분포의 빈도수라고 본다면 데이터가 2천개 정도의 길이를 가지는 데이터는 거의 없고 뭐 천 개 정도의 데이터도 상당히 드문 것을 확인할 수 있습니다. 데이터가 긴 길이를 반영하면 할수록 학습에 있어 더 많은 데이터를 더 많은 정보를 학습할 수 있으니까 이 장점이 있을 수도 있겠지만 트랜스포머 모델들의 구조 특성상 어 인풋 랭스가 길어지면 길어질수록 랭스에 대해서 쿼드라틱하게 시간 복잡도 공간 복잡도가 증가하기 때문에 결국에 적절한 그 스위스팟을 찾는 것이 중요합니다. 지금 같은 경우에는 단순하게 500시비 정도로 잘라서 사용을 하도록 하겠습니다. 그러면은 이 프롬프트의 맥스랭까지 고려해서 포맷팅을 해보자면 입력 데이터들에 대해서 토크나이징을 해줄 텐데 이때 맥스 랭스 만큼만 잡아주게 될 거고 어 맥스랭스보다 길면은 트링케이션 트루를 통해서 잘라주게 되고 짧으면은 패딩을 통해서 맥스랭스만큼을 패드 토큰으로 채워주게 될 겁니다. 그리고 이때 중요한 것은 인풋과 동일한 값을 레이블로 설정을 해주고 있는 건데요. 어차피 우리가 학습하게 된 거는 모델 오브젝티브 펑션이 복잡한 게 아니라 그냥 입력 데이터에 각각의 토큰들이 다음 토큰을 예측하도록 즉 지금 시점에 들어온 토큰을 입력을 받은 모델은 그다음에 생성될 토큰을 예측하면 되기 때문에 레이블과 어 인풋 값이 동일한 값이다라는 걸 알 수가 있습니다. 그러면 이 과정을 통해서 전처리를 수행해 주도록 하겠습니다. 네 실제로 맥스 램까지 고려해서 데이터 전처리가 끝났다면 토크나이즈드 트레인 데이터 셋 그리고 토크나이즈드 펠 데이터 셋이 만들어졌을 겁니다. 이제 데이터셋을 확인해 보면 이런 식으로 패드 토큰이 0이기 때문에 0이 쭉 있고 토큰 패딩 사이드가 레프트였기 때문에 왼쪽에 패딩이 되어 있고 그 뒤로 쭉 이렇게 실제 인풋 토큰들이 있는 것을 확인할 수 있습니다. 그리고 이 토큰의 길이를 확인해 보면 맥스렌과 동일한 512로 되어 있는 걸 볼 수가 있습니다. 이제 그런 모든 데이터가 512보다 짧더라도 패딩으로 512가 맞춰졌고 512보다 길더라도 512만큼 잘려 있는 걸 알 수 있습니다. 이제 학습을 진행하도록 할 텐데요. 그레디언트 체크 포인팅 방법을 소개해 드렸는데 이 그레디언트 체크 포인팅을 사용을 하는 방법으로 얘기를 해보도록 하겠습니다. 우선 모델에 대해서 그레디언트 체크 포인팅 엔에이블을 통해서 그리던트 체크 포인팅을 사용하도록 할 거고 우리가 모델 학습을 할 때 로라를 사용을 해서 효율적으로 학습하기 위해서 프리페어 모델을 케이비트 트레이닝으로 전환해 주도록 하겠습니다. 그다음에 실제로 전체 파라미터 중에 학습되고 있는 파라미터 그러니까 모델 파라미터 중에서 리카이얼스 그레이드가 트윈 파라미터 수만 세주는 함수를 직접 선언을 해서 이 파라미터 수를 세주게 되면 전체 파라미터가 거의 세븐빌리언에 달하는 상황임에도 불구하고 학습되는 파라미터가 없는 상황입니다. 왜냐하면 저희가 모델 트레인을 원래 입력을 해서 이렇게 해서 모델 트레인을 시키잖아요. 근데 이렇게 되면 이 풀 파라미터를 학습시키게 되면 너무 브램을 많이 차지하고 속도도 느려지니까 이걸 사용하지 않고요. 이거에 대해서 직접적으로 파라미터들을 하나씩 살펴보면 인베딩 토큰 인베딩 레이어부터 0번 레이어의 큐케브 그리고 멀티헤드 어텐션을 해줄 때 쿼리 키 밸류 프로젝션 레이어나 아니면 이 피드 포워드 네트워크에서 업 해주는 네트워크 다운해주는 네트워크 레이어 n 해주는 네트워크 같은 것들 모두 다 그레디언트가 꺼져 있는 걸 확인할 수 있습니다. 그러면 이 중에서 저희는 로라를 적용하기는 할 텐데 로라를 어디다 적용하냐면 각각의 트랜스포머 모듈 중에서 멀티헤드 어텐션에 적용되는 쿼리 프로젝션 키 프로젝션 그리고 밸류 프로젝션 이 세 가지 레이어에만 로라를 적용해서 학습을 진행하도록 하겠습니다. 이를 위해서 다시 모델의 각각의 레이어를 프린트해 보면 모델 레이어 같은 경우에는 이런 식으로 구성이 되어 있는데요. 전체 라마 모델의 구조를 가지고 있지만 인베딩 레이어가 5만 2천 개의 포켓 사이즈에 대해서 4096의 디멘전을 가지고 있고 그다음에는 0번 레이어부터 31번 레이어까지 동일한 이 구조를 가지고 있는데 이 구조에는 셀프 어텐션 쿼리키 밸류 아웃풋 레이어와 피드 포워드 레이어로 구성이 되어 있는 걸 볼 수가 있습니다. 그중에서도 우리는 요거 요거 요거 이 세 가지 쿼리 키 밸류 프로젝션 레이어에 대해서 로라를 적용하겠습니다. 이때 알은 로우 랭크를 의미하게 되고요. 로라 알파는 실제로 로라 논문에 나와 있는 학습률 정도로 이해를 해 주시면 될 것 같습니다. 우리는 결국에 그러면은 전체 파라미터가 아니라 로라 파라미터만 학습을 하게 될 거고 이 로라 파라미터라고 하는 건 랭크를 32로 학습률을 64로 해서 쿼리 키 밸류 프로젝션에 대해서만 로라를 적용할 거다 라는 거를 컴피그 내에서 설정을 해주게 됩니다. 그리고 로라에도 바이어스를 적용할 수 있는데 지금은 적용하지 않겠습니다. 학습될 때 드롭 아웃을 0 0으로 적용을 하게 되는데 이 외에도 테스크가 현재 사실 거의 대부분의 라즈 랭귀지 모델들이 디코더 온리 모델이기 때문에 커즈얼 엘엠으로 학습을 하게 됩니다. 이렇게까지 설정된 로라 컴피그를 가지고 겟 해프트 모델 함수를 실행하게 되면 아 이 모델에 대해서 로라를 적용할 거고 그중에서도 랭크는 이거, 알파는 이거 그리고 그게 적용된 레이어 같은 경우에는 이런 레이어들에 적용이 되겠구나 하고 자동적으로 적용이 됩니다. 그 결과 전체 파라미터 수가 이 위에서 보시면 5 6 16으로 끝나는 숫자였는데 이게 1449로 바뀌었죠. 즉 파라미터가 조금 늘었지만 그 파라미터 수들이 요만큼인데 이게 전체 파라미터 중에서 0.3% 정도의 파라미터만 차지하는 파라미터가 추가되고 이 파라미터들만 학습을 시킬 겁니다. 즉 전체 파라미터 중에 매우 극소수의 파라미터만 학습시킴에도 LLM 전체를 훈련시키는 것과 비슷한 효과를 거둘 수 있는 로라를 직접적으로 확인할 수 있습니다. 그러면 모델이 어떻게 변하는지 살펴보자면 모델 구조가 원래 셀프 어텐션 레이어에는 쿼리 프로젝션 그리고 키 프로젝션 이런 것들이 쭉 있었는데 여기에서 원래 기존 모델의 웨이트인 베이스 레이어 외에도 드롭 아웃을 위한 로라 드롭아웃 그리고 로라가 이렇게 차원을 늘렸다가 줄어드는 두 가지 레이어가 존재하기 때문에 로라 에 레이어 로라 비 레이어가 존재하는 걸 확인할 수 있고 각각의 레이어가 랭크 32로 되어 있는 걸 확인할 수 있습니다. 그러면은 만약에 멀티 지표 상황이면 이걸 실행을 하게 될 텐데요. 이러면은 모델이 여러 지표의 병렬적으로 올라가게 되고 잘려서 자동으로 올라가게 될 겁니다. 그러면 엑셀러레이터에서 모델을 학습시키기 위해서 엑셀러레이터의 모델을 등록하는 프리페어 모델 펑션까지 메소드까지 실행을 했습니다. 사실 이 과정까지 하면 이제 학습을 위한 모든 준비는 끝났습니다. 데이터 전처리도 끝났고 모델 선언이나 로라 적용까지도 모두 마무리되었기 때문에 학습을 하게 될 텐데요. 학습 코드가 복잡할까 봐 걱정을 하실 수도 있지만 사실 라즈 랭귀지 모델들에 대해서 그냥 슈퍼바이드 파인튜닝 즉 에프티를 하는 과정은 트랜스포머에 있는 트레이너를 통해서 아주 단순하게 할 수 있습니다. 여러 가지 컨피규레이션들만 이렇게 적용을 하면 되는데요. 프로젝트명을 로라로 하고 베이스 모델 이름이 코 라마라고 할 때 우리는 런네임을 이렇게 설정을 해주게 될 겁니다. 트레이너 같은 경우에는 이 모델에 대해서 훈련을 할 거고 트윈 데이터는 이거고 이밸류에이션 데이터는 이걸로 설정 되어 있습니다. 그리고 아규먼트로 들어가는 것들은 피닝 아규먼트들이 들어가게 되는데요. 이 아웃풋 트렉토리의 모델 웨이트가 저장이 될 거고 웜업 스텝을 이 정도로 해놓고 배치 사이즈가 2고 그레디언트 어큐뮬레이션을 5로 했으니 실제 배치 사이즈는 10으로 돌아가게 될 겁니다. 그리고 학습 스텝은 3천 스텝이 돌아가게 될 거고 러닝 네이트는 이 정도가 될 겁니다. 그리고 원래 다른 수많은 라즈 랭귀지 모델들의 프리시전이 비프16이기 때문에 그 경우에는 트루로 해야 되지만 저희가 현재 사용할 모델 같은 경우에는 프피16이기 때문에 펄스로 설정을 해놨습니다. 옵티마이저 같은 경우에는 아담w를 사용하게 될 거고요. 로깅은 100 스텝마다 하게 될 겁니다. 실제로 학습을 할 때는 1 스텝마다 찍겠지만 지금은 저희가 실습을 위해서 빠르게 이 스텝들에 대한 결과물들을 보기 위해서 5 스텝으로 수정을 해보도록 하겠습니다. 그리고 세이브 스텝은 100스텝으로 하고 이밸류에이션은 100스텝마다 이루어지도록 하겠습니다. 그리고 데이터 컬렉터 역시도 이 배치 단위로 입력된 데이터를 합쳐주는 함수라고 할 수 있는데요. 이것 역시도 트랜스포머에서 사전에 잘 정의가 되어 있기 때문에 해당 함수를 사용하도록 하겠습니다. 그러면 이렇게 트레이너를 쭉 해서 선언을 해 주게 되고요. 선언된 트레이너에 대해서 트레인을 시켜보도록 하겠습니다. 네 트레이너를 통해서 현재 트레인이 되고 있는 상태이고요. 그 때문에 이 브이램 사용량이 현재 30기가까지 늘어난 걸 볼 수가 있습니다. 현재 두 번째 스텝이 돌고 있는 걸 확인할 수 있고 스텝이 계속 돌아가게 될 겁니다. 이게 그레디언트 어큐뮬레이션도 쓰고 있는 상황에다가 모델 사이즈도 크기 때문에 한 스텝 한 스텝이 매우 오래 걸립니다. 그래서 보시게 되면 시간당 0.06 이터레이션이 돌고 있는 상황이 되죠. 그러면 이 학습 상황 역시도 너무 느려서 현재 시간 내에 모든 걸 다 설명드리기는 힘들 수도 있으니까 조금 더 아큐먼트를 수정을 해서 현재 실습에서는 보여드리도록 하겠습니다. 우선 학습 스텝을 다섯 스텝만 시켜보도록 하고요. 1 스텝만 시켜볼까요? 크레디언트 어큐뮬레이션도 두 번만 적용하도록 하겠습니다. 그다음에 세이브 스텝은 스텝 단위로 두 스텝마다 저장을 하게 할 거고 이밸류에이션 는 한 번마다 이밸류에이션을 하도록 하겠습니다. 그럼 이걸 실행을 해보면 현재 열 스텝만 학습을 했으니 이렇게 되는데 열 스텝이 도는 동안 이밸류에이션이 돌고 있는 걸 볼 수가 있습니다. 이제 데이터도 준비가 되어 있고 모델과 모델에서 학습된 로라 파라미터 역시도 모두 준비가 되었으니 학습을 시키면 되는데요. 실제 학습 코드는 아래와 같이 트레이너 클래스를 이용을 해서 진행이 되게 됩니다. 하지만 전체 코드를 다 실행을 하고 다 학습하는 건 너무 오래 걸리니까 저희는 이 밸리데이션 데이터 앱 중에서도 일부만 사용을 해서 빠르게 확인을 해보도록 하겠습니다. 코드에 대해서 조금만 설명을 드리자면 일반적으로 NLP 특히나 현재 LLM과 같이 슈퍼바이저도 파인튜닝을 시킬 때는 디코더 온리 모델에 대해서 랭귀지 모델링으로 학습이 되기 때문에 트레이너 클래스로 매우 간단하게 학습을 시킬 수 있습니다. 학습할 모델 그리고 학습할 데이터셋과 평가할 데이터셋을 선언해 주게 되고 학습에 대한 구체적인 아규멘트들 역시 트랜스포머스의 트레이닝 아규먼트로 입력을 할 수 있습니다. 이 중에서도 퍼트 바이스 트레인 배치 사이즈라고 하면 만약에 여러 개의 GPU를 사용할 때 각각의 GPU마다의 배치 사이즈를 의미하게 되는데요. 현재 GPU가 1개니까 어 하나의 지표에서 2개 배치 사이즈를 2로 가져가게 될 거고 이에 대해서 그레디언트 어큐뮬레이션을 두 번만 적용해서 총 배치 사이즈가 4일 겁니다. 실습을 위해서 간소화한 거고요. 실제로는 여러분들 거에서는 5로 설정이 되어 있을 겁니다. 맥스 스텝은 열 스텝만 학습을 시켜 볼 거고요. 옵티마이저로는 아담블를 사용을 하고 로깅 스텝도 한 번씩 해보도록 하겠습니다. 이밸류에이션도 한 번마다 매 스텝마다 이밸류에이션을 진행을 해보도록 하겠습니다. 학습을 위한 준비는 모두 끝났습니다. 모델도 준비가 되어 있고 모델에 대해서 학습될 로라 파라미터도 설정이 되어 있고요. 데이터도 모두 전처리가 되어서 준비가 된 상태라서 학습을 진행해 보도록 하겠습니다. 근데 학습을 하면서 저희가 실습이기 때문에 이제 시간 관계상 밸리데이션 데이터 셋을 전체 데이터가 아니라 이 중에서 일부 데이터만 사용해 보도록 하겠습니다. 10개 데이터에 대해서만 진행을 해보도록 하겠습니다. 사실 라즈 랭귀지 모델의 에프 튜닝 학습 같은 경우에는 매우 단순하게 코드로 진행을 할 수가 있습니다. 트랜스포머스의 트레이너 클래스를 이용을 하면 되는데요. 이 트레이너 클래스를 이용을 해서 어떤 모델을 학습시킬지 학습 데이터가 무엇인지 이밸류에이션 데이터가 무엇인지만 설정해 주면 됩니다. 그 외에 학습에 필요한 여러 가지 컴퓨레에이션들은 큐에닝 어그먼트 클래스를 통해서 입력을 할 수가 있고요. 이 중에서 폴 디바이스 트레인 배치 사이즈라고 하는 것은 지피유가 여러 개일 때 각각의 지표에 올라가게 될 배치 사이즈를 의미하게 되는데 저희는 현재 실습이나 여러분들 서버 환경상 하나의 지표만 사용하기 때문에 배치 사이즈가 2라는 거는 한 번에 2개의 데이터 스 포워딩을 하겠다라는 얘기가 될 것이고 그레디언트 어큐뮬레이션을 적용을 한다는 건 이거를 두 배씩 배치 사이즈를 늘리도록 그레디언트 어큐뮬레이션 를 사용하겠다는 겁니다. 즉 배치 사이즈가 현재는 4가 되겠죠 그리고 총 10스텝만큼 학습을 시키게 될 거고 대부분의 LLM 모델들은 원래 프리시즌이 bf16이지만 현재 저희가 사용하는 모델은 프피16이기 때문에 비프16을 펄스로 설정하고 옵티마이저는 아담블 로깅은 매 스텝마다 그리고 두 스텝마다 웨이트를 저장을 하고 이별을 하도록 하겠습니다. 그리고 데이터 컬렉터는 이 하나의 데이터들을 각각 배치로 묶어주기 위한 함수라고 할 수 있는데요. 이것 역시도 트랜스포머스에서 이미 제공하는 게 있기 때문에 이를 이용하도록 하겠습니다. 그러면 이 모든 것들을 이용을 해서 트레이너를 설정을 해주고 설정된 트레이너에 대해서 트레인을 하게 된다면 밑에 보시는 바와 같이 이게 NVIDIA 에스엠아인데요 어 학습 과정에서 31기가 정도를 현재 차지하고 있는 걸 볼 수가 있습니다. 그러면은 첫 번째 스텝에서 트레이닝 노스가 5.39 정도가 나왔네요. 매 스텝마다 이밸류에이션을 진행을 하고 있기 때문에 사실 로스가 거의 떨어지지는 않을 겁니다. 밸리 트레이닝 로스 역시도 플록츄이션이 있기 때문에 무조건 꾸준히 하락한다는 보장은 없고요. 네 그런데 이렇게 지금 세이브 스텝이 2로 되어 있기 때문에 두 스텝마다 모델이 저장되고 있을 겁니다. 그래서 실제로 살펴보면 두 번째 체크 포인트에서 저장된 모델들이 이렇게 실제로 저장되고 있는 걸 볼 수가 있습니다. 그러면 모델 학습이 진행되는 동안 이 실제로 이 웨이트가 어떤지 한번 살펴보도록 할 텐데요. 이 웨이트가 저장된 곳을 가서 한번 살펴보자면 재밌게도 모델 웨이트는 결국 어댑터 모델 세이프 텐서스 이게 모델 웨이트가 저장된 부분일 텐데 모델 웨이트가 97메가바이트밖에 되지 않습니다. 허깅페이스에서 사용하는 트레이너가 저장할 때는 이 저희가 사용하고 있는 코라마를 모든 웨이트 즉 세븐빌리언의 웨이트를 저장하는 게 아니라 어댑터의 웨이트 즉 로라의 웨이트만 저장을 하게 됩니다. 왜냐하면은 사실 세븐빌리언의 웨이트는 학습이 되고 있지 않습니다. 저희가 학습하는 건 로라 웨이트만 학습하고 있기 때문에 학습 도중에 지속적으로 변하는 웨이트는 로라 웨이트고 그렇기 때문에 실제로 저장해야 될 웨이트 역시도 로라의 값만 저장하면 돼서 매우 작은 값 모델 웨이트만 저장되고 있는 걸 확인할 수가 있을 겁니다. 그래서 실제로 확인을 해봐도 트레이닝 로스 역시 어느 정도 떨어지는 걸 확인할 수 있고 그에 비해서 밸리데이션 로스는 막 엄청 드라마틱한 감소를 보이고 있지는 않지만 조금씩 감소하는 걸 확인할 수가 있습니다. 여러분들이 이 실습 동영상 이후에 실제로 학습을 시켜 보시면 매우 오랜 시간 학습을 하게 될 겁니다. 10시간 넘게 학습이 진행될 것 같은데 그 과정 중에서 뭐 완디이나 아니면은 그냥 주피터 노트북으로 확인을 해 보셔도 매우 꾸준하게 로스가 잘 떨어지고 밸리데이션 로스도 어느 정도 잘 감소하는 걸 확인할 수 있으실 겁니다. 자 만약에 학습이 다 마무리되었다고 친다면 엘엘엠을 학습할 때 사용했던 데이터가 특정 테스크에 대해서 타겟팅을 해 가지고 인풋과 아웃풋이 정해져 있지 않고 정말로 거의 모든 도메인과 모든 테스크에 대해서 열려 있는 다양한 테스크들이 섞여 있는 형태였습니다. 그러면 이 상황에서 어떻게 모델을 평가해야 될지 어떤 테스크로 평가해야 될지에 대해서는 이밸류에이션 실습을 통해서 한번 살펴보도록 하겠습니다. 지금까지 라지 랭귀지 모델을 불러오고 로라에 대해서 적용을 하고 인스트럭션 튜닝 학습을 위한 데이터 확보 전처리 그리고 실제 학습과 로깅 과정에 대해서 살펴보는 실습이었습니다. 네 그럼 이어서 이밸류에이션 실습에서 뵙겠습니다. 감사합니다."
}