{
  "lecture_name": "[MRC] (2강)Extraction-based MRC",
  "source_file": "[MRC] (2강)Extraction-based MRC_104.mp4_2025-12-04-104441646.json",
  "text": "네 안녕하세요 머신 리딩 컴프레션 수업 2강 시작하도록 하겠습니다. 오늘 수업은 총 4개의 파트로 구성이 됩니다. 첫 번째로는 저희가 MRC 특히 익스트랙션 베이스 형태로 접근을 한다는 게 어떤 의미인지 살펴보도록 하고요. 파트 2에서는 이런 MRC 모델을 학습하기 위해 준비해야 되는 단계 프리 프로세싱이 어떤 것들이 있는지 알아보도록 하겠습니다. 파트3에서는 실제로 모델을 파인튜닝 한 방법론을 알아보고요. 그리고 파트 4에서는 이 모델이 출력 값을 어떻게 포스트 프로세스에서 저희가 원하는 최종 형태로 바꿀 수 있을지 알아보도록 하겠습니다. 네 파트 1입니다. 저희가 간단하게 1강에서 살펴본 것과 같이 이제 익스트랙션 밸리스 MRC라 함은 질문과 답변이 질문에 대한 답변이 항상 주어진 지문 내에 존재한다고 보시면 될 것 같습니다. 물론 이런 어썸션이 항상 맞는 건 아닌데요. 하지만 예를 들면 아래와 같이 저렇게 질문이 워스 커미셔널 얼리 투니 투 이렇게 써 있었을 때 정답 라 굿데이 실제로 문서 내에 존재하는 걸 보실 수가 있는데요. 이렇게 포뮬레이션을 가정을 하게 되면은 저희가 텍스트를 생성하는 것이 아닌 텍스트의 위치만 파악하는 것으로 문제를 포뮬레이트 할 수 있기 때문에 상당히 여러모로 편리한 경우가 많습니다. 그래서 이런 방식을 채택하는 MRC 데이터셋이 스쿼드 콜쿼드 뉴스큐에 내추럴 퀘스천스 등이 있습니다. 그리고 실제로 저희가 이 데이터셋들을 제작한 웹사이트에서 다운받을 수도 있겠지만 편리성을 위해서 허깅페이스라는 웹사이트에서 제공하는 데이터 셋을 다운받도록 합니다. 실제로 데이터 셋이 모두 다운받기 쉬운 형태로 되어 있고 그리고 또 포맷 또한 다루기 쉬운 형태로 일관적으로 돼 있기 때문에 이번 수업을 들으실 때 활용하시게 되면 상당히 유용하실 거라 생각합니다. MRC의 평가 방법은 크게 두 가지가 있는데요. 이것도 또한 지난 넥처에서 말씀을 드렸죠. EM이랑 F1이 그것인데요. EM 같은 경우는 익젝트 매치 즉 정확하게 일치하는 경우에만 점수를 부과하고요. 1 같은 경우는 좀 더 소프트한 매트릭으로 부분적으로 정답과 예측 값이 겹친다 하더라도 점수를 일부 줄 수 있는 방식입니다. 그래서 보시다시피 현재 리더보드에도 2m보다 프1이 조금 더 높은 걸 보실 수가 있죠. 그리고 왼쪽에 보시는 리더보드 같은 경우는 오리지널 영어 스쿼드 데이터셋의 리더 보드이고 오른쪽 같은 경우는 이런 영어 데이터 셋을 한국어 버전으로 변형하고 새로 만들어 어 엘지 씨엔스에서 배포하고 있는 퀄쿼드라는 데이터셋의 리더 보드입니다. 네 다음은 이젠 매치 스코어를 좀 더 엄밀하게 알아보면요. 보시다시피 정답이 조금이라도 예측값과 다른 경우에는 점수를 0점을 부여합니다. F1 스코어 같은 경우는 예측 값과 정답의 오버 레블 비율로 계산하게 됩니다. EM과 달리 0점과 1점 사이에 부분 점수를 받을 수 있습니다. 오른쪽에 계산하는 방법이 나와 있는데요. 프리시전 같은 경우는 분자에는 두 예측값과 실제 정답 두 사이에 겹치는 단어의 개수를 계산을 하고요. 분모에는 예측값의 토큰 총 개수를 계산하여 두 숫자를 나누도록 합니다. 리콜 같은 경우는 분자는 똑같은데요. 분모가 좀 다르죠. 분모 같은 경우는 실제 정답 값의 토큰 개수를 계산을 하여 두 숫자를 나누게 됩니다. 그래서 보시게 되면은 프리시전 같은 경우는 필요 없이 예측 값이 좀 길어지게 되면은 아무리 겹치는 값이 많다 하더라도 분모가 커지게 되기 때문에 프리시전이 내려가게 되고요. 리콜 같은 경우에는 반대로 둘이 겹치는 단어의 개수가 너무 적으면 리콜이 내려가게 됩니다. 그리고 F1은 이 두 개를 곱한 값에 2를 곱해주고 이 두 개를 더한 값으로 나눠주는 방식으로 산수를 하게 됩니다. 그래서 저희가 계산해 볼 수 있는 예제인데요. 보시다시피 위에 이그젬플 같은 경우는 정답 정답이 널브스 브레이크 다운이고요. 또 널브스이기도 하고요. 그리고 프리딕션 같은 경우는 널브스였습니다. 만약에 저희가 이 경우 첫 번째 답 즉 nevest 브레이크 다운이 완전 유일한 정답이라고 만약 가정을 한다면 저희 예측값 같은 경우는 실제로 프리시션은 1점이겠죠. 왜냐하면은 두 개 겹치는 게 하나고 그리고 프리딕션의 토큰 개수도 하나니까요. 하지만 리콜 값이 0.5가 되는데요. 겹치는 건 하나의 토큰 널브스지만 정답 값 같은 경우는 브레이크 다운이라는 단어가 추가적으로 하나 더 있으므로 분모가 2가 되어 리콜 값은 0.5가 됩니다. 마지막으로는 F1은 이 두 숫자를 곱한 다음에 이를 또 곱해주고 이 두 개의 수치를 더한 값으로 나눠주는 최종 0.67이 나오는 것을 알 수가 있습니다. 물론 실제로 그라운트스 엔서가 하나만 있는 게 아니기 때문에 이런 방식으로 계산을 각각 각 그라운트스 엔서 하나마다 계산을 하여 실제 이밸류에이션은 그중 가장 높은 숫자를 내보내는 방식으로 합니다. 보시다시피 그라운트 홀스 앤서가 널브스 브레이 다운, 널브스 브레이 다운 널브스 브렉 다운 이렇게 밖에 없기 때문에 이 경우는 다 0.67로 계산이 될 거고요. 따라서 만약에 프리딕션이 널브스였으면 최종 F1 스코어도 0.67이 됩니다. 물론 0.67이라는 숫자를 보통 67로 편의상 표현하기도 합니다. 이제 저희가 어떻게 이밸류에이션 할지를 알아봤으니 이런 이밸류에이션을 잘 할 수 있는 모델을 만드는 방법을 알아보도록 하겠습니다. 저희가 만든 모델의 오버 뷰인데요. 사실 좀 복잡해 보이실 수 있지만 좀 들여다보면 상당히 심플해요. 일단 먼저 밑에 인풋이 있죠. 컨텍스트 쪽에 인풋이 있고 퀘스천 쪽에 인풋이 있고요. 그리고 정답 같은 경우는 이제 인풋이라기보다는 저거는 저희가 학습할 때 필요한 데이터고요. 그러면 실제로 모델에서는 컨텍스트와 퀘스천이 저렇게 단어가 쪼개져 가지고 먼저 나가게 됩니다. 저희가 1강에서 배웠던 토크나이제이션을 활용해서요. 그래서 컨텍스트 같은 경우는 씨 토크 원 씨 토크 2 이렇게 해서 씨 토크 엔까지 가게 되고요. 퀘스천 같은 경우는 큐 토크 1부터 시작해서 큐토크 엠까지 가게 됩니다. 이거를 저희가 월드 인베딩이라는 컨셉을 이용을 해서 벡터화를 시키고요. 그다음에 이 벡터들이 저 파란색 모델 안에 들어가게 돼서 최종적으로는 모델이 내보내는 값은 시작점과 끝점이 됩니다. 여기서 시작점과 끝점을 내보낸다고 하는 것은 저 숫자를 내보내는 것이 아니라 저 컨텍스트와 퀘스천에 해당하는 각각 단어에 해당하는 컨텍스셜라이즈 벡터가 나오게 됩니다. 이 컨텍스셜라이즈 벡터들을 각각 스케일러 밸류로 내보냄으로써 일종의 각 포지션이 시작 또는 끝 값이 될 수 있는 점수가 되겠죠. 그 점수 중에 가장 높은 위치를 찾는 방식으로 스타트 포지션과 앤디 포지션을 예측하게 됩니다. 스타트랑 앤드가 예측이 되고 나서부터는 최종 예측값은 쉽게 구할 수 있겠죠. 말 그대로 시작과 끝 사이에 있는 스팸을 그대로 가져와서 탑으로 내보내게 됩니다. 네 그러면 제가 먼저 첫 번째 단계로 프리 프로세싱을 알아보도록 하겠습니다. 프리 프로세싱 단계에서 저희가 볼 텍스트의 예시입니다. 보시다시피 타이틀과 컨텍스트 그리고 퀘스천 앤서 페어즈가 주어지는데요. 여기서 정답은 주황색에 해당하는 부분입니다. 첫 번째 질문 같은 경우는요. 그리고 두 번째 질문 같은 경우는 파란색에 해당하는 질문입니다. 저희가 토크나이제이션이라는 단계를 먼저 거쳐야 할 텐데요. 토크나이제이션 같은 경우는 텍스트를 작은 단위로 나누는 것이라고 보시면 됩니다. 가장 간단한 방법으로는 띄어쓰기 기준으로 텍스트를 나눌 수 있을 것 같아요. 또는 조금 더 복잡하게 들어가기 시작하면 형태소나 서브워드 등으로 기준을 바꿀 수도 있습니다. 요즘 최근에는 사실 AR 보케블러리 이슈나 여러 가지 정보학적인 관점에서 이점을 가진 바이트 페어 인코딩을 활용한 서브워드 토크나이제이션이 가장 주류로 사용되고 있습니다. 그리고 본 강의에서도 비피 방법론 중 하나인 워드 피스의 토크나이저를 사용할 건데요. 이렇게 말씀드리면 좀 와닿지가 않으시겠죠 밑에 예시를 보시면 됩니다. 워드 피스 토크나이저를 사용하게 되는 경우 미국 군대 내 두 번째로 높은 직위는 무엇인가라는 텍스트를 저희가 분할하게 되면은 이렇게 단어가 나눠지게 됩니다. 미국이란 단어로 나눠지고 처음에 그다음 군대 내 두 번째 그리고 샵 샵 로고라는 단어가 나오는데요. 여기서 샵샵이 의미하는 의미는 바로 앞에 단어랑 스페이스가 없다는 뜻입니다. 즉 붙어 있다는 뜻이죠. 그래서 저희가 이 단어를 나중에 원래 문장을 reconstruct 하고 싶을 때도 좀 더 편리하게 할 수가 있겠죠. 그리고 그다음으로 높은 직위는 같은 경우도 s sak이 붙어 있죠 무엇인가 이렇게 되는 것을 보실 수가 있습니다. 그리고 하나 중요한 거는 좀 어떤 어떤 개념으로 보시면 되냐면 워드피스 같은 경우는 결국에는 자주 나오는 단어 같은 경우는 저렇게 미국 군대처럼 하나의 단어로 이루어지고 비교적 자주 나오지 않는 단어 같은 경우는 좀 따로 나눠지는 걸 볼 수가 있습니다. 예를 들면 직위는이라는 단어는 좀 덜 자주 나오겠죠 그러다 보니까 직위은이 한 단어가 되지 못하고 직과 위은이 나눠진 것을 보실 수가 있습니다. 그래서 실제로 저희가 프리퍼레싱을 하게 되면은 이제 이런 토크나이제이션을 하게 되고요. 그리고 또한 중요한 거는 실제로 저희가 스페셜 토큰을 이용을 해서 퀘스천과 컨텍스트를 구분하도록 합니다. 보시다시피 질문이 미국 군대 내 두 번째로 높은 직위는 무엇인가이고 그리고 그다음에 지문이 나오죠 컨텍스트가 나오는데 이 두 개를 sep라는 저 토큰이 분리하고 있는 것을 보실 수가 있습니다. 그리고 마찬가지로 또 CLS라는 토큰으로 시작하는 걸 알 수가 있는데 저희가 편의상 사용을 하고 있는 그런 토큰이라고 보시면 되고요. 그리고 이런 스페셜 토큰들이 결국은 실제로 토크나이즈 이후에도 계속 존재하는 걸 볼 수가 있습니다. 그래서 이렇게 토크나이즈 된 형태로 모델에 들어가신다고 보시면 됩니다. 그리고 어텐션 마스크라는 그 개념이 또 있는데요. 이거 같은 경우는 저희가 입력 시퀀스 중에서 어텐션을 연산할 때 무시할 토큰을 표시하는 방법론으로 보시면 됩니다. 그래서 0은 무시하고 1은 연산에 포함하게 됩니다. 그럼 보통 무시하는 토큰들은 어떤 걸까요? 저희가 방금 봤던 것처럼 스페셜 토큰들 중에 특히 패드라는 토큰 같은 경우는 저희가 그 문장의 길이를 똑같이 맞춰주기 위해서 사용을 하는데요. 패드 같은 경우는 당연히 의미가 없는 이 특수 토큰이기 때문에 실제로 어텐션 마스크에서도 0으로 처리가 됩니다. 그래서 저희가 입력 값을 모델에 줄 때 어텐션 마스크도 같이 주게 되는데 즉 입력 값을 주면서 여기서 모델 입장에서 무시할 토큰들은 어떤 것이야 이렇게 알려주는 거라 보시면 될 것 같아요. 이제 그런다고 했었을 때 보시면은 처음에는 다 전부 다 숫자 1로 되어 있는 걸 보실 수가 있고 나중 가면 패드에 해당되는 토큰들은 전부 다 0인 것을 아실 수가 있습니다. 마지막으로는 토큰 타입 아이디즈라는 것을 또한 입력 값에 같이 주어 주는데요. 이거 같은 경우는 아까 보셨다시피 저희가 입력이 2개 이상일 때 예를 들어 저희 MRC 같은 경우는 질문과 지문 이렇게 두 개가 들어가게 되겠죠. 저희가 이 두 개를 항상 붙이게 되는데 그 붙였을 때 그래도 두 개를 구분할 수 있어야 되겠죠. 구분하는 방법이 두 가지가 있는 거죠. 첫 번째는 아까 보여드린 것처럼 s이라는 토큰을 통해서 그 두 개 사이를 나눠주는 게 있고 두 번째로는 좀 더 직접적으로 각 단어의 또 어떤 플래그를 이렇게 0과 1로 줌으로써 하는 방식이 있고요. 그래서 보시다시피 이제 처음에 질문에 해당하는 토큰들 같은 경우는 0을 주고 이제 컨텍스트에 해당하는 토큰들 같은 경우 1을 주는 걸 보실 수가 있습니다. 패드 같은 경우는 편의상 그냥 0을 주도록 합니다. 이렇게 프리 프로세싱을 하고 난 다음에 저희가 이제 그 정답 같은 경우도 학습할 때는 같이 프로세스를 해서 줘야겠죠. 여기서 생길 수 있는 문제가 정답은 저희가 텍스트 레벨에서 캐릭터 위치로 파악을 하고 있는데 토크 아이즈를 하고 난 다음에 학습할 때 시그널을 주는 방식은 이 토큰이 정답 토큰이 어디에 있는가로 되기 때문에 조금의 약간의 프로세싱이 필요합니다. 보시다시피 저기서 정답이 미국 육군 부참모총장이라고 했었을 때 그 원래 오리지널 텍스트에 해당하는 토큰들을 찾을 수 있어야 되는데요. 찾는 방법은 여러 가지가 있지만 가장 간단하게는 해당 텍스트를 그대로 포함하는 모든 단어를 가져와서 시작과 끝을 표시하는 방식이 있습니다. 대부분의 경우는 문제가 없는데 가끔 가다가 예를 들면은 정답은 미국 육군 부참모총장에서 이제 예를 들면 국 육군 부참모총장 이렇게 좀 부분적으로 나눠질 수도 있겠죠. 이렇게 되면은 정확하게 일치하는 정답은 없기 때문에 조금 이제 애매할 수도 있긴 한데 그렇다 하더라도 최소한으로 포함하는 단어들을 전부 다 정답으로 포함하여 학습할 때 사용한다고 생각하시면 될 것 같습니다. 네 세 번째 파트로는 파인 튜닝인데요. 제가 아까 보여드렸던 오버 뷰입니다. 사실 저희가 이번 수업에서는 버트 내부의 모델이 어떻게 돌아가는지에 대해서 자세하게 들어가지 않으려고 해요. 버트라는 모델이 일종의 블랙박스 또는 매직 박스라고 생각을 하시고 그 매직 박스를 MRC에 어떻게 잘 활용할 수 있을까가 이번 강의에 이번 전체 수업의 목적입니다. 그래서 버트라는 블루 박스는 저기서 블루 박스죠. 블루 박스는 인풋은 컨텍스트와 퀘스천 같은 어떤 토큰들의 임베딩이 인풋으로 들어오게 되고요. 저 박스가 내보내는 값도 마찬가지로 컨텍스트 퀘스천들의 임베딩을 내보내게 되는데 인풋과 다르게 이 인베딩들은 컨텍스트 라이즈 돼 있는 인베딩들이고 활용도가 상당히 높습니다. 그래서 저희가 이걸 MRC에 적용하고 싶다고 한다면은 컨텍스트에서 나온 인베딩의 아웃풋 값을 점수화시켜야 되는데요. 프리딕션을 낼 수 있도록 정답을 사실 여러 가지 방법이 있겠지만 가장 간단한 방법 중 하나는 그 지문 내에서 정답에 해당되는 각 인베딩을 저희가 실제로 어떤 리니어 트랜스포메이션을 통해서 각 단어마다 하나의 숫자가 나올 수 있도록 바꿔주고 물론 이 리니어 트랜스포메이션도 학습할 대상이겠죠. 그러면은 저희가 각 버트 지문 토큰마다 하나의 숫자가 아웃풋에 나오게 되고요. 그 숫자는 점수로 볼 수 있습니다. 높은 거일수록 저희가 정답으로 볼 수 있는 점수들 그래서 시작 포지션을 이런 방식으로 구하게 되고 마찬가지로 엔드 포지션도 이런 식으로 구하게 돼서 최종 답안을 내는 방식을 취합니다. 실제로 학습할 때는 이 수치를 소스 맥스를 위해 어플라이를 해서 거기에 negative 로그 라이클리 후드로 학습하는 방식을 취하는데요. 이거는 예전 다른 수업에서 많이 보셨을 거라 생각해서 코드에서 다루는 걸로 하겠습니다. 마지막으로 포스트 프로세싱입니다. 크게 두 가지로 나눌 수 있는데요. 첫 번째로는 불가능한 답을 제거하는 부분이고요. 그래서 여러 가지 이제 케이스가 있겠죠. 예를 들면 엔드 포지션이 스타트 포지션보다 앞에 있는 경우 당연히 불가능한 답변이겠죠. 뒤로 갈 수는 없으니까 또는 예측한 위치가 가끔 컨텍스트를 벗어난 경우가 있습니다. 저희가 인풋을 질문과 지문 두 개를 같이 붙여서 주기 때문에 예를 들면 질문 쪽에 정답이 있다고 나올 수도 있겠죠. 상당히 드물겠지만 그런 경우는 답에서 제거하게 됩니다. 그리고 또 정답이 너무 길어질 수는 또 없기 때문에 예를 들면 정답의 토큰 개수가 30개 이상인 경우는 또 불가능한 답에서 제거하도록 합니다. 그다음에는 최적의 답안을 찾게 되는데요. 가장 심플한 방법론으로는 저희가 스타트랑 엔드 포지션 각각 프리딕션을 아까 말씀드렸듯이 스코어로 나온다고 말씀드렸죠. 이 스코어 중에서 가장 높은 엔겔을 각각 찾습니다. 그리고 이 엔겔를 조합을 만들어 엔스퀘어의 조합이 생기면 그 엔스퀘어 중에서 불가능한 조합을 앞에 기준대로 적용해서 제거를 합니다. 그다음에 가능한 조합들을 스코어의 합이 큰 순서대로 정렬하도록 하고요. 다음으로 스코어가 가장 큰 조합을 최종 예측으로 선정하도록 합니다. 마지막으로는 tk가 필요한 경우에는 4번에서 구한 것을 차례대로 내보내는 방식을 취하도록 합니다. 네 강의 마치도록 하겠습니다. 감사합니다. 네 2강 실습 시작하겠습니다. 자 먼저 저희 오늘 실습에 활용할 인바이먼트는 콜랩이라고 하는 구글에서 제공하고 있는 그런 이제 인바먼트인데요. 그 GPT 와이파이서 노트북을 써보신 분들이 계실 텐데 그거를 저희가 GPU까지 연결해서 조금 더 편리하게 사용할 수 있는 그런 플랫폼이에요. 그래서 혹시 아직까지 사용해 보시지 않은 분들이 있다면은 오늘 좀 강의에서 강의 초기에서 조금 더 어떻게 사용하는지에 대한 말씀도 같이 드리도록 하겠습니다. 그래서 일단은 콜랩 닷 리서치 닷 그고 닷 컴으로 가시면 되고요. 주소 이렇게 있고요 가시면은 이제 저 같은 경우는 이제 프로서스크레션을 하고 있기 때문에 조금 표시가 왼쪽이 다르긴 한데 그 서브스크립션이 없이도 모두 사용이 가능하고요. 그리고 여기에서 뭐든 하는 것들은 기본적으로 구글 드라이브랑 연동이 됩니다. 그래서 좀 편리한 부분들이 있습니다. 예를 들면 저희가 어떤 모델을 학습하고 나서 그거에 대한 그 학습한 모델을 저장을 하고 싶다 하면은 구글 드라이브에 저장을 하는 방식을 취할 수가 있고요. 먼저 이렇게 들어가시면은 여러 가지 이제 그 버튼들이 보이실 텐데 먼저 이 노트북을 저희가 특정 컴퓨터에 연결을 하셔야 돼요. 그래서 이제 커넥트를 가시면 되는데 이제 그냥 커넥트를 하시게 되면은 CPU에 연결하게 됩니다. 근데 저희가 GPU를 사용하고 싶잖아요. GPU를 통해 더 빠르게 학습하고 더 빠르게 모델을 돌릴 수가 있으니까 그래서 보시면은 이제 여기서 그냥 단순하게 커넥터를 누르시는 게 아니라 런타임에 가셔 가지고 여기에 런 타임 타입을 먼저 고르셔야 돼요. 이제 하드웨어 엑설레이터라고 써 있죠 이걸 논을 하시게 되면 CPU가 되고요. 여기서 GPU를 쓰도록 할게요. 물론 이제 GPU를 쓰시게 되면은 런타임이 타임아웃이 된다고 해 가지고 이제 오랫동안 안 쓰시거나 하면은 뺏어갑니다. GPU를 그래서 혹시 이거를 돌려놨다가 다시 갔다 왔더니 GPU가 더 이상 안 돌아가고 있을 때 놀라지 마시고 기본적으로 이걸 모두가 좀 셰어하는 그런 GPU다 보니까 그때그때 좀 사용을 할 수가 있고 오랫동안 사용을 하지 않거나 또는 너무 오랫동안 사용을 하게 되면 뺏어가는 방식이라고 이해를 하시면 될 것 같습니다. 그래서 이제 세이브를 하게 되고요. 자 그러면 이제 저희가 런타임 타입을 정했고 그다음에 커넥터를 누르시면 돼요. 이제 이제 연결을 시작을 하고요. 커넥티드라는 표시가 뜨면서 램이랑 디스크 표시가 뜨면은 자 이제 여러분은 해당 노트북을 구글이 갖고 있는 컴퓨터로 연결을 했다고 보시면 됩니다. 그럼 이제부터는 저희가 자유롭게 여기 위에서 파이썬 언어들을 수행을 할 수가 있어요. 지표도 사용할 수 있고요. 예를 들면은 저희가 뭐 그냥 파이썬을 해볼게요. 이렇게 하면은 나오죠. 답변이 그리고 GPU가 있는 걸 확인해 보고 싶으시다면은 NVIDIA SRI라고 치시면은 이렇게 GPU가 나오는 걸 볼 수가 있고 보시다시피 저 같은 경우는 콜의 프로여서 비 100이 할당이 됐는데 콜의 프로가 아니시면은 아마도 k 80이나 더 낮은 게 할당이 될 수도 있습니다. 그걸 참고하시고요. 먼저 저희가 패키지를 인스톨 하도록 할게요. 방금 제가 썼던 커맨드처럼 이게 비록 파이썬 코드이긴 하지만 파이썬 노트북이긴 하지만 앞에 이 익스크레메이션 마이크를 써주시면은 그 해당 컴퓨터에서 배시 명령을 수행을 할 수가 있어요. 네 제가 설치를 먼저 해줄 텐데 이제 설치를 두 가지 라이브러리를 하시도록 할게요. 먼저 그 익스크메이션 마이크를 써주시고요. 배시 명령을 위해서 그리고 확인 페이스에 트랜스포머스라는 라이브러리를 설치를 해 주시고요. 그다음에 확인 페이스에서 또 마찬가지로 제공을 하는 데이터 셋이라는 또 파이썬 라이브러리를 설치해 주시면 됩니다. 이건 금방 설치가 되고요. 특히 이제 구글 상에 있는 컴퓨터다 보니까 다운로드나 이제 이런 것들이 상당히 빠르게 진행이 됩니다. 보시다시피 인스톨레이션이 완료됐다는 거를 아실 수가 있습니다. 자 다음으로는 저희가 좀 활용할 이 허깅 페이스 웨퍼스토리 내에서 활용할 좀 몇 가지 코드가 있기 때문에 레퍼 스토리를 다운 받은 다음에 이 웨퍼스토리를 인포트하도록 할게요. 그래서 보시면 먼저 허깅페이스 레퍼 스토리 같은 경우는 깃 홉 닷컴 허깅 페이스 트랜스포머스라는 깃으로 가고요. 그다음에 여기서 저희가 가져올 레퍼스토리 그 파일들은 시스패스닷 어펜드라는 명령어로 특정 폴더에 있는 파이썬 코드들을 전부 다 저희 여기 현재 파이썬 저희 인바먼트에 인폴트 해올 수 있습니다. 이렇게 입력을 하시게 되면은 먼저 트랜스포머라는 라이브러리를 이제는 파이썬 패킷을 다운 받는 게 아니라 기타 리퍼스토이를 다운을 받게 되고요. 거기에 대해서 파일들을 가져와 가지고 전부 다 인포트를 끝냈죠. 자 다음으로는 저희가 데이터셋 확인 페이스 데이터셋을 활용하는 방법 알아볼게요. 저희가 프롬 데이터셋 인폴트 로드 데이터셋을 하고요. 아까 저희 수업에서 보셨죠? 그다음에 데이터 세트는 저희가 콜코드를 활용할 거기 때문에 로드 데이터셋이라는 함수를 활용을 해서 콜 코드를 입력을 해 줍니다. 그러면 이제 다운을 받게 되는데 네 다운은 금방 되고요. 보시다시피 다운로드를 현재 컴퓨터상의 캐시 폴더 안에 저장을 한 걸 아실 수가 있죠. 그다음에 저희가 한번 데이터셋을 좀 들여다 볼까요? 데이터셋을 들여다본다고 하면은 이제 이 데이터셋을 저희가 프린트를 해보시면 이런 형태를 띠고 있는 걸 알 수가 있습니다. 데이터셋이라는 오브젝트를 저희가 들여다보면요. 보시다시피 데이터셋이라는 오브젝트는 트레인과 밸리데이션으로 이루어져 있는데 각각이 하나의 테이블이라고 보시면 될 것 같아요. 하나의 테이블이고 각 테이블에 헤더가 있겠죠 헤더가 아이디랑 타이틀이랑 컨텍스트랑 퀘스천이랑 앤서즈 이렇게 있고요. 그리고 개수가 밑에 적혀 있는 걸 볼 수가 있습니다. 그래서 아이디 같은 경우는 고유의 아이디고 그리고 타이틀은 문서의 타이틀 그리고 컨텍스트는 지문을 말씀드리는 거고 퀘스천이랑 또 해당되는 답변 답변이 여러 개일 수도 있는 점 기억하시죠? 그래서 이렇게 볼 수가 있고요. 그래서 이거에 대한 각각에 대한 액세스를 이렇게 하실 수 있어요. 보시면은 이렇게 하면 트레인 데이터를 액세스 할 수 있게 되고요. 마찬가지로 밸리데이션도 액세스를 하실 수가 있고 그리고 첫 번째 이 샘플을 보고 싶으시다 하면은 이렇게 서스캡션을 써주시면은 첫 번째 이샘플을 보실 수가 있습니다. 보시다시피 답변이 있고 컨텍스트가 있고 아이디가 있고 퀘스천이 있고 타이틀이 있죠 다음으로는 저희가 데이터 셋을 불러왔으니 다음으로는 저희가 이 데이터 셋을 평가하는 펑션을 불러올 텐데요. 이것도 마찬가지로 데이터 셋 라이브러리 안에 있어요. 여기에서 인포트를 하시게 되는데 로드 매트릭이라는 이제 아까 쓰셨던 게 로드 데이터셋이었죠. 마찬가지 로드 메이트릭이라는 데 그 펑션을 활용을 해서 매트릭을 불러오시게 되는데 매트릭은 그 스쿼드 매트릭을 활용을 하도록 하겠습니다. 그래서 스쿼드 같은 경우도 어떻게 보면 콜코드에서 쓰이는 비슷한 매트릭이기 때문에 실제로 이 매트릭 펑션을 다운받게 되고요. 그다음으로는 저희가 트랜스포머에서 이제는 저희가 모델을 불러오도록 할게요. 그래서 이젠 트랜스포머 쪽에서 저희가 인포트를 하시게 될 텐데 아까 저희 수업에서 들으신 것처럼 일단 먼저 컴플리게이션을 불러올 수 있는 그런 클래스를 불러오고요. 그다음에 모델을 불러올 수 있는 특히 쿠션 엔서링 쪽 모델 불러올 수 있는 클래스를 불러오고 그다음에 오토 토크나이저를 불러옵니다. 그다음에 이 클래스들을 활용을 해서 실제로 저희가 특정 모델을 불러오게 될 텐데 모델 이름을 먼저 정의를 해 주시고요. 저희 같은 경우는 버트 베이스 멀티 링구얼 케이스들을 활용을 할 거고요. 그리고 그다음에 실제 컨피규레이션은 이 모델에 해당하는 걸 가져오면 되겠죠. 그래서 오토 컴피그를 활용을 해서 해당 네임을 입력을 해 주시고 마찬가지로 토크나이저도 해당 네임에 해당하는 토크나이저를 가져와 주시면 됩니다. 마지막으로 모델도 가져올 텐데 모델 같은 경우는 저희가 한 가지 더 중요한 거는 오토 토크나이저가 아니라 오토 모델 폴 쿠션 앤서링을 하고 여기서는 인풋으로 모델 이름과 모델 이름뿐만이 아니라 컴플리게이션을 같이 넣어줍니다. 이렇게 하면 실제로 이 모델들을 다운받는 걸 보실 수가 있죠 제가 오타가 났네요. 프리프라 프리 트레인드를 해주셔야 되고요. 마찬가지로 여기도 다운 받는 걸 보실 수가 있고요. 그럼 이게 실제로 웹상에 있는 모델들 모델의 웨이트랑 토크나이저랑 그리고 컴퓨듀레이션을 전부 다 가져오는 거라고 보시면 됩니다. 상당히 컨비넌트하죠 이렇게 가져오시게 되면은 이제 각각을 한번 들여다보실 수 있는데 예를 들면 저희가 모델이라는 오브젝트를 들여다보시게 되면요. 이렇게 길게 레이아웃 노 리니얼 이런 것들이 나오게 되는데 이게 결국 이 모델의 실제 아키텍처라고 보시면 될 것 같아요. 이게 엄청 복잡해 보이긴 하지만 실제로는 트랜스포머라는 그런 아키텍처를 활용한 모델이고 보시다시피 그 똑같은 형태의 레이어가 여러 개 있는 걸 보실 수가 있어요. 그래서 제가 수업에서 말씀을 드린 것처럼 실제 버튼은 버트의 인풋은 텍스트라고 보시면 되고 버트의 아웃풋 같은 경우 각 텍스트 토큰에 해당되는 벡터라고 보시면 될 것 같아요. 이게 저희는 일종의 이 버트를 저희가 좀 매직 박스처럼 활용을 하는 거고요. 즉 원하는 인풋을 저희가 관심 있어 하는 텍스트를 인풋으로 넣었을 때 그 인풋을 토크나이즈 하고 난 다음에 각 토큰에 대한 인베딩이 나오는 게 이 모델의 역할이라고 보시면 되겠습니다. 길게 있고요. 다음으로 저희가 몇 가지 좀 중요한 설정들을 하도록 할게요. 이건 제가 제 그 샘플 페이지에서 카피를 해올 텐데 이거 같은 경우는 그 파라미터들이고요. 보시다시피 일단 먼저 시퀀스 랭스의 한계를 정하는 게 중요해요. 결국은 일정 개수를 넘어가지 않는다는 걸 알고 있어야 저희가 그 개수 내에서 모델을 만들고 여러 가지 그 패딩 같은 것들을 작업할 수 있기 때문에 저희가 맥스 시퀀스 랭스를 정의를 하고요. 그다음에 저희가 어떤 이 맥스 랭스에서 남는 부분이 있다. 예를 들면 인풋이 랭스가 200이었어요. 근데 맥시코 랭스는 384면은 나머지를 저희가 채워줍니다. 184개 같은 경우는 이제 패딩으로 채워주게 되는데 이거를 패트 MA lx라는 저희 플랙으로 컨트롤 하고요. 그리고 문서가 너무 긴 경우가 있겠죠. 문서가 너무 긴 경우는 d 스트라이드라는 저희가 숫자로 결국은 일부 겹치게 해서 나누는 방식을 택합니다. 예를 들면 저희 문서 길이가 500이라 한다면은 저 시퀀스 랭크 384 내에 들어가지가 않기 때문에 문서를 2개로 쪼개고 그 문서 2개가 오버랩 되는 게 128개를 이제 함으로써 저희가 이제 결국은 좀 2개로 나눠서 퀘스천 앤서링 시스템을 진행할 수 있도록 하고요. 그런 경우는 문서가 하나라 하더라도 각각 문서에서 답변을 구한 다음에 그 답변을 취합하는 방식을 택하는 거죠. 뭐 첫 번째 500 단어 500개짜리 문서가 있다고 하셨을 때 첫 번째 384개짜리 문서랑 두 번째 그 나머지의 128개 겹치는 나머지의 문서에서 답변을 구하고 두 개 답변 중 더 확률이 높은 거를 가져가는 방식인 거죠. 그리고 저희가 이건 샘플 코드이기 때문에 트레인 샘플의 개수를 맥스를 여기서 정합니다. 이제 이거 이상 넘어가지 않는 거고 이거 같은 경우는 저희가 개수를 적당히 정함으로써 좀 학습이 빨리 끝나도록 할게요. 실제로는 이것보다 훨씬 더 많은 개수를 학습을 해야 되겠죠. 밸리데이션도 마찬가지로 개수를 저희가 한 한정을 짓도록 하고요. 그리고 이제 프리 프로세싱 넘 멀컬즈 같은 경우는 간단하게 말씀을 드리면은 이제 그 CPU의 트레드를 몇 개를 활용할 거냐 그러니까 프로세스를 몇 개를 활용할 거냐라고 보시면 될 것 같고 많이 쓸수록 더 빨라지긴 하는데 다만 보통 일반적으로는 그 컴퓨터가 갖고 있는 그런 그 하드웨어에 디펜드 하기도 하고요. 그 이상 필요가 없는 경우도 많습니다. 4 이상으로 그리고 배치 사이즈 같은 경우는 저희 학습할 때 쓰는 미니 배치 사이즈를 말씀드리는 거고 트레이 애프워크 같은 경우는 학습 데이터를 총 몇 번 재활용할 거냐라고 보시면 되고 이제 그리고 엠베 사이즈와 이제 맥스 앤셀 랭스가 있는데 결국에는 이제 맥스 앤셀 랭스 같은 경우는 답변이 최고의 길이가 몇이냐 답변 길이의 그 최대 길이를 제안함으로써 저희가 어쨌든 너무 긴 답변이 나오지 않을 수 있도록 미리 좀 컨트롤하는 거라고 보시면 되겠습니다. 다음으로는 제가 코드 하나를 카피앤 페이스트 할 텐데 이거는 일단은 여기 수업에서 자세히 설명하지는 않고요. 간단하게 말씀을 드려서 아까 보셨던 그 c스비 테이블 파일 있잖아요. 그 테이블 파일을 적절하게 프로세싱을 해 주어 가지고 모델에 넣기에 적합하도록 바꾸는 거라고 생각하시면 되고 어 이거에 대한 설명은 제가 코멘트를 달아 놨기 때문에 그 첨부된 파일에서 보시면서 좀 이해를 하시면 좋을 것 같아요. 제가 수업 내에서는 이걸 전부 다 커버하기에는 좀 시간이 없어서 카피 앤 페이스 하도록 하고요. 어쨌든 이 그 펑션의 역할은 여기 들어오는 이샘플들이 사실상 아까 제가 보여드렸던 데이터 셋들의 각각의 로우라고 보시면 되고요. 얘 아웃풋 같은 경우는 버트에 들어갈 수 있는 형태의 인풋이라고 보시면 될 것 같아요. 버트 모델에 이런저런 좀 프로세싱을 하는 거고 새로운 정보를 더하는 건 아닙니다. 일단 이거는 펑션을 정의를 하고요. 그래서 저희가 보시면은 이제 그걸 좀 예제를 드릴 수 있는 게 아까 보시다시피 저희가 트레인 데이터 셋을 다시 정의를 하고 아까 보셨죠 이거 이렇게 한 다음에 첫 번째 이그젬플을 프린트 하시게 되면 아까 보신 것처럼 이런 JSON 형태로 그러니까 즉 딕션의 형태로 첫 번째 로우에 해당하는 저의 학습 데이터를 볼 수가 있는데 그리고 저희가 이거를 어쨌든 부분적으로 그 고를 거예요. 전부 다 활용하지 않으려고 하니까 그래서 아까 보시다시피 실제 트레인 데이터 셋은 네 셀렉트를 하셔 가지고 레인지 해서 어 맥스 트레인 샘플을 이렇게 하게 되고요. 그럼 보시면은 개수가 원래는 6만 개였는데 이거를 다시 보시면 이제 이렇게 하시게 되면 아 네 잠깐만요. 제가 오타가 있었던 것 같아요. 넥스트레인 샘플즈가 위에 올라가시면은 넥스트레인 샘플인데 이거 아까 누르지 않았었군요. 네 죄송해요. 누르고 밑에서 이제 이렇게 정의를 하시게 되면은 보시다시피 16으로 나오죠. 원래 트레인 데이터셋의 길이는 6만 개였는데 16으로 줄은 거를 아실 수가 있습니다. 그다음에 저희가 이 트인 데이터셋을 지금 이 트인 데이터셋은 일종의 CSB 또는 그 파이썬 딕션의 형태로 버트가 활용할 수 있는 형태가 아니기 때문에 이거를 저희가 변형을 시켜줄 텐데 방금 보여드렸던 프리 프로세스 펑션을 활용을 할 거예요. 그래서 이것도 제가 카페인 페이스트를 합니다만 보시다시피 이렇게 저희가 해 주게 되면은 트레인 데이터셋을 다시 정의를 했죠. 그리고 보시면 이 트레인 데이터 셋을 정의를 하는 방법으로는 트레인 데이터셋에 매핑을 하는데 이 프리페어 트레인 피처즈는 아까 저희가 정의했던 그 긴 펑션을 활용을 하고요. 이제 배치들이라는 걸 활용을 해서 실제로 저희가 이제는 이 데이터셋을 들여다볼 때는 하나씩 보는 게 아니라 이제 여러 개가 합쳐진 방식으로 보는 거겠죠. 그리고 이제 프리 프로세싱을 파이프라인화시킴으로써 어쨌든 저희가 아까 말씀드렸던 것처럼 이 이거를 프로세싱을 할 때 한 번에 다 하는 방식보다 좀 그때그때 필요할 때 온디맨드를 하는 방식을 통해서 저희가 조금 더 효율적으로 그 데이터를 다룰 수가 있게 됩니다. 그래서 이제 이렇게 하고 난 다음에 실제로 트레인 데이터 셋 0을 보시면 이제 많이 달라졌을 거예요. 아까 위에서 보셨을 때는 이렇게 사람이 읽을 수 있는 형태로 보였는데 이제 이걸 한 번 더 하시게 되면은 완전히 숫자밖에 안 나오는 걸 볼 수 있게 될 거예요. 이렇게 이게 결국에는 저희가 같은 형태의 정보를 담고 있는 거지만 이걸 버트가 이해할 수 있는 형태로 바꿨다고 보시면 될 것 같고요. 저희 모델이 이해할 수 있는 형태로 바꿔준 거고요. 그다음에 저희가 제가 하나 더 설명은 안 하고 넘어갈 그 펑션을 디파인을 할 텐데 마찬가지로 이걸 트레인 데이터를 저희가 프리 프로세스를 했다고 한다면 마찬가지로 밸리데이션 쪽도 똑같이 할 수 있겠죠 그래서 이것도 지금 보시면은 펑션의 이름이 프리퍼 밸리데이션 피처즈고 그 하는 역할은 똑같습니다. 다만 다른 점은 방금 전은 학습 데이터를 프리 프로세스 하는 거였다면 이제는 밸리데이션 데이터를 프리 프로세스 하는 거고요. 네 마찬가지로 정의를 해준 다음에 똑같은 이제 트인 데이터랑 똑같은 방식으로 저희가 매핑을 해줍니다. 모델이 이해할 수 있는 형태로 이렇게 그러면 프로세스가 이렇게 진행이 되고요. 자 이제 그러면 저희가 데이터를 가져왔고 모델도 가져왔고 데이터를 모델에 적합한 형태로 바꿔주었어요. 그럼 이제는 저희가 데이터를 모델에 적용시켜서 학습을 해야겠죠. 저희가 수업에서 들었던 것처럼 사실 학습을 한다는 거는 저희가 완전히 스크래치부터 학습하는 것이 아니라 어떤 위키피디아 콜퍼스 같은 거대한 콜퍼스에 미리 학습을 시켜 놓은 모델을 가져온 거고요. 그 모델을 추가적으로 학습하는 것 즉 파인 튜닝이라고 부릅니다. 파인튜닝 프로세스를 보여드릴 텐데요. 이제 마찬가지로 이제 좀 더 그 허깅페이스 라이브러리에서 트랜스포머 라이브러리에서 좀 몇 개의 중요한 그 펑션들을 가져오게 될 텐데 먼저 첫 번째로 볼 수 있는 게 디포 데이러 컬레이터라고 돼 있죠. 이거 같은 경우는 저희가 학습을 할 때 그 다른 이그샘플들 즉 여러 개의 이그샘플들을 콜레이트 해주는 그런 역할을 하게 되고요. 이 트레이닝 아규먼트라는 클래스 같은 경우는 저희가 학습할 때 어쨌든 컨피규레이션들이 있겠죠. 주는 아규먼트들 뭐 배치 사이즈라든지 이런 것들을 합쳐 가지고 한 번에 줄 수 있는 그런 좀 컨비니언트한 펑션이고 마지막으로 이밸레이션 할 때 프리딕션을 좀 쉽게 할 수 있도록 도와주는 펑션이 있고요. 그다음에 트레이닝을 하는 루틴에서 저는 퀘스천 엔스트로잉 트레이너라는 이 펑션을 쓸 거예요. 이거를 쓰게 되면 조금 더 편하게 학습을 할 수 있는데 좀 이따 보시면 아시게 될 거고요. 마지막으로 저희가 답변을 낼 때 나온 답변을 또 한번 포스트 프로세스를 해줘야 됩니다. 저희 렉처에서 커버했던 것처럼 이거를 여기 유틸리스트 큐에이라는 그런 그 곳에서 이게 지금 보시면 트레이얼 큐에이랑 유틸리스 큐에이가 어디서 나오는지 궁금해하실 수 있는데 이게 아까 저희가 그 시스로 그 트랜스포머 라이브러리에서 특정 파일들을 저희 인포트 해왔잖아요. 그 파일들이라고 생각하시면 될 것 같아요. 그래서 이거를 저희가 이제 먼저 인포트를 해오고요. 그다음에 포스트 프로세스 펑션 같은 경우는 제가 임의로 여기다가 이제 정의를 할게요. 이것도 마찬가지로 그 답이 나왔었을 때 답을 다시 우리가 원하는 형태로 매핑을 해주는 거고 즉 모델이 이해할 수 있는 형태에서 사람이 이해할 수 있는 형태로 매핑을 해준다고 보시면 될 것 같아요. 이것도 일단은 자세한 설명은 일단 지금 넘어가도록 하고요. 정의를 해주도록 하고 그리고 이제 매트릭을 제가 정의를 할 텐데 매트릭은 결국에는 어떤 특정한 그 모델의 프리딕션이 답변과 비교했을 때 얼마큼 좋은지를 보는 거죠. 이것도 마찬가지로 저희가 정의를 해 주는데 이런 식으로 정의를 해줄 수가 있습니다. 네 다음으로 이제 거의 이제 막바지인데요. 어 아규먼트들을 몇 가지를 정의해 주도록 할게요. 그래서 이거 같은 경우는 트레이닝 아규먼트라는 클래스를 활용을 하고 단순하게 이거는 여러 가지 저희가 정의할 아규먼트들을 그냥 모아준 일종의 그런 컨비니언트한 클래스라고 보시면 될 것 같아요. 그래서 보시다시피 저희가 어디다가 아웃풋을 내보낼지 아웃풋 디렉토리를 정의를 하고 그리고 학습을 할지 안 할지에 대한 부분도 정의를 하고요. 두 트레인 이밸류에이션을 할지 안 할지도 정의를 하고요. 학습할 때 러닝 웨이트를 정의를 해 주고 학습할 때 배치 사이즈가 얼마일지 이밸류에이션 할 때 배치 사이즈를 얼마를 쓸지 그리고 학습할 때 몇 번 재사용할지 에폭스가 얼마일지 그리고 웨이디케이까지 이렇게 정의를 해 줍니다. 다음으로는 트레이너인데요. 이 트레이너를 활용하면 저희가 상당히 편리하게 학습을 하실 수가 있어요. 그래서 트레이너를 이렇게 정의를 하실 텐데 이제 보시다시피 퀘스천 앤슬링 트레이너라는 클래스가 받는 아규먼트는 첫 번째로 모델이고요. 즉 저희가 모델을 이 여기 넣음으로써 정의를 하게 되고 그리고 학습할 때 필요한 아규먼트들을 이제 알기스로 넣어주시게 되고 데이터셋을 여기서 정의를 해줍니다. 학습 데이터셋 아까 근데 저희가 그 학습 데이터셋을 오리지널 데이터셋에서 프리 프로세싱 펑션을 통해서 모델을 이해할 수 있는 형태로 바꿔주었죠. 그래서 그 새로운 데이터셋이 들어가게 되는 거고 마찬가지로 이밸류에이션 데이터 셋도 원래 형태의 사람이 이해할 수 있는 형태에서 모델의 이하 수 있는 형태로 저희가 변경을 해 주었고요. 그다음에 실제로 저희가 어 이밸류에이션 할 때 쓸 이그 샘플을 여기 넣어주게 되고 그리고 어떤 토크나이저를 쓸지 이것도 저희가 아까 불러왔죠. 기존 모델에서 그리고 어떻게 데이터를 콜레이트 할지인데 이거 같은 경우는 사실은 웬만한 경우는 거의 디폴트 콜레이터를 쓰게 됩니다. 어떻게 이 샘플들을 같이 이렇게 콜레이트 즉 붙여줄지인데 이거는 거의 디폴트를 쓰신다고 보시면 되고 마지막으로 포스트 프로세스 펑션 즉 어떤 저희가 답변이 나오게 되면 그 답변을 어떻게 다시 사람이 이해할 수 있는 형태로 해석할지 반대 펑션인 거죠. 위에랑을 여기서 펑션을 정리해 주게 되고요. 이거는 보시다시피 저희가 위에서는 트레인 데이터 셋을 인풋으로 받았는데 여기 같은 경우는 데이터 셋을 인풋으로 받는 게 아니라 펑션을 인풋으로 받아요. 즉 여기 같은 경우는 이 펑션을 받고 내부에서 이 펑션을 활용해서 그때그때 온디맨드로 나온 답변을 이제 사람이 이해할 수 있는 형태로 바꿔준다고 보시면 되고 마지막으로는 저희가 이밸류에이션을 진행하게 된다면 이밸류에이션을 어떻게 진행할지 어떤 점수를 낼지에 대한 펑션을 여기서 똑같이 마찬가지로 넣어주게 됩니다. 이제 이거를 이렇게 정의를 해 주시게 되면은 학습에 필요한 모든 것들을 이제 준비를 하게 되고요. 이제 준비가 끝났으면 이제 실제로 학습을 진행하면 되겠죠. 그러면은 학습은 아주 간단합니다. 이렇게 진행을 하게 되고 트레인 리저트는 트레인 널 다 트레인이고 이거 이거를 이제 부르시게 되면 학습이 진행이 돼요. 저희가 GPU를 사용하고 있기 때문에 상당히 빨리 아마 끝날 거예요. 네 상당히 빠르죠. 실제로는 12개만 하시면 안 되고 이제 만 2천 개 또는 이제 12만 개 이렇게 이런 스케일로 하셔야 될 거고요. 그래서 트레인 리졸트를 보시면 당연히 학습 잘 안 됐기 때문에 결과가 잘 나오지는 않겠지만 저희가 어떤 결과가 나오는지 볼 수가 있어요. 네 그리고 트레임 리졸트를 이렇게 프린트 하시게 되면은 학습에서 지금 트래킹하고 있는 여러 가지 수치들이 나오게 됩니다. 가장 아마 중요한 거는 현재 스텝이 몇 번째냐 즉 몇 번째 배치를 학습에 활용했냐 글로벌 스텝이죠. 10 저가 12개만 활용했기 때문에 12인 거고요. 그리고 현재 스텝에서 로스가 얼마인지 4.96 이렇게 나오죠. 이 두 개가 가장 중요하다고 보시면 될 것 같습니다. 하지만 이거는 학습에 대한 그 수치들이고 실제로 평가를 하려면 저희가 이발리엣을 해야 되겠죠. 그래서 저희가 이 트레이너 에서 이밸류에이션라는 펑션을 활용을 해서 현재 타임 스텝에서 현재 현재 모델에서 이 이발 데이터셋을 활용을 해가지고 성능이 얼마큼 잘 나오는지를 볼 수 있게 됩니다. 이거 같은 경우 상당히 빠르게 진행이 되겠죠. 저희가 GPU를 활용하고 있으니까요. 그리고 아까 말씀드린 것처럼 이밸류에이션도 저희가 그 개수를 제한해 놨기 때문에 전부 이 데이터 전부를 쓴 건 아니고 거기 일부만 사용을 했고요. 그래서 이게 그 나온 결과를 보시면 보시다시피 저희가 중요하게 생각하는 두 개의 매트릭을 볼 수가 있습니다. 이그젝 매치랑 프1인데 저희가 학습을 별로 안 했기 때문에 이색 매치는 0점이고 다만 프1은 약간의 점수를 받은 걸 볼 수가 있네요. 그리고 저희가 2개의 에포크를 돌았기 때문에 위에서 그렇기 때문에 에폭도 같이 나옵니다. 네 저희 오늘 그 실습은 여기까지고요. 전반적인 코드의 스트럭처를 이해하기 쉽도록 좀 최대한 간결하게 실습을 진행을 하였고요. 실제로는 제가 스킵했던 부분들 특히나 그런 펑션의 세부적인 내용들은 살펴보시는 걸 추천을 드려요. 저희가 제공해 드릴 이 파이썬 노트북에 실제로 코멘트도 해드렸으니 이 코멘트를 보시고 예를 들면은 위에 가보시면요. 너무 기네요. 이게 이제 아까 말씀드렸던 학습 데이터를 프리 프로세싱 하는 경우에는 이렇게 제가 코멘트를 드렸어요. 시간 관계상 저희가 실습 내에서는 이걸 커버하지 못했지만 보시면서 하나하나 이제 그 실제로 그 내용을 들여다보시면서 이해를 하시는 걸 추천을 드립니다. 네 그러면 실습 마치도록 하겠습니다. 감사합니다."
}