{
  "lecture_name": "[RecSys 이론] (5강) Item2Vec and ANN",
  "source_file": "[RecSys 이론] (5강) Item2Vec and ANN_133.mp4_2025-12-04-11012672.json",
  "text": "안녕하세요. 추천 시스템 강의를 맡은 강사 이준원입니다. 이번 시간은 5강 아이템 투백과 에이엔엔 에 대해서 다뤄보겠습니다. 먼저 아이템 투 백 같은 경우에는 엔엘피 분야에서 처음 등장한 워드 투백 기법을 추천에 응용하는 방법입니다. 따라서 이번 시간에는 아이템 투백을 이해하기 위해서 워드 투백 관련된 내용도 다룰 예정입니다. 그 이후에는 에엔 약자로 어프록시메이트 니얼리스트, 네이버 후드라고 불리는 기법인데요. 주어진 벡터와 가장 가까이 있는 벡터를 근사적으로 어프록시메이트하게 찾는 방법입니다. 엠프와 같은 추천 모델이 학습된 이후에 모델을 실제 서빙할 때 많이 사용되는 기법입니다. 통상적인 머신 러닝과 딥러닝에서 다루는 주된 내용은 아니지만 추천 시스템의 실제 활용에서는 상당히 중요한 부분이기 때문에 관련 내용을 가볍게 다루도록 하겠습니다. 아이템 투 백을 이해하기 위해서는 먼저 워드 투 백 모델에 대한 이해가 필수적입니다. 워드 투 백에 대한 내용을 배운 뒤에 아이템 투 백 추천 모델에 대해 살펴보겠습니다. 그 이후에는 방금 언급했던 ann 이 왜 추천 시스템이 필요하고 어떠한 기법과 라이브러리가 있는지를 학습하겠습니다. 먼저 워드투 백입니다. 이번 시간에는 인베딩과 워드 임베딩을 다시 한 번 이해해 보고 워드 임베딩의 대표적인 방법론인 워드 투 백에 대해서 알아보겠습니다. 먼저 인베딩이란 무엇일까요? 사실 우리가 매트리스 백터라이제이션에서 레이턴트 팩터 모델을 배웠지만 인베딩도 같은 개념입니다. 주어진 데이터를 그보다 낮은 차원의 벡터로 만들어서 표현하는 방법입니다. 먼저 이 인베딩을 이용하기 위해서는 스파스 레프레젠테이션과 댄스 레프레젠테이션의 차이점을 살펴볼 텐데요. 스퍼스 레프레젠테이션 같은 경우에는 아이템 전체의 가짓수가 차원의 수와 동일합니다. 그 벡터는 모두 0과 1로 이루어진 벡터로 표현되어 있고 그것은 보통 원핫 인코딩 혹은 멀티엣 인코딩으로 표현되는데요. 아래 예시와 같이 면도기나 가위라는 아이템을 표현할 때 모든 아이템 개수로 벡터를 만들고 면도기에 해당하는 차원에만 1 가위에만 해당하는 차원에만 1을 넣어놓고 나머지는 모두 0으로 두는 것입니다. 이렇게 스프러스 레프레젠테이션을 할 경우 아이템 개수가 많아지면 벡터의 차원이 한없이 커지고 공간이 낭비되는데요. 반대로 댄스 한 레프레젠테이션은 아이템의 전체 가짓수보다 훨씬 더 작은 차원으로 해당 피처를 표현하는 방법입니다. 뭐 이진 값으로 표현할 수도 있겠지만 보통은 실수 값으로 이루어진 벡터로 컴팩트하게 표현하고요. 그래서 이 위에 있는 면도기와 가위 같은 벡터를 이렇게 5차원 정도의 작은 벡터로 댄스하게 표현합니다. 자 그렇다면 워드 임베딩이란 엔엘피 분야에서 텍스트를 분석하기 위해서 단어를 댄스한 인베딩 벡터로 표현하는 기법입니다. 원핫 인코딩 즉 스퍼스 한 레프레젠테이션으로 표현되어 있는 단어 벡터를 댄스 한 레프레젠테이션으로 바꾸는 방법입니다. 사람들은 단어를 이해할 때 그 단어의 의미를 알고 그 의미적인 유사도를 구할 수 있습니다. 그래서 우리가 단어를 벡터 공간에 임베딩을 통해 표현한다면 비슷한 의미를 가진 단어일수록 임베딩 벡터는 비슷한 위치에 분포하게 될 것입니다. 뭐 예를 들면 예쁘다 귀엽다 같은 단어들은 비슷한 벡터 값을 가지고 비슷한 인베딩을 가져서 인베딩 공간 내에 비슷한 위치에 분포될 것으로 기대하는 것이죠. 단어를 임베딩하기 위해서는 학습 모델 필요한데요. 사실 지난 시간에 배운 매트리스 팩토라이제이션도 원핫 인코딩된 유저와 아이템 개별 유저 아이템을 레이턴트한 팩터 즉 인베딩으로 표현하기 때문에 이것을 유저 아이템 인베딩으로 볼 수 있고 그때 우리가 학습했던 유저 아이템 매트릭스가 곧 유저 아이템의 인베딩이 됩니다. 자 그래서 워드투 백에 대해서 이제 살펴볼 텐데요. 이 워드 투백의 특징은 일단 뉴럴넷 기반의 랭귀지 모델입니다. 구조는 이따 보시겠지만 가장 간단한 하나의 레이어만으로 인풋과 아웃풋 레이어 사이에 하나의 레이어만으로 이루어진 가장 간단한 뉴럴 랭귀지 모델이고요. 이제 이 모델을 학습하기 위해서 대량의 문서 데이터셋을 벡터 공간에 투영하게 됩니다. 그럼 각각의 단어가 압축된 형태로 댄스 한 인베딩으로 표현이 되고요. 이 모델 자체는 모델 스트럭처가 굉장히 간단하기 때문에 굉장히 학습이 빠르고 효율적인 구조를 가집니다. 그래서 오른쪽 예시를 보시면은 이 아래 부분이 단어를 워드 투백 모델로 학습한 다음에 그 각각의 단어의 인베딩을 3차원 공간에 투영한 시각화 자료입니다. 그래서 각 단어의 인베딩 관계를 보시면은 킹 퀸이라는 단어가 인베딩 되어 있고 맨 모맨이라는 단어가 인베딩이 되어 있습니다. 그럼 이 퀸에서 퀸으로 가는 벡터와 맨에서 원맨으로 가는 벡터는 굉장히 비슷한데요. 이 단어가 인베딩을 하면서 그 의미적인 유사도를 학습하기 때문에 남자에서 여자로 이동하는 벡터는 거의 동일한 값을 갖게 됩니다. 마찬가지로 동사와 명사도 마찬가지고요. 이 오른쪽에 있는 수도와 나라에 대한 예시도 왼쪽에는 나라들이 쭉 임베딩이 되어 있고 오른쪽에는 수도들이 임베딩이 되어 있는데 그 차이의 벡터는 굉장히 비슷함을 알 수 있습니다. 그래서 각각의 벡터 쌍이 굉장히 관계가 유사하다 즉 단어들이 인베딩이 잘 되어 있음을 확인할 수 있습니다. 그래서 우리는 이 워오투 백 모델을 학습할 때 총 3가지 학습 방법이 있는데요. 이 세 가지에 대해서 하나하나 다뤄보겠습니다. 자 워드 투 백 모델을 통해서 수행하는 테스크는 단어를 예측하는 것입니다. 물리적으로 어떤 문장 안에 가까이에 있는 단어는 서로 연관성이 있습니다. 그래서 그 단어를 예측하는 테스크를 수행하면서 자연스럽게 각각의 단어에 임베딩이 학습되게 되고 그 인베딩은 단어 고유의 의미를 내포하게 됩니다. 그 첫 번째 방법은 컨티뉴스 백 어스 시바우라고 불리는데요. 단어를 예측하기 위해서는 가운데에 있는 단어를 기준으로 앞뒤 n 개의 단어를 사용해야 합니다. 그래서 이 윈도우 사이즈가 n이라고 하는데요. 그 윈도우 사이즈가 n이면은 가운데에 있는 단어로 뒤에 있는 n개의 단어와 앞에 있는 n개의 단어를 사용해서 가운데에 있는 단어를 예측합니다. 아래와 같이 단어들로 이루어진 문장들이 있습니다. 그럼 윈도우를 이동해 가면서 가운데에 있는 단어를 가지고 가운데에 있는 단어를 예측하는데 바로 주변에 있는 단어들을 활용해서 가운데에 있는 단어를 동시에 예측하게 됩니다. 예시에서 폭스라는 단어를 예측하기 위해서는 주변에 있는 퀵 브라운 점스 오버라는 4개의 단어를 가지고 가운데 있는 폭스라는 단어를 예측하는 테스크를 수행하면서 이 각각의 단어의 인베딩이 학습됩니다. 그래서 그 다음은 s바웃 모델의 구조를 그림으로 표현해 보았습니다. 방금 전에 설명했듯이 우리의 아웃풋 우리가 예측해야 하는 단어는 가운데에 있는 폭스라는 단어고 우리의 인풋 은 주변에 있는 단어들 이 4개의 단어인 거죠. 그래서 모델을 구조를 살펴보면 여기 부분이 각각의 인풋 레이어에 들어가는 인풋 값입니다. 4개의 단어가 되겠죠 네 그래서 주변에 있는 4개의 단어를 가지고 프로덕션 레이어를 거쳐서 마지막에 있는 폭스라는 단어를 예측하는 구조를 가지게 됩니다. 네 이 모델에서 학습하는 파라미터는 총 2가지인데요. 먼저 단어의 총 개수를 v라고 하고 임베딩 벡터 즉 우리가 댄스하게 표현할 중간에 있는 프로젝션 레이어의 노드 개수를 m이라고 합시다. 그러면 총 브개의 유니크한 단어가 엠개로 인베딩되는 것이기 때문에 학습 파라미터의 개수는 v 곱하기 m이 됩니다. 그리고 학습 파라미터는 매트릭스의 형태로 존재하게 되겠죠. 이 매트릭스는 두 개가 생기는데요. 인풋 레이어 즉 기존에 있는 인풋 값을 댄스 한 레이어로 바꿔준 더블와 이제 이 바뀐 댄스한 값이 다시 스퍼스 레프레젠테이션으로 표현된 더블 프라임 두 개의 학습 파라미터가 존재하게 됩니다. 이 두 개의 사이즈는 모두 브 곱하기 엠 그리고 엠 곱하기 브로 같습니다. 자 하나의 원 핫 벡터는 인베딩 매트릭스를 통해서 다음과 같이 댄스하게 표현됩니다. 이 가운데 있는 인베딩 매트릭스가 우리가 모델에서 학습해야 할 파라미터라고 말씀드렸는데요. 이 브라운이라고 있는 원핫 인코딩된 단어의 표현을 인베딩 매트릭스를 통해서 댄스한 표현으로 바꿔주는데요. 그렇기 때문에 이 인베딩 매트릭스를 룩업 테이블이라고도 합니다. 그래서 각각의 단어가 인베딩 매트리스를 통해서 각각의 고유한 m 차원의 벡터로 표현되게 됩니다. 이 m 차원의 벡터 총 4개가 되겠죠 이 4개는 가운데 프로덕션 레이어로 모여서 그 개수만큼 더해서 평균을 구해주게 됩니다. 다음과 같이 이 스몰 v가 최종적으로 프로젝션 레이어에 표현되는 벡터 4개의 형태입니다. 그래서 이 4개를 가지고 다시 최종 예측 단어인 폭스를 예측하는 것입니다. 네 구해진 프로젝션 레이어의 레프리젠테이션은 다시 브 차원의 벡터로 표현돼서 최종적인 폭스 예측 값과 비교가 돼야 되는데요. 그래서 이 가운데에 있는 브이 벡터가 더블 프라임이라는 파라미터와 곱해지게 되면 제라는 벡터로 표현되게 되고 이 z라는 벡터는 다시 v 차원 전체 단어의 개수 차원으로 표현됩니다. 이제 여기에 이제 소프트맥스 펑션이 존재하는데요. 그 이유는 최종적으로 이 v라는 벡터가 단어 v에 대한 발생 확률로 표현돼야 되고 이 확률이 원래 폭스라는 원 핫 인코딩된 벡터와 크로스 저 앤트오피를 구해야 하기 때문이죠. 그래서 최종적으로 주어진 4개의 단어를 가지고 폭스라는 단어를 맞추는 멀티클래스 클래시피케이션 테스크를 수행하기 때문에 마지막 레이어의 소프트맥스가 사용되게 됩니다. 다음은 스킵 그램이라는 워드 투 백 모델을 학습하는 또 다른 방법입니다. 씨 바우와 입력층과 출력층이 서로 반대로 된 모델임을 오른쪽 그림을 통해 확인할 수 있습니다. 하지만 수행하는 테스크는 동일한데요. 주어진 폭스를 가지고 예측해야 하는 주변 단어를 클래시피케이션 하는 멀티클래스 클래시피케이션 문제입니다. 보시면 하나의 단어가 인풋으로 사용되기 때문에 아까와 같이 씨바오처럼 벡터의 평균을 모두 구해서 더하는 과정은 프로젝션 레이어에는 없습니다. 일반적으로 씨바오보다는 스킵 그램이 성능이 더 좋다고 알려져 있는데요. 여기서 이 성능이라고 하는 것은 이 수행되는 멀티 멀티 클래스 클래시피케이션 테스크의 로스를 이야기하는 것이 아니라 워드 트랙 모델이 학습된 이후에 이 단어가 인베딩 되는 그 인베딩의 레프레젠테이션 표현력이 더 좋다는 것을 의미합니다. 그래서 가운데에 있는 단어가 인풋으로 들어가게 되고 주변에 있는 단어가 아웃풋 그래서 총 4개의 데이터가 생성되게 되고 이 4개의 데이터를 가지고 워드 투백 모델을 학습해서 최종적으로 우리는 이 더블 각각의 단어의 임베딩을 사용하게 됩니다. 마지막으로 설명할 방법은 스킵 그램에다가 negative 샘플링을 적용한 SGS 방법입니다. 이 학습 방법이 우리가 뒤에서 최종적으로 추천 모델에 사용할 아이템 투 백 에서 사용하는 학습 방법과 같습니다. 스키 그램과 negative 샘플링을 그림을 통해서 비교해 봤는데요. 스키 그램은 주어진 입력 단어에 대해서 주변 단어를 예측하는 멀티클래스 클래시피케이션 문제 가 됩니다. 그래서 박스가 들어갔을 때 퀵이나 브라운이나 점스나 오버를 예측해야 하는 문제입니다. 스키 그 negative 샘플링에서는 이 인풋과 아웃풋을 조금 바꿔서 다른 문제를 구성하였는데요. 주변에 있는 단어 즉 원래 입력과 레이블의 값을 모두 입력 값으로 바꾸고 주변에 있다면 1 그리고 그 단어가 주변에 없다면 0을 예측하는 바이너리 클래시피케이션 문제로 바꾸었습니다. 따라서 가운데 있는 단어와 주변에 있는 단어가 인풋으로 들어가고 이 둘은 주변에 있기 때문에 1 최대한 가깝게 예측해야 하는 문제입니다. 그래서 수행하는 테스크가 멀티 클래스 클래시피케이션에서 바이너리 클래시피케이션으로 바뀌었기 때문에 모델 구조도 이에 적합하게 변경되어야 합니다. 자 그렇다면 왜 negative 샘플링이라는 이름이 붙었을까요? 먼저 그림을 보시면 이 가운데에 있는 단어를 가지고 주변에 있는 단어를 예측하는 문제가 원래 여기에 있는 스킵 그램이었습니다. 근데 이게 이제 스킵 그램 negative 샘플링으로 바뀌면서 여기에 있는 입력과 레이블이 모두 입력 1과 입력 2 즉 둘 다 입력 값이 되었고 이 둘이 주변에 있기 때문에 우리는 1을 예측해야 하는데요. 이 스키 그램에서 사용한 모든 데이터를 그대로 바꿔줄 경우에는 레이블이 모두 1만 존재합니다. 왜냐하면은 우리는 주변에 있는 단어는 사용했지만 주변에 있지 않은 데이터는 스킵 그램에서 사용하지 않았기 때문이죠. 따라서 가운데 있는 단어를 중심으로 주변에 있지 않은 단어를 강제로 샘플링해야 합니다. 이것이 바로 negative 샘플링이고 그 샘플링한 단어의 레이블은 1이 아닌 0이 됩니다. 그래서 이 negative 샘플링을 몇 개 하냐가 이 모델의 하이퍼 파라미터가 되는 것이죠. 우리가 학습하는 파라미터가 아니라 우리가 정해주는 파라미터 그래서 positive 샘플 하나 즉 브라운 하나당 몇 개의 샘플링을 할 거냐가 하이퍼 파라미터고 학습 데이터가 적은 경우에는 이 negative 샘플링이 좀 많이 필요하고요. 학습 데이터가 충분히 큰 경우에는 2에서 5 정도가 적당하다고 논문은 이야기하고 있습니다. 그래서 스킵 그램 negative 샘플링은 각각의 입력 원과 입력 투가 서로 다른 인베딩 레이어를 갖습니다. 그래서 첫 번째 입력에 해당하는 임베딩 벡터를 룩업하고 두 번째 입력에 해당하는 임베딩 벡터를 룩업해서 이 각각의 임베딩 벡터를 다음 레이어로 사용하게 됩니다. 자 그래서 다음은 모델이 어떻게 최종적으로 예측을 수행하고 학습을 이루어지는지 에 대한 내용인데요. 방금 전에 인베딩 매트릭스를 통해서 룩업 된 각각의 벡터는 모두 같은 차원입니다. 그럼 이 중심 단어를 기준으로 인풋 원과 인풋 2의 dot 프로덕트 내적을 통해서 최종 예측 값을 구하게 되고 이 최종 예측값은 실수 값이기 때문에 시그모이드를 통해서 0과 1 사이로 출력하게 합니다. 그래서 0과 1 사이로 출력된 값을 실제 레이블이 1과 0이기 때문에 이 둘의 로스를 크로스 전 추피로 계산해서 그 로스를 가지고 백 프로포게이션을 수행합니다. 그러면 우리가 학습해야 하는 파라미터 임베딩 매트릭스죠. 각각의 입력 원과 입력 2에 해당하는 임베딩 매트릭스가 업데이트가 되고 결국 최종적으로 모델이 이 과정을 반복하면서 수렴하게 됩니다. 최종적으로 생성된 워드 임베딩은 입력 원과 입력 2 각각 생기기 때문에 2개고요. 뭐 사용하는 방식은 선택적으로 하나만 사용할 수도 있고 이 둘의 평균을 사용하는 방법도 있습니다. 그 둘의 성능 차이는 그렇게 크지 않기 때문에 별로 중요한 부분은 아닙니다. 참고로 헷갈리지 않으셔야 하는 것은 모델이 수행한 테스크는 바이너리 클래시피케이션 그 전 학습 방법은 멀티 클래스 클래시피케이션이지만 실제 워드 투 백 모델의 학습 결과물이라고 보통 이야기하는 것은 그 예측 모델 전체가 아니라 이렇게 학습된 워드 임베딩 값이라는 것입니다. 이 인베딩 매트릭스가 우리가 워드 투 백을 통해서 최종적으로 원하고자 하는 단어들의 인베딩이 되는 거고 이제 이 인베딩을 적절한 다른 다운스트림 테스크에 잘 사용하는 것이 일반적인 방법입니다. 네 다음은 워드 투백 모델을 추천 시스템에 적용한 모델인 아이템 투백에 대해서 살펴보겠습니다. 이 아이템 투 백은 워드 투 백 스지앤스 스키 그립 negative 샘플링 모델에서 영감을 받은 모델입니다. 논문의 명칭 그대로 자연어 처리에 있는 워드가 아니라 추천 시스템에서 사용하는 아이템을 임베딩하는데 이 워드 투백 모델을 그대로 사용한 논문입니다. 아이템 투 백을 살펴봅시다. 아이템 투 백은 NLP의 단어 워드가 아니라 추천 시스템에서의 아이템을 워드 투 백 모델 동일한 모델을 사용하여서 인베딩을 합니다. 아까 처음에 말한 대로 이 매트리스 팩토라이제이션도 유저 아이템 을 인베딩하는 방법인데요. 그래서 본 논문에서는 아이템 투 백과 엠프의 인베딩 성능을 비교하는 부분을 제시하고 있는데요. 뒤에서 설명을 드리겠습니다. 자 그러면 데이터를 어떻게 구성할까요? 워드 투 백을 학습하기 위해서는 문장과 단어가 필요했는데요. 추천 시스템에서는 이 문장과 단어를 다르게 구성합니다. 유저가 소비한 아이템 리스트가 문장이 되고요. 그 아이템 리스트에 있는 하나하나의 단어는 하나하나의 아이템은 단어가 됩니다. 이렇게 데이터를 바꿔서 워드 투 백 모델을 사용합니다. 참고로 아이템 투 백은 유저 식별자를 따로 사용하지 않습니다. 그래서 유저와 아이템의 관계를 사용하지 않고 개별 유저가 소비한 아이템들만을 학습 데이터로 사용합니다. 그래서 때로는 유저 식별 없이 그냥 세션 단위로 어떤 아이템을 소비했다라는 데이터도 아이템 투백의 학습 데이터로 사용할 수 있습니다. 그래서 앞서 배운 스킵 그 negative 샘플링 기반의 워드 투 백을 사용하여서 이 아이템 하나하나를 벡터화 하는 것이 이 논문의 최종 목표이고 그 인베딩 값을 추천에 활용했을 때 기존의 매트리스 팩토라이제이션의 아이템 인베딩보다 더 좋은 표현력과 양질의 추천 결과를 제공한다고 말하고 있습니다. 자 그래서 아이템 투 백 모델을 학습하기 위해서는 학습 데이터를 생성해야 하는데요. NLP에서는 문장과 단어를 사용했지만 여기서는 유저나 혹은 세션별로 소비한 아이템 집합과 그 아이템 집합을 구성하는 아이템을 사용합니다. 여기서 집합을 사용한다고 했는데요. 원래 문장에서는 a b c d e로 이루어진 단어 문장이 있을 때 이 문장의 순서가 그대로 반영이 되었죠. 하지만 이 아이템 드 백에서는 집합을 사용합니다. 그래서 아이템을 순서대로 사용한 그 시퀀스를 사용하지 않고 집합으로 바꾸면서 공간적 시간적인 정보가 사라지게 됩니다. 대신 이 집합 안에 존재하는 아이템은 서로 유사하다고 가정합니다. 이렇게 하는 이유는 세션별로 시퀀스의 의미가 조금씩 다르기 때문에 그냥 다 동일하게 집합으로 만들어서 그 집합 안에 있는 모든 아이템의 페어에 대해서 학습 데이터를 생성하기 위함입니다. 방금 얘기했던 것처럼 공간적인 정보 시간적인 정보를 무시하고 아이템 집합 내에 존재하는 모든 아이템 쌍, 두 개의 아이템 쌍에 대해서 이 스키 그룹 negative 샘플링의 학습 데이터를 생성을 하게 됩니다. 기존 스킵 구m은 가운데에 있는 데이터와 주변에 있는 데이터를 가지고 positive 샘플을 생성했습니다. 하지만 아이템 투 백 같은 경우에는 이 전체를 다 집합으로 만들어서 가능한 모든 조합에 대한 positive 샘플을 생성합니다. 다음 예시를 통해 살펴보겠습니다. 하나의 동일한 세션에서 유저가 acb를 순서대로 소비했다고 가정했을 때 이 a씨의 순서 정보를 다 날려버리고 아이템 ab씨의 집합으로 만듭니다. 그럼 가능한 모든 조합은 a b b c a 씨 BA b c 씨에 총 6개의 조합이 가능하고요. 이 6개의 조합으로 positive 샘플을 만든 이후에 negative 샘플링을 통해서 a b c와 다른 조합 즉 d 같은 negative 샘플을 추가해서 전체 데이터를 구성하고 이 모델 이 데이터를 가지고 아이템트 백 모델을 학습하게 됩니다. 네 그래서 위의 과정을 통해서 아이템트 백 모델이 학습될 텐데요. 그 결과를 기존의 VD 이 SVD는 여기서는 매트리스 팩터라이제이션을 의미합니다. 매트리스 팩터라이제이션의 아이템 인베딩과 비교한 그림입니다. 각각의 아이템 투 백과 엠프의 모델의 아이템 투 벡터를 아이템 팩터를 티스엔이라는 차원 축소 기법을 통해서 2차원으로 인베딩하고 이것을 시각화한 것입니다. 이 티스엔이란 참고로 고차원 데이터를 2차원 시각화에 흔히 사용하는 차원 축소 기법입니다. 실제 여기에 2차원으로 표현돼 있는 임베딩 벡터들은 실제로 훨씬 더 큰 차원의 벡터임을 기억하시기 바랍니다. 이 그림에서 각각의 벡터 각각의 점 색깔의 의미는 아이템의 카테고리를 의미합니다. 이 아이템 카테고리는 어떤 모델을 통해 생성된 것이 아니라 이미 사람이 잘 정의해 놓은 예를 들면 음악의 장르 같은 혹은 커머스의 상품 카테고리와 같은 이미 사람이 정의한 카테고리를 의미하는 것이고 같은 카테고리는 같은 색을 갖도록 시각화한 그림입니다. 그래서 좌측이 아이템 2 1이고 우측이 매트리스 팩토라이제이션인데요. 그래서 모든 아이템 벡터를 2차원 평면에 시각화한 결과 비슷한 색상 즉 비슷한 카테고리를 가지는 점들끼리 모여 있는 것이 우측에 있는 MF보다 아이템 투 백이 더 잘 모여 있음을 볼 수 있습니다. 즉 우측에 있는 MF보다 왼쪽에 있는 아이템 투백이 더 비슷한 카테고리끼리 비슷한 표현을 가진다는 것이죠. 물론 이 표현 이 비교가 정량적인 비교는 아니지만 시각적으로 보았을 때도 아이템 투백이 보통 우리가 사람이 알고 있는 그 아이템의 특성을 잘 표현하는 임베딩이다라고 볼 수 있습니다. 다음은 각각의 아이템 투백과 SVD 모델이 학습된 이후에 아이템 벡터를 사용하여서 주어진 아이템에 대해서 가장 비슷한 아이템을 찾아 추천해 준 결과입니다. 즉 주어진 아이템과 비슷한 아이템을 찾아주는 연관 추천 테스크인데요. 이 다음에 데이터가 우리에게 익숙한 아티스트들은 아니지만 이 우측에 있는 SVD로 찾은 비슷한 아티스트보다 좌측에 있는 아이템 투 백 모델로 표현하여서 찾은 아이템 즉 아티스트들이 이 주어진 아이 아티스트와 더 비슷하다고 볼 수 있습니다. 자 그럼 마지막으로 이 아이템 투백이 각각의 서비스에서 어떻게 적용될 수 있을까요? 이 서비스에서 아이템 투백 그걸 학습하기 위한 문장과 단어를 구성하는 단위가 모두 다릅니다. 예를 들면 우리가 아프리카 TV에서 추천을 한다고 할 때 문장은 유저의 시청 이력이 될 것이고 단어는 그 시청 이력을 구성하는 라이브 방송이 될 수 있습니다. 스포티 파일 같은 경우에는 시청 플레이 리스트 그러니까 유저가 플레이를 한 내역을 쓸 수도 있지만 유저가 이미 만들어 놓은 플레이 리스트 자체를 문장으로 볼 수 있고 그 플레이 리스트 안에 있는 노래를 단어로 볼 수 있는 거죠. 그 밑에 있는 쇼핑 세션도 비슷한 예시인데요. 서비스마다 그 문장과 단어를 구성하는 형태가 조금 달라지긴 하지만 그 문장과 단어를 제대로 정의한 이후에 아이템 투 백 모델을 학습하는 방법은 모두 동일합니다. 이제 어떻게 문장과 단어를 구성하느냐가 각각의 서비스에서 추천을 잘 하는 노하우라고 볼 수 있습니다. 네 다음은 마지막 파트 어프록시메이트 니얼리스 네이버입니다. 이 ann의 정의와 필요성에 대해서 배우고 왜 추천 시스템에 에엔이 필요한지 그리고 ann 알고리즘을 제공하는 라이브러리들을 하나씩 알아봅시다. 네 먼저 이 리얼리스트 네이버가 무엇인지 어떤 문제를 풀려고 하는 것인지 봅시다. 리얼리스트 네이버 문제 자체는 추천 시스템과는 원래는 관련은 없습니다. 주어진 벡터 스페이스에서 내가 원하는 쿼리 벡터와 가장 유사한 벡터를 찾는 알고리즘인데요. 이제 이것이 왜 추천과 관련이 있을까요? 이 매트릭스 팩터라이제이션 모델을 가지고 추천 아이템을 서빙한다고 해봅시다. 유저에게 아이템을 추천하는 테스크 같은 경우에는 해당 유저 벡터에 대해서 후보 아이템 벡터들과의 유사도 연산이 필요합니다. 그중에 가장 높은 아이템 벡터가 추천되겠죠. 마찬가지로 비슷한 아이템을 추천하는 연관 추천 테스크 같은 경우에는 해당 아이템 벡터가 주어졌을 때 그 해당 아이템 벡터와 모든 후보가 될 수 있는 아이템 벡터 간의 유사도 연산이 필요합니다. 그리고 그 가장 유사도가 높은 아이템 벡터가 추천되게 되겠죠. 그래서 이 추천 모델 서빙 자체는 리얼리스트 네이버 서치와 굉장히 비슷합니다. 어떤 모델 학습을 통해서 유저 아이템 벡터가 굉장히 많이 생성됐고 이제 추천 모델을 서빙하기 위해서는 유저 아이템 벡터가 딱 주어졌을 때 그 주어진 쿼리 벡터와 가장 유사도가 높은 즉 가장 인접한 이웃을 찾아주는 문제가 바로 추천 모델 서빙에서 활용되는 기법과 일맥상통하기 때문입니다. 네 그렇다면 가장 무식하게 이 이웃들을 찾아주는 방식이 블루트 포스 KNN입니다. 정확한 리얼리스트 네이버 호두를 구하기 위해서 주어진 이 빨간색 쿼리 벡터가 있을 때 이 빨간색 벡터와 가능한 모든 벡터들을 다 유사도를 구해서 가장 유사도가 가까운 이 3개를 찾아주는 방법입니다. 이 브루트 포스 KNN는 벡터의 차원이 커질수록 유사도 연산 자체가 오래 걸리고 벡터의 개수가 많아질수록 모든 벡터에 대해서 선형적으로 연산 속도가 늘어나게 됩니다. 그래서 만약에 하나의 머신의 싱글 스레드 기준으로 100차원의 벡터가 약 100만 개가 있을 때 주어진 1개의 벡터에 대해서 100만 개의 벡터와 모든 유사도 연산을 하고 가장 가까운 리얼리스 네이버를 찾는 데 0.166초나 걸립니다. 그리고 이 100만 개에 대해서 모두 구하기 위해서는 0.16초에 100만을 곱하면은 총 46시간이 소요가 되는 것이죠. 그래서 너무 많은 시간이 걸리기 때문에 이 모든 연산을 실제 현업에서 수행하기엔 굉장히 어렵습니다. 벡터의 개수가 100만 개가 아니라 더 많이 늘어나게 되면은 추천 모델을 우리가 잘 학습했을지라도 그 학습한 추천 모델을 실제 서빙에 사용을 못할 수도 있는 것이죠. 따라서 정확한 리얼리스트 네이버 호드가 아니라 케이계에 근접한 이웃, 즉 어프록시메이트 네이버를 계산하는 그래서 정확도는 약간 포기하면서 대신 아주 빠른 속도로 원래 이웃과 굉장히 비슷한 근접 이웃을 찾아보는 니즈가 생기게 되었습니다. 따라서 정확한 리얼리스트 네이버를 찾는 것이 아니라 근사적으로 근접 이웃을 찾는 필요성이 높아지게 되었고 그에 맞는 다양한 기법과 라이브러리들이 등장하게 되었습니다. 이제 이 아래 그림은 다양한 에엔의 라이브러리들에 대한 성능을 나타내는 표입니다. 여기서 먼저 x축과 y축을 볼 텐데요. x축은 실제 가장 가까운 근접 이웃에 비해 얼마나 정확하게 근접 이웃을 탐색했는지에 대한 정확도의 결과고요. y축은 주어진 벡터에 대한 리얼리스트 네이버를 탐색하는 속도 스피드 부분입니다. 그래서 x축은 우측으로 갈수록 기존 정답과 최대한 비슷하게 근접 이웃을 찾았다는 것이고요. y축은 올라갈수록 속도가 아주 빠르다는 것입니다. 그래서 그래프를 보면은 정확도가 점점 높아질수록 속도는 점점 느려지고 반대로 정확도가 낮아질수록 속도는 점점 빨라짐을 볼 수 있습니다. 네 그래서 이 100%의 정확도로 200ml 세 를 소유해서 정확한 이웃을 구하는 것보다는 이 초록색인 99%의 정확도로 그보다 훨씬 적은 거의 100분의 1 수준인 2~3m밀리세컨드 안에 근접한 이웃을 구할 수가 있고요. 속도를 더 빠르게 하면 정확도는 조금 떨어지지만 1미리세컨드 만의 90%의 정확도로 근접한 이웃을 찾을 수 있게 됩니다. 따라서 굳이 100%의 정확도로 내가 가장 가까운 이웃을 찾는 것보다는 실용적으로 정확도를 조금 낮추면서 속도는 100배 200배 빠르게 하는 이 에엔엔 기법이 실제 추천에 많이 활용되고 있습니다. 그리고 이 그림에서 방금 설명했던 시간이 빨라질수록 정확도는 낮아지고 시간이 오래 걸릴수록 정확도는 높아지는 이 어큐러시와 스피드 트레이드 오프 를 잘 기억하시고 그 이후에 라이브러리들이 이 트레이드 오프를 적절하게 해결하기 위해서 다양한 접근법을 사용했다는 것을 알아두시기 바랍니다. 네 다음은 an의 라이브러리 중에서 하나인 어노이입니다. 어노이는 어프록시메이트 얼리스, 네이버 우드 오에라는 다소 재미있는 이름으로 구성되어 있는데 그와 달리 이름은 탐색 기법과는 전혀 관련이 없습니다. 다양한 에에에 라이브러리들이 있지만 이번 시간에는 이 어노이에 대해서 자세히 설명하고 다른 기법들은 간단히 리뷰하는 정도로 넘어가겠습니다. 어노이를 요약하면 주어진 벡터들을 여러 개의 서셋 여러 개의 부분 집합으로 나누고 이를 트리 형태의 자료 구조로 구성하여서 이 트리 형태의 자료 구조를 가지고 효율적으로 탐색하는 방법입니다. 그럼 아래에 그림을 통해서 학습 과정을 설명하겠습니다. 먼저 편의상 모든 벡터는 이렇게 2차원 공간에 임베딩된 벡터라고 가정하고 시각화를 통해 표현하겠습니다. 먼저 첫 번째 스텝은 임의의 두 점을 선택해서 두 점 사이에 하이퍼 플레인으로 벡터 스페이스를 나눕니다. 점이 이렇게 이렇게 선택됐을 때 이 두 점을 나누는 하이퍼 플레인을 구하는 것인데요. 여기서 하이퍼 플레인이란 2차원이 아닌 일반화된 엔차원에 대해서 사용하는 정의이지만 우리가 지금 시각화한 차원은 2차원이기 때문에 2차원의 하이퍼 플레인은 1차원 직선이 됨을 기억하시기 바랍니다. 그래서 이렇게 하이퍼 플레인으로 나눌 때마다 섭스페이스가 생기는데요. 이 s 스페이스는 다음과 같이 바이너리 트리로 구성할 수 있습니다. 그 이 바이너리 트리에 노드의 밑에 있는 60 185와 같은 숫자는 각 해당하는 색깔의 서 스페이스에 포함되어 있는 벡터의 개수입니다. 그래서 한두 번 나눠진 이후에 계속해서 점을 정하고 그 점을 하이퍼 플레인으로 나눠주는 과정을 반복하다 보면은 이 오른쪽에 있는 그림과 같이 많은 점들이 아주 작은 서브 스페이스로 나눠지게 됩니다. 이제 이 모두 나누어진 s스페이스는 다음과 같은 바이너리 트리의 형태로 구성됩니다. 각각의 s 스페이스는 바이너리 트리의 노드에 해당하고 그 노드 안에 서 스페이스에 벡터가 몇 개가 들어가 있는지 어떤 벡터들이 들어가 있는지를 이 트리라는 자료 구조를 사용해서 저장합니다. 이제 이렇게 했을 때 주어진 쿼리 벡터가 있을 때 그 쿼리 벡터와 가장 가까운 서스페이스, 그 쿼리 벡터가 속해 있는 섭스페이스를 로그 엔 시간 만에 찾을 수가 있습니다. 그리고 이렇게 이 쿼리 벡터가 어떤 스페이스에 있는지를 찾고 나면 이 해당 스페이스에 있는 벡터들과 쿼리 벡터의 유사도 연산만을 수행하게 됩니다. 즉 모든 서브 스페이스를 다 비교하지 않고 이 서브 스페이스 안에 있는 벡터들만 가지고 유사도 연산을 수행해서 가장 가까운 이웃을 찾는 것입니다. 이제 여기서 어노이의 문제가 발생하는데요. 가장 근접한 점이 아슬아슬하게 하이퍼 플레인에 의해 잘못 나눠져서 다른 서브셋에 포함될 수도 있고 그렇게 되면은 애초에 그 점은 유사도 연산을 계산하는 후보에도 들어가지 못합니다. 이 하이퍼 플레인을 랜덤하게 나눴기 때문에 충분히 이런 상황이 발생할 수 있고요. 이를 해결하기 위해서는 하나의 서브 스페이스에 있는 벡터만 가지고 탐색하는 것이 아니라 서치 스페이스를 늘려서 좀 더 많은 벡터를 탐색해야 합니다. 그래서 첫 번째 방법은 이 프라이어이티 q와 트리 구조를 사용하여서 트리 내에 가까운 노드도 탐색을 추가하는 것입니다. 원래 처음에 우리는 이 노드만 탐색하였는데요. 추가적인 노드를 탐색한다고 했을 때 이 노드와 가장 가까이 있는 다른 노드를 추가하여서 탐색하게 됩니다. 그럼 그림과 같이 쿼리 벡터가 포함돼 있는 서 스페이스가 원래 하나였는데 이제 옆에 있는 포함되어 있지 않지만 물리적으로 가장 가까이 있는 서치 스페이스의 벡터도 추가돼서 서치 스페이스는 원래보다 늘어나게 되고 이렇게 될 경우에 정확도는 조금 더 올라가겠지만 탐색 속도는 조금 늘어나게 되겠죠. 이제 이러한 트레이드 오프가 아까 얘기했던 어큐레쉬와 스피드 사이의 트레이드 오프라고 볼 수 있습니다. 두 번째 방법은 바이너리 트리를 하나만 생성하는 것이 아니라 여러 개를 생성해서 병렬적으로 탐색하는 것입니다. 아까 만들었던 트리를 오른쪽 그림과 같이 하나가 아니라 여기서는 3개겠죠. 여러 개를 만들어서 이 여러 개의 트릴을 병렬적으로 탐색하는 것입니다. 일종의 앙상블 효과라고도 볼 수 있겠죠. 그래서 여러 개의 트릴에 있는 서브 스페이스를 모두 탐색하기 때문에 서치 스페이스는 더 많이 늘어나게 되고 따라서 정확도가 향상되겠죠. 그래서 유저가 정해야 하는 어노이의 파라미터는 총 2개입니다. 첫 번째는 바로 방금 전에 설명한 이 바이너리 트리의 개수 넘버 오브 트리즈입니다. 이 트리의 개수가 늘어날수록 그만큼 사용하는 메모리가 많아지고 선형적으로 비례해서 이 트리의 인덱스 사이즈가 늘어나게 됩니다. 두 번째는 트리 전체에 대해서 탐색해야 하는 노드의 개수인데요. 여기서는 노드의 개수가 총 하나, 둘, 셋, 넷, 다섯, 여섯, 일곱, 여덟 개가 되겠죠. 이제 이 서치 케라는 숫자를 더 높이게 되면은 바이너리 트리에서 더 많은 노드를 탐색하게 됩니다. 그래서 정확도는 올라가겠지만 탐색 시간은 그에 따라서 더 많이 증가하게 되겠죠. 네 그래서 어노이에 대한 요약과 특징을 정리해 보자면은 이 어노이를 학습하는 것, 즉 서치 인덱스를 생성하는 것이 다른 에엔엔 기법과 라이브러리에 비해서 굉장히 간단하고 가볍습니다. 특히 아이템 개수가 많지 않고 벡터의 차원이 100보다 낮은 경우에 간단하게 사용하기 적합하고요. 다만 이 어노인은 GPU 연산을 지원하지 않습니다. 그래서 다른 에엔 기법에 비해서 어노이가 제일 간단하기 때문에 적은 데이터 셋을 가진 추천 모델을 학습했을 때 보통 어노이로 빠르게 적용해 보는 경우가 많습니다. 또한 이 서치해야 할 이웃의 개수를 알고리즘이 보장해 준다는 것입니다. 아까 탐색해야 할 서 스페이스의 개수가 서치 k라는 파라미터를 통해 사용자가 조정할 수 있다고 했는데요. 우리가 반드시 케이 개의 이웃을 찾는 것을 보장하려면 적어도 케이 개의 노드만 탐색해도 케이 개의 이웃, 즉 가장 가까운 케게의 이웃을 보장할 수 있습니다. 또 방금 언급한 이 모델의 파라미터 조정을 통해서 어큐레시와 스피드의 트레이드 오프를 사용자가 직접 조정할 수 있다는 것인데요. 원하는 목적에 따라서 어큐레시를 늘리거나 반대로 스피드를 빠르게 하거나와 같은 어플리케이션을 할 수 있습니다. 다만 어노이의 경우 한 번 생성한 트리 한 번 생성한 인덱스에 새로운 데이터를 추가할 수 없는데요. 즉 새로운 데이터가 추가돼서 이 데이터를 가지고 어노이를 사용해야 될 경우 다시 전체 데이터를 가지고 트리를 빌드해서 새로운 데이터 새로운 트리를 가지고 리얼리스트 네이버 호드를 서치해야 합니다. 자 다음은 다른 ann 기법과 라이브러리에 대해서 간단하게 리뷰하겠습니다. 사실 ann에도 다양한 방법이 있지만 본 수업에서 모두 다루기에는 어려움이 있기 때문에 언어에 대해서 비교적 자세히 설명을 하였고 그 외의 기법들은 여기 나와 있는 자료를 참고해서 추가로 스터디를 해보시기를 권장합니다. 먼저 이름이 다소 긴데요. 하이어라키컬 네비어블 스몰 월드 그래스라는 에이앤앤 기법입니다. 여기서는 벡터를 그래프로 표현하는데요. 벡터 하나하나가 그래프의 노드가 되고요. 그리고 벡터가 가까이 있는 즉 유사도가 가까운 물리적으로 가까이 있는 벡터는 엣지로 연결됩니다. 자 여기서 이 스몰 월드 그래프 그리고 내비거블 그리고 하이레키컬에 대해서 차근차근 설명해 보겠습니다. 먼저 스몰 월드 그래프 같은 경우에는 전체 점들 즉 전체 벡터들 가운데서 물리적으로 가까이 연결되어 있는 점만 연결한 그래프입니다. 그래서 멀리 있는 벡터 즉 멀리 있다는 것은 유사도가 작다는 것이죠. 유사한 노드들끼리만 엣지를 갖고 유사하지 않은 노드들끼리는 엣지를 갖지 않는 것이 스몰 월드 그래프입니다. 이제 여기에 내비거블이라는 말이 추가가 되는데요. 이 네비거블은 스몰 월드 그래프 각각을 서로 연결해 주는 롱 엣지가 있어서 이 롱 엣지를 통해서 유사하지 않은 거리가 멀리 떨어져 있는 노드들도 서로 네비어블하다. 즉 탐색할 수 있다는 것입니다. 그리고 이제 이거 여기 이 네비어블 스몰 워드 그래프를 계층적으로 만들어 준 것이 결국 하이러키컬 네비어블 스몰워드 그래프라고 할 수 있습니다. 이 오른쪽 그림처럼 계층을 만들어준 이유는 결국에는 계층적으로 탐색을 진행해서 이 ann 서치의 속도를 향상시키기 위함인데요. 이 오른쪽에 가장 아래에 있는 레이어 는 이제 우리가 가진 모든 노드를 사용해서 내비 어브 스몰 월드 그래프를 생성해 줬고요. 이제 여기서 랜덤 샘플링을 통해서 아래에 있는 모든 벡터 가운데 일부 벡터만 살려서 레이어 원을 구성해 주고요. 이 레이어 원에 있는 벡터들 중에 또 일부 벡터만 샘플링해서 레이어 2로 구성해 줍니다. 그래서 가장 상위에 있는 레이어의 가장 적은 벡터가 존재하게 됩니다. 그래서 이 하나의 레이어를 기준으로 이 밑에 있는 레이어들은 여기에 있는 벡터들은 반드시 이 밑에 노드로 존재하게 되겠죠. 그럼 이 그래프를 통해서 어떻게 근접 이웃을 빠르게 탐색하는지 작동 작동 방식을 살펴봅시다. 먼저 우리가 탐색해야 하는 노드, 즉 주어진 커리 벡터는 이 녹색입니다. 그럼 이 녹색과 가장 유사한 근접 이웃을 찾아야 하는데요. 작동 방식은 처음에 가장 위에 있는 즉 노드가 제일 적은 레이어에서 탐색이 시작됩니다. 임의로 하나의 빨간색 노드가 선택되면은 현재 레이어에서 이 초록색과 가장 가까운 노드로 이동을 합니다. 그리고 더 이상 가까워질 수 없다면 밑에 있는 레이어로 내려와서 다시 이 녹색 레이어와 가까운 곳으로 이동하게 됩니다. 그래서 다시 또 가까워질 수 없다면 또 밑으로 내려가게 되겠죠. 이렇게 해서 마지막에 있는 레이어 제로까지 내려올 때까지 이 탐색 과정을 계속 반복하고요. 이제 마지막 레이어에서 더 이상 가까워질 수 없을 때 이 탐색을 멈추게 됩니다. 그러면 이제 그동안 방문했던 모든 노드들 즉 하나 둘, 셋, 넷 이 4개에 대해서만 근접 이웃을 계산하기 위해 유사도를 구합니다. 그래서 모든 점이 아니라 그동안 트래버스 했던 노드들만 후보로 하여서 최종적으로 유사도를 탐색하고 근접 이웃을 계산하는 것이죠. 그래서 이 기법에 지원하는 대표적인 라이브러리로는 NMS 립과 파이스 가 있습니다. 다음 기법은 인버티드 파일 인덱스 기법입니다. 이것도 결국에 서치 스페이스를 줄여서 속도를 빠르게 하는 방법의 일부인데요. 이 아프를 빌드하기 위해서는 오른쪽에 있는 그림과 같이 주어진 벡터 공간을 파티션으로 나눠야 하는데요. 보통의 케이민지와 같은 클러스터링을 통해서 전체 벡터를 n개의 클러스터로 나누어서 저장합니다. 그리고 각각의 벡터의 인덱스를 클러스터별로 구성해서 이 벡터들을 인버트 드 리스트로 저장합니다. 그래서 탐색을 하게 되면은 이 오른쪽에 그림처럼 어떤 쿼리 벡터가 주어졌을 때 이 쿼리 벡터가 포함돼 있는 클러스터 안에서만 탐색이 이루어지게 됩니다. 여기선 빨간색 영역에 있는 이웃들만 가지고 유사도를 계산해서 가장 가까운 근접 이웃을 찾겠죠. 근데 문제는 여기서도 클러스터로 나눠진 경계에 아슬아슬하게 존재하는 이런 벡터가 존재할 수 있다는 것입니다. 이럴 경우에 탐색해야 되는 클러스터가 하나가 아니라 다른 클러스터로 확장해야지만 더 정확한 근접 이웃을 구할 수 있습니다. 따라서 이런 아래에 있는 그림처럼 하나의 클러스터가 아니라 탐색해야 하는 클러스터의 개수를 점점 증가시킬수록 정확도는 증가하게 되고 반대로 탐색 속도는 느려지게 되는 이 트레이드 오프가 발생합니다. 네 마지막 기법인데요. 프로덕트 퀀타이제이션이라는 것인데 이 기법은 앞에 서의 서치 스페이스를 줄이는 것과는 조금 다른 아이디어를 사용합니다. 앞에 기법들은 탐색 공간을 줄여서 속도를 빠르게 했지만 여기서는 기존 벡터가 가지고 있는 고유한 값들을 압축하여서 표현하였습니다. 이렇게 해서 압축하여서 표현하게 되면은 유사도를 계산하는 연산 자체를 훨씬 더 빠르게 수행할 수 있게 합니다. 그래서 어떻게 압축하는지 간단히 그림을 통해 살펴봅시다. 기존의 벡터 여기서 8차원 벡터를 n개의 서브 벡터로 나눠줍니다. 여기서 총 4개의 서브 벡터가 나눠져 있죠. 그리고 각각의 서브 벡터 군에 대해서 케이민지 클러스터링을 통해서 각각 벡터의 센트로이드를 구해주고 이 벡터를 각각 센트로이드로 매핑시켜 줍니다. 그럼 원래 이 복잡한 8차원의 데이터 8차원 벡터가 단순하게 4개의 센트로이드의 형태로 표현되어 있음을 알 수 있습니다. 그래서 이 아래 그림은 2차원 벡터를 프로덕트 퀀타이제이션을 통해 압축한 것인데요. 각 벡터의 x y의 고유한 값들이 분포하고 있지만 이 고유한 값 대신에 여기 있는 9개의 센트로이드 값만 사용합니다. 이렇게 될 경우에 각 영역에 포함돼 있는 벡터들은 실제로는 다른 값을 갖지만 압축을 통해서 같은 값 즉 센트로이드 하나로 표현되는 것이지요. 이제 이렇게 해서 기존 임베딩 벡터의 정확한 값은 잃어버리지만 최대한 센트로이드를 통해 유사하게 유지시킬 수 있고요. 이제 이랬을 때의 장점은 두 벡터의 유사도를 구하는 연산이 거의 요구되지 않습니다. 왜냐하면 우리가 미리 다 센트로이드를 구해놨기 때문에 센트로이드 사이에 유사도만 가지고 두 벡터의 유사도를 구할 수 있기 때문에 이 두 벡터의 유사도를 구하는 연산 자체가 거의 5원의 연산에 수렴하게 됩니다. 그래서 앞에서 배운 이 아브프와 피큐는 서로 각각 독립된 기법이기 때문에 이 두 기법을 동시에 사용해 줄 수 있는데요. 아브프를 통해서 탐색 공간을 좀 더 넓혀주면서 피큐를 가지고 유사도 연산을 더 빠르게 하면은 이 에엔엔 즉 가장 가까운 이웃을 근사하게 찾아주는 방법을 아주 빠르고 효율적으로 수행할 수 있습니다. 그래서 이 파이스 라이브러리에서는 아브프와 pq 그리고 이 둘을 합친 ivfpq를 각각 ann 연산으로 제공하고 있습니다. 네 그래서 여기까지가 다섯 번째 강의 에 대한 내용이었고요. 모두 수고하셨습니다."
}