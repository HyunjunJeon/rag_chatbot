{
  "lecture_name": "[RecSys 이론] (3강) Collaborative Filtering 1",
  "source_file": "[RecSys 이론] (3강) Collaborative Filtering 1_131.mp4_2025-12-04-110032266.json",
  "text": "안녕하세요. 세 번째 강의 추천 시스템에서 가장 중요한 개념인 컬라버레이트 필터링을 배워봅시다. 저는 강의를 맡은 이준원입니다. 먼저 이번 시간에는 컬라버레이트 필터링 줄여서 CF라고 불리는데요. 이 씨프 문제가 무엇이고 어떤 원리에 의해 작동하는지 그리고 우리가 앞으로 어떤 모델을 배울 것인지에 대해서 이야기해 보겠습니다. 그 이후에 가장 기본적인 CF 모델인 네이버 우드 베이스 시프 이웃 기반 c프라고 불리는데요. 이 모델에 대해 자세히 다뤄보겠습니다. 이어서 케이엔엔 즉 케이계에 근접한 이웃 정보를 활용하는 CF인 케엔 CF 와 KNN을 측정하기 위한 유사도 펑션을 다루고 마지막으로 이 기법을 사용하여서 어떻게 평점을 예측하는지까지의 문제를 풀어보겠습니다. 먼저 추천 시스템에서 가장 중요한 개념이자 기본적인 추천 모델인 콜라보이트 필터링 씨프에 대해서 배워보겠습니다. 콜라보이트 필터링이란 많은 유저들로부터 얻은 아이템에 대한 기호 정보를 활용해서 유저의 관심사를 자동으로 예측하는 방법입니다. 여기서 컬래버레이티브라는 단어를 직역하면 집단적인 협업인데요. 즉 다수의 의견을 활용한다는 뜻입니다. 그래서 더 많은 유저와 더 많은 아이템이 축적될수록 이 협업의 효과는 커지고 이 추천에는 정확해질 것이라는 가정에서 출발합니다. 데이터가 계속 축적될수록 협력이 커진다는 것은 사실 기본적인 머신러닝 딥러닝 모델을 학습할 때 학습 데이터가 많아질수록 좋다는 것과 같은 의미라고 볼 수 있죠. 그래서 이 CF의 원리를 추천한 아이템의 예시를 보자면 아래와 같이 제가 노트북이라는 아이템을 클릭해서 들어갔을 때 이 노트북을 봤던 다른 유저들이 함께 본 상품들이 추천됩니다. 지금 보고 있는 노트북과는 다른 종류의 노트북들이 추천됨을 알 수 있죠. 비슷하게 노트북을 본 게 아니라 그 노트북을 실제로 구매한 유저들이 구매한 다른 상품들도 추천되는데요. 아래와 같이 노트북이 추천되기도 하지만 이미 노트북을 구매했기 때문에 노트북과 같이 구매하는 노트북 파우치나 노트북 스탠드 같은 상품들도 추천되게 되죠. 이와 같이 나의 정보만 활용하는 것이 아니라 다른 많은 사용자와 많은 아이템 데이터의 상호작용 정보까지 활용하는 것이 컬라버레이트 필터링 입니다. 자 그렇다면 CF를 활용해서 풀려는 최종적인 문제는 무엇일까요? 간단한데요. 유저 u에 대해서 아이템 i에 부여할 평점을 예측하는 것입니다. 물론 평점이 아닌 어떤 선호도나 혹은 클릭할 확률 같은 것을 예측할 수도 있는데요. 그렇다면 어떠한 방법으로 예측을 할까요? 먼저 주어진 데이터 즉 학습 데이터를 활용하여서 유저와 아이템 행렬을 생성을 합니다. 이 행렬의 원소는 유저가 아이템에 기록한 선호도나 평점으로 채워지게 되는데요. 문제는 이 행렬이 모두 채워져 있지 않다는 것입니다. 모든 유저가 모든 아이템에 대해서 평가를 내리지 않기 때문에 반드시 빈칸이 존재할 것이고 우리는 이 빈칸을 채워 나가야 되는 것입니다. 이미 과거에 내려진 평점과 우리가 정한 유사도 기준을 가지고 이 비어 있는 평점을 예측하게 되는데요. 기존에 비어 있는 평점을 예측한다는 것은 유저가 아직 소비하지 않은 아이템에 대한 예측 평점을 구하는 것이고 이 평점을 가지고 최종적으로 추천에 활용하는 것입니다. 네 방금 CF의 원리를 설명하면서 유저와 아이템의 유사도를 사용해서 평점을 예측한다고 했는데요. 그 원리를 그림을 통해 쉽게 이해해 봅시다. 다음 그림처럼 유저 에가 있고 이 유저 에는 빨간색 동그라미와 빨간색 세모를 소비했다고 즉 선호했다고 가정합시다. 근데 이 유저 a 말고 다른 유저 b가 등장합니다. 이 유저 b도 유저 a와 마찬가지로 빨간색 동그라미와 빨간색 세모를 선호했습니다. 이러면 이 유저 a와 유저 b는 서로 비슷한 아이템 정확히는 같은 아이템을 선호했기 때문에 이 둘은 비슷한 유저라고 정의할 수 있게 되겠죠. 이제 이 상황에서 유저 b가 파란색 네모라는 아이템을 선호했다고 가정합시다. 그리고 이 파란색 네모는 유저 a가 아직 소비하지 않은 것이죠. 이 유저 a와 유저 b가 비슷한 취향을 가졌기 때문에 이 비슷한 취향을 가진 유저들이 선호한 아이템 즉 파란색 네모를 이 유저한테 추천해 주는 것입니다. 반대로 유저 비가 만약에 노란색 동그라미라는 아이템을 선호하지 않았다면은 유저 에도 유저 비와 비슷한 취향을 가졌을 것이라고 생각하기 때문에 이 노란색 동그라미는 유저 에에게 추천해 주지 않습니다. 그래서 보시면은 아이템이 가지고 있는 색깔 즉 뭐 빨간색이라든지 동그라미라든지 네모라든지 이런 특징은 전혀 활용하지 않았습니다. 지난 시간에 콘텐트 베이스 레코멘데이션 같은 경우에는 아이템이 가지고 있는 특징을 활용해서 비슷한 아이템을 추천했지만 씨에프 같은 경우에는 아이템이 가진 속성을 하나도 사용하지 않으면서도 좋은 추천 성능을 보인다고 알려져 있습니다. 네 그래서 씨프는 다음과 같이 분류할 수 있는데요. 먼저 제일 쉽고 간단한 기법인 네이버 우드 베이스 CF, 메모리 베이스 CF라고도 불리는데요. 이 이웃 기반 CF를 이번 시간에 다루게 될 것입니다. 그 이후에 4강부터 7강에 걸쳐서 모델 베이스 CF 머신러닝과 딥러닝, 매트리스 팩토라이제이션과 딥러닝으로 확장된 모델에 대해서 다루게 될 것입니다. 그리고 하이브리드 CF라는 개념이 있는데요. 지난 시간에 배운 콘텐츠 베이스트 레코멘데이션 그리고 오늘 배우는 컬래버레이트 필터링을 결합한 추천 모델 를 의미합니다. 하지만 보통 모델을 하나로 결합하기보다는 각각의 모델의 추천 결과를 각각 생성하여서 그 결과를 합쳐서 앙상블하는 방식으로 현업에서는 많이 사용하기도 합니다. 다음은 CF 계열의 모델에서 가장 초기 모델인 이 네이버 우드 베이스 CF 이웃 기반 협업 필터링에 대해서 배워봅시다. 네 이웃 기반 CF는 두 가지로 분류할 수 있는데요. 유저 베이스 CF와 아이템 베이스 c에프입니다. 두 기법은 유저 간의 유사도를 사용하냐 혹은 아이템 간의 유사도를 사용하느냐에 따라 나뉘게 되는데요. 먼저 유저 베이스를 기준으로 살펴보겠습니다. 이 유저 베이스트 CF는 주어진 유저 a 유저 b 이렇게 유저 두 유저 간에 얼마나 유사한 아이템을 선호하는가 즉 두 유저 간의 선호 도를 가지고 유사도를 구하게 됩니다. 유저 간의 유사도를 구한 뒤에 타겟 유저와 유사도가 높은 유저가 선호하는 아이템을 추천합니다. 혹은 그 선호하는 아이템의 평점을 참고해서 평점을 예측하게 됩니다. 여기서 유저 a b, c d와 아이템이 총 5개가 있는데요. 이 가운데 있는 유저 비의 스타워즈에 대한 평점만 우리가 모르는 상태에서 다른 평점들은 다 기록되어 있죠. 이제 여기서 우리는 이 유저 비의 스타워즈에 대한 평점을 예측하는 것입니다. 유저 베이스 CF 관점으로 예측을 한다고 하면은 이제 유저 b는 아이언맨 헐크와 같은 이런 히어로 영화에 대해서는 높은 평점을 주고, 이런 비포 선라이즈나 노팅힐 같은 이런 로맨틱 영화, 드라마 영화 같은 영화에 대해서는 평점을 낮게 줬죠. 다른 유저 a c d를 살펴보면 유저 b와 가장 비슷한 유저는 바로 유저 a임을 대략적으로 볼 수 있습니다. 꼭 수치적으로 계산하지 않더라도 유저 에는 앞에 두 개 영화에는 높은 평점을 줬고 뒤에 2개 영화에는 낮은 평점을 주었기 때문입니다. 따라서 이 유저 a와 유저 비는 유사도가 높다 혹은 수학적으로는 서로 연관도가 높다 하이리 코롤레이트 되어 있다라고 표현할 수 있죠. 그래서 이 유저 b의 스타워즈에 대한 선호도는 유저 a를 참고하면 아마 비슷하게 높을 것으로 예측된다. 이런 방식으로 작동하는 것이 유저 베이스드 씨프입니다. 다음은 아이템 베이스드 씨프입니다. 방금 전에 유저와 유저 사이의 유사도를 구했다면 이번에는 아이템과 아이템 사이의 유사도를 비교해야 합니다. 즉 두 아이템이 유저들로부터 얼마나 유사하게 평점을 받았는가가 유사도를 측정하는 기준이 됩니다. 아까는 가로로 이렇게 유사도를 구했다면 이번에는 이렇게 세로로 된 벡터 사이에 유사도를 구하는 것이죠. 그래서 아이템 간의 유사도를 구하고 내가 지금 예측하려고 하는 타깃 아이템과 유사도가 높은 아이템의 선호도를 참고하여서 평점을 예측하고 추천이 이루어지게 됩니다. 그래서 다음과 같이 스타워즈라는 아이템을 봤을 때 이 유저 a c d에 대한 평점이 나와 있고, 이 스타워즈와 비슷한 아이템 을 찾아보면은 직관적으로는 이 유저 a가 스타워즈 그리고 아이언맨 헐크에 대해 같은 평점이 높게 좋고요. 그리고 이 유저 CD는 아이언맨 헐크에 대해서 상대적으로 낮은 평점을 줬죠. 가장 유사한 아이템은 헐크라고 볼 수 있지만 2개를 뽑았을 때는 이 아이언맨과 헐크가 스타워즈와 유사도가 높다는 것을 직관적으로 알 수 있습니다. 반대로 뒤에 있는 비포 선라이즈와 노팅힐 같은 영화는 스타워즈와 비교했을 때 평점의 분포가 약간 반대됨을 알 수 있죠. 따라서 이 유저 b의 스타워즈에 대한 평점은 이 아이언맨과 헐크 를 참고할 것이고, 이 유저 b가 아이언맨과 헐크의 평점을 높게 주었기 때문에 이 4점과 5점을 참고해서 스타워즈의 평점도 높게 예측될 거다라고 예측하는 방식이 아이템 기반 CF입니다. 그래서 처음에 언급한 대로 이 콜라버티 필터링을 통해서 우리가 풀려는 문제는 간단한데요. 결국 최종적으로 유저 u가 아이템 i의 v여할 평점을 예측하는 것입니다. 방금 언급했던 대로 이 유저 베이스트 혹은 아이템 베이스 CF를 사용해서 이 문제를 풀 것인데요. 이 이웃 기반 컬라버티 필터링의 특징은 구현이 굉장히 간단하고 이해가 쉽다는 것입니다. 방금 전에 설명했던 그 두 가지 유저 베이스트 아이템 베이스트 씨프의 원리가 이 전체 모델 기법의 거의 전부라고 할 수 있을 정도로 아주 간단한 모델입니다. 그러나 이 두 가지 문제가 있는데요. 사실 이 두 가지 문제는 꼭 이 네이버 오드 베이스 CF 뿐만이 아니라 어떤 추천 시스템이든 어떤 모델을 사용하든 반드시 풀어야 하는 문제입니다. 먼저 아이템과 유저가 계속 늘어날 경우에 어떻게 대응할 것인가에 대한 스케일러벨티 이슈인데요. 사실 이웃 기반 CF의 경우에는 유저와 아이템 수가 기하급수적으로 많아질수록 굉장히 취약합니다. 유저 개수와 아이템의 개수 자체가 그 유저와 아이템을 표현하는 벡터가 되고요. 그 차원이 늘어날수록 벡터의 크기는 점점 커지게 되겠죠. 그래서 보통 이럴 경우에는 다음 시간에 배운 모델 베이스 CF인 매트리스 팩토라이제이션을 사용해서 이 문제를 해결하기도 합니다. 그 다음은 스파시티가 클 경우에 성능이 저하되는데요. 이 경우에도 방금 언급했던 매트리스 팩토라이제이션과 같은 모델 베이스 CF를 사용하는 것이 좋습니다. 이제 방금 전에 짧게 언급한 스프라시티 이슈에 대해서 좀 더 자세히 설명해 보겠습니다. 이 스프라시티 상황은 유저 아이템 숫자에 비해서 우리가 가지고 있는 평점이나 선호도 데이터가 아주 적을 경우를 말합니다. 실제로 우리가 유저 데이터를 활용해서 유저 아이템 매트릭스를 만들었다고 가정하면 아까 예시 같은 경우에는 하나의 엘리먼트를 빼고는 다 평점이 채워져 있지만 실제 대부분은 다 비어 있습니다. 이것을 스퍼스 매트릭스라고 하는데요. 우리가 넷플릭스라는 예시를 봤을 때 유저가 굉장히 많고 영화도 거의 수천 개 수만 개가 있겠죠. 근데 평균적으로 한 명의 유저가 영화를 몇 개나 봤을까요? 뭐 많이 봐봤자 몇백 개 정도의 수준이겠죠. 그렇기 때문에 이 유저와 영화를 가지고 매트릭스를 만들게 되면은 대부분의 원소는 비어 있게 됩니다. 그래서 이웃 기반 CF를 사용하려면 이 스파 시티 웨이시오라는 값이 99.5%를 넘지 않는 것이 좋습니다. 이 스파 시티 웨이시오란 전체 행렬 원소 즉 유저 아이템 매트릭스 가운데 비어 있는 원소의 비율이 얼마나 되는가인데요. 이 스파스 리에이지가 99.5%가 넘는 경우 굉장히 스파스한 데이터라고 볼 수 있고 이 경우에는 모델 기반 CF 즉 4강에서 배우는 매트리스 팩토라이제이션 같은 콜라보레이트 필터링 기법을 사용해야 합니다. 네 다음은 KNN 케 니어리스트 네이버 후드를 활용한 콜라보이트 필터링입니다. 여기서 이 KNN은 근접 이웃을 말하는데요. 이 근접 이윤을 말하는 케엔엔이 무엇인지 그리고 이 케엔엔 씨프가 어떻게 작동하는지 그 원리를 이해하고 그 이웃을 측정하는 매트릭인 유사도 추천 시스템에서 어떤 유사도가 있는지를 알아보겠습니다. 방금 바로 전 파트에서 우리는 이웃 기반 CF MBCF에 대해서 배웠는데요. 이 MBCF를 사용해서 아이템 i에 대한 평점을 구하기 위해서는 이 아이템 i 를 평가한 모든 유저 집합을 사용했습니다. 여기서는 이 스타워즈라는 아이템을 사용하기 위해서 스타워즈라는 아이템을 평가한 다른 유저가 a c d 3명만 있었죠. 이 3명의 유저가 바로 오메가 아에 속하게 됩니다. 다음 표에서는 유저가 총 4명밖에 없었고 그중에 오메가 아이는 3명밖에 없었기 때문에 간단하게 계산할 수 있었지만 이 유저가 계속 늘어난다면 수천 수백 수만 이렇게 늘어난다면 이 모든 유저의 정보를 다 활용하는 데는 무리가 있습니다. 물론 계산할 수 있기는 하지만 계산 속도가 굉장히 느려지고 오히려 많은 유저를 사용할수록 성능은 떨어지기도 합니다. 정확한 예측 값을 구하지 못하게 되는 것이죠. 그래서 이 MBC 이웃 기반 CF에 다가 KNN의 아이디어를 더하게 됩니다. KNN은 케이 니어리스트 네이버스 즉 어떤 주어진 타겟과 가장 가까운 케 명의 이웃의 데이터를 참고하는 방법인데요. 이제 이 그림을 보시면요. 가운데에 있는 녹색 점이 타겟이고요. 이 녹색 동그라미와 가장 가까운 점 k개를 찾아서 그 k개의 정보를 사용하는 것입니다. 이 문제는 이제 분류 문제 추천 문제는 아니지만 분류 문제에서 예시를 가져왔는데요. 이 초록색이 과연 초록색 타겟 데이터가 파란색 클래스인지 빨간색 클래스인지를 예측하는 바이너리 클래시피케이션 문제입니다. 이 초록색 데이터와 가장 가까운 데이터는 총 3개 케는 3이라고 했기 때문에 이 3개의 데이터를 참고해서 보니까 빨간색이 2개고 파란색이 하나죠. 그렇기 때문에 이 동그라미 타겟 데이터는 빨간색으로 분류될 확률이 높다 혹은 빨간색으로 분류할 수 있는 것입니다. 그래서 이러한 아이디어를 우리가 방금 배운 컬라버레이트 필터링 이웃 기반 CF에 그대로 활용하는 것인데요. 아까 언급한 대로 해당 아이템을 소비한 유저 오메가 i에 속한 유저 가운데 가장 유사한 k 명의 유저만을 이용해서 평점을 예측하는 것입니다. 이 오메가 i에 속한 모든 유저가 아니라 그중에 가장 가까운 유사도를 구해서 유사도가 가장 높은 유저 k명을 만을 이용하는 것이죠. 지금 언급하다는 이 유사하다라는 것은 우리가 정의한 유사도의 값이 크다는 것이고 앞으로 추천 시스템에서 사용하는 유사도 기법을 바로 뒤에 배울 것입니다. 그래서 보통 MBCF에서는 k는 25에서 50을 많이 사용하지만 이 케 값은 자동으로 학습되는 모델의 파라미터가 아니라 모델을 설계하는 사람이 직접 튜닝하고 정해줘야 하는 하이퍼 파라미터임을 기억하시기 바랍니다. 그래서 다음은 KNCF가 구현되는 가장 간단한 예시입니다. 여기서 케는 1이라고 가정을 했고요. 이 유저 비의 스타워즈에 대한 아이템을 예측하는 방법인데요. 이 유저 비와 가장 유사한 유저 한 명만의 데이터를 사용하는 것이죠. 이 유저 b와 가장 유사한 유저는 직관적으로 유저 a가 되겠죠. 그럼 유저 a의 스타워즈 평점이 5점이기 때문에 이 5점을 그대로 활용해서 유저 비의 예측 평점은 5점이 되는 것이죠. 이제 이거는 1 NCF가 됩니다. 네 다음은 유사도 측정 방법인데요. 방금 전 표에 있는 데이터를 통해서 유저 비와 가장 유사한 유저는 에인지를 대략적으로 알 수 있었지만 정확한 유사도를 측정하기 위해서는 어떤 수치적인 표현으로 나타내야 합니다. 그래서 유사도는 두 개체 뭐 두 데이터 간의 유사성을 수치로 나타낼 수 있는 그래서 어떤 실수 값으로 나타낼 수 있는 함수나 척도 매트릭을 의미합니다. 오른쪽 그림과 같이 굉장히 다양한 유사도가 있는데요. 두 데이터 포인트 벡터 간의 유사도를 구할 수도 있고요. 두 분포 간의 유사도를 구할 수 있고요. 두 집합 간의 유사도를 구할 수도 있습니다. 이 유사도에 대한 다양한 정의가 존재하지만 기본적으로 유사도를 직관적으로 이해한다는 것은 거리의 역수 즉 거리가 길면 길수록 유사도는 오히려 작아지고 거리가 가까우면 가까울수록 역수가 되기 때문에 유사도는 커지게 되는 것이 그래서 두 개체 사이의 거리를 어떻게 측정하느냐에 따라서 동일한 데이터 2개에 대한 다양한 유사도 값이 존재할 수 있습니다. 우리는 그 다양한 유사도 측정 방법 중에서 추천 시스템에서 자주 사용하는 총 4가지의 유사도 측정 방법을 바로 다음 슬라이드부터 학습하겠습니다. 첫 번째는 MSD 민 스퀘어드 디퍼런스 유사도입니다. 이 유사도는 다른 분야가 아니라 거의 추천 시스템에서만 주로 사용되는 유사도입니다. 유저나 아이템을 기준으로 그 유저의 아이템의 각각의 레이팅의 차이를 빼서 제곱해서 전체 유저와 전체 아이템 개수 레이팅의 개수로 나눠준 값입니다. 사실 이 미인 스퀘어드 연산은 이 유클리드 거리와 비슷한 연산이기 때문에 이 유사도는 유클리드 거리에 반비례하게 됩니다. 평점이 비슷하면 비슷할수록 이 둘의 값은 작아지게 되겠죠. 그리고 이 유사도 MSD 값이 결국에는 분모로 들어가기 때문에 두 개의 평점이 비슷하면 비슷할수록 분모는 작아지게 되고 가장 두 평점이 일치할 때 이 MSD는 0이 돼서 유사도 값은 1이 되게 됩니다. 분모가 0이 되는 것을 방지하기 위해서 스무딩을 사용해서 분모에 1을 더해서 유사도의 최댓값이 1이 되게 만든 것입니다. 다음은 코사인 유사도입니다. 이 코사인 유사도는 지난 2강에서 유저 프로파일 벡터를 가지고 아이템을 추천할 때도 간단하게 배웠었는데요. 코사인 유사도는 추천 시스템에서 가장 많이 사용되는 유사도입니다. 같은 차원을 가진 두 벡터 의 각도를 이용해서 유사도를 구하는 것인데요. 직관적으로 표현하면은 두 유사도의 두 벡터의 가리키는 방향 즉 각도가 같을 때 그 유사도는 1이 되고요. 가장 작을 반대로 작을 때는 마이너스 1 이 되는 것이죠. 앞에서 배웠던 MSD 민 스퀘어드 디퍼런스 유사도와는 다르게 이론적으로 두 벡터의 평점의 크기가 많이 달라도 유사도는 아주 높을 수 있습니다. 첫 번째 벡터가 이쪽 방향을 가리키고 두 번째 벡터는 두 배의 크기를 가지고 같은 방향을 가리키게 되면은 윈 스퀘어드 디퍼런스는 각각의 벡터의 크기가 중요하기 때문에 이 유사도가 크게 작게 나오지만 코사인 시뮬러이트 같은 경우에는 두 벡터의 크기가 달라도 가리키는 방향 즉 각도가 비슷하면 유사도는 거의 1에 가깝게 됩니다. 다음은 피어슨 유사도입니다. 이 피어슨 유사도는 피어슨 코릴레이션 값과 같은데요. 피어슨 유사도는 코사인 유사도와 동일하게 같은 차원의 벡터 XY에 대해서 유사도를 정의할 수 있습니다. 각 벡터를 벡터의 표본 평균으로 정규화한 뒤에 그 정규화한 벡터의 코사인 유사도 값을 구한 것이 피어슨 유사도입니다. 코사인 시뮬레이터와의 차이점은 표본 평균으로 정규화한다는 점인데요. 그렇기 때문에 각각의 벡터의 웨이팅의 크기 차이를 고려할 수 있습니다. 어떤 유저는 평점을 굉장히 미묘하게 다르게 주고 예를 들면 3.5점에서 4.5점 사이로만 주는 유저가 있고 어떤 유저는 1점부터 5점 사이로 굉장히 크게 주는 유저가 있을 때 이 둘의 유사도를 다이렉트로 구하는 것보다는 PS 유사도를 사용하면 그 둘의 유사도의 크기 차이를 보정해 주는 효과를 냅니다. 그렇기 때문에 다른 유사도에 비해서 어떤 도메인에서도 이 피어슨 유사도를 사용했을 때는 무난하게 어느 정도의 추천 성능을 내는 것으로 알려져 있습니다. 다음은 자카드 유사도입니다. 자카드 유사도는 조금 다른데요. 이 자카드 유사도는 벡터의 개념이 아니라 집합에 대한 유사도입니다. 그래서 유사도 연산도 벡터가 아니라 집합에 대해서 정의가 되는데요. 두 집합 a b에 대해서 분모에는 a와 b의 합집합의 크기 그리고 분자에는 a와 b의 교집합의 크기 가 들어갑니다. 집합의 개념을 사용한 유사도이기 때문에 코사인 피어슨 유사도와 달리 벡터가 아니라서 서로 길이가 달라도 이론적으로 유사도의 계산이 가능합니다. 직관적인 의미는 두 집합이 얼마나 같은 아이템을 공유하고 있는지 나타내는 것인데요. 두 집합이 완전히 동일할 경우에는 1이 되고 두 집합의 겹치는 아이템이 하나도 없을 경우에는 0이 되는 것이죠. 그래서 추천 시스템에 어떻게 자카드 유사도 사용이 가능하냐 예를 들면은 유저가 영화에 대해서 4.5점 이상의 평점을 준 영화만을 가지고 자카도 유사도를 계산했을 때 1번 유저는 4.5점, a라는 유저는 4.5점으로 영화를 준 평점을 준 영화가 뭐 m1 m3 3개가 2개가 있고 b라는 유저는 m1 m3 m4라고 했을 때 이 a와 b의 유사도는 어떤 벡터 연산이 아니라 이 집합의 자카드 시밀러리티 그래서 자카드 에 콤마 비는 이 두 개의 합집합은 3이 되겠죠. 그리고 둘의 교집합은 하나 둘 2 즉 0.66 이렇게 자카드의 유사도 계산을 구할 수 있습니다. 그래서 지금까지 추천 시스템 즉 KNNCF에서 주로 사용되는 유사도에 대해서 살펴보았습니다. 대표적으로 MSD 코사인 시뮬러리티, 피어슨 유사도 그리고 자카드 시밀러리티 4가지가 존재하며 보통 어떤 유사도를 사용할지는 딱 정해진 것이 없습니다. 우리에게 주어진 데이터 그리고 해당 추천 시스템이 가진 특징, 그 서비스의 특징 등을 고려해서 정하는 것이 적합하고 우리가 1강에서 배웠던 오프라인 테스트를 거쳐서 가장 좋은 추천 성능을 보인 유사도를 고르는 것이 일반적인 방법입니다. 마지막으로 지금까지 배운 이웃 기반 c프와 케엔씨프를 활용하여서 유저 아이템 매트릭스의 빈칸을 채우는 즉 레이팅 프리딕션을 수행하는 테스크를 수행해 보겠습니다. 네 첫 번째는 에버리지 방법인데요. 아까와 같은 데이터죠. 유저 b의 스타워즈에 대한 레이팅을 예측하는 것입니다. 가장 간단한 방법은 이 스타워즈를 본 다른 유저들의 평점을 다 더해서 평균을 내는 것입니다. 그래서 유저 비는 아직 스타워즈를 보지 않았지만 스타워즈를 본 유저 a 씨 d의 평점을 가져와서 총 3명의 유저이기 때문에 3으로 나누어서 평점을 구하는 것입니다. 근데 마음에 안 드는 것은 유저 b 입장에서는 유저 a c d의 레이팅을 똑같이 반영하는 것이 과연 적절할까 내 나와 비슷한 유저는 더 많이 반영하고 나와 비슷하지 않은 유저는 덜 반영하는 것이 오히려 좋지 않을까 그래서 이를 보완하는 기법이 웨이티드 에버리지 방법입니다. 유저 간의 유사도 즉 앞에서 정의했던 유사도를 가중치로 사용하여서 웨이팅의 평균을 가중 평균으로 구하는 것입니다. 유저 b의 입장에서는 자신과 비슷한 a의 평점이 많이 반영되고 비슷하지 않은 c의 평점은 덜 반영되는 것이 더 정확한 예측을 할 수 있겠죠. 그래서 이를 수식으로 표현하면 각각의 평점 5점 13점을 사용하는 건 동일하되 이때 이 b를 기준으로 b와 a의 유사도 비와 씨의 유사도 비와 디의 유사도를 각각 구해줍니다. 여기서 각각의 유사도는 0.95 0.6 0.85라고 했는데요. 보시면 가장 높은 유사도는 b와 a의 유사도 0.95가 되겠죠 그래서 이 0.95가 가장 많이 반영을 시키고 b와 가장 다른 유사도를 가진 c는 0.6 즉 그만큼 덜 반영하는 것이 그래서 이렇게 구해서 좀 더 정확한 평점을 예측하는 방법이 웨이티드 에버리지 방법입니다. 네 방금 설명한 평점을 예측하는 방법을 수식으로 표현하면 다음과 같습니다. 이제 유저 유의 아이템 아이에 대한 평점 알 햇 유아를 예측하는 것인데요. 먼저 에버리지는 이 오메가 아이 즉 아이템 아에 대한 평점이 있으면서 유저 유와 유사한 집합에 있는 평점들을 그대로 다 평균 내서 더하는 것이죠. 이게 레버리지 방식이고요. 두 번째는 그냥 더하지 않고 우리가 예측하려고 하는 유저와 그 아이템을 소비한 다른 유저들의 유사도를 각각 구해줘서 그 유사도를 가지고 가중 평균을 구하는 방식입니다. 자 우리가 이전 슬라이드까지 배운 평점을 예측하는 방법을 앱솔루트 레이팅이라고 명명했는데요. 이 앱솔루트 레이팅은 유저가 아이템에 매긴 평점의 절대값 즉 이 r 값을 그대로 활용해서 평균을 구하거나 가중 평균을 구했기 때문이죠. 이 방법에는 한계가 있는데요. 바로 유저가 아이템에 평점을 주는 기준이 유저마다 제각각 다르기 때문입니다. 어떤 유저는 전체적으로 높게 평점을 줄 수도 있고 반대로 어떤 유저는 되게 낮게 평점을 줄 수 있는 것이죠. 그래서 어떤 긍정적인 유저는 대부분을 다 5점으로 주고 부정적인 평가로 3점을 줄 수도 있고요. 부정적인 유저는 대부분을 다 1점 2점을 주되 가끔 4점 5점은 거의 주지 않는 유저가 있을 수 있죠. 이럴 경우에 이 유저의 평점 절대값을 그대로 활용하게 되면은 각각의 유저별로 가지는 유저의 편차를 반영할 수 없게 됩니다. 그래서 이 릴레이티브 레이팅 상대적 평점의 개념이 등장합니다. 그래서 개별 유저의 평균 평점을 각각 구하고 그 평점으로부터 얼마나 높은지 혹은 얼마나 낮은지에 그 편차를 가지고 이 모델을 구성하는 것입니다. 어떤 유저의 평균이 2.5점인데 5점을 줬다는 것은 아주 높게 평가했다는 것이고요. 반대로 모든 아이템이 평점을 5점으로 줬다는 유저는 그 5점을 그대로 사용하는 것이 아니라 오히려 아이템 간의 비교가 없기 때문에 그 평점 값을 많이 반영하지 않는 것이죠. 그래서 이 편차를 구해주는 것인데요. 유저가 아이템에 내린 평점 을 그대로 사용하지 않고 거기서 개별 유저의 평균 평점을 뺀 이 디비에이션 값을 사용하는 것입니다. 이전에는 웨이팅을 가지고 그대로 예측 평점을 구했는데요. 지금은 이 디비에이션 값을 구해서 예측 디비에이션을 구하는 방식으로 평점을 예측합니다. 그래서 최종 예측 평점을 구하기 위해서는 먼저 데이터로부터 유저의 평균값 레이팅을 구하고요. 그리고 예측된 디비에이션 값을 더 더해줘서 최종 예측 값을 구합니다. 아래 수식을 보시면 먼저 우리가 가지고 있는 유저 유어 아이템의 아이에 대한 모든 평점 레이팅 값에서 각각 유저의 평균을 뺀 디비에이션 값을 모두 구해줍니다. 그래서 이걸 사용하지 않고 이 디비에이션 값을 사용하게 되는 것이죠. 그 이후에 에버리지 방식이던 웨이트드 에버리지 방식이든 우리가 사용하는 평점의 예측 수식을 디비에이션에 대해서 정의하게 됩니다. 그래서 이 구해진 예측 디비에이션 값을 가지고 최종적으로 원래 해당되는 유저의 평점에다가 이 디비에이션 값을 더하게 되는 것이죠. 그래서 마지막으로 요약하면 이 예측 디비에이션의 의미는 유저 유가 자신의 평균 평점 ru바보다 이 아이템을 얼마나 높게 평가했는가 혹은 얼마나 낮게 평가했는가에 대한 차이 값이고요. 그래서 최종 이 디비에이션 값을 평균에 더해야지만 이 유저 유의 최종 아이에 대한 절대 평점 값이 얻어지게 됩니다. 그래서 다음 예시를 통해 릴레이티브 레이팅을 구하는 과정을 살펴봅시다. 앱솔루트 레이팅과 마찬가지로 유저 비를 기준으로 유저 a c d의 유사도를 가중치로 사용하는 것은 동일합니다. 이전 슬라이드에서 사용했던 동일한 코사인 유사도를 사용할 것이고요. 이제 여기서 이 유저 b의 평균 평점은 3.0임을 기억해 두시기 바랍니다. 그럼 이 유저 b의 스타워즈에 대한 예측 평점은 각각의 유저 CAC d에 대해서도 모두 디비에이션 값을 다 계산하게 됩니다. 이 유저 의 디비에이션은 자신의 유저 평균 평점보다 스타워즈를 1.6점 높게 줬다는 것이고요. 이 유저 c에 해당하는 마이너스 1.6은 자신의 평균 평점 즉 c의 평균 평점보다 스타워즈를 1.6점 낮게 줬다는 것이죠. 그러면 이 b의 예측 디비에이션은 각각의 유사도를 가중치로 사용하되 이 디비에이션 값 즉 1.6과 마이너스 1.6 그리고 이 01 값을 사용하여서 예측 디비에이션을 구하게 됩니다. 그리고 이 예측 디비에이션에 원래 유저 비의 평균이 3점이었죠. 그 3점을 더해서 최종 평점인 3.23을 예측하게 됩니다. 그래서 이렇게 구하는 것이 윌리티브 레이팅을 구하는 방식이고요. 그래서 이 릴레이티브 레이팅과 랩솔루트 레이팅을 비교하면 앱솔루트 레이팅은 절대 평점인 이 알 값을 그대로 사용했지만 릴레이티브 레이팅은 먼저 ru 즉 유저의 평균 평점을 앞에 더해 주게 되고요. 그리고 이 평균을 계산하는 뒷부분은 절대 평점이 아니라 절대 평점에서 각각 유저의 평균 평점을 뺀 즉 이 값이 디비에이션 유저 프라임 아 값이 되겠죠. 이 값을 사용해서 최종 평점을 구하는 것입니다. 근데 지금까지의 예시는 모두 유저 베이스 CF를 기준으로 설명했는데요. 아이템 베이스 시프도 작동하는 방식과 원리는 완전 동일합니다. 다만 유저 a와 유저 b의 유사도를 구하는 방식, 즉 가로 벡터의 유사도를 구하는 것이 아니라 아이템과 아이템 벡터 여기서는 세로 방향의 벡터의 유사도를 구하는 방식으로 평점을 예측합니다. 그래서 아래 예시는 아이템 베이스 CF로 접근하는 것이고 앱솔루트 레이팅 즉 절대 평점을 그대로 가중 평균 내는 방식을 사용하였습니다. 그래서 여기서는 아이템 베이스 CF 플러스 웨이티드 에버리지 예측을 한 것인데요. 거기서 추가가 된 것은 이 아이템의 가장 유사한 이웃 아이템 즉 두 개의 아이템만 사용한 것입니다. 그래서 2nn을 사용했다는 것인데요. 이 유저가 가지고 있는 모든 아이템 즉 이 4개의 아이템이 아니라 스타워즈와 가장 유사한 아이언맨과 헐크만을 이용한 KNN CF인 거죠. 그래서 예측 과정을 수식으로 간단히 살펴보면 이 스타워즈와 아이언맨의 유사도는 0.9가 되고요. 스타워즈와 헐크의 유사도는 0.95가 됩니다. 그래서 각각 0.95와 0.95를 가중치로 사용하여서 이 유저 비가 아이언맨에 매긴 평점 4점과 5점을 그냥 평균 구하는 것이 아니라 웨이티드 에버리지 방식으로 구하면 다음과 같은 4.5점의 예측 값을 구할 수 있습니다. 이제 다음은 동일하게 아이템 베이스 CF의 2엔엔인데요. 이제 앱솔루트 웨이팅이 아니라 디비에이션을 활용한 릴레이티브 레이팅입니다. 그래서 마찬가지로 여기서 가중치를 사용하는 유사도는 동일합니다. 그러나 아이템 베이스 시프는 이제 유저별 디비에이션이 아니라 아이템 전체의 평균에서 각각 유저가 아이템에 매긴 평점을 빼준 아이템별 디비에이션을 사용합니다. 사용하는 유사도는 다음과 같이 0.9 0.95가 되고요. 여기서 의미하는 요 1.2는 아이언맨의 전체 평균 평점보다 유저 b가 1.2점 높게 평점을 매겼다. 이 2.4점은 헐크 전체의 평균 평점보다 유저 비가 그 2.4점을 높게 매겼다는 정보이고요. 그래서 스타워즈와 가장 비슷한 두 개의 아이템에 대한 가중 평균을 구하면은 1.8 즉 이건 예측 디비에이션이 되고요. 1.8에다가 스타워즈 즉 아이템의 원래 평균이 되겠죠. 아까 유저 베이스에서는 유저의 평균을 구했지만 아이템 베이스에서는 이 아이템의 평균 평점에다가 이 디비에이션을 더해야 최종 4.8점이라는 예측 평점을 구할 수 있습니다. 그래서 이 아이템 베이스 CF로 평점을 예측하는 것을 수식으로 표현하면 다음과 같습니다. 유저 베이스 CF의 수식과 차이점은 첫째로 아이템 간의 유사도를 사용한다는 점이고요. 두 번째는 이제 유저 유가 평가한 아이템들의 집합을 사용한다는 것입니다. 방금 전 예시에서 스타워즈를 예측하기 위해서 그 스타워즈와 비슷한 유저 b가 평가한 아이템의 집합은 헐크와 아이언맨이었죠. 그 헐크와 아이언맨이 이 파유에 해당하는 것이고 그래서 이 5u에 해당하는 아이템들의 가중치 와 평점을 사용하여서 최종 아이템 i에 대한 예측 평점을 구하는 것입니다. 여기까지 CF MBCF 그리고 KNNCF에 대해서 배웠고요. 이제 이 CF의 최종 목적은 결국 유저 u의 아이템 i에 대한 평점을 계산하는 것이고 유저 베이스, 아이템 베이스 CF 모두 다 사용을 해서 평점을 예측할 수 있었습니다. 이렇게 해서 예측된 평점은 결국 레코멘데이션 시스템 추천 시스템에 활용되어야 합니다. 유저를 기준으로 가장 평점이 높은 엔개의 아이템을 정렬해서 그 엔개의 아이템을 최종적으로 추천해 주는 것이 탑 앤 레코멘데이션이라고 볼 수 있습니다. 그래서 이 씨프엘을 통해 구한 평점을 탑 앤 추천에 활용해야 되는데요. 아주 간단합니다. 타겟 유저에 대해서 아이템들의 모든 평점이 계산 완료되면 그 평점을 높은 순서대로 정렬하여서 가장 평점이 높은 상위 n개만 뽑아서 추천을 해주는 것입니다. 이 우측에 있는 오늘의 쇼핑 제안은 이제 쿠팡에 접속했을 때 아무런 컨텍스트 없이 첫 화면에 등장하는 화면 추천 아이템들인데요. 이 나에게 가장 적합한 아이템들이 탑 엔 개가 5개가 추천되었음을 볼 수 있습니다. 이제 이런 식이 탑 엔 레코멘데이션에 활용이 됩니다. 결과적으로 예측 평점이 가장 높은 아이템을 n개를 뽑아서 추천해 주게 되면 최종 추천이 완료가 됩니다. 네 컬래버레이티 필터링 첫 번째 파트는 여기까지입니다. 들어주셔서 감사합니다."
}