{
  "lecture_name": "[RecSys 이론] (4강) Collaborative Filtering 2",
  "source_file": "[RecSys 이론] (4강) Collaborative Filtering 2_132.mp4_2025-12-04-110125210.json",
  "text": "안녕하세요. 추천 시스템 강의를 맡은 강사 이준원입니다. 이번 시간은 컬래버레이터 필터링 두 번째 시간입니다. 이번 시간에는 지난 시간에 배운 이웃 기반 네이버 우드 베이스 CF와는 다른 모델 베이스드 CF를 학습합니다. 먼저 모델 베이스 CF가 무엇이고 이 모델 베이스 CF가 이웃 기반 c프와는 어떻게 다른지 어떤 개념을 사용하여 학습하는지 를 배워봅시다. 그리고 모델 기반의 CF의 대표적인 모델인 이 매트리스 팩토라이제이션에 대해서 대부분의 내용을 다룰 텐데요. 이 메트리스 팩토라이제이션의 기본 아이디어가 되는 싱귤러 밸류 디컴포지션 스브디의 개념과 원리를 이해한 다음에 엠프 문제를 정의하고 어떻게 모델이 학습하는지 다루겠습니다. 그리고 나서 이 인플리시 피드백 데이터셋에 대해서 매트리스 팩토라이제이션이 어떤 모델링을 사용하여 학습하는지 그리고 모델의 학습 파라미터를 어떻게 업데이트하는지 그 학습 방법인 ALS와 BPR에 대해서도 이야기하겠습니다. 추천 시스템에서 가장 중요한 모델이 매트리스 팩토라이제이션인데요. 다소 내용이 많지만 중요한 이론적인 베이스 가 되기 때문에 잘 이해하고 학습하시길 바랍니다. 첫 번째로 모델 베이스 콜라보레이트 필터링에 대해 이해하고 이를 적용해 봅시다. 먼저 추천 시스템에서 반드시 해결해야 하는 이 두 가지 이슈가 있다고 지난 시간에도 말씀드렸습니다. 바로 스파 시티와 스케일러빌리티 이슈입니다. 지난 강의에서 다룬 이 이웃 기반 CF, 네이버 우드 베이스 CF는 이 두 가지 이슈에 대해서 각각 문제가 있다고 했는데요. 먼저 스파 시티 이슈 같은 경우에는 데이터가 충분하지 않은 경우에 추천 성능이 떨어지거나 혹은 네이버 우드 베이스 CF와 같은 경우에는 유사도 계산이 부정확해집니다. 데이터가 굉장히 부족하거나 혹은 아예 학습 데이터에는 없는 유저나 아이템이 등장할 경우에는 추천이 불가능한데요. 이제 이런 문제를 콜드 스타트 문제라고 합니다. 두 번째로는 스케일러빌리티 이슈입니다. 유저 아이템이 계속해서 늘어나서 유저 아이템의 숫자가 굉장히 클 때를 의미하는데요. 보통 대형 서비스 같은 경우에는 유저 아이템의 숫자가 뭐 수십만 수백만에 이르기 때문에 계속 아이템의 개수가 늘어날수록 유사도 계산도 계속 늘어나고 그 계산량도 많아지게 됩니다. 유저 아이템이 많아야 데이터가 많이 쌓이고 그 CF의 효과가 올라가지만 그래서 정확한 예측을 할 수 있지만 반대로 그 계산 시간은 오래 걸리는 문제가 스케일로베티 이슈입니다. 그래서 모델 기반 CF 모델 베이스 CF가 등장하게 되는데요. 이 모델 기반 CF는 항목 간의 유사성을 단순히 비교하는 것에서 벗어나서 데이터에 내재한 패턴을 이용해서 추천을 하는 씨프 기법입니다. 그래서 여기 네 가지 차이점이 있는데요. 기존의 네이버 워드 씨프는 파라미터를 학습하지 않았는데요. 이 모델 기반 시프는 파라미트릭 머신 러닝을 사용합니다. 사실 저희가 이미 딥러닝을 배웠기 때문에 딥러닝에서도 다양한 웨이트 즉 파라미터를 학습하는데요. 매트리스 팩토라이제이션은 그보다 간단한 모델이긴 하지만 어쨌든 파라미터를 학습하는 머신 러닝을 사용합니다. 그래서 주어진 데이터를 사용하여서 모델을 학습하고 그 모델에 가지고 있는 파라미터 에 데이터의 정보가 압축된 형태로 저장됩니다. 그래서 학습된 모델의 이 파라미터는 데이터의 패턴을 나타내고 이 파라미터는 최적화 문제를 통해서 업데이트가 돼서 최종적으로 모델이 완성되게 됩니다. 그래서 모델 기반 CF의 특징은 데이터에 숨겨져 있는 유저와 아이템의 관계에 잠재적인 특성과 패턴을 찾아서 이를 파라미터에 반영하는 것입니다. 그래서 이웃 기반 CF와 비교하면 이웃 기반 c프는 유저 아이템 벡터를 통해서 데이터를 계산된 형태로 저장하고 있습니다. 즉 파라미터를 따로 학습하지 않고 유저 아이템의 레이팅을 그대로 유저 아이템 벡터로 사용하는 기법이기 때문에 메모리 베이스 시에프라고 불리기도 합니다. 데이터를 그대로 메모리에 올려놓고 콜라보레이트 필터링을 수행하기 때문이죠. 그러나 이 모델 베이스 CF 우리가 앞으로 배울 기법은 유저 아이템 벡터를 모두 학습을 통해서 어떤 파라미터로 저장하게 됩니다. 네 현업에서는 매트리스 팩토라이제이션 기법을 가장 많이 사용하는데요. 특히 최근에는 이 매트리스 팩토라이제이션의 원리를 딥러닝 모델에 응용하는 기법이 높은 추천 성능을 내고 있습니다. 자 그래서 이 이웃 기반 CF에 비해서 모델 기반 CF가 가진 장점을 몇 가지 설명해 보겠습니다. 먼저 모델의 학습과 서빙이 되는 측면인데요. 우리가 사용하는 유저 아이템 데이터 즉 학습 데이터는 학습에만 사용되고 그 데이터의 패턴이 학습된 모델의 압축된 형태 즉 파라미터의 형태로 저장되게 됩니다. 따라서 이미 학습된 모델을 가지고 추천하기 때문에 서빙 속도가 이웃 기반 CF보다 빠르게 됩니다. 또한 아까 전에 언급했던 이 스파 시티 스케일러빌리티 이슈 추천 시스템에서 풀어야 하는 중요한 문제를 개선했는데요. 이웃 기반 CF의 경우 스파 시티 웨이co가 99.5% 즉 너무 스파스한 데이터에 대해서는 추천 성능이 좋지 않다고 언급을 드렸는데요. 이 매트리스 팩토라이제이션 모델 베이스 시프 같은 경우에는 스파시티 베이시오가 99.5%가 넘는 굉장히 스파스한 데이터에 대해서도 상대적으로 좋은 성능을 보입니다. 또한 사용자와 아이템 숫자가 몇십만 몇백만으로 늘어나게 되더라도 괜찮은 추천 성능을 보입니다. 그다음은 오버피팅을 방지한다는 것인데요. 이웃 기반 CF 같은 경우에는 전체 데이터의 패턴을 학습하는 것이 아니라 특정 주변 이웃 자신과 가장 가까운 이웃의 데이터 패턴을 바로 반영해서 평점을 예측하거나 추천을 수행하는데요. 이웃 기반 CF와 달리 모델 베이스 CF는 전체 데이터의 패턴을 학습하기 때문에 특정 이웃에 오버피팅 되지 않고 조금 더 일반화된 모델의 성능을 보여줍니다. 그리고 마지막으로 리미티드 커버리지 를 극복했다는 것인데요. 이웃 기반 CF 같은 경우에는 공통의 유저나 공통의 아이템을 많이 공유해서 결국 그 값으로 유사도를 정확하게 구해야 하는데요. 이 커버리지가 아무리 데이터가 많다고 하더라도 워낙 스프레스 한 매트릭스이기 때문에 어느 정도의 적당한 유사도를 가진 두 유저 혹은 두 아이템 벡터의 쌍은 많지 않습니다. 따라서 이 커버리지로 인해서 아주 높은 추천 성능을 낼 수는 없는 건데요. 이제 이 모델 베이스 CF 같은 경우에는 특정 유저나 특정 아이템 페어의 유사도를 직접 계산하지 않고 모델이 전체 데이터의 패턴을 학습하기 때문에 이 MBCF와 같은 유사도 값이 정확하지 않은 이런 케이스는 발생하지 않습니다. 네 다음은 조금 다른 이야기인데요. 어떤 모델을 사용하냐가 아니라 유저 아이템의 상호작용 데이터인 피드백 데이터가 어떻게 이루어져 있는지에 대한 이야기입니다. 우리 지난 시간에는 주로 평점 즉 익스플리시 피드백 을 주로 다뤘습니다. 영화 평점이나 맛집 별점과 같은 아이템에 대한 유저의 선호도를 직접 알 수 있는 데이터 1점부터 5점까지 이런 식으로 다루었죠. 그래서 우리가 지난 시간에 이웃 기반 CF 를 배울 때 다루었던 토의 데이터들은 대부분 익스플리시 피드백 데이터였고 레이팅 프리딕션도 익스플리시 피드백을 예측하는 것이었습니다. 그러나 실제로 추천 시스템에는 익스플리스 피드백보다 인플리스 피드백 데이터가 훨씬 많습니다. 어떤 아이템을 클릭했거나 어떤 동영상을 시청했다는 그런 여부들 은 유저가 선호도를 직접 1점에서 5점으로 매기지 않더라도 혹은 좋아요를 누르지 않더라도 자동으로 수집이 되기 때문에 유저의 선호도를 간접적으로 알 수 있는 데이터는 굉장히 많이 수집되게 됩니다. 그래서 그 유저가 아이템을 소비했다 즉 상호작용이 있었다고 하면은 1 positive한 원소를 갖는 행렬로 표현할 수 있습니다. 다만 여기서 상호 작용하지 않았다라는 값을 0으로 놓지는 않습니다. 말씀드린 대로 현실에서는 익스플리시 피드백보다 인플 인플리시 피드백 데이터가 훨씬 많고 크기가 크기 때문에 훨씬 많이 사용되고요. 그래서 이 인플리시 피드백 데이터를 학습하기에 적합한 모델링 방식도 중요한 부분입니다. 그래서 각각의 익스플리시 피드백, 인플리시 피드백을 가지고 유저 아이템 매트릭스를 그림과 같이 만들어 보았습니다. 이 빈칸을 어떻게 예측할까요? 일단 위에 부분은 우리가 지난 시간에 레이팅 프리딕션을 배웠던 것처럼 빈칸에 유저 아이템에 대한 예측 평점을 채워 넣을 수 있겠죠. 근데 인플리시 피드백 같은 경우에는 동그라미 즉 이 유저가 이 아이템에 대해 선호했다라는 데이터만 있는데요. 그래서 이 매트릭스를 채워 넣을 때는 이 유저가 이 아이템을 선호할 것이다 혹은 선호하지 않을 것이다. 사실 여기는 동그라미와 x로 표현했지만 어떤 선호도 값으로 예측하게 됩니다. 어떤 정확한 레이팅이 아닌 이 유저가 이 아이템을 얼마나 좋아할 것인가 혹은 얼마나 좋아하지 않을 것인가에 대한 값으로 예측하게 됩니다. 다음은 레이턴트 팩터 모델이라는 개념입니다. 처음에 이 개념이 등장할 때는 레이턴트 팩터라는 말을 많이 사용했는데요. 요즘에는 이제 딥러닝 AI로 넘어오고서는 레이턴트 팩터라는 표현보다는 인베딩이라는 워딩을 훨씬 많이 사용하는 것 같습니다. 이제 앞으로 배우는 모델 베이스트 CF 같은 경우에는 모두 레이턴트 팩터 즉 인베딩으로 표현되는 모델이기 때문에 이 개념을 짚고 넘어가겠습니다. 이 레이턴트 팩터 모델의 의미는 유저와 아이템의 관계를 어떤 잠재적인 요인으로 표현할 수 있다고 보는 모델입니다. 다양하고 복잡한 유저의 와 아이템의 특성을 몇 개의 벡터로 굉장히 댄스하게 컴팩트하게 표현하는 것이죠. 그래서 유저와 아이템 행렬 이 유저 아이템 행렬 같은 경우에는 우리가 갖고 있는 트레이닝 데이터가 되겠죠. 방금 전에 그림에서 보았던 그 행렬을 저차원의 행렬로 분해하는 방식으로 작동합니다. 이때 유저 아이템 행렬이 분해되는 저차원의 각 의미는 모델 학습을 통해 생성되지만 표면적으로는 알 수 없습니다. 그래서 이제 유저와 아이템을 같은 차원으로 레이턴트하게 만들어서 같은 벡터 공간에 임베딩하게 될 경우 유저 아이템 벡터가 같은 공간에 놓이게 되는데요. 이를 통해서 유저와 아이템이 얼마나 유사한 정도를 가지는지 확인할 수 있고 유저 아이템 벡터가 유사한 공간에 임베딩 된다면 해당 유저에게 해당 아이템이 추천될 확률이 높다고 이해할 수 있습니다. 다음 페이지의 그림을 통해서 레이턴트 팩터 공간에 놓인 유저와 아이템 벡터의 예시를 살펴봅시다. 네 다음 그림은 유저와 아이템 벡터를 편의상 간단한 2차원으로 인베딩 했다고 가정합시다. 여기에 파란색으로 보이는 것들이 다 아이템 벡터 즉 영화를 인베딩한 것이고요. 여기에 보이는 이 사람 한 명 한 명은 유저 벡터인 거죠. 그래서 여기에 보이는 이 2차원 공간에 유저 아이템을 임베딩 했을 때 첫 번째 차원은 이 영화가 남성형인지 여성형인지, 두 번째 차원은 이 영화가 심각한 영화인지 혹은 코미디와 같이 웃긴 영화인지이고요. 이제 레이턴트 팩터 모델을 사용하여서 유저 벡터와 아이템 벡터를 인베딩 시켰을 때 각각의 영화의 특징에 따라서 인베딩이 이루어짐을 볼 수 있습니다. 그럼 이 유저 같은 경우에는 심각한 남성형 영화, 전쟁 영화 같은 그것을 좋아하는 유저라고 볼 수 있고요. 그래서 이 유저에게는 전쟁 영화 같은 아이템들이 추천될 것이고요. 반대로 이 유저 같은 경우에는 가벼우면서 여성향 영화들이 추천되게 됩니다. 여기에서는 이 레이턴트 팩터의 차원의 의미를 남성 여성 심각한 웃김으로 표현했지만 아까 언급했던 것처럼 실제 모델로 학습을 하게 될 경우 각 차원의 의미가 무엇인지는 우리가 알 수 없습니다. 이 그림은 우리의 이해를 돕기 위해서 각 차원의 의미가 무엇인지 적었을 뿐이지 실제 레이턴트 팩터 모델을 학습하고 나서 각각의 차원의 의미는 익스플리시트하게는 우리가 알 수 없음을 참고하시기 바랍니다. 그래서 이번 강의에서는 컬라버레이트 필터링, 또 그 가운데서도 모델 베이스 시프인 에브디와 매트리스 팩토라이제이션 각각의 학습 기법들에 대해서 자세히 다뤄보겠습니다. 이제 그 밑에 있는 딥러닝 기반의 모델은 다음 강의인 5강부터 7강까지 다양하게 다룰 예정입니다. 다음은 SVD가 무엇이고 이 원리를 이해해 보고 추후에 이 개념이 어떻게 매트리스 팩토라이제이션 모델로 확장되는지 알아봅시다. 먼저 s브디는 싱귤러 벡터 디컴포지션의 약자로서 2차원 행렬을 분해하는 기법입니다. 우리가 추천 시스템에서 사용하는 데이터는 유저와 아이템으로 이루어진 2차원 레이팅 매트릭스인데요. 이 웨이팅 매트릭스에 대해서 유저와 아이템의 잠재 요인 즉 레이턴트 팩터를 표현할 수 있는 행렬로 분해하는데요. 행렬은 총 3가지의 행렬로 이루어집니다. 유저 잠재 요인 행렬 그리고 잠재 요인의 중요도를 나타내는 잠재 요인, 대각 행렬 그리고 아이템을 레이턴트 팩터 잠재 요인으로 나타내는 행렬 3개로 나타낼 수 있는데요. 사실 이 싱글레벨러디 컴포지션 s브디는 추천 시스템에 처음 활용됐다기보다는 선형 대수학에서 차원을 축소하는 기법으로 분류하는 것이 더 정확하고요. 이제 우리가 알고 있는 주성분 분석 피씨에도 이런 선형대수학의 차원 축소 기법 중에 하나입니다. 그러면 SVD를 그림으로 살펴봅시다. 사실 SVD를 완벽하게 이해하기 위해서는 선형대 수학의 아이겐 밸류 디컴포지션이나 싱귤러 벡터, 싱귤러 밸류와 같은 개념을 알아야 정확하게 이해할 수 있습니다. 하지만 이 강의에서 이런 선형대 수학의 내용을 모두 다루기는 어렵고요. 따라서 그림을 통해서 각각의 행렬이 어떠한 의미를 가지는지를 직관적으로만 살펴봅시다. 먼저 첫 번째 유저 행렬입니다. 이 유저 아이템으로 이루어진 m 곱하기 엔 차원의 행렬을 3개의 매트릭스로 분해하는데요. 첫 번째 유저 행렬은 한 명 한 명 유저에 대해서 엠 차원짜리 팩터로 분해하게 됩니다. 그리고 마찬가지로 아이템 팩터도 아이템 전체 개수인 n 차원짜리 행렬로 벡터로 분해하게 됩니다. 이 둘의 차원은 맞지 않기 때문에 가운데에 있는 이 잠재 요인 행렬, 레이턴트 팩터, 직사각, 대각 행렬이 각각의 팩터의 중요도를 어떠한 수치적인 값으로 갖습니다. 이 행렬은 대각행렬이기 때문에 대각선에 있는 원소를 제외한 나머지 원소들은 다 0이 됩니다. 그래서 이렇게 행렬을 분해하고 이 분해된 행렬을 다 곱했을 때 원래 레이팅 매트릭스로 다시 복원하는 개념이 SVD입니다. 그래서 전체 우리가 갖고 있는 레이팅 매트릭스를 온전하게 3개의 매트릭스로 분해하는 걸 풀스브디라고 하는데요. 이제 여기서 우리가 알아야 되는 것은 이 풀 스브디가 아니라 이것을 좀 더 압축시킨 트렁케이티드 스브디입니다. 여기서는 전체의 레이턴트 팩터 가운데 대표 값으로 사용될 케 개의 특이치, 즉 케이개의 값만 활용합니다. 그래서 전체 특이값이 여기에 있으면 이 중에서 가장 값이 높은 몇 개의 값만 골라서 압축하게 됩니다. 그럼 이 압축된 레이턴트 팩터에 따라서 유저 매트릭스와 아이템 매트릭스도 원래는 엔 곱하기 m 행렬 n 곱하기 n 행렬이었지만 m 곱하기 케 케 곱하기 엔으로 압축됩니다. 이렇게 압축된 행렬은 원래 행렬 알과 완벽하게 동일하지는 않지만 최대한 유사한 알 햇으로 복원될 수 있고요. 그래서 이 몇 개의 특이치만을 가지고도 기존 알과 최대한 비슷하게 유지하는 그 유용한 정보를 유지시키는 방법이 트로 KT 에스비입니다. 그래서 이 분해된 행렬은 다시 곱해져서 알 셋이 되고 이 알 햇은 부분 복원되긴 하지만 가장 중요한 정보는 남아 있는 매트릭스가 되는 것이죠. 아까 얘기한 대로 이 케이크의 레이턴트 팩터는 어떤 의미인지 유추할 수는 있지만 정확히 무엇을 의미하는지는 우리가 알 수는 없습니다. 요약하면은 이 트렁케이티드 스프디는 각각의 유저와 아이템을 요 케이계에 적은 훨씬 적은 차원만 활용해서 압축할 수 있고 표현할 수 있다는 것인데요. 이제 이렇게 해서 실제 평점 매트릭스인 알과 최대한 유사한 알 햇을 복원할 수 있고 이 알 햇을 추천 시스템의 예측 값으로 사용한다는 것이 이 SVD의 핵심적인 아이디어입니다. 그래서 주어진 행렬을 주어진 유저 아이템 매트릭스를 어떤 적은 차원의 매트릭스로 분해한다는 것은 다음에 배울 매트릭스 팩토라이제이션의 핵심 아이디어와 일맥상통합니다. 그러나 전통적인 SVD를 사용하여 평점을 예측하는 것은 한계가 있습니다. 먼저 아까 우리가 분해해야 되는 유저 아이템 매트릭스 이 논레지가 불안정할 때 이 SVD는 작동하지 않습니다. 실제 우리가 유저 아이템 매트릭스를 구성하면은 스프레이시티가 굉장히 높고 결측치가 굉장히 많기 때문에 대부분의 데이터는 비어 있겠죠. 그 비어 있는 상태로는 SVD를 수행할 수 없습니다. 완전히 유저 아이템에 대한 매트릭스의 원소가 다 채워져 있을 때만 에브디가 작동하기 때문에 따라서 결칙된 엔추, 결칙된 원소를 모두 채우는 인퓨테이션을 통해서 유저 아이템 매트릭스를 댄스하게 만들어야만 에브디를 수행할 수 있습니다. 그래서 결칙된 n추를 0으로 채우거나 혹은 유저 아이템의 평균으로 채우게 되는데요. 이제 이럴 경우에 실제 유저가 그 아이템에 대해서 평가를 한 것이 아니지만 결칙되었다는 이유만으로 0이나 유저 아이템의 평균 평점으로 채우는 것은 데이터 양도 상당히 증가시키면서 또한 그 데이터를 왜곡시키기 때문에 결과적으로 예측 성능을 떨어뜨립니다. 특히 행렬의 n추이가 매우 적은 특히 스파이시티 메이저가 굉장히 높을 때는 이렇게 인퓨테이션을 통해서 에브디를 적용하면 과적합되기 쉽습니다. 그래서 우리는 이 SVD의 원리, 실제 유저 아이템 매트릭스를 적은 차원의 행렬로 분해하는 원리는 차용하되 다른 접근 방법이 필요한데요. 이것이 매트릭스 팩토라이제이션의 등장 배경입니다. 그래서 다음은 에브디의 원리를 사용하여 이 말 그대로 매트릭스를 팩토라이제이션, 즉 행렬을 분해하는 엠프의 원리를 이해해 보고 적용해 봅시다. 이 매트리스 팩토라이제이션은 유저 아이템 행렬을 저차원의 유저 행렬과 레이턴트 행렬의 곱으로 분해하는 방법입니다. 방금 배웠던 SVD 개념과 유사하지만 채워져 있지 않은 관측되지 않은 선호도를 강제로 인퓨테이션 하지 않고 오로지 유저 아이템의 관측된 선호도만 모델링에 활용하고 관측되지 않은 선호도를 예측하는 일반적인 모델을 만드는 것이 목표입니다. 그래서 SVD와 다르게 레이팅 매트릭스를 유저 매트릭스와 아이템 매트릭스 2개의 매트릭스로 분해하여서 이 분해된 p와 q의 곱이 실제 알과 최대한 유사하도록 알 햇을 추론하게 됩니다. 그래서 기본 모델을 그림으로 나타냈습니다. 알과 최대한 유사한 알 햇이 만들어지도록 피와 큐 즉 유저 매트릭스와 아이템 매트릭스를 학습하는 것인데요. 이 유저 매트릭스는 전체 유저의 수 곱하기 케이 개의 레이턴트 팩터, 아이템 매트릭스는 전체 아이템 개수 곱하기 케이의 레이턴트 팩터 로 이루어져 있습니다. k 레이턴스 섹터가 같아야만 행렬 곱이 수행되기 때문이죠. 그래서 이렇게 p와 q를 정의하면 이 p와 q 안에 있는 각각의 벡터들 유저 벡터들과 아이템 벡터들은 모두 학습 파라미터가 됩니다. 그래서 우리 이 학습 파라미터를 구하기 위해서 최적화 문제를 풀어야 하는데요. 이제 우리가 실제로 알고 있는 트루 에이팅이 알유아고요. 우리가 p와 q로 행렬을 분해해서 모델링한 그렇게 해서 예측한 값은 rui의 h 값입니다. 그리고 이 rui h은 어떤 유저 벡터와 아이템 벡터를 내적한 값입니다. 그래서 이 둘의 차이를 mse로 민 스퀘어드 에러로 정의하여서 이 값이 최대한 작아질 수 있도록 최적화 문제를 풀게 되면은 그에 해당하는 파라미터인 피유와 큐아가 학습됩니다. 이 수식에 대해서 조금 더 자세히 다음 슬라이드에서 살펴봅시다. 그래서 우리 최적화 문제를 풀기 위해서 오브젝티브 펑션을 정의해야 하는데요. 이제 이 부분이 실제 레이팅과 예측 레이팅의 차이이고요. 이제 뒤에 있는 부분은 각각의 파라미터가 오버피팅 되지 않도록 레귤러라이제이션 텀을 적용한 부분입니다. 말씀드린 대로 이 rui는 학습 데이터에 있는 실제 유저 유의 아이템 i에 대한 레이팅이고요. 우리가 학습해야 될 pu와 qi가 유저 그리고 아이템 i의 레이턴트 벡터 입니다. 그리고 이 뒤에 있는 부분은 우리가 학습해야 할 파라미터에 대해서 람다를 곱해서 엘투놈을 적용했는데요. 이 l2 정규화를 통해서 pu와 qi가 너무 많이 커지지 않도록 즉 학습 데이터에 과적합돼서 실제로 예측 데이터에 대해서는 성능이 예측 성능이 떨어지지 않도록 사용하는 기법입니다. 여기 보시면은 오브 디 UI 즉 실제 관측된 데이터만을 사용해서 이 모델을 학습하는데요. 아까 전에 SVD 같은 경우에는 행렬을 분해하기 위해서 결측된 엔트리를 인퓨테이션을 통해서 강제로 채워 넣었는데 이 부분이 빠지고 우리는 실제 관측된 데이터만을 사용해서 모델을 정확하게 학습하는 방법입니다. 방금 전에 나왔던 엘2 레귤러라이제이션을 언급하면서 정규화라는 개념을 잠시 얘기할 텐데요. 아마 아시는 분도 있겠지만 우리가 학습해야 될 웨이트 혹은 파라미터를 로스 펑션에 넣어주면은 이 웨이트가 너무 커지지 않고 제한이 걸려서 학습 데이터에 오버피팅 되는 것이 방지가 됩니다. 이때 이 레귤러라이제이션 텀에 곱해진 람다의 크기에 이 레귤러라이제이션의 강도가 영향을 받는데요. 람다 또 너무 크게 하면은 웨이트가 제대로 변하지 않아서 학습이 아예 일어나지 않는 언더 피팅이 일어나게 됩니다. 그래서 대표적인 방법으로는 l1과 l2 정규화가 있는데요. 우리는 매트리스 팩터라이제이션에서는 l2 정규화를 사용했음을 짚고 넘어가겠습니다. 네 매트리스 팩토라이제이션의 최적화 문제를 정의했으니 이제 파라미터를 업데이트해야 하는데요. 이 파라미터를 업데이트하는 방법은 스토캐스틱 그레디언트 디센트 여러분들이 아마 이전에 강의에서 배우셨던 확률적 경사 하강법을 사용합니다. 그래서 다음과 같은 로스가 있을 때 그 로스를 우리가 학습해야 하는 그래디언트로 편미분한 값이 그래디언트가 되고요. 이 그레디언트의 반대 방향으로 웨이트를 업데이트해 줍니다. 그래서 MF 모델의 SGD를 적용해 보면 아까 정의했던 로스 펑션 이 다음과 같이 있고요. 일로스 펑션을 우리가 그래디언트로 계산하기 위해서 학습 파라미터인 pu로 편미분을 했을 때 이 수식을 통해서 다음과 같이 미분이 이루어지고 그래서 결국에 이런 수식을 얻게 되는데요. 여기서 이 rui 마이너스 pu 트랜스포스 qi는 실제 값과 예측값의 차이인 에러가 됩니다. 그래서 최종적으로 이 에러 텀을 사용해서 그래디언트를 이와 같이 표현할 수 있고요. 우리는 그래디언트의 반대 방향으로 업데이트를 해야 하기 때문에 이 그래디언트의 반대 방향 마이너스가 플러스가 돼서 더해지게 되고 여기서 이 값은 러닝 웨이트가 됩니다. 네 여기까지가 매트리스 팩토라이제이션을 gd로 학습하는 방법이었고요. 다음은 이 MF의 다양한 테크닉을 추천 시스템에 사용한 논문 매트리스 팩토라이제이션 테크닉 레코멘데이션 시스템이라는 아주 유명한 논문입니다. 기본적인 매트리스 팩토라이제이션 모델과 거기에 다양한 테크닉을 추가하여서 성능을 향상시킨 논문입니다. 기본적인 엠프 모델과 수식은 방금 전에 다뤘기 때문에 이 논문에서 어떠한 테크닉을 추가하였는지를 위주로 리뷰해 보겠습니다. 먼저 첫 번째는 에딩 바이어스입니다. 어떤 유저는 모든 영화에 대해서 평점을 짜게 줄 수도 있고 어떤 유저는 모든 영화에 대해서 평점을 후하게 줄 수도 있겠죠. 즉 유저별로 편향이 생길 수가 있습니다. 마찬가지로 아이템도 편향이 생길 수 있는데요. 그래서 매트리스 팩토라이제이션 모델 텀에 전체 평균과 유저 아이템별로의 바이어스 편향을 반영할 수 있는 파라미터를 추가하여서 예측 성능을 높인 방법입니다. 이 목적 함수가 기존에 있는 목적 함수이고요. 아래에 보시면 이 뮤 bu BI가 새로 포함된 추가되었음을 알 수 있습니다. 이 뮤는 글로벌 평균이고요. 이 뮤는 학습하는 파라미터는 아닙니다. 이제 이 bu와 BI가 학습 파라미터인데요. 개별 유저와 개별 아이템에 대한 편향을 학습할 수 있는 파라미터를 추가하였고 마찬가지로 이 편향이 레귤러라이제이션 텀에 추가돼서 오버피팅 되지 않게 학습이 됩니다. 그래서 SGD를 사용하여서 똑같이 파라미터를 업데이트할 수 있는데요. 이제 이 아랫 부분은 원래 기존에 유저와 아이템 벡터를 업데이트한 부분이고 거기에 이 바이어스 두 개의 텀이 추가되었습니다. 그래서 이 추가된 그레디언트 값들은 모두 로스 펑션을 각각의 파라미터로 편미분한 값이고요. 여러분들이 계산해 보면 쉽게 각각의 값을 구할 수 있습니다. 다음은 컨피던스라는 개념을 추가한 테크닉입니다. 보통은 우리가 가진 모든 데이터가 있을 때 데이터 하나하나에 대해서 동일한 비중으로 학습하게 됩니다. 그러나 때로는 모든 데이터가 다 동일하게 중요하거나 혹은 동일한 신뢰도를 갖지 않습니다. 그래서 이 신뢰도 의 개념을 사용하여서 때로는 특정 데이터는 모델이 더 많이 반영해 주길 바라고 특정 데이터는 신뢰도가 낮기 때문에 모델이 덜 반영해 주길 바라는 것인데요. 뭐 대규모의 광고 집행과 같이 특정 아이템이 굉장히 많이 노출되는 경우에 학습 데이터의 대부분이 그 아이템으로 이루어져 있습니다. 그럴 경우 그 아이템은 컨피던스를 좀 낮게 주어서 모델이 좀 덜 학습하게 만들 수 있는데요. 이 수식을 통해 살펴봅시다. 이 위에 있는 것이 아까 바이어스까지 사용한 모델이고요. 보시면은 실측값과 예측값의 차이 앞에 cui라는 컴피던스가 추가됩니다. 그래서 데이터에 따라서 컨피던스가 높은 데이터는 이 CU가 높겠죠. 그러면 로스 펑션에 더 많이 반영되기 때문에 학습이 더 많이 될 것이고 cui가 낮은 경우에는 아무리 데이터가 많더라도 로스 펑션에 덜 반영되기 때문에 그만큼 그 데이터는 모델의 파라미터에 덜 반영되게 됩니다. 이렇게 사용할 수가 있고요. 이제 또 다른 사용법은 유저의 아이템에 대한 평점이 정확하지 않은 경우에도 사용할 수 있습니다. 이때도 컨피던스를 추가하는데요. 인플리시 피드백 같은 경우에도 컨피던스를 사용하여서 매트리스 팩토라이제이션 모델링을 합니다. 이 뒤에 4강에서 이 부분을 자세히 다루도록 하겠습니다. 이제 마지막 마지막 테크닉은 엠스 모델링에 템퍼럴한 피처를 추가한 기법입니다. 템퍼럴이란 시간적인 개념인데요. 어떤 시간에 따라서 유저와 아이템의 특성을 반영하고 싶은 것입니다. 예를 들면 아이템은 초반에 인기가 많았지만 점점 시간이 지날수록 인기도가 떨어질 수도 있고요. 반대로 올라갈 수도 있죠. 또 유저 같은 경우에는 초반에는 굉장히 평점을 후하게 주다가 시간이 흐르면서 평점을 내리는 기준이 엄격해질 수 있습니다. 따라서 학습 파라미터가 시간을 반영하도록 모델을 설계하는 것인데요. 원래 기존에는 비유 BI p qi가 모두 모두 티와는 관계없는 독립적인 파라미터였는데요. 여기에 티라는 veriebe를 추가하여서 시간이 지날수록 선형적으로 이 beu가 증가하거나 혹은 감소할 수 있는 선형 수식을 세워서 어떤 시간적인 개념이 파라미터의 반영될 수 있도록 하는 것이죠. 네 여기까지가 매트릭스 팩토라이제이션에 대한 기본적인 내용이었고요. 다음은 방금 언급했던 인플리시 피드백을 다루고 이를 학습할 수 있는 매트릭스 팩토라이제이션 모델과 학습 기법에 대해서 살펴봅시다. 다음 논문은 컬래버레이트 필터링 폴 인플리시 피드백 데이터셋 즉 인플리시 피드백 데이터셋을 위해서 매트리스 팩토라이제이션을 모델링하고 어떤 학습 기법을 사용해야 하는지를 제시한 논문입니다. 앞서 엠프의 다양한 테크닉을 제시했던 페이퍼의 동일 저자인 코렌이 썼던 논문인데요. 이 MF 논문 가운데 가장 잘 알려진 논문 중에 하나입니다. 말 그대로 인플리시 피드백 데이터에 적합한 매트리스 팩토라이제이션 모델을 설계하여서 성능을 향상시켰습니다. 먼저 이 논문에서 사용한 altonati 리스 퀘어라는 학습 기법에 대해서 설명해 보겠습니다. 본 논문에서 처음 제시한 방법은 아니지만 MF를 학습할 때 가장 많이 사용하는 방법입니다. 방금 전 파트에서는 우리가 SGD 스톡캐스트 그레디언트 디센트를 이용해서 엠프 모델을 학습했는데요. 유저 아이템 차원이 계속 늘어나고 데이터가 아주 많아지게 되면 학습 속도는 그만큼 느려지게 됩니다. 그러나 이 ALS 학습 방법은 SGD보다 훨씬 더 빠른 방법으로 학습이 이루어집니다. 먼저 기본적인 컨셉은 우리가 업데이트해야 되는 매트릭스가 유저 피 그리고 아이템 큐가 있었는데요. 이 피와 큐를 번갈아 가면서 업데이트하는 것입니다. 한 번의 피와 큐를 동시에 업데이트하지 않고 두 매트릭스 가운데 하나를 상수로 두고 q를 업데이트하고 또 q를 상수로 두고 하나를 업데이트하는 것이죠. 이렇게 pu나 qi 즉 한쪽에 매트릭스를 한쪽의 파라미터를 고정했을 때 다른 하나에 대해서는 리스트 스퀘어의 수식이 되기 때문에 리스트 스퀘어 문제를 디터미니스틱하게 풀 수 있습니다. 그래서 방금 언급했던 SGD와 비교해 보면은 이 스프레스한 데이터 즉 유저 아이템은 굉장히 많은데 거기에 그에 비해 채워져 있는 데이터는 굉장히 적은 경우에 더 학습이 로버스트하게 잘 이루어집니다. 또 대용량 데이터를 병렬로 처리할 수 있기 때문에 SGD보다 훨씬 빠른 학습 속도도 보여줍니다. 아까 전에 리스트 스퀘어 문제를 푼다고 했는데요. pu나 qi를 상수로 고정할 때 이 목적 함수가 쿼드 투 폼이 돼서 이 폼이 컨벡스 하기 때문에 리스트 스퀘어 문제를 풀 수 있다고 했습니다. 사실 이 개념은 컴백스 옵티마이제이션과 리니어 알제브라에 대한 이해도가 필요한데요. 한글로는 최소 제곱법이라고도 표현하고 있고요. 이 키워드로 검색을 해서 한번 스터디 해보시기를 추천드립니다. 네 다음 수식을 보시면 엠프와 동일한 수식입니다. 여기서 에엘스는 피와 큐를 번갈아 가면서 상수로 가정하고 나머지 파라미터를 업데이트하는데요. 이제 여기서 p를 만약에 상수로 둘 경우 이 목적 함수는 q를 q로 변수를 갖는 컴백스한 쿼드러틱 폼이 되므로 본 목적 함수를 최소로 만드는 q에 대한 해 를 디터미니틱하게 한 번에 구할 수 있습니다. 디터미니스틱하게 구한다는 것은 어떠한 수식 한번으로 인해서 계산할 수 있다는 것이고요. 쉽게 얘기해서 한 번의 행렬 연산으로 바로 피나 q에 대한 해를 구할 수 있는 것입니다. 그래서 p와 q 가운데 하나를 고정시키게 되면은 리스트 퀘어로 반대편 파라미터를 구할 수 있기 때문에 아래에 있는 두 시기를 번갈아 가면서 pu 한 번 업데이트하고 그다음 다시 큐를 업데이트하고 이렇게 계속 업데이트를 하게 되고요. 정해진 이터레이션을 모두 돌거나 혹은 로스 펑션이 더 이상 작아지지 않을 때까지 계속 돌다가 학습이 멈추게 되면은 그때 이 피유와 큐아에 대한 최종 업데이트가 마무리되게 됩니다. 그래서 여기까지가 매트리스 팩토라이제이션의 또 다른 학습 방법인 에스에 대한 내용이었습니다. 그래서 다음은 이 인플리시 피드백 데이터를 고려한 MF의 모델링인데요. 기존의 MF는 유저 아이템에 대한 레이팅만 고려했다면 이 인플리시 피드백은 이 레이팅을 두 가지로 나눕니다. 프리퍼런스와 컨피던스 두 가지로 나눠서 모델에 반영하게 됩니다. 먼저 프리퍼런스 같은 경우에는 유저가 아이템을 선호하는지의 여부를 10인 바이너리로 표현한 방법입니다. 그래서 실제 이 매트리스 팩토라이제이션의 피유 큐아의 내적은 이 프리퍼런스 값 10을 예측하게 됩니다. 그리고 컨피던스 같은 경우에는 이 유저 유가 아이를 얼마나 선호하는지를 나타내는 증가 함수로 정의해야 하는데요. 이제 가장 간단하게는 선형적인 형태로 cui는 1 플러스 기울기 곱하기 rui로 표현할 수 있습니다. 그래서 이 알파를 크게 하는 경우 파지티피티브 피드백과 negative 피드백의 상대적인 중요도를 더 크게 할 수 있고, 이 알파를 작게 하는 경우 커피던스 값을 낮게 해서 상대적으로 레이팅이 높던 낮던 관계없이 비슷하게 모델이 학습할 수 있도록 만들어 줍니다. 그래서 이 프리퍼런스 컨피던스 방법은 방금 전에 파트 3에서 배웠던 요 에딩 컨피던스 레벨에서 응용한 방법이기 때문에 둘을 비교하여서 참고하시길 바랍니다. 네 그래서 익스플리시 피드백에 대한 수식과 새로 구한 인플리시 피드백에 대한 목적 함수 수식을 비교해 봅시다. 기존의 엠프는 유저와 아이템 벡터의 내저 값을 가지고 곧바로 웨이팅을 예측했는데요. 인플리시 피드백 같은 경우 유저 아이템의 내저 값은 유저의 프리퍼런스 값 01을 예측하게 됩니다. 그리고 예측값과 실제 값의 차이를 전체 목적 함수에 얼마나 반영할지는 이 컨피던스 를 통해 조절되는 것이죠. 그래서 컨피던스가 이러한 인크리징 펑션이라고 앞에서 이야기했는데요. 웨이팅이 높을 경우에는 컨피던스가 높기 때문에 그 해당 데이터에 대해서 예측이 좀 더 정확해지도록 모델이 학습될 것이고요. 웨이팅이 낮을 때에는 컨피던스가 낮아지므로 그 해당 데이터에 대해서 모델이 조금 덜 정확해지도록 학습될 것입니다. 이제 새로운 목적 함수는 모델의 수식이 익스플리시 피드백과 달라졌지만 파라미터를 업데이트하는 최적화 방식은 우리가 파트 3에서 배운 SGD, 그리고 방금 전에 슬라이드에서 배운 에이스로 둘 다 동일하게 풀 수 있습니다. 본 논문에서는 이 as 매트릭스 연산이 더 효율적이기 때문에 이 방법으로 제안하였습니다. 그래서 이 위에 있는 수식이 아까 언급했던 익스플리시 피드백 기준의 MF를 ALS 얼터너티브 리스팅 스퀘어로 업데이트한 수식이고요. 아래는 기존에 알만 있었다면은 이제 이 컨피던스 매트릭스와 프리퍼런스 벡터가 포함된 해의 형태로 표현할 수 있습니다. 마찬가지로 리스트 스퀘어 문제를 풀게 되면은 해가 디터미니스틱하게 구해지기 때문에 데이터 포인트 하나하나 업데이트하지 않고 한 번에 피유와 큐아를 번갈아 가면서 업데이트할 수 있습니다. 마지막 파트는 인플리시 피드백을 고려한 또 다른 MF의 학습 방법인 베이지안 퍼스널라이즈 랭킹입니다. 이 베이지안 퍼스널라이즈 랭킹은 인플리시 피드백을 데이터 를 사용하여서 MF를 학습할 수 있는 또 다른 관점을 제시한 논문입니다. 이 논문은 이름대로 이 베이지 안 추론에 기반하고 있으며 인플리시 데이터를 사용하여서 서로 다른 아이템에 유저의 선호도를 반영할 수 있도록 모델링하였습니다. 네 먼저 이 베이지안 퍼스널라이즈 랭킹 BPR을 살펴보기 전에 퍼스널라이즈 랭킹이란 무엇인지 알아봅시다. 퍼스나이즈 랭킹 문제란 어떤 하나의 사용자에게 순서가 있는 아이템 리스트를 제공하는 문제인데요. 이것은 바로 아이템 추천 문제라고 볼 수 있겠죠 그래서 퍼스널라이즈 랭킹을 반영하여서 최적화를 한다는 것은 유저가 아이템 아이보다 제를 좋아한다는 데이터가 있을 때 이 정보를 활용해서 매트리스 팩토라이제이션 파라미터를 학습하고 목적 함수를 최적화시키는 것입니다. 그래서 유저 유에 대해서 아이템 아가 제보다 좋아한다면 이는 유저 유의 퍼스널라이즈 랭킹이다라고 정의할 수 있습니다. 자 우리가 다루는 데이터는 익스플레스 피드백이 아닌 인플리스 피드백 데이터라고 이야기했습니다. 그 서비스에서 우리가 쉽게 얻을 수 있는 데이터는 다음과 같은 사용자의 클릭이나 구매와 같은 로그이고요. 이제 이런 것들은 평점 이랑 다르게 아이템의 선호 정도가 분명하게 드러나지 않습니다. 단순히 클릭했다 구매했다 즉 어떤 관측된 데이터 퍼시티브 오브제레이션만 존재하고 그 아이템을 상호 작용하지 않은 데이터 는 따로 없기 때문에 관측되지 않은 negative 옵저베이션에 대해서는 두 가지가 다 가능합니다. 실제로 유저가 그 아이템에 관심이 없기 때문에 클릭하지 않아서 데이터가 쌓이지 않은 것이고요. 반대로는 실제로 관심이 있을 수도 있지만 아직 그 유저에게 아이템이 노출되지 않았기 때문에 혹은 그 유저가 그 아이템을 아직 존재를 모르기 때문에 상호 작용하지 않았을 수 있죠. 그래서 이 두 가지를 모두 고려해서 선호 정보를 생성해야 하는데요. 이 유저에 대한 아이템의 선호도 랭킹을 생성하여서 이를 매트리스 팩토라이제이션 학습 데이터로 사용해 보겠습니다. 그렇다면 아이템에 대한 선호도 랭킹을 생성해서 이 선호도 랭킹 데이터를 매트리스 팩토라이제이션 모델의 학습 데이터를 사용해야 하는데요. 어떻게 생성해야 할까요? 이제 이 우측에 주어진 유저 아이템 매트릭스가 있을 때 이 유저 아이템 매트릭스를 선호도 정보로 바꾸기 위해서 본 논문에서는 몇 가지 가정을 합니다. 먼저 관측된 아이템은 관측되지 않은 아이템보다 선호한다라고 데이터를 바꿉니다. 이것이 가장 중요한 정보인데요. 이제 그 외에 관측된 아이템끼리 혹은 관측되지 않은 아이템끼리는 무엇을 더 선호한다고 말할 수 없기 때문에 그 선호도를 추론할 수 없고 그 정보를 만들지 않습니다. 그래서 이 데이터를 통해 어떻게 선호도를 생성하는지 살펴봅시다. 이 데이터는 우리가 잘 알고 있는 유저와 아이템으로 이루어진 매트릭스입니다. 이제 이 유저 아이템 매트릭스를 각각의 유저별로 선호도 정보를 생성합니다. 그래서 우리는 첫 번째 예시인 유저 원에 대한 선호도 정보를 살펴볼 건데요. 유저 원 같은 경우에는 아이템 2와 아이템 3만 선호한다고 했습니다. 이제 이렇게 했을 때 관측되지 않은 아이템은 아이템 1과 아이템 4가 되겠죠 그럼 이 빨간색으로 어노테이션 한 아이템끼리와 검은색으로 어노테이션 아이템끼리의 선호도 정보는 우리가 구할 수 없습니다. 그러나 빨간색 아이템 아이템 투 아이템 쓰는 아이템 원 아이템 4보다 더 선호한다고 그 선호도 정보를 만들 수 있는 것이죠. 이렇게 할 경우에는 관측되지 않은 아이템 즉 유저 원을 기준으로는 유저 아이템 원과 아이템 4에 대해서도 선호도 정보가 부여가 돼서 이 매트리스 팩토라이제이션이 관측되지 않은 아이템들에 대해서도 파라미터가 학습되고 랭킹을 할 수 있게 된다는 장점이 있습니다. 이제 그 학습 데이터는 DS라는 형태로 표현됩니다. 그래서 정해진 유저 1명에 대해서 유저가 관측된 아이템 즉 선호하는 아이템 아이와 관측되지 않은 아이템 즉 선호하지 않는 아이템 제로 3개의 원소를 가진 트라이프리스 집합으로 학습 데이터를 구성합니다. 그래서 유저 원에 대해서는 아까 설명했듯이 i2와 i3는 소비를 했다. 즉 관측된 아이템이고 아이템 1과 아이템 4는 아직 소비하지 않은 관측되지 않은 아이템이기 때문에 총 4개의 데이터 가 생성되고 이 4개의 데이터는 이 트라이플리스 집합 학습 데이터 안에 들어가게 됩니다. 이제 학습 데이터를 모두 생성했으면 파라미터 학습을 수행해야 하는데요. 이제 여기서 본 논문의 제목에 등장한 이 베이지안 개념이 사용됩니다. 데이터에서 모델 파라미터를 학습하는 방법은 크게 두 가지가 있는데요. 맥시멈 라이클리후드 이스티메이션과 본 논문에서 사용하는 맥시멈 a 포스트리어 최대 사후 확률 추정이라는 방법입니다. 줄여서 map라고 부를 텐데요. map는 말 그대로 사후 확률이 최대가 되는 파라미터를 학습하는 것입니다. 여기서 사후 확률은 주어진 유저 선호 정보에 대해서 파라미터 확률을 의미하는데요. 근데 이 파라미터 확률 즉 사후 확률을 직접 구할 수 없기 때문에 여기 이 확률을 구하기 위해 선회하는 방법이 바로 베이지 정리입니다. 이 사후 확률은 베이지 정리에 의해서 사후 사전 확률 곱하기 라이클리 후드에 비례합니다. 사전 확률이란 우리가 학습할 파라미터에 대한 사전 정보 이고요. 라이클로 후드는 우리가 파라미터를 고정했을 때 그때 유저 선호 정보가 나타날 확률입니다. 이 두 가지의 확률은 모두 적절한 가정과 계산을 통해서 우리가 얻을 수 있고 따라서 우리가 최종적으로 최대화해야 되는 사후 확률을 수식으로 표현할 수 있습니다. 아무튼 다시 말해서 포스테리어 즉 사후 확률을 최대화한다는 것은 주어진 유저 정보를 최대한 잘 나타내는 파라미터를 추정하는 것이고 우리는 이 포스테리어를 최대화하는 최적화 문제를 풀어야 합니다. 그래서 포스테리어를 계산하기 위해서는 먼저 라이클리우드를 계산해야 하는데요. 여기서 우리의 학습 모델의 MF의 예측 수식이 사용됩니다. 우리가 가지고 있는 유저의 퍼스널라이즈 랭킹 정보 그 모든 트라이플스의 정보를 활용해서 라이클리우드를 표현해야 하는데요. 주어진 유저에 대해서 유저가 아이템 아이를 제보다 선호할 확률을 우리가 메트리스 팩토라이제이션의 수식을 활용해서 표현합니다. 이 확률은 0과 1 사이이기 때문에 시그모이드 펑션 이 시그모이드 펑션은 어떤 실수 값을 0과 1 사이로 투영합니다. 그래서 이 시그모이드 펑션을 사용하는데요. 이 가운데에 있는 이 x h 값이 바로 매트리스 팩토라이제이션의 평점 예측을 통해서 유저 유와 아이템 아이에 대한 평점 예측 값 그리고 유저 유와 제에 대한 평점 예측 값의 차이가 됩니다. 그래서 이 차이 값을 시그모이드 펑션에 넣게 되면 최종적으로 유저 유가 아이템 아보다 제를 선호할 어떤 0과 1 사이의 확률로 표현되는 것이고 이 확률을 모두 곱한 값이 전체 라이클리후드 즉 전체 데이터에 대한 라이클리 후드가 됩니다. 다음은 프라이어인데요. 프라이어는 우리가 학습해야 할 파라미터에 대한 사전 확률입니다. 보통 이 사전 확률은 어떤 분포를 따른다라는 가정을 통해 나타내는데요. 보통 정규 분포를 많이 사용합니다. 그래서 본 모델은 파라미터에 대한 사전 확률을 정규 분포를 따른다고 가정하였습니다. 우리가 사용하는 학습 파라미터는 유저 벡터 아이템 벡터 인데요. 벡터의 형태로 존재하기 때문에 정규 분포도 행렬의 형태로 정의해야 합니다. 그래서 평균이 모두 0이고 공분산 행렬이 람다 곱하기 단위 행렬 아로 정의합니다. 여기서 이 람다 세타는 모델링할 때 정해야 하는 하이퍼 파라미터인데요. 추후에 수식을 통해서 이 람다 세타 공분산 행렬의 크기가 레귤러라이제이션 즉 정규화 역할을 수행한다는 것을 이후 슬라이드를 통해 알 수 있을 것입니다. 그래서 계산을 쭉 해주면은 이 뒤에 있는 텀은 우리가 아까 사전 확률로 정의했던 그 프라이어의 공분산 행렬에 들어가는 람다 세타 값입니다. 이것이 다음과 같이 정규화 엘투 레귤러레이제이션처럼 작동하게 되고요. 이 앞에 있는 부분은 라이클리 후드 텀이 되겠습니다. 그래서 이 목적 함수는 미분 가능하기 때문에 학습 파라미터로 그래디언트 를 구할 수가 있고요. 따라서 그레디언트 디센트 옵티메이제이션을 수행할 수 있습니다. 그래서 오른쪽 수식과 같이 파라미터가 있고 파라미터에 대한 그레디언트와 러닝 웨이트를 곱해서 SGD를 수행할 수 있습니다. 목적 함수가 일반적인 로그 함수 시그모이드 l2 놈으로 이루어져 있기 때문에 아래와 같이 미분하게 되면은 이런 수식을 구할 수 있습니다. 그러나 본 문제에서는 일반적인 그래디언트 디센트가 적절한 방법은 아니라고 이야기하고 있습니다. 그래서 이 논문에서는 런 BPR이라는 학습 방법을 사용합니다. 이 학습 방법은 그냥 SGD가 아니라 boot stra 기반의 SGD인데요. 아까 우리가 유저의 선호 정보를 사용하여서 학습 데이터 셋을 만든다고 했죠. 그것은 DS로 표현된다고 했습니다. 이 DS는 유저에 대해서 유저가 관측된 아이템과 관측되지 않은 아이템 제로 이루어져 있는 데이터 집합이라고 했는데요. 보통 일반적인 SGD를 사용하게 되면은 유저가 소비했던 아이템 즉 관측된 아이템 보다 관측되지 않은 제 아이템이 훨씬 크기 때문에 가능한 모든 학습 데이터 셋을 사용하면 학습의 비대칭이 발생합니다. 제이는 훨씬 많은 데 비해서 아이템 아이는 적기 때문에 계속해서 동일한 유저와 아이템에 대한 유저와 그 아이템 아이 관측된 아이템에 대한 업데이트가 계속 일어나게 되고 수렴이 잘 되지 않습니다. 그래서 이것을 트라이 플리스 단위로 샘플링을 해서 이 문제를 해결합니다. 유저가 선호하는 아이템 하나, 유저가 선호하지 않는 아이템 제 하나를 이렇게 하나씩 동일한 웨이트로 샘플링해서 이 학습 데이터에 동일한 유저 유와 선호하는 아이템 아이가 계속 등장하지 않도록 데이터 샘플링을 하는 것이죠. 이렇게 하게 되면 모델 학습이 잘 되어서 우수한 성능을 보이게 됩니다. 그래서 이 런 BPR을 최종적으로 MF 모델링에 적용하면 학습 파라미터인 p q를 업데이트해야 하는데요. 그 다음 수식과 같이 아까 우리가 언급했던 그 시그모이드 안에 들어가는 x 값 씨 r유아 마이너스 알유 제라고 했고요. 그래서 이렇게 구할 수가 있고요. 그래서 이 피와 q에 따라서 그래디언트는 다음과 같은 수식으로 구할 수 있습니다. 사실 이 수식은 굉장히 간단해서 충분히 유도할 수 있는 부분이고요. 이 수식이 중요하다기보다는 결국에 우리가 런 BPR을 통해서 업데이트하려고 하는 것은 매트리스 팩토라이제이션의 유저 아이템 벡터, 유저 아이템 파라미터임을 기억하시길 바랍니다. 그래서 마지막으로 BPR 논문을 요약하면 다음과 같습니다. 인플리시 피드백 데이터만을 활용해서 유저를 기준으로 아이템 간의 선호도를 도출했고 이 선호도 데이터를 가지고 엠프 모델을 학습하였습니다. 그리고 본 논문에 있는 베이지 안이라는 이름처럼 맥시멈 에포스트리아 사후 확률을 최대로 추정하는 방법을 통해서 파라미터를 최적화하였고요. 그리고 런 BPR이라는 방식을 통해서 그냥 SGD가 아니라 부트스트랩 기반의 SGD를 활용하여서 데이터의 비대칭을 해결하였습니다. 그래서 결과적으로 이 매트리스 팩토라이제이션의 BPR 옵티미제이션을 적용했을 때 추천 성능이 우수했다고 본 논문은 밝히고 있습니다. 이상 4강 강의를 모두 마쳤습니다. 긴 내용 들어주시느라 모두 수고 많으셨습니다."
}