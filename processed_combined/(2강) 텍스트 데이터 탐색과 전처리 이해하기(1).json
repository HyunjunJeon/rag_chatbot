{
  "lecture_name": "(2강) 텍스트 데이터 탐색과 전처리 이해하기(1)",
  "source_file": "(2강) 텍스트 데이터 탐색과 전처리 이해하기(1)_18.mp4_2025-12-04-103105553.json",
  "text": "안녕하세요. 도메인 공통 프로젝트 2강 텍스트 데이터 탐색과 전처리 이야기 시작하겠습니다. 이번 강의에서는 텍스트 데이터의 본질과 전처리의 필요성, 텍스트 정제 텍스트의 구조화 및 수치화에 대해서 다뤄보겠습니다. 먼저 텍스트 데이터의 본질과 전처의 필요성입니다. 정형 데이터와 비정형 데이터에 대해서 이야기해 보겠습니다. 정형 데이터는 사전에 정의된 명확한 스키마가 있는 데이터를 이야기를 하고요. 관계형 데이터베이스에 저장되거나 SQL 혹은 bi2를 통한 직접적인 분석이 가능합니다. 그리고 SQL과 같은 어떤 정해진 방법으로 우리가 검색을 하기 때문에 정확하고 매우 빠르게 검색을 할 수 있고요. 그리고 데이터의 양 부분은 이제 전체 인터넷에 있는 데이터의 양을 기반으로 봤을 때 전체 데이터의 약 20% 정도를 우리가 정형 데이터를 사용하고 있다라고 조사되어 있습니다. 대표적인 예시로는 판매 기록, 재무 데이터, 이알피 혹은 씨알엠 데이터 등을 이야기를 합니다. 다음은 비정형 데이터입니다. 비정형 데이터는 스키마가 없거나 유연합니다. 이제 여기에서 스키마가 없거나 유연하다라고 하는 것은 우리가 정형 데이터에서 가지고 있는 어떤 스키마 어떤 테이블 형태가 아니라 다양한 형태의 데이터라는 것을 이야기를 합니다. 그래서 저장 방식은 관계형 데이터베이스가 아니라 노에스큐엘 데이터 레이크 혹은 어떤 파일로 저장이 되는 객체 스토리지의 데이터가 저장이 됩니다. 분석 및 처리 부분에서는 이제 엔엘피를 통해서 데이터를 가공해서 사용을 하거나 혹은 컴퓨터 비전 이런 것과 같이 복잡한 전처리 및 AI 모델을 필요로 합니다. 검색 용이성 부분에서는 여러분들이 사용하는 데이터에 따라 어떤 내용인지 이거를 검색하는 게 매우 복잡할 수 있습니다. 예를 들어서 우리가 사진첩이 있고 그 사진첩에서 텍스트로 검색한다고 한번 해보겠습니다. 실제로 제 핸드폰에서 비행기에 타고 있는 사진이라고 검색을 했을 때 이 텍스트와 사진을 매핑하는 것이 어렵습니다. 그렇기 때문에 이런 기반 검색에 복잡한 기술을 요구하기 때문에 정형 데이터 대비 검색 용이성이 어렵다라고 말할 수 있습니다. 데이터의 양은 전체 데이터의 약 80% 이상을 차지하면서 점점 더 빠르게 증가하고 있습니다. 대표적인 예시로는 소셜미디어 게시물, 동영상, 오디오 문서 파일 등이 있습니다. 비정형 데이터에서도 비전 데이터냐 혹은 텍스트 데이터냐는 본질적인 차이가 있습니다. 특히 텍스트는 구조가 없고 의미 단위가 문맥과 단어의 순서에 따라 변하기 때문에 기계가 바로 이해하기 매우 까다로운 데이터입니다. 밑에 표를 보겠습니다. 비전 데이터는 기본 단위는 픽셀이고요. 텍스트 데이터는 단어 혹은 형태소입니다. 구조의 경우는 비전 데이터는 여러분들이 보는 어떤 화면 이제 고정된 픽셀 그리드 형태로 되어 있고요. 텍스트 데이터 같은 경우는 별도의 구조는 없고 길이가 길어질 수도 작아질 수도 있습니다. 또한 비전 데이터는 공간적 정보가 중요합니다. 픽셀이 어디에 어떤 값으로 배치되느냐에 따라 정보가 달라진다라고 할 수가 있고요. 텍스트 데이터는 순차적인 정보가 중요합니다. 단어의 순서가 중요하다라는 뜻입니다. 의미 해석 부분에서는 비전 데이터는 시각적 패턴, 객관적으로 보여지는 시각적 패턴이 있고요. 그리고 텍스트 데이터 같은 경우는 문맥에 따라 의미가 변화되는 중의적이거나 주관적인 의미 해석이 가능합니다. 텍스트는 앞선 페이지에서 가변적인 길이를 가질 수 있다라고 말씀을 드렸는데요. 한 줄짜리 영화 리뷰부터 수천 자에 달하는 평론까지 텍스트는 길이가 매우 다양하기 때문에 일관된 입력 형태로 만들기가 어렵습니다. 혹은 분석 목적에 맞지 않는 불필요한 노이즈가 존재합니다. 화면에 보여지는 예시를 보시면 이제 HTML 태그도 들어가 있고요. 어떤 느낌표와 같은 글자도 들어가 있고 이제 별과 같은 특수 문자 이런 것들이 들어가 있습니다. 그래서 HTML 태그가 들어가 있다라고 하는 거는 여러분들이 이 데이터를 웹에서 수집했다 라는 거를 이야기할 수 있고요. 특수문자 같은 경우는 물론 유저가 감정 표현이나 어떤 기호로서 사용할 수 있지만 여러분들이 이 텍스트 데이터를 분석하는 데는 방해가 될 수도 있습니다. 그리고 또한 키 키 키과 같이 웃음소리를 표현하는 인터넷 용어 이런 것들 같은 경우는 어떤 키윽이라는 하나의 문자로 정교화할 수 있습니다. 예시에 보여지는 URL 주소 같은 경우는 외부 링크 정보로 이 텍스트 자체의 의미 분석에는 불필요할 수가 있습니다. 또한 단어의 의미는 고정되어 있지 않고 문맥의 구조에 따라 달라지기 때문에 해석이 복잡할 수 있습니다. 첫 번째로는 문맥 의존성인데요. 같은 단어라도 문맥에 따라 의미가 달라집니다. 첫 번째 예시로 맛있는 사과를 먹었다. 진심 어린 사과를 건넸다 에서는 앞에는 어떤 먹는 사과를 이야기를 하지만 뒤에서는 죄송하다와 같은 사과를 의미하게 됩니다. 다음과 같은 동음이요 예시들도 있을 수 있습니다. 다음으로는 표현의 모호성입니다. 의미는 같지만 표현이 다른 단어들을 컴퓨터는 모두 다른 것으로 인식합니다. 예시로 사과, 애플, 애플 z는 모두 같은 과일을 의미하지만 컴퓨터에게는 완전히 다른 3개의 문자열일 뿐입니다. 구조적 의존성에 대해서도 한번 이야기해 보겠습니다. 단순한 단어의 나열이 아닌 단어 간의 관계와 문법적 구조가 전체의 의미를 결정합니다. 아이스 어 맨 위 텔레스코프라고 하는 문장에서 왼쪽에 있는 예시는 이제 나는 이 망원경을 통해서 어떤 남자를 봤다 라고 이야기할 수 있고요. 오른쪽에 있는 문장은 나는 망원경을 가지고 있는 어떤 남자를 봤다 이런 식으로 같은 문장이지만 다른 의미를 가진 문장이 될 수 있습니다. AI ML 쪽에서 많이 사용하는 용어인데요. 갈비지 갈비지 아웃이라고 합니다. 노이즈나 표현의 모호성 등 여러 문제점을 가진 원본 텍스트를 그대로 사용하면 모델의 성능을 보장할 수가 없습니다. 따라서 여러분들이 텍스트 데이터를 정제하고 가공하는 전처리는 모델의 성능을 좌우하는 선택이 아닌 필수 과정임을 인지하시면 좋겠습니다. 다음은 전처리 파이프라인입니다. 텍스트 전처리 파이프라인의 일반적인 과정은 다음과 같습니다. 원본 텍스트를 정제를 통해서 필요 없는 글자들을 제거를 합니다. 예시에서는 HTML 태그와 이 느낌표와 같은 글자들이 제거됐고요. 그다음에는 이 단어를 토큰화를 통해서 영화와 강추라고 하는 이제 두 개의 덩어리로 쪼갭니다. 그리고 영아와 강추라고 하는 거를 우리가 표준화를 통해서 우리의 이제 워드백에 들어있는 어떤 문자로 어떤 토큰으로 이제 변화를 하게 되는데요. 이제 표준화를 한 이 토큰들을 우리가 벡터화를 통해서 어떤 숫자 형태로 만들어 냅니다. 그럼 이제부터는 숫자로 표현된 이 벡터를 모델의 입력으로 처리를 할 수 있게 됩니다. 이 파이프라인은 절대적인 규칙은 아닙니다. 여러분들이 풀고자 하는 문제의 종류, 그리고 사용하고자 하는 데이터의 특성, 그리고 사용하는 모델에 따라서 몇몇 단계가 생략되거나 혹은 순서가 바뀌거나 혹은 새로운 단계가 추가될 수 있습니다. 또한 모델의 예측 오류를 분석하여 전처리 규칙을 지속적으로 개선하는 과정이 필요합니다. 우리 이전에 배웠었던 에아의 경진대회 사이클과 약간 비슷하다라고 생각하시면 될 것 같아요. 전처리의 기대 효과로는 인사이트 발굴 및 데이터 이해도 증진입니다. 자주 쓰이는 단어나 노이즈 패턴 등 데이터의 특성을 파악하여 더 나은 모델링 전략을 수립할 수 있도록 돕습니다. 다음은 학습 효율 증대입니다. 데이터 표현을 일관되게 만들어 학습 속도를 높이고 필요한 데이터의 양을 줄일 수 있습니다. 다음은 일반화 능력 향상입니다. 다양한 표현을 하나로 통일해 모델이 처음 보는 데이터에도 안정적으로 반응하는 능력을 키워줍니다. 마지막으로는 성능 향상입니다. 모델이 이 데이터를 학습하는 데 필요 없는 노이즈를 제거해 모델의 예측 정확도를 직접적으로 향상시킵니다. 다음은 텍스트 정제입니다. 텍스트 정제 첫 단계로 표현을 일관되게 통일하여 모델의 혼란을 줄이는 기본 정제에 대해서 이야기해 보겠습니다. 첫 번째로는 소문자 변환입니다. 대문자 a로 시작하는 애플과 소문자 a로 시작하는 애플은 의미가 같지만 컴퓨터는 다른 단어로 인식합니다. 이를 하나의 표현으로 통일하여 단어 집합의 크기를 줄이고 모델이 단어를 더 효율적으로 학습하게 합니다. 만약에 이를 통일하지 않는다면 우리가 사용하는 단어 집합의 개수가 늘어나고 모델이 인지해야 되는 단어의 개수가 여러 개 같은 의미지만 여러 개가 되기 때문에 모델이 효율적으로 학습을 할 수가 없게 된다라고 이해하시면 되겠습니다. 다음은 구두점 제거입니다. 분석에 불필요한 마침표, 쉼표, 물음표 등의 구두점을 제거합니다. 이는 단어 자체의 의미에 집중하고자 할 때 유용합니다. 다음은 불필요한 공백 제거입니다. 문장 중간에 포함된 여러 개의 공백이나 문장 앞뒤의 공백은 의미가 없으므로 하나의 공백으로 통일하거나 제거해 주는 것이 좋습니다. 코드 예시를 보겠습니다. 예시 텍스트에는 보시면 아시겠지만 대문자도 포함되어 있고요. 그리고 느낌표 혹은 어떤 이제 이모티콘 그리고 이전에 봤었던 HTML 태그와 URL 되게 다양한 정보들이 포함이 되어 있습니다. 그래서 여러분이 원본 텍스트를 출력하면 아래와 같이 출력이 될 수가 있습니다. 방금 봤던 예시 텍스트를 기반으로 우리가 정제를 하는 예시 코드를 보겠습니다. 먼저 소문자 변환을 할 겁니다. 여러분들이 가지고 있는 저 텍스트 저 베이직 텍스트 닷 로어라고 하는 이제 메소드를 호출을 하게 되면 이 텍스트 내에 존재하는 모든 대문자를 소문자로 변환을 해줍니다. 밑에서 1-1 소문자 변환을 보시면 이전에 있었던 대문자들이 모두 소문자로 변환된 것을 보실 수가 있습니다. 다음은 스트링 펑츄레이션에 포함된 모든 구두점을 빈 문자열로 대체하는 방법입니다. 베이직 텍스트에 있는 문자 하나하나를 순회를 하면서 스트링 펑쉐션에 포함되지 않았다면 텍스트 노 펑트라고 하는 빈 문자열에 이 문자 하나하나를 붙여 나갑니다. 이런 식으로 기본적인 구두점들을 제거할 수 있고요. 밑에 예시에서 1 2 구도점 제거 부분을 보시면 이제 느낌표나 따옴표 같은 것들이 사라진 것을 보실 수가 있습니다. 다음은 양 끝 공백 제거를 해 보겠습니다. 이제 베이직 텍스트 스트립 스플릿이라고 하는 명령어를 사용을 하시면 양 끝에 있는 공백을 제거한 결과를 보실 수가 있습니다. 다음은 정규 표현식을 활용한 패턴 정제입니다. 우리 강의에서 정규 표현식을 자세히 다루지는 않겠지만 간단하게 설명드리면 특정 패턴을 가진 문자를 찾아내고 변경하는 강력한 도구입니다. 이를 통해서 복잡한 노이즈를 효과적으로 제거할 수 있습니다. 일반적으로 다음과 같은 패턴들을 정제 대상으로 삼습니다. HTML 태그 URL 및 이메일 주소 해시태그 및 사용자 맨션 숫자 반복되는 문자 등을 대상으로 삼습니다. 정규 표현식 예시를 보겠습니다. 주로 re sup이라고 하는 메소드를 활용할 건데요. re는 이제 LG x라고 하는 파이썬의 기본 정규식 패턴 모듈이고요. 서은 섭스트랙트라고 해서 이제 제거할 때 사용을 하게 됩니다. 그래서 원본 텍스트를 이제 예시와 같이 출력을 해보면 이제 똑같이 어떤 대문자도 포함되어 있고요. 뭐 이제 구두점도 포함되어 있고 이모티콘 등등 다 포함되어 있는 걸 보실 수가 있고요. 자 이제 두 번째에서 보면 레그엑스 텍스트 로얼 스트립을 했을 때 이제 기본 정제를 통해서 이제 소문자로 바꿔주는 거를 확인을 할 수가 있습니다. 다음으로는 치티엠엘 태그 제거인데요. 여러분들이 이제 예시에서 보실 수 있듯이 아이 다 서브 해서 앞에 들어가는 좀 이상해 보이는 문자열 저 부분이 정규식입니다. 그래서 이제 HTML 태그 같은 경우는 이제 꺾쇠로 표현되는 패턴을 가지고 있기 때문에 저 꺽쇠 패턴을 가진 녀석들을 빈 문자열로 대체해라 이제 두 번째 인자로 들어가고요. 세 번째 인자로는 이제 그 대상 텍스트를 전달을 합니다. 그래서 2 다시 치티엠엘 태그 제거를 보시면 실제로 비알 태그가 제거되는 것을 확인을 하실 수가 있습니다. 다음으로는 URL 제거입니다. HDPS 혹은 HTPS가 아닌 것을 포함한 그리고 WWW로 시작하는 이런 패턴들을 제거하는 예시입니다. 똑같이 빈 문자열로 대체를 하도록 되어 있고요. 2 3 결과를 보시면 이제 HTPS 슬래시 슬래시 이그샘플 닷컴이라고 적혀 있던 부분이 사라진 것을 확인하실 수가 있습니다. 마지막으로 이메일 주소 제거입니다. 이메일 주소 제거에서는 우리가 이제 골뱅이라고 부르는 저 앱을 포함한 이메일 패턴을 모두 제거하도록 합니다. 맨 마지막 2-4 결과를 보시면 이제 테스트 닷 이메일 닷컴이라고 하는 텍스트가 사라진 것을 보실 수가 있습니다. 네 다음으로는 해시태그 및 사용자 맨션 제거하는 정규식 표현입니다. s 그리고 앱으로 시작하는 이 텍스트들을 제거하도록 하는 부분이고요. 2 5의 결과를 보시면 이제 해시태그 존재하던 것과 으로 사용자 맨션 되어 있던 것들이 제거된 걸 보실 수가 있습니다. 다음으로는 반복 문자 제거입니다. 자음 모음 반복되는 부분 혹은 한글 글자가 반복되는 부분을 이제 하나의 글자로 통합시키는 정규식 패턴입니다. 2-6을 보시면 아시겠지만 키윽키윽키윽이 반복되어 있던 게 키 하나로 바뀌었고요. 진짜 아아가 진짜 좋다로 바뀐 것을 보실 수가 있습니다. 다음으로는 숫자 제거입니다. 물론 분석 목적에 따라서 숫자가 필요할 수도 숫자가 필요하지 않을 수도 있지만 이제 다음과 같이 숫자를 제거하는 식으로 처리할 수 있습니다. 다음으로는 특수문자 이모지 제거 부분이고요. 특수 문자랑 이모지 같은 경우는 알파벳 한글 공백만 남기는 식으로 처리를 합니다. 그래서 2 8 결과를 보시면 이모지가 사라진 것을 보실 수가 있고요. 마지막으로는 여러 개의 공백을 하나의 공백으로 변경을 해서 이제 합쳐주는 연산입니다. 앞서 우리가 여러 가지 방법을 통해서 이 텍스트를 정제하는 것을 봤는데요. 여기서 주의하실 점이 있습니다. 혹은 앞서 진행하는 단계를 보면서 어 이건 이렇게 하면 안 될 것 같은데라는 생각이 드셨을 수도 있습니다. 그게 뭐냐면 모든 노이즈를 항상 제거하는 것이 정답이 아닐 수 있다라는 겁니다. 여러분들이 하고자 하는 분석의 목적에 따라서 정제 전략을 다르게 수립해야 됩니다. 첫 번째로 여러분들이 하고자 하는 테스크가 감성 분석이었다면 문장부호, 느낌표 혹은 물음표 또는 이모티콘과 같은 것들은 감정의 강도와 뉘앙스를 담고 있는 핵심 데이터일 수 있습니다. 정말 좋아요 느낌표 그리고 정말 좋아요요. 느낌표가 없는 이 예시는 느낌표의 유머가 감성의 정도를 나타낼 수 있습니다. 이런 것들을 단순 노이즈로 간주하여 무조건 제거를 하게 되면 모델의 예측 정확도가 오히려 떨어질 수 있습니다. 특별한 토큰으로 치환하거나 그대로 유지하는 전략을 고려할 수 있습니다. 다음으로는 정보 추출입니다. 뉴스 기사에서 기업의 주가나 실적을 분석하는 경우 숫자는 가장 핵심적인 정보입니다. 제품 리뷰에서 모델 번호나 가격을 추출할 때도 마찬가지입니다. 이런 상황에서 숫자를 모두 제거하면 중요한 정보를 잃게 됩니다. 따라서 정제를 시작하기 전 내가 이 분석을 통해 무엇을 얻고 싶은가를 먼저 고민하고 그에 맞는 정제 규칙을 설계하는 것이 매우 중요합니다. 다음은 텍스트의 구조화 및 수치화에 대해서 다뤄보겠습니다. 첫 번째로는 토큰화입니다. 사람이 글을 읽을 때 자연스럽게 단어 문장 단위로 이해하는 것처럼 컴퓨터가 처리할 수 있는 의미 있는 최소 단위 토큰으로 쪼개는 과정을 말합니다. 예시에서 러닝 이스 펀이라고 하는 이 문장을 토크나이제이션을 통해서 토큰화를 한다면 러닝 이즈 펀 그리고 구두점이 들어가게 됩니다. 토큰의 단위는 여러 가지가 될 수 있는데요. 첫 번째로는 단어입니다. 단순히 띄어쓰기를 기준으로 텍스트를 분리할 수 있습니다. 예를 들어 입력으로 에아는 인공지능의 줄임말입니다라고 했을 때 토크는 에아는 인공지능의 줄임말입니다. 이 3개가 될 수 있습니다. 혹은 형태소로 쪼개는 경우가 있습니다. 의미를 갖는 가장 작은 단위로 분해하여 조사를 분리하고 어간을 추출하게 됩니다. 똑같은 입력을 토큰화했을 때 AI는 인공지능 의 줄임말입니다가 됩니다. 다음으로는 서브 워드입니다. 단어를 더 작은 단위로 분리해 신조어 및 ro 버케블러리의 문제를 해결할 수 있는 방법입니다. 현재 AI 언어 모델의 표준이라고 생각하시면 됩니다. 똑같은 입력을 출력을 했을 때 결과는 AI는 인공 그리고 oo 지능 그리고 의 주임 땡땡, 말 땡땡입니다. 이런 식으로 분리가 됩니다. 코드 예시를 보겠습니다. 영어 토큰화에서는 내추럴 랭귀지 툴킷이라고 하는 엔엘티케 라이브러리를 사용해 보겠습니다. 예시 문장이 2개가 있고요. 이 두 개의 문장을 토크나이즈 처리를 했을 때 결과는 다음과 같습니다. 첫 번째로는 기본 공백 분리를 하기 위해서 단순히 텍스트 닷 스플릿으로 공백을 기준으로 이제 분리를 한 걸 보실 수가 있고요. 이제 그 밑에는 엔엘티케 워드 토크나이즈 처리를 해 가지고 분리한 걸 보실 수가 있습니다. 그래서 각각 문장의 1번과 2번을 비교해 보시면 엔엘티케 토크나이즈를 한 게 훨씬 더 세부적으로 토크나이즈가 된 것을 확인하실 수가 있습니다. 물론 거의 비슷할 수도 있습니다. 영어와 달리 한국어는 띄어쓰기만으로 토큰화를 하면 심각한 문제가 발생합니다. 그 이유는 바로 조사 어미 등이 단어에 직접 붙는 교착어의 특성 때문입니다. 예를 들어서 나는 밥을 먹었다 이 예시를 공백 기준으로 분리를 하게 되면 나는 밥을 먹었다로 토큰화가 됩니다. 이런 경우에 AI는 나는 과 나를 이게 또 다르게 인식을 하게 되고요. 밥은과 밥을 이걸 모두 다른 단어로 인식하게 됩니다. 이렇게 되면은 학습해야 되는 이 단어들이 매우 많아지기 때문에 학습 효율이 떨어지게 됩니다. 그래서 이 부분을 형태소를 기반으로 토큰화를 하게 되면 나는 밥을 먹었다와 같이 분리가 되고요. 나 밥 먹다라는 단어의 실질적인 의미를 분리를 하게 돼서 모델이 훨씬 효율적으로 학습할 수 있게 됩니다. 여기에서 n 을 다 점 뭐 이런 것들은 이제 우리가 정제를 통해서 제거를 하게 될 겁니다. 그래서 한국어 처리에서는 코앤엘파이, 맥 캡과 같은 형태소 분석기 사용이 매우 중요합니다. 코넬 파이 예시를 좀 보겠습니다. 예시 문장으로 성공은 준비가 기회를 만났을 때 일어난다라고 하는 예시를 보겠습니다. 기본 공백으로 분리를 하게 되면 성공은 준비가 기회를 만났을 때 일어난다 라고 할 수가 있고요. 이제 컨테이터 분석기 2개를 사용을 해서 확인을 해봤을 때 이 서로 다른 토큰이 발생하는 것을 볼 수가 있습니다. 하지만 기본 공백 분리보다는 조금 더 자세하게 일어나는 것을 보실 수가 있습니다. 다음은 표준화에 대해서 이야기 한번 해보겠습니다. 왜 단어의 표현을 통일해야 될까요? AI 모델이 텍스트를 효율적으로 학습하고 더 정확하게 분석할 수 있도록 만들기 위해서입니다. 이전에 언급한 대문자, 애플, 소문자, 애플, 애플처럼 형태는 다르지만 의미가 비슷한 단어들을 하나의 기준으로 통일하는 과정입니다. 표준화에 적용되는 주요 방법은 두 가지가 있습니다. 불용어 제거와 어휘 표준화입니다. 불용어 제거는 분석에 불필요한 단어들을 제거하는 방법이고요. 어휘 표준화는 단어의 형태를 원형으로 통일합니다. 어간을 추출하거나 표제어를 추출하는 방법이 있습니다. 불용어 제거하는 것을 예시로 보겠습니다. 불용어란 문장에서 자주 등장하지만 핵심 의미를 분석하는 데에는 큰 기여를 하지 않는 단어들입니다. 영어 같은 경우는 어 더 이스 인 온이 있고요. 한국어에서는 은는이가 을를 주로 조사, 접속사 어미 등이 있습니다. 그래서 똑같이 코넬 파이에 있는 오케티 토크나이저를 사용을 해서 이제 확인을 한번 해 보겠습니다. 부용어를 제거를 하게 되면 다음과 같이 는 에라고 하는 토큰이 제거된 것을 볼 수가 있습니다. 어휘 표준화에서 어간 추출과 표제어 추출에 대해서 한번 보겠습니다. 핵심 원리를 비교를 해보면 어간 추출은 규칙 기반으로 어미를 잘라냅니다. 표제어 추출은 사전과 문법을 이용해 원형을 찾게 됩니다. 정확도 측면에서는 어간 추출은 규칙 기반으로 어미를 잘라내기 때문에 사전에 실제로 존재하지 않는 단어가 생성될 수 있습니다. 하지만 표제어 추출에서는 문법적으로 정확한 원형을 반환을 하게 됩니다. 방금 설명드렸던 이 내용에 의거하면 당연하게도 속도는 어간 추출이 훨씬 빠릅니다. 표자원 추출은 사전과 문법에서 검색을 하고 원형을 찾아야 하기 때문에 상대적으로 느린 속도를 보여줄 수가 있고요. 예시는 다음과 같이 나올 수가 있습니다. 다음은 이제 어간 추출 표절 추출 예시를 좀 보겠습니다. 이제 준비한 예시로 스터디 스터딩 스터디 고즈, 고잉 온이 있고요. 여기에서 어간 추출과 표제어 추출에 따라서 결과가 달라지는 것을 보실 수가 있습니다. 다음은 벡터화입니다. 이제 여러분들이 전 처리를 잘한 이런 토큰들을 모델이 학습할 수 있도록 숫자 벡터로 변환하는 최종 단계입니다. 벡터화를 진행하기 전에 먼저 텍스트에 있는 모든 고유한 토큰들을 모아서 단어 사전을 만듭니다. 그걸 우리가 보케뷸러리라고 이야기를 하고요. 이 단어 사전을 통해 기본적인 벡터화가 가능해집니다. 대표적인 방법으로는 워너 인코딩과 백 오브 월스가 있습니다. 그래서 예시를 보면 각각의 문장 2개를 토큰화를 한 뒤에 이것을 기반으로 보케블러리를 만드는 예시를 볼 수가 있습니다. 기초적인 벡터화 방법인 원더 인코딩에 대해서 살펴보겠습니다. 하나의 단어를 표현하는 가장 기본적인 방식입니다. 단어 사전에 있는 단어 중 표현하고 싶은 단어의 위치만 1로 나머지는 0으로 표시하게 됩니다. 두 번째로는 백 오브 워스입니다. 문장이나 문서 전체를 표현하는 방식입니다. 문장에 포함된 단어들이 단어 사전에서 어디에 해당하는지 1로 표시하거나 단어의 등장 횟수를 직접 숫자로 기록합니다. 단어의 순서를 무시하는 것이 특징입니다. 다음은 고급 벡터화 방법입니다. 단어의 의미를 벡터에 담는 것입니다. 대표적인 방법으로 티프아디프라고 하는 방법이 있는데요. 단순 빈도 그러니까 백 오브 워드의 한계를 보완하는 기법입니다. 문서 내 단어의 중요도를 계산하여 가중치를 부여합니다. 문서에서는 자주 등장하지만 다른 문서에서는 잘 나오지 않는 단어에 높은 점수를 줍니다. 다음은 워 투 백 글로브라고 하는 예측 기반 임베딩입니다. 비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가질 것이라는 아이디어를 기반으로 합니다. 단어의 의미를 밀집벡터로 표현하여 단어 간 의미적 관계를 개선할 수 있게 됩니다. 예를 들어서 서울에서 한국을 빼고 일본을 더한다면 그 벡터는 도쿄라고 하는 이 단어의 인베딩과 유사하게 됩니다. 다음은 벌츠 엘모라고 하는 문맥 기반 임베딩입니다. 최신 언어 모델들이 사용하는 방식으로 문맥에 따라 단어의 의미가 달라지는 것을 반영합니다. 예를 들어 맛있는 사과와 진심 어린 사과에서 사과의 벡터를 문맥에 맞게 다르게 표현합니다. 이번 강의 요약을 해 보겠습니다. 텍스트 데이터의 본질과 전처리 필요성에 대해서 이야기를 했었습니다. 텍스트는 비정형 데이터로서 의미의 모호성, 구조적 의존성 등으로 처리에 어려움이 있습니다. 자연어 모델링의 성능 향상을 위해 전처리는 필수적인 단계이다라고 이야기를 했었고요. 텍스트 데이터 전처리의 흐름으로 정제, 토큰화, 표준화 벡터가 기본 구조가 된다고 말씀드렸습니다. 단 목적에 따라 유동적 구성 및 지속적 개선이 필요합니다. 다음은 텍스트 정제입니다. 기본 정제 소문자 변환, 구두점 및 불필요한 공백 제거 등 혹은 패턴 정제, 정규식 표현을 활용한 HTML 태그, 반복 문자 등 제거 활용이 가능했고요. 분석 목적을 고려한 전략적 정제가 중요했습니다. 대표적으로는 이모티콘이나 느낌표, 물음표와 같은 것들이 테스크에서는 중요한 정보일 수 있다라고 말씀을 드렸었고요. 다음으로는 텍스트의 구조화 및 수치화입니다. 구조화에서는 토큰화를 이야기했었습니다. 컴퓨터가 이해 가능한 최소 단위인 토큰으로 텍스트를 분리하는 작업을 했었고요. 표준화에서는 불용어 제거, 어휘 통일 등으로 모델의 학습 효율 및 정확도를 향상시키는 방법에 대해서 이야기했습니다. 다음은 수치화 벡터화에서 이야기를 했습니다. 분리된 토큰을 모델이 학습할 수 있는 숫자 벡터로 변환하는 부분에 대해서 이야기했었습니다. 이번 수업은 여기까지입니다. 다음 수업에서 뵙겠습니다. 감사합니다."
}