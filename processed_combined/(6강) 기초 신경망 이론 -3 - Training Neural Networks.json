{
  "lecture_name": "(6강) 기초 신경망 이론 -3 - Training Neural Networks",
  "source_file": "(6강) 기초 신경망 이론 -3 - Training Neural Networks_40.mp4_2025-12-04-103437472.json",
  "text": "네 안녕하세요 여러분 저번 시간에는 저희가 뉴럴넷을 트레이닝 하기 위한 가장 중요한 기술인 그 로스에 대한 그레디언트를 계산하는 백프로파게이션에 대해서 배웠습니다. 그래서 어 저번 시간에 이어서 오늘에는요 그 뉴럴넷을 학습시키기 위한 되게 부수적인 하지만 중요한 많은 테크닉들을 배워보도록 하겠습니다. 그래서 오늘 배울 내용은 주로 세 가지인데요 액티베이션 펑션이랑 웨잇 이니셜라이제이션이랑 러닝 레이 스케줄링에 대해서 배워보도록 하겠습니다. 자 그럼 먼저 시작하겠습니다. 어 액티베이션 펑션 저희가 어 그 액티베이션 펑션이라고 하면요 그 뉴럴넷을 넌 리니어한 데이터 패턴을 학습할 수 있게 하는 키 컴포넌트라고 생각하시면 됩니다. 그래서 기본적으로 액티베이션 펑션은요 저희가 데이터를 처리할 때 뉴럴넷을 쓰잖아요 근데 그 예전에 우리 첫 번째 강의랑 두 번째 강의 세 번째 강의까지는 리니어 모델을 주로 썼어요. 즉 데이터랑 그 아웃풋의 인풋과 아웃풋의 관계가 선형적이라고 가정을 하고 그 선형식 y 이퀄 블x 더하기 비라는 이 선형식에 맞춰 가지고 모델링을 했습니다. 근데 세상에 많은 데이터들이 그렇게 선형적인 관계를 가지고 있지 않잖아요 그럴 때 이렇게 넌 리니어한 어떤 데이터의 패턴을 학습시키기 위해서 이 넌 리니어한 함수를 배워야 되는데 이 런 리니얼한 함수에 어떻게 넌 리니어리티를 추가하느냐를 고민했을 때 이 액티베이션 펑션을 이용해 가지고 어 요 넌리니어리티를 추가하게 돼요. 그래서 여기 써있는 걸 보시면 알 수 있겠지만 우리가 데이터를 처리하는 뉴럴 네트워크를 만들 때 이 뉴럴 네트워크이 넌 리니어한 함수를 어떻게 활성화할 것인지를 결정해 하는 과정에서 나온 개념이라고 생각하시면 됩니다. 그래서 어 여러분 전 시간에 배웠지만 사실 뉴럴 네트워크은요 실제 그 신경망에서의 그 뉴런이랑 굉장히 비슷한 동작을 하는데 이렇게 인풋 시그널들이 쭉 들어왔을 때 그 인풋 시그널들에다가 웨일 파라미터를 곱해 가지고 다 더한 다음에 그다음에 액티베이션 펑션을 넣어요. 그래가지고 액티베이션 펑션이 하는 일은요 이 아웃풋이 어떤 트레숄드가 넘으면 액티브 되고 그러니까 신호가 전달이 되고 어떤 그 시그널이 그 트레시 홀드보다 밑이면은 어 이 아웃풋으로 전달되지 않게 하는 게이트 같은 역할을 한다고 보시면 됩니다. 자 그래서 그 액티비션 펑션이 굉장히 여러 가지가 있고 우리가 그 문제에 따라서 다양한 액티베이션 펑션을 우리가 갖다가 쓸 수가 있어요. 그래서 이게 시그모이드 펑션인데 시그모이드 펑션은 기본적으로 이렇게 인풋이 들어오면 아웃풋은 0 아니면 1로 이렇게 레귤러 라이즈 됩니다. 그래서 인풋이 이렇게 들어왔을 때 아웃풋은 1 아니면 0이 되고 이렇게 익스포넨셜한 펑션으로 표현될 수 있습니다. 그리고 이 탄젠트 치 하이퍼블릭 탄젠트는요 이 시그모이드 펑션을 이렇게 시프트 했다고 생각하시면 돼요. 이거는 0과 1 사이에서 이렇게 바운드 되잖아요. 근데 하이퍼블릭 탄젠트는 마이너스 1과 1 사이에서 이렇게 바운드가 되고요. 렐로 펑션이라는 게 있는데 얘 렐로 펑션은 이런 식으로 생겼습니다. 그래서 이 뒷장에서는요 이런 다양한 종류들의 액티베이션 펑션에 대해서 좀 더 자세히 알아보도록 할 거예요. 자 먼저 시그모이드 펑션 우리가 이 시그모이드 펑션을 쓰는 이유는요. 기본적으로 우리가 예를 들어서 그 이미지 클래시피케이션 같은 테스크를 봤을 때 우리가 원하는 거는 이 아웃풋이 어떤 확률 분포로 나왔으면 좋겠어요. 그래서 이 아웃풋을 0과 1 사이로 이렇게 강제를 한 거예요. 그래서 출력 값의 범위는 0과 1이고 만약에 인풋이 어 양도 음도 아닌 어떤 애매한 경우에는 0.5가 나오게 이런 식으로 디자인한 액티베이션 펑션이 시그모이드 펑션이에요. 그리고 이렇게 익스포넨셜에 대한 펑션으로 표현이 되게 됩니다. 그런데 이 시그모이드 펑션을 사실 잘 쓰지는 않아요. 왜 그럴까요? 여기 여러 가지 이유들이 있는데 먼저 베니싱 그레디언트 프로블럼이라고 하는데 이게 되게 중요한 거예요. 이 액티베이션 펑션을 보면은 여기 기울기를 보면은 이 가운데에서는 기울기가 있지만 인풋이 굉장히 작거나 아니면 굉장히 컸을 때는 그레디언트가 거의 0이죠. 왜냐하면 이 펑션이 여기서 이 부분이랑 이 부분에서 완전 플랫 하기 때문이에요. 그래서 인풋의 크기가 크거나 작을 때는 그레디언트가 즉 기울기가 0이에요. 그 말은 우리가 백프로파게이션 할 때 어 그 각각의 변수에 대해 그레디언트를 구하는데 이 그레디언트들이 다 0으로 된다는 거예요. 근데 우리가 전 시간에 백프로파게이션 계산하면서 봤겠지만 뒷부분에서 그레디언트가 0이면 즉 업스트림 그레디언트가 0이면 어 앞쪽에서 가는 그레디언트도 다운스트림 그레디언트도 0이죠. 왜냐면은 어 로컬 그래디언트랑 업스트림을 곱해야지 다운스트림이 되기 때문에 그 뒤에 단계 어디든 하나에서 그레디언트가 0이 돼버리면은 그 인풋 단으로 그레디언트가 흘러가지가 않아요. 그래 가지고 트레이닝이 안 됩니다. 네 이해되시죠? 그레디언트가 0인 게 굉장히 큰 문제예요. 그래서 이거를 베니싱 그레디언트 프로블러밍이라고 그래요. 그래서 한국말로 말하면은 이 그레디언트가 점점 없어진다는 거죠. 그러니까 뒷부분에서 그레디언트가 0이 돼 버리면은 그거가 앞으로 흘러들어갈 때 다 앞으로 다 0으로 만든다는 거예요. 그래서 트레이닝이 안 됩니다. 그리고 두 번째 문제는 이 출력 값들이 제로 센터도 하지 않는다는 건데 이거는 이제 뒤에서 말씀드릴게요. 왜 이 제로 센터도 하지 않은 출력 값들이 문제가 되는지 그리고 익스포넨셜의 연산이 하기가 어렵다는 이런 컴퓨테이션이 비싸다는 어떤 단점이 있습니다. 그래서 이제 또 이 단점들 하나하나 또 자세히 볼 거예요. 먼저 베니싱 그레디언트 제가 아까 말씀드렸죠. 여기랑 여기에서 그레디언트가 0이다 그게 굉장히 큰 문제다라고 생각을 할 수 있을 거예요. 그래서 그걸 좀 더 자세하게 수학적으로 볼게요. 자 시그모이드 펑션은 이런 식으로 표현이 돼요. 1 나누기 1 더하기 익스포넨셜의 마이너스 엑스 이런 식으로 표현이 됩니다. 근데 이 시그모이드 펑션을 갖다가 우리가 어 스에 대해서 미분을 하면은 부분 적분 식을 이용해 가지고 이런 식으로 표현이 될 거예요. 이제 이 식을 갖다가 한번 해석을 해볼게요. 만약에 x가 굉장히 큰 값이라고 생각을 해봐요. 그러면은 x가 예를 들어서 10이라고 생각을 해봅시다. 그러면은 x가 10일 때에 시그모이드의 값은 뭐예요? 1이죠 그렇죠 그러면은 그 1을 갖다가 이 그레디언트 식에다가 넣으면 이 두 번째 항이 0이 될 거예요. 그렇죠 그래서 그레디언트는 0에 수렴합니다. 그렇죠 그리고 스가 마이너스 10일 때 x가 0일 때도 어 여전히 이 시그모이드의 마이너스 10은 0이기 때문에 그래디언트는 0에 수령을 해요. 시그모이드를 x에 대해서 미분한 거를 함수로 이렇게 표현을 해보면은 이런 식으로 돼요. 즉 스가 작을 때만 어떤 값이 있고 x가 5보다 크거나 아니면 마이너스 5보다 작으면 다 0이 된다는 거예요. 그래서 제가 말씀드렸듯이 우리가 어떤 시그모이드 펑션을 가지고 오퍼레이션을 하면은 이 업스트림 그레디언트는 로스를 시그모이드 펑션에 대해서 미분을 한 거죠. 그리고 로컬 그래디언트는 이 시그모이드 펑션을 인풋 x에 대해서 미분을 한 거란 말입니다. 그래서 이 시그모이드를 인풋 x에 대해서 미분한 펑션이 이거예요. 그래서 로컬 그래디언트가 거의 그냥 모든 대부분의 x에 대해서 0으로 돼버리기 때문에 업스트림이랑 이 로컬 그래디언트를 곱하면 또 여전히 다운 스트링 그레디언트도 0이 돼 버린다는 단점을 가지게 돼요. 그래서 기본적으로 이 시그모이드의 단점은요 로컬 그래디언트가 거의 대부분 0이라서 요 로스가 앞으로 프로파게이션을 안 합니다. 그래서 이거를 베니싱 그레디언트라고 그러고 제가 여기에서 키드 그레디언트라고 썼습니다. 그래서 트레이닝이 잘 안 돼요. 그래서 시그머리드를 잘 쓰지 않는 데는 이런 이유가 있습니다. 자 그리고 아까 앞에서 넌 제로 센터드 아웃풋이라고 그랬는데 이게 약간 어 이해하기가 좀 어려우니까 한번 잘 따라와 보세요. 요 어 넌 제로 센터드 아웃풋이 뭘 무슨 말이냐면요. 우리 시그모이드 펑션 여기서 보시면은 이런 식으로 생겼어요. 근데 이 중심이 0.5지 0은 아니잖아요 이게 문제가 됩니다. 자 입력 x가 모두 양수라고 가정을 한다면 값의 범위가 항상 0과 1 사이가 되겠죠 그러니까 예를 들어서 뭐 예를 들어 이런 경우가 자주 일어나는데 인풋이 모두 양수예요. 뭐 컴퓨터 비전 같은 경우에 인풋은 이미지의 픽셀 값이 되니까 대부분 0과 1 사이 그러니까 음수의 값이 없을 거예요. 그러면은 인풋이 양수니까 당연히 아웃풋도 0.5랑 1 사이로 바운드가 된단 말입니다. 그래서 값의 범위가 항상 양수가 되는 문제가 있어요. 그러면은 이게 뭐가 문제냐 이게 또 이제 백프로파게이션의 그 시기인데요. 여기에서 우리가 이 로컬 그래디언트를 보면은요. 이 시그모이드를 어떤 인풋 파라미터 w에 대해서 미분을 하는 걸 보면은요. 이 미분이 이렇게 쭉 풀어보면 이런 식으로 나타나져요. 즉 이거를 갖다가 시그모이드 펑션을 더블에다가 미분을 하면은 아까 앞에 있었던 그 부분 적분 식을 이용해 가지고 이런 식으로 쭉 써지게 되고 또 여기에 블스를 갖다가 더블에 대해서 미분을 하니까 스가 이렇게 앞에 나오게 돼요. 그렇죠 부분 적분의 이론을 이용해서 그런데 여기에서의 문제는 여기에 스가 곱해진다는 거예요. 여기 앞에 요 시그모이드의 어떤 값 그리고 1 마이너스 시그모이드 이렇게 x 이렇게 3개의 곱으로 나타내지는데 이거를 보시면은요. 시그모이드는 기본적으로 다 아웃풋이 양수죠. 그렇죠 왜냐하면 넌 제로 센터드 아웃풋이니까 0.5센트도 되고 0과 1 사이에 바운드 되니까 이 시그모이드 값은 항상 양수예요. 그래서 이 첫 번째 텀은 항상 0과 1 사이의 양수야. 그리고 두 번째 텀을 보시면 1 마이너스 시그모이드 썸팅이에요. 그러니까 요 요 항이 기본적으로 0과 1 사이니까 1 마이너스 0과 1 사이도 여전히 그쵸 양수죠. 그리고 우리는 입력 x가 양수라고 가정을 했으니까 이것도 양수예요. 그래서 기본적으로 요 시그모이드를 더블에 대해서 미분한 값이 다 양수라는 거예요. 그래서 이렇게 그레디언트가 항상 양수로 가정이 돼요. 근데 이게 왜 문제냐 왜 왜 문제냐 보면은요. 여기 써 있죠. 모든 파라미터 더블에 대한 업스트림 그레디언트의 부호가 변하지 않아요. 왜냐 이렇게 업스트림이 이렇게 들어오잖아요. 그리고 업스트림에 다운스트림을 구하기 위해서는 업스트림이랑 로컬 그래디언트를 곱해야 되겠죠. 우리가 여기서 봤다시피 이 로컬 그래디언트는 무조건 양수예요. 그러면은 이 업스트림에 있는 부호가 똑같이 다운스트림으로 간단 말입니다. 그래서 뒤에서 어떤 그레디언트가 계산되든 그거랑 같은 부호의 로스의 프로파게이션이 앞으로 아 가게 됩니다. 그러니까는 그 뒤에 있는 그 그레디언트의 부호가 바뀌지 않아요. 그래서 여기서 쓰여져 있는 것처럼 이렇게 되기 때문에 모든 그레디언트가 다 양수거나 아니면 다 음수예요. 그래서 그게 뭐가 문제냐 그래디언트가 항상 특정 방향으로만 업데이트가 돼요. 그래서 예를 들어서 이게 예를 들어 이렇게 로스의 어떤 서페이스라고 생각해 봐요. 근데 이게 x축이고 이게 y축이에요. 근데 우리 같은 경우에는 항상 그래디언트가 같은 부호로만 업데이트가 되니까 이 스 방향 업데이트도 양수 와 방향 업데이트도 양수인 경우 즉 이쪽 방향이죠. 그리고 스 방향도 음수 와 방향도 음수 이쪽 방향이죠. 이 방향 아니면 이 방향으로 밖에 움직이지 않는다는 그런 문제가 있어요. 근데 만약에 이 옵티멀 그레디언트가 사실은 스는 이 방향 스는 양수이고 y는 음수인 이 방향이라고 할 때 이 방향으로 절대 갈 수가 없다는 거예요. XY는 양수 아니면 x 아이유는 둘 다 음수 요 이쪽 방향 아니면 이쪽 방향으로만 움직이게 된다는 거죠. 그래서 할 수 없이 이 옵티멀 그래디언트 방향을 배우기 위해서 이렇게 지그재그로 밖에 움직일 수가 없어요. 그래서 굉장히 비효율적입니다. 로스를 업데이트하기 위해서 실은 이런 거는 이제 여러 개의 이그 샘플을 보면서 트레이닝을 할 때 문제가 완화되기도 해요. 하지만 펀디멘탈이 이 로컬 그래디언트가 항상 양수이기 때문에 이 업스트림의 사인을 업스트림의 그 부호를 똑같이 다운스트림으로 가져가는 문제가 생깁니다. 네 그리고 마지막으로 아까 앞에서 익스포넨셜 계산이 어렵다고 그랬죠. 그래서 결국 이 시그모이드가 이런 문제가 있어서 실제로 현업에서는 시그모이드 펑션은 그 뉴럴넷 디자인할 때 각 레이어의 레이어 사이에다가 끼는 거는 잘 쓰지를 않아요. 네 그래서 요 탄젠트 치는 이 시그모이드의 단점을 약간 보완한 형태의 액티베이션 펑션인데요. 요 시그모이드 펑션이 이렇게 생겼잖아요. 탄젠트 치는 얘를 갖다가 밑으로 이렇게 시프트한 형태로 생긴 거예요. 그래서 넌 제로 센터드 되지 않는 그런 장점을 가지고 있습니다. 그 시그모이드가 제로 센터드가 아니라서 문제가 생겼잖아요. 그래서 시그모이드의 특성은 그대로 가되 제로 센터드 된 액티베이션 펑션을 갖다 쓰는 겁니다. 그래서 이 플롯에서 이 펑션의 그 모양에서 알 수 있듯이 이 하이퍼블릭 탄젠트의 출력 값의 범위는 마이너스 1과 1 사이고 우리가 이렇게 제로 센터도 되게 이렇게 디자인을 한 펑션을 갖다 썼죠. 그런데 여기에 단점은 저희 아까 베니싱 그레디언트 문제 말했잖아요. 인풋과 아웃풋이 클 때 그레디언트가 0이라서 로스가 앞으로 전달이 안 된다. 이것도 여기에서 똑같이 발생을 합니다. 펑션에서 볼 수 있듯이 인풋과 아웃풋이 인풋이 크고 인풋이 아니면 되게 작을 때 이 그레디언트가 0인 거를 볼 수 있죠. 그래서 베니싱 그레디언트 문제는 여전히 발생하는 단점을 가지고 있습니다. 자 그래 가지고 탄젠트 h보다 조금 더 나은 게 요 렐로입니다. 그래서 렐로는 기본적으로 여기에서 많은 장점을 가지고 있어요. 베니싱 그레디언트 문제가 없습니다. 베니싱 그레디언트가 이 액티베이션 펑션의 그 미분 값이 0일 때 생기는 그런 문제죠. 그래 가지고 여기에서는 미분했을 때 항상 0이 되지 않도록 이렇게 의도적으로 이 x가 0보다 큰 경우에 항상 기울기 값을 갖도록 이렇게 만들어 놨어요. 그래서 여기서 보시면 특징이 어 스가 양수일 때 절대 세츄레이 되지 않습니다. 스가 양수일 때 항상 기울기가 있기 때문에 베니싱 그레디언트 프로블럼이 없죠. 그리고 연산이 효율적입니다. 왜 연산이 효율적이냐 여기에서는 미분할 필요가 없어요. 미분을 하면 항상 1이니까 어차피 양수에는 그리고 음수일 때는 미분을 하면 0이죠. 그래서 연산이 효율적이죠. 그래서 연산이 효율적이기 때문에 시그모이드나 하이퍼블릭 탄젠트보다 빨리 수렴하는 그런 장점이 있습니다. 근데 이 랠루도 역시 단점을 가지고 있어요. 이것도 여전히 제로 센터도 되지 않는 단점을 가지고 있죠. 시그모이드랑 똑같이 그리고 데드 렐루 프러블럼이라고 하는데 이게 뭐냐면은요. 출력값이 음수라면 모든 것이 세츄레이션 되는 문제가 발생해요. 여기서 출력 값이 음수면은 아웃풋은 완전 0이죠. 그래서 음수의 출력 값을 갖다가 완전 다 없애버리는 거죠. 그리고 제일 큰 문제가 x가 0일 때 미분 불가의 점이 있어요. 우리가 백프로파게이션을 하려면 모든 영역에서 미분 가능해야 돼요. 그런데 여기 그 만약에 스가 0이 나오면은 이 뾰족한 점에서는 미분이 불가하잖아요. 그래서 그레디언트를 구할 수가 없습니다. 그 그래서 이런 미분 불가한 그런 특성이 있기 때문에 또 단점을 가지고 있습니다. 그래서 그 렐로에 개선된 형태가 요 리키 렐로입니다. 흘러내리는 렐로라고 그러죠. 그래서 그 인풋이 음수일 때 모든 값이 0이 돼버리는 문제를 해결하기 위해서 인풋이 음수여도 약간의 기울기를 이렇게 주는 겁니다. 그래서 노 데드 렐로 문제 이 데드 렐로 문제를 해결한 거죠. 근데 단점은 우리가 여기에서 기울기를 의도적으로 줬기 때문에 이 추가적인 기울기 파라미터가 드는 약간 마이너한 단점을 가지고 있습니다. 그리고 이거는 이제 새로운 이제 어 렐로의 어드벤스드 형태인 엘로입니다. 익스포넨셜 리니얼 유닛인데 그 음수인 그러니까 랠로랑 똑같은데 렐로의 음수인 부분에 이런 식으로 장난질을 쳐서 이런 식으로 식을 해 가지고 기울기가 저렇게 변하도록 이렇게 디자인을 한 겁니다. 그래서 익스포넨션을 이용해 가지고 음수인 영역에서 이렇게 펑션을 만들어서 액티베이션으로 쓰도록 만들었어요. 단점이라고 하는 거는요 익스포넨셜의 연산이 비싸요. 예 익스포넨셜을 미분할 때는 복잡하기 때문에 그 연산이 비싼 단점이 있습니다. 그래서 어 여러 가지 이제 액티베이션을 우리가 배웠죠. 그래 가지고 렐루 시그모이드 탄젠트h 렐루의 변형형인 엘루 뭐 이런 것들을 배웠는데 실제로 현업에서 시그모이드나 탄젠트 h는 잘 사용하지 않아요. 그 각각의 뉴럴넷 레이어 레이어 사이에서 이 시그모니드나 탄젠트 치는 잘 사용을 하지 않고 올리 레스에어 가장 마지막 단에서 뭐 클래시피케이션이나 뭐 그런 리그레션의 결과를 딱 내오기 위해서 마지막 단에 사용은 하지만 중간중간에는 쓰지 않아요. 이게 되게 중요해요. 왜 그러죠? vaning gradent pro블럼 때문에 그러죠 미분 값이 0이기 때문에 그래서 그 각각의 레이어 중간중간에는 주로 랠루를 사용하는 게 뭐 어떤 프랙티컬 어떤 그 노하우고요 시그모이드나 탄젠트 치는 거의 안 사용하지만 마지막 레이어에 종종 사용하기도 합니다. 이제 웨이트 이니셜라이제이션을 배워보도록 할게요. 이건 뭐냐면요 우리가 뉴럴레 트레이닝 할 때 여러 가지 스텝들에 대해서 우리가 전에 배웠는데 가장 첫 번째 스텝이 wit 이니셜라이제이션이었어요. 우리가 w라는 파라미터를 어떤 식으로든 초기화를 시켜야 되겠죠 뭐 무조건 다 0으로 체계화 할 수도 없고 랜덤으로 할 수도 없어요. 그러면은 어떻게 초기화하는 것이 학습할 때 수렴이 가장 잘 될까 학습이 제일 잘 될까 이거를 한번 생각을 해볼게요. 여기에 수학적인 게 많이 나와요. 그래서 좀 어려울 수 있지만 저랑 같이 천천히 잘 보도록 할게요. 자 가장 쉬운 방법이 랜덤 노이즈일 거예요. 그래서 웨잇 파라미터를 랜덤한 방법으로 다 채우는 거예요. 그래서 이렇게 웨잇 파라미터 EC 0.2 0.01 곱하기 엔피 랜덤 랜드 디엔 이렇게 했는데 기본적으로 이 두 번째 항이 랜덤 넘버를 이렇게 쭉 뽑아내는 거예요. 어떤 어 어 사이즈를 가진 랜덤 넘버를 뽑아내고 거기다가 어 작은 파라미터인 0.01을 곱해 가지고 표준 정규 분포를 가장 작은 상수 0.01로 곱한 거예요. 그래서 기본적으로 그 랜덤 이니셜라이제이션을 하되 어 그 맥시멈 값이 0.01이 되도록 만든 겁니다. 자 이렇게 되면 어떻게 될까요? 이렇게 했더니 그 이렇게 이니셜라이제이션을 하고서 각각의 레이어들을 이제 인풋을 통해서 쭉 통과하게 만들었어요. 이렇게 랜덤하게 이니셜라이제이션 된 레이어들을 몇 개를 몇 개를 몇 개를 쌓아 가지고 인풋을 넣었을 때 어떻게 아웃풋이 바뀌는지 봤더니 자 첫 번째 레이어를 통과했을 때는 그래도 리저너블하게 아웃풋 값이 이렇게 나왔어요. 아웃풋의 범위가 마이너스 1부터 1까지 예쁘게 이렇게 나왔어요. 근데 두 번째 통과할 때부터 이 아웃풋이 점점 없어지는 거를 볼 수가 있어요. 예 이게 큰 문제죠. 왜냐면은 wat이 굉장히 작기 때문에 아웃풋 값이 거의 0으로 수렴하는 이런 나쁜 특성을 가지고 있어요. 그래 가지고 이걸 이렇게 되면 학습이 되지 않아요. 그래서 포 디폴 네트워크 되게 깊게 깊게 싸면은 이렇게 어 랜덤하게 작은 값으로 이니셜라이즈 하면은 트레이닝이 잘 되지 않습니다. 왜 그럴까 어 왜냐면은 기본적으로 이 여기에서 탄젠트 액티베이션 펑션을 썼잖아요. 근데 TGH액티베이션 펑션이 이렇게 생겼죠 우리 아까 앞에서 봤죠 그러면은 이 witt을 x에다가 곱하면은 기본적으로 아웃풋이 굉장히 작아진다는 거예요. 즉 이렇게 어 이 마이너스 1과 1 사이로 이렇게 바운드가 돼 가지고 아웃풋이 나옵니다. 그리고 더 작아지면 더 이렇게 스쿼시가 되죠. 그래서 그 아웃풋이 작아짐에 따라 점점 점점점점 아웃풋 값이 없어지는 이런 나쁜 특성을 가지고 있습니다. 그래서 결과적으로 x가 0일 때 모든 기울기가 0에 가까워서 학습이 되지 않을 것입니다라고 여기 쓰여 있었죠. 그래서 이렇게 표준 정규 분포를 작은 상수로 곱했더니 잘 트레이닝이 되지 않는 것을 볼 수가 있어요. 네 큰 문제죠 자 그러면은 아예 엄청 큰 값을 갖다가 곱해주면 어떨까요? 크기를 키우면 어떻게 될까요? 그러면은 극단적으로 이렇게 아예 그 아웃풋이 마이너스 1 아니면 1로 보내지는 특성을 가지게 됩니다. 만약에 첫 번째 레이어를 통과했을 때 wiht 값이 너무 커져버리면 아웃풋이 익스플로드가 돼요. 그러니까 아웃풋이 너무 커져요. 그러면 아웃풋이 또 너무 커지면 이 tgen h에서 봤을 때 아웃풋이 예를 들어서 굉장히 뭐 10에 가까운 거예요. 그러면은 여기에서 또 그레디언트가 0이죠. 그래가지고 학습이 안 되는 거예요. 여기에서 그레디언트가 아예 프로파게이션이 안 되니까 그래 가지고 아웃풋을 쭉 보면은 마이너스 1로 모인다든지 1로 모인다든지 이렇게 아웃풋이 익스플로드 되는 그런 문제가 있습니다. 예를 들어서 이렇게 크기를 키우면은 어 이 모든 기울기가 0에 가까워져 가지고 또 학습이 안 돼요. 그래서 하비어라는 아저씨가 옛날 과학자죠. 이 하비어라는 아저씨가 하비어 이니셜라이제이션이라는 어 이론을 만들었어요. 그래서 우리가 이렇게 웨잇 파라미터를 랜덤하게 줄 때 이 웨이 파라미터 앞에다가 1 나누기 루트 인풋 사이즈만큼의 그 값을 갖다가 넣으면은 이렇게 예쁘게 아웃풋이 어 없어지지도 않고 그래디언트가 베니시 되지도 않고 익스플로드 되지 않고 잘 나올 것이다라는 이론입니다. 그래서 기본적으로 랜덤한 어 웨이 이니셜 라이즈를 한 다음에 거기에다가 상수 값으로 인풋 사이즈의 루트 씌운 거에 역수를 취해서 곱해줍니다. 그러면은 이렇게 어 그 웨잇 값들이 어 뭐 이렇게 랜덤하게 잘 이니셜 라이즈가 되고 레이어를 여러 개 통과를 해도 값이 없어지거나 익스플로드 되지 않고 예쁘게 잘 나오는 특성을 가지고 있습니다. 그래서 이거는 이제 참고로 알아두시면 되고요. 실제로 우리가 현업에서는 그 파이토치에 이니셜라이제이션 펑션이 있습니다. 그래서 그 펑션을 불러다 쓰면은 거기에서 이런 현상이 나오지 않도록 이런 이론적인 테크닉들이 다 인플리멘트가 되어 있습니다. 자 그리고 마지막으로 어 러닝 레이 스케줄링에 대해서 배워볼게요. 이게 사실 앞에서 배운 1번 2번 같은 경우에는요 여러분들 실제로 현업에서 뭐 파이토치나 텐서플로우 갖다 쓰잖아요 그때 그 라이브러리에서 대부분 해결이 되는 방법들입니다. 그런데 이 러닝 레이 스케줄링 같은 거는요 우리가 그 경험에 따라서 트라이얼 앤 에러로 우리가 알아서 해야 돼요. 그래서 어떻게 보면 이 3번이 여러분들이 실제로 현업 가서 일할 때는 제일 중요한 어떤 상황이 되지 않을까라는 생각이 듭니다. 자 러닝 넷을 어떻게 우리가 설정을 해야 될까요? 먼저 러닝 레잇을 한번 리뷰를 해볼게요. 저희가 그 로스를 갖다가 미니마이즈 할 때 앞 강의에서 배웠듯이 그레디언트 디센트를 쓰죠. 그래서 그레디언트 디센트는 예를 들어서 우리가 뭐 산에 올라갔어요. 그래서 산의 어떤 윗부분에 있는데 산에 가장 낮은 부분을 갈 때 내가 있는 어떤 로케이션에서 발을 여러 방향으로 내딛은 다음에 가장 가파르게 내려가는 방향을 선택해서 한 걸음 내딛을 때 그 걸음의 보폭이 러닝 레잇이에요. 그러니까는 그레디언트 디센트는 한 지점에서 주위에 있는 곳들의 그레디언트를 구해서 가장 많이 내려가는 그레디언트를 픽하죠. 그 방향을 픽하죠. 그리고 얼마나 갈지를 정하는 게 이 러닝 레잇입니다. 그래서 이 러닝 레잇을 적절하게 하는 게 되게 중요해요. 예를 들어서 러닝 넷이 굉장히 굉장히 크다고 생각을 해봐요. 그러면 우리가 산에서 내려갈 때 보폭이 엄청 큰 거죠. 그러니까 걷지 않고 뛴다고 생각을 해봐요. 그러면은 너무 넓게 뛰면 멀리 뛰기를 너무 넓게 하면은 우리가 가장 가파르게 내려가는 방향으로 뛴다고 해도 엄청 많이 뛰기 때문에 더 높은 지점으로 갈 수도 있어요. 여기 그게 이 노란색의 경우에요. 만약에 우리가 너무 러닝 레잇을 크게 하면 베리 하이 러닝 레잇 하면은 이렇게 LS가 오히려 이렇게 늘어날 수가 있어요. 우리가 원하는 식으로 그 밑에 골짜기를 가지를 못하고 오히려 반대로 위로 올라갈 수도 있다는 거죠. 그리고 그러면은 반대로 너무너무 작은 러닝 레이을 한다고 생각해 봐요. 예를 들어서 우리가 산에서 밑으로 내려가는데 그레디언트 디센트를 써 가지고 밑으로 내려가는데 한 걸음 스텝이 예를 들어서 1센치야 그러면은 이게 세워라 내워라 절대 내려갈 수가 없죠. 그래서 베리 로우 러닝맨 파란색 부분입니다. 너무 어 러닝 넷이 느리면 이렇게 아무리 에포크를 돌려도 러닝 그 로스가 확 줄어들지를 않고 이렇게 너무 오래 걸리게 지글지글지글지글 줄어든다는 거예요. 자 여기 에포크라는 개념이 나왔어요. 여러분 에포크라는 거 여러분들 실제로 코딩할 때 굉장히 많이 쓸 텐데요. 에포크라는 게 우리가 가지고 있는 학습 데이터를 처음부터 끝까지 한번 쭉 보는 거를 on 에포크라고 그래 그래서 트레이닝 할 때 원 에포크를 했다 그러면 우리가 트레이닝 셋을 한 번을 봤다는 거고 투 에포크를 했다 그러면 두 번을 봤다는 거죠. 자 그러면은 이렇게 베리 하이 러닝 레이 매우 높은 학습률도 나쁘고 베리 로우 러닝 레잇도 나빠요. 매우 낮은 학습률도 나빠요. 그럼 어떻게 해야 되냐 어느 정도 러닝 레잇 하이하게 하거나 아니면 옵티멀한 빨간색을 해야 되겠죠 어느 정도 옵티멀 하게 큰 러닝 레잇을 하면은 러닝 레이 파란색에 비해서 되게 리즈너블하게 빨리 줄어드는 걸 볼 수 있어요. 이 파란색은 줄어드는 데 아주 세열한 에어라 걸리죠. 왜냐하면 러닝 넷이 너무 작기 때문에 근데 러닝 넷을 어 여기에서 높여줬더니 이 초록색처럼 러닝 레이 초반에 확 줄고 그다음에 이렇게 세츄레이션이 돼요. 근데 이게 옵티멀이 아닐 수도 있어요. 여기에서 우리가 조금 러닝 레잇을 높이면은 그러니까 옵티멀 한 러닝 레을 하면은 이렇게 빨간색처럼 훨씬 더 로스를 더 많이 줄일 수가 있어요. 그래서 이거를 어떻게 빨간색을 발견하는 게 어떻게 보면 아트예요. 거의 여러분들이 트라이얼 앤 에러를 해가면서 러닝 넷을 높였다가 낮았다가 하면서 가장 최적의 값을 찾아내야 되는데 저는 현업에서 어떻게 하냐면요 러닝 레잇을 되게 듬성듬성하게 여러 개를 갖다가 시도를 해서 뭐 예를 들어 5개의 실험을 돌려봐요. 그러면은 그 5개의 러닝 커브가 그려지잖아요 그중에서 이제 감을 잡는 거죠. 내가 러닝 네일을 어느 정도 해야지 좋겠다 그래서 대부분 큰 초기 학습률에서 시간이 지날수록 감소하죠 그러니까 예를 들어서 러닝 레잇을 초기에는 굉장히 크게 한 다음에 조금씩 조금씩 조금씩 늘어나고 나가는 식으로 우리가 트레이닝을 할 수가 있습니다. 네 그래서 이렇게 어 대부분 이렇게 큰 초기 학습률을 시작을 해가지고 하이 러닝 레잇에서 시작을 해서 제대로 막 줄어드는 로스가 줄어드는 거를 발견한 다음에 세츄레이션이 되면은 조금 더 러닝 라이스를 낮춰가지고 하는 식으로 이렇게 여러분들이 실제로 트레이닝을 해가면서 러닝 레이 조절해 가면서 하시면 됩니다. 자 그리고 여기서 이제 러닝 레잇 디케이드에 대해서 배울 건데요. 우리가 러닝 레잇을 트레이닝 하면서 이제 조금씩 줄어들 수가 있는데요. 우리가 트레이닝 시작할 때부터 끝까지 같은 러닝 레잇으로 갈 수도 있지만 러닝 레잇을 처음에 크게 하고 그다음에 조금씩 조금씩 줄여가는 방향도 있습니다. 그래서 어 이 오른쪽 그림이 사실 러닝 레잇 디케이 안 하고 러닝 레잇을 항상 그 트레이닝 시작부터 끝까지 같은 값으로 컨스턴트 값으로 줬을 때의 그 패턴입니다. 그래서 보시면 우리가 예를 들어서 산에서 어 그 가장 밑으로 내려올 때 러닝 넷이 항상 똑같으면 보폭이 똑같은 거죠. 그래서 이렇게 어 이런 식으로 오른쪽에 있는 것처럼 같은 보폭으로 이렇게 해가지고 밑으로 내려갈 수 있지만은 내려가는 그 골짜기가 항상 옵티멀은 아닐 수가 있어요. 보폭이 클 수도 있으니까 그래서 러닝 네잇 디케를 쓰면은 초반에는 큰 러닝 레이을 써서 보폭을 크게 해서 빨리빨리 내려올 수 있는 만큼 밑으로 내려와요. 그다음에 거의 밑으로 내려왔을 때 보폭을 점점점점 줄여가는 겁니다. 이렇게 큰 러닝 레에서 작은 러닝 레으로 작은 러닝 넷으로 조금씩 줄어가요. 그러면은 자연스럽게 가장 옵티멀 한 그 가장 밑에 그 골짜기로 내려올 수가 있겠죠. 그게 바로 러닝 레이 디케이의 개념입니다. 그래서 우리가 옵티멀한 어떤 최적의 해에 접근하기 위해서는 어 이 러닝 메이 디케를 쓰는 게 유리합니다. 그래서 사실 이거는 약간 저의 그런 어떤 팁인데요. 실제로 우리가 이제 뉴럴넷 트레이닝 한다고 생각을 해보세요. 저는 어떻게 하냐면요. 처음에 작은 러닝 내일에서 시작해서 점점점점점점 늘려가요. 그래서 굉장히 로스가 잘 줄어든다고 생각될 때 그 러닝 레이에 고정을 해가지고 그 러닝 레잇으로 줄일 수 있는 로스까지 쭉 줄여 나가요. 그러면 어느 순간 이 로스가 세츄레이션 되고 더 이상 줄어들지를 않거든요. 그러면은 그 상황에서 러닝 레잇을 반을 줄입니다. 그러면은 로스가 다시 줄어들기 시작해요. 여기 이 포인트에서 갑자기 로스가 딱 점프해서 줄어드는 걸 볼 수 있죠. 그래서 러닝 넷을 확 줄이면 로스가 다시 줄어들어요. 그리고 또 세츄레이션이 돼요. 그럼 또 세츄레이션 됐을 때 러닝 넷을 또 줄여갑니다. 반을 반을 줄여 들어가요. 그러면은 또다시 로스가 이렇게 내려가요. 이런 식으로 해가지고 최적의 로스를 갖다가 저는 이렇게 발견을 합니다. 그래서 이게 사실 저의 어떤 그런 경험적인 노하우인데요. 이게 꽤 잘 돼요. 그래서 여기다 써 놨는데 트레이닝을 시작할 때 너무 큰 러닝 넷을 너무 큰 학습률을 사용하면은요. 로스가 너무 급격하게 상승하는 이런 역효과가 일어날 수 있기 때문에 저는 초기에 0에 가까운 작은 학습률을 사용한 다음에 리니어하게 쭉 러닝 레잇을 줄여가지고 로스가 리저너블하게 줄어든다. 그러면은 그 로스를 가지고 이제 쭉 트레이닝을 해 가지고 로스가 세츄레이트 할 때까지 그 러닝 레잇을 유지하고 그다음에 로스가 세츄레이션 된 것 같다 그러면은 그때 러닝 레잇을 확 줄여가지고 이렇게 줄여가는 식으로 이렇게 저는 트레이닝을 합니다. 그리고 초반에 0에 가까운 러닝 레잇으로 했다가 리니어하게 쭉 올려가는 거를 이니셜 웜업이라고 그래요. 그래서 이거는 사실 저의 노하우가 아니고 많은 사람들이 쓰고 있는 그런 방법이라서 여러분도 한번 나중에 이제 뉴럴넷 트레이닝을 할 때 이런 테크닉을 한번 써보세요. 그러면은 이제 로스를 가장 최적으로 발견할 수 있을 거예요. 자 그래서 그 러닝넷 디케이 할 때 그냥 리니어하게 디케이 하지 않고 이런 코사인 함수를 따라서 디케이 할 수도 있고 인벌트 루트 방법으로 이렇게 러닝 넷을 이런 방법으로 dk 할 수도 있습니다. 이거는 어떤 인페리컬한 팁이에요. 리니어하게 dk 하는 거는 가장 간단할 수 있겠지만 코사인이나 인벌트 스퀘어 루트 같은 거를 쓰면은 더 리저너블하게 이제 dk가 된다. 그래서 이거는 코드 안에서 우리가 이 식을 불러와 가지고 인플리멘트 하면은 쉽게 러닝 레이 tk 펑션을 구현을 할 수 있을 거예요. 그래서 오늘은 트레이닝을 하기 위한 어떤 포인트들 가장 중요한 포인트는 아니지만 가장 중요한 건 백프로파게이션이에요. 실제로 현업에서 트레이닝을 할 때 뭐 러닝 레잇을 어떻게 하는지 웨이트 어떻게 이니셜라이제이션 하는지 뭐 액티베이션 펑션 어떤 거 갖다 써야 될지 이런 거를 각각의 특성들을 학습하면서 배워보는 시간을 가졌습니다. 다음 시간에는 또 이어서 어떻게 뉴럴라 트레이닝 하는지에 대해서 더 테크니컬한 부분을 배우도록 하겠습니다. 네. 긴 시간 집중해 주셔서 감사합니다."
}