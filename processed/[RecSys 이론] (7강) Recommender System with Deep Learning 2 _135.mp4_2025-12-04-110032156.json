{"result":"SUCCEEDED","message":"Succeeded","token":"e2dd9eb9a5db4728a2334032b5e4af6c","version":"ncp_v2_v2.4.6-c00dd1b-20250528__v4.2.20.1_ko_firedepartment_20250923_","params":{"service":"ncp","domain":"general","lang":"ko","completion":"sync","callback":"","diarization":{"enable":false,"speakerCountMin":-1,"speakerCountMax":-1},"sed":{"enable":false},"boostings":[],"forbiddens":"","wordAlignment":true,"fullText":true,"noiseFiltering":true,"priority":0,"userdata":{"_ncp_DomainCode":"tpc-boostcamp","_ncp_DomainId":13807,"_ncp_TaskId":42975812,"_ncp_TraceId":"e0b7790b6ab341c899cfcba314918a64"}},"progress":100,"keywords":{},"segments":[{"start":0,"end":7100,"text":"안녕하세요.","confidence":0.9996,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[5570,6060,"안녕하세요."]],"textEdited":"안녕하세요."},{"start":7100,"end":20200,"text":"이번 시간 일곱 번째 강의 지난 시간에 이어서 딥러닝 기반 추천 모델에 대해 살펴보겠습니다. 지난 시간에는 뉴럴넷의 가장 기본적인 구조인 MLP 멀티레이어 퍼셉트론 기반 추천 모델과","confidence":0.8751,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[7310,7520,"이번"],[7550,7780,"시간"],[7850,8080,"일곱"],[8090,8360,"번째"],[8410,8660,"강의"],[9090,9320,"지난"],[9370,9680,"시간에"],[9680,9940,"이어서"],[10150,10560,"딥러닝"],[10560,10760,"기반"],[10990,11280,"추천"],[11310,11640,"모델에"],[11640,11840,"대해"],[12030,12780,"살펴보겠습니다."],[13110,13340,"지난"],[13410,13860,"시간에는"],[14490,15000,"뉴럴넷의"],[15030,15280,"가장"],[15430,15900,"기본적인"],[16050,16380,"구조인"],[16970,17420,"MLP"],[17610,18180,"멀티레이어"],[18210,18680,"퍼셉트론"],[18680,18860,"기반"],[18970,19260,"추천"],[19260,19680,"모델과"]],"textEdited":"이번 시간 일곱 번째 강의 지난 시간에 이어서 딥러닝 기반 추천 모델에 대해 살펴보겠습니다. 지난 시간에는 뉴럴넷의 가장 기본적인 구조인 MLP 멀티레이어 퍼셉트론 기반 추천 모델과"},{"start":20200,"end":33500,"text":"오토 인코더를 사용한 추천 모델에 대해서 다루었습니다. 오늘은 그보다 조금 더 복잡한 두 가지 딥러닝 기법, 두 가지 딥러닝 모델 아키텍처를 사용한 추천 모델에 대해서 학습하겠습니다.","confidence":0.9531,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[20410,20720,"오토"],[20750,21200,"인코더를"],[21200,21460,"사용한"],[21510,21760,"추천"],[21760,22100,"모델에"],[22100,22460,"대해서"],[22670,23320,"다루었습니다."],[24210,24540,"오늘은"],[24590,24900,"그보다"],[24930,25160,"조금"],[25160,25300,"더"],[25410,25820,"복잡한"],[26270,26420,"두"],[26420,26700,"가지"],[26950,27360,"딥러닝"],[27410,27700,"기법,"],[28190,28307,"두"],[28307,28540,"가지"],[28590,28960,"딥러닝"],[28970,29200,"모델"],[29230,29860,"아키텍처를"],[29860,30140,"사용한"],[30710,30980,"추천"],[30990,31287,"모델에"],[31287,31600,"대해서"],[32090,33380,"학습하겠습니다."]],"textEdited":"오토 인코더를 사용한 추천 모델에 대해서 다루었습니다. 오늘은 그보다 조금 더 복잡한 두 가지 딥러닝 기법, 두 가지 딥러닝 모델 아키텍처를 사용한 추천 모델에 대해서 학습하겠습니다."},{"start":33500,"end":47400,"text":"바로 그래프 뉴얼 네트워크라고 불리는 지엔엔과 시퀀스 모델 등에 주로 사용되는 알앤엔 기반 추천 모델입니다. 먼저 그래프 뉴얼 네트워크라는 다소 생소할 수도 있는 모델에 대해서 학습하겠습니다.","confidence":0.7846,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[33730,33980,"바로"],[34150,34500,"그래프"],[34500,34700,"뉴얼"],[34700,35540,"네트워크라고"],[35540,35800,"불리는"],[35910,36520,"지엔엔과"],[37070,37500,"시퀀스"],[37500,37720,"모델"],[37720,37920,"등에"],[37920,38120,"주로"],[38120,38540,"사용되는"],[38910,39520,"알앤엔"],[39710,39940,"기반"],[40090,40340,"추천"],[40350,40860,"모델입니다."],[42010,42280,"먼저"],[42390,42687,"그래프"],[42687,42900,"뉴얼"],[42900,43540,"네트워크라는"],[43690,43980,"다소"],[44030,44440,"생소할"],[44590,44900,"수도"],[44900,45100,"있는"],[45330,45627,"모델에"],[45627,45940,"대해서"],[46270,47000,"학습하겠습니다."]],"textEdited":"바로 그래프 뉴얼 네트워크라고 불리는 지엔엔과 시퀀스 모델 등에 주로 사용되는 알앤엔 기반 추천 모델입니다. 먼저 그래프 뉴얼 네트워크라는 다소 생소할 수도 있는 모델에 대해서 학습하겠습니다."},{"start":47400,"end":58200,"text":"그래프가 무엇이고 어떤 식의 데이터를 표현할 때 이 그래프가 적합한지, 또 그래프 자료 구조를 어떻게 뉴럴넷 추천 모델에 적용하였는지, 뉴럴 그래프 컬래버티 필터링","confidence":0.9419,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[47670,48100,"그래프가"],[48130,48600,"무엇이고"],[48930,49180,"어떤"],[49190,49440,"식의"],[49450,49880,"데이터를"],[49910,50240,"표현할"],[50240,50380,"때"],[50390,50540,"이"],[50540,50920,"그래프가"],[50930,51520,"적합한지,"],[52330,52480,"또"],[52510,52820,"그래프"],[52870,53140,"자료"],[53140,53480,"구조를"],[53670,54040,"어떻게"],[54070,54460,"뉴럴넷"],[54490,54760,"추천"],[54760,55040,"모델에"],[55040,55780,"적용하였는지,"],[56350,56620,"뉴럴"],[56650,57000,"그래프"],[57070,57500,"컬래버티"],[57510,57880,"필터링"]],"textEdited":"그래프가 무엇이고 어떤 식의 데이터를 표현할 때 이 그래프가 적합한지, 또 그래프 자료 구조를 어떻게 뉴럴넷 추천 모델에 적용하였는지, 뉴럴 그래프 컬래버티 필터링"},{"start":58200,"end":71000,"text":"NGCF라는 모델과 이보다 조금 더 가벼운 라이트 GCN 이 두 가지 모델을 살펴보겠습니다. 그 이후에 RNN 계열의 모델을 간단히 리캡하고 GRU 레이어를 활용한 시퀀스 모델링","confidence":0.9096,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[58510,59560,"NGCF라는"],[59560,59960,"모델과"],[60330,60680,"이보다"],[60710,60920,"조금"],[60920,61040,"더"],[61090,61400,"가벼운"],[61550,61880,"라이트"],[61890,62280,"GCN"],[62370,62520,"이"],[62520,62607,"두"],[62607,62840,"가지"],[62890,63260,"모델을"],[63390,64100,"살펴보겠습니다."],[64210,64360,"그"],[64360,64640,"이후에"],[65090,65640,"RNN"],[65750,66120,"계열의"],[66230,66560,"모델을"],[66630,66980,"간단히"],[67010,67620,"리캡하고"],[68150,68700,"GRU"],[68850,69220,"레이어를"],[69230,69540,"활용한"],[69990,70400,"시퀀스"],[70400,70780,"모델링"]],"textEdited":"NGCF라는 모델과 이보다 조금 더 가벼운 라이트 GCN 이 두 가지 모델을 살펴보겠습니다. 그 이후에 RNN 계열의 모델을 간단히 리캡하고 GRU 레이어를 활용한 시퀀스 모델링"},{"start":71000,"end":84300,"text":"추천 기법인 gr 4 r을 살펴보겠습니다. 먼저 그래프 뉴널 네트워크를 활용한 추천 모델입니다. 그래프 신경망 그리고 그래프 컨볼루션 뉴럴 네트워크의 개념을 이해하고 이를 적용한 추천 시스템 모델에 대해서","confidence":0.8568,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[71210,71480,"추천"],[71490,71840,"기법인"],[71950,72220,"gr"],[72310,72460,"4"],[72470,73060,"r을"],[73230,73960,"살펴보겠습니다."],[74190,74440,"먼저"],[74910,75220,"그래프"],[75220,75400,"뉴널"],[75400,75900,"네트워크를"],[75900,76160,"활용한"],[76190,76420,"추천"],[76420,76880,"모델입니다."],[77590,77880,"그래프"],[77930,78300,"신경망"],[78750,78980,"그리고"],[79010,79320,"그래프"],[79370,79820,"컨볼루션"],[79870,80100,"뉴럴"],[80100,80840,"네트워크의"],[80890,81220,"개념을"],[81250,81660,"이해하고"],[82330,82540,"이를"],[82590,82900,"적용한"],[82930,83160,"추천"],[83190,83500,"시스템"],[83500,83767,"모델에"],[83767,84080,"대해서"]],"textEdited":"추천 기법인 gr 4 r을 살펴보겠습니다. 먼저 그래프 뉴널 네트워크를 활용한 추천 모델입니다. 그래프 신경망 그리고 그래프 컨볼루션 뉴럴 네트워크의 개념을 이해하고 이를 적용한 추천 시스템 모델에 대해서"},{"start":84300,"end":98900,"text":"살펴보겠습니다. 네 먼저 그래프 뉴얼 네트워크를 이야기하기 전에 이 그래프가 무엇인지에 대해서 정의해 봐야겠죠. 그래프의 정의는 굉장히 간단합니다. 이 꼭짓점인 노드와 노드와 노드 사이를 잇는 엣지","confidence":0.9622,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[84570,85280,"살펴보겠습니다."],[85570,85720,"네"],[85730,86000,"먼저"],[86270,86600,"그래프"],[86600,86760,"뉴얼"],[86760,87260,"네트워크를"],[87260,87740,"이야기하기"],[87740,88080,"전에"],[88410,88560,"이"],[88570,89100,"그래프가"],[89390,90020,"무엇인지에"],[90020,90360,"대해서"],[90850,91147,"정의해"],[91147,91620,"봐야겠죠."],[91950,92360,"그래프의"],[92360,92660,"정의는"],[92670,92920,"굉장히"],[92990,93540,"간단합니다."],[94010,94160,"이"],[94210,94720,"꼭짓점인"],[94770,95940,"노드와"],[96690,97060,"노드와"],[97090,97380,"노드"],[97390,97720,"사이를"],[97730,97920,"잇는"],[98130,98460,"엣지"]],"textEdited":"살펴보겠습니다. 네 먼저 그래프 뉴얼 네트워크를 이야기하기 전에 이 그래프가 무엇인지에 대해서 정의해 봐야겠죠. 그래프의 정의는 굉장히 간단합니다. 이 꼭짓점인 노드와 노드와 노드 사이를 잇는 엣지"},{"start":98900,"end":107500,"text":"로 이루어진 자료 구조를 의미합니다. 따라서 데이터들이 서로 연결되어 있을 때 이러한 데이터를 표현하기에 적합한 자료 구조입니다.","confidence":0.9894,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[99110,99260,"로"],[99270,99640,"이루어진"],[99730,100020,"자료"],[100020,100340,"구조를"],[100390,100920,"의미합니다."],[101470,101820,"따라서"],[102390,102927,"데이터들이"],[102927,103120,"서로"],[103170,103660,"연결되어"],[103670,103880,"있을"],[103890,104040,"때"],[104230,104520,"이러한"],[104590,105060,"데이터를"],[105270,105840,"표현하기에"],[105890,106360,"적합한"],[106590,106880,"자료"],[106930,107440,"구조입니다."]],"textEdited":"로 이루어진 자료 구조를 의미합니다. 따라서 데이터들이 서로 연결되어 있을 때 이러한 데이터를 표현하기에 적합한 자료 구조입니다."},{"start":107500,"end":114900,"text":"아래에 보이는 제일 기본적인 그래프는 노드의 집합과 엣지들의 집합으로 이루어져 있습니다. 일반적으로 그래프는","confidence":0.9795,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[107790,108100,"아래에"],[108100,108380,"보이는"],[108470,108720,"제일"],[108790,109300,"기본적인"],[109330,109820,"그래프는"],[110370,110820,"노드의"],[110890,111400,"집합과"],[111550,112000,"엣지들의"],[112010,112440,"집합으로"],[112440,112740,"이루어져"],[112740,113140,"있습니다."],[113450,113980,"일반적으로"],[113990,114440,"그래프는"]],"textEdited":"아래에 보이는 제일 기본적인 그래프는 노드의 집합과 엣지들의 집합으로 이루어져 있습니다. 일반적으로 그래프는"},{"start":114900,"end":128200,"text":"노드들의 집합, 엣지들의 집합인 abcd라는 4개의 노드가 존재하고요. 이 abcd 서로서로 연결되어 있는 엣지가 총 4개가 존재하고 이 4개는 다음과 같은 그림으로 나타낼 수 있습니다.","confidence":0.951,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[115210,115740,"노드들의"],[115770,116060,"집합,"],[116350,116740,"엣지들의"],[116740,117080,"집합인"],[117310,118080,"abcd라는"],[118450,118840,"4개의"],[119150,119760,"노드가"],[119790,120340,"존재하고요."],[120790,120940,"이"],[120970,121500,"abcd"],[121530,121900,"서로서로"],[121930,122334,"연결되어"],[122334,122480,"있는"],[122530,122880,"엣지가"],[122950,123100,"총"],[123650,124100,"4개가"],[124170,124740,"존재하고"],[124890,125040,"이"],[125050,125420,"4개는"],[125510,125840,"다음과"],[125840,126060,"같은"],[126150,126580,"그림으로"],[127030,127400,"나타낼"],[127430,127580,"수"],[127580,127980,"있습니다."]],"textEdited":"노드들의 집합, 엣지들의 집합인 abcd라는 4개의 노드가 존재하고요. 이 abcd 서로서로 연결되어 있는 엣지가 총 4개가 존재하고 이 4개는 다음과 같은 그림으로 나타낼 수 있습니다."},{"start":128200,"end":143200,"text":"그렇다면 그래프를 왜 사용할까요? 방금 언급했듯이 그래프는 연결되어 있는 데이터를 표현하기에 적합합니다. 따라서 어떤 객체 사이의 관계나 상호 작용 같은 다소 추상적인 개념을 표현할 수 있습니다. 이러한 상호 작용을","confidence":0.9709,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[128510,128940,"그렇다면"],[129090,129540,"그래프를"],[129550,129700,"왜"],[129750,130340,"사용할까요?"],[131090,131320,"방금"],[131330,131980,"언급했듯이"],[132530,133020,"그래프는"],[133110,133720,"연결되어"],[133730,133900,"있는"],[133970,134400,"데이터를"],[134410,134900,"표현하기에"],[134930,135540,"적합합니다."],[135870,136220,"따라서"],[136350,136560,"어떤"],[136730,137060,"객체"],[137060,137340,"사이의"],[137450,137900,"관계나"],[138510,138720,"상호"],[138750,139000,"작용"],[139010,139280,"같은"],[139730,140000,"다소"],[140050,140500,"추상적인"],[140550,140880,"개념을"],[140930,141280,"표현할"],[141290,141394,"수"],[141394,141760,"있습니다."],[141950,142220,"이러한"],[142350,142580,"상호"],[142590,142940,"작용을"]],"textEdited":"그렇다면 그래프를 왜 사용할까요? 방금 언급했듯이 그래프는 연결되어 있는 데이터를 표현하기에 적합합니다. 따라서 어떤 객체 사이의 관계나 상호 작용 같은 다소 추상적인 개념을 표현할 수 있습니다. 이러한 상호 작용을"},{"start":143200,"end":157700,"text":"테이블 형태의 정형화된 데이터를 표현하면 오히려 표현형이 복잡해지는데요. 그래프를 사용하면 오히려 간단하게 표현할 수 있고 또 다른 관점으로도 표현할 수 있습니다. 아래와 같이 친구 관계, 소셜 네트워크라든지","confidence":0.9767,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[143490,143820,"테이블"],[143910,144320,"형태의"],[144410,144860,"정형화된"],[144910,145280,"데이터를"],[145290,145720,"표현하면"],[145870,146200,"오히려"],[146490,146940,"표현형이"],[147010,147740,"복잡해지는데요."],[148110,148600,"그래프를"],[148600,149000,"사용하면"],[149030,149340,"오히려"],[149930,150440,"간단하게"],[150490,150840,"표현할"],[150890,151040,"수"],[151040,151360,"있고"],[151890,152040,"또"],[152050,152280,"다른"],[152410,153040,"관점으로도"],[153370,153700,"표현할"],[153710,153827,"수"],[153827,154200,"있습니다."],[154550,154880,"아래와"],[154890,155180,"같이"],[155710,155960,"친구"],[155990,156240,"관계,"],[156330,156620,"소셜"],[156620,157540,"네트워크라든지"]],"textEdited":"테이블 형태의 정형화된 데이터를 표현하면 오히려 표현형이 복잡해지는데요. 그래프를 사용하면 오히려 간단하게 표현할 수 있고 또 다른 관점으로도 표현할 수 있습니다. 아래와 같이 친구 관계, 소셜 네트워크라든지"},{"start":157700,"end":169500,"text":"그 팔로워 팔로잉 관계가 있는 데이터겠죠. 그리고 현재 코로나 바이러스 상황과 같이 바이러스의 확산 같은 상태도 이 그래프를 통해서 표현할 수 있습니다. 그 외에 추천 시스템에서","confidence":0.9803,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[157890,158040,"그"],[158050,158400,"팔로워"],[158430,158760,"팔로잉"],[158770,159040,"관계가"],[159040,159220,"있는"],[159550,160120,"데이터겠죠."],[160790,161000,"그리고"],[161000,161220,"현재"],[161370,161640,"코로나"],[161730,162100,"바이러스"],[162110,162400,"상황과"],[162400,162640,"같이"],[162710,163200,"바이러스의"],[163290,163580,"확산"],[163770,164060,"같은"],[164590,165020,"상태도"],[165370,165520,"이"],[165520,165940,"그래프를"],[165940,166240,"통해서"],[166390,166700,"표현할"],[166700,166807,"수"],[166807,167200,"있습니다."],[167510,167660,"그"],[167660,167920,"외에"],[168270,168560,"추천"],[168630,169220,"시스템에서"]],"textEdited":"그 팔로워 팔로잉 관계가 있는 데이터겠죠. 그리고 현재 코로나 바이러스 상황과 같이 바이러스의 확산 같은 상태도 이 그래프를 통해서 표현할 수 있습니다. 그 외에 추천 시스템에서"},{"start":169500,"end":183900,"text":"가장 많이 사용하는 유저가 아이템을 소비했다는 데이터도 그래프로 표현할 수 있는데요. 유저와 아이템을 각각의 노드로 두고 유저가 아이템을 소비했다는 데이터를 엣지로 한다면 이 데이터를 표현할 수 있습니다.","confidence":0.9883,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[169770,170020,"가장"],[170020,170200,"많이"],[170230,170620,"사용하는"],[170890,171280,"유저가"],[171310,171840,"아이템을"],[171910,172560,"소비했다는"],[173210,173700,"데이터도"],[173890,174320,"그래프로"],[174320,174580,"표현할"],[174580,174667,"수"],[174667,175000,"있는데요."],[175450,175860,"유저와"],[175890,176340,"아이템을"],[176410,176800,"각각의"],[176830,177260,"노드로"],[177270,177560,"두고"],[178270,178640,"유저가"],[178670,179080,"아이템을"],[179110,179700,"소비했다는"],[179790,180240,"데이터를"],[180410,180840,"엣지로"],[180840,181180,"한다면"],[181750,181900,"이"],[181910,182380,"데이터를"],[182730,183080,"표현할"],[183090,183240,"수"],[183240,183740,"있습니다."]],"textEdited":"가장 많이 사용하는 유저가 아이템을 소비했다는 데이터도 그래프로 표현할 수 있는데요. 유저와 아이템을 각각의 노드로 두고 유저가 아이템을 소비했다는 데이터를 엣지로 한다면 이 데이터를 표현할 수 있습니다."},{"start":183900,"end":190200,"text":"네 그래프를 사용하는 이유는 흔히 우리가 자주 다루는 이미지나 텍스트","confidence":0.993,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[184330,184480,"네"],[184550,184960,"그래프를"],[184970,185380,"사용하는"],[185430,185740,"이유는"],[186370,186660,"흔히"],[187350,187620,"우리가"],[187690,188000,"자주"],[188010,188440,"다루는"],[188970,189420,"이미지나"],[189490,189920,"텍스트"]],"textEdited":"네 그래프를 사용하는 이유는 흔히 우리가 자주 다루는 이미지나 텍스트"},{"start":190200,"end":199300,"text":"혹은 엑셀 시트로 표현할 수 있는 정형화된 데이터와는 조금 다른 형태의 데이터를 사용하기 때문이다. 이제 이러한 이미지, 텍스트, 정형 데이터 같은 데이터들은","confidence":0.9815,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[190410,190600,"혹은"],[190690,190960,"엑셀"],[191030,191340,"시트로"],[191340,191620,"표현할"],[191620,191727,"수"],[191727,191900,"있는"],[192510,193000,"정형화된"],[193030,193640,"데이터와는"],[194010,194220,"조금"],[194250,194460,"다른"],[194530,194860,"형태의"],[194860,195240,"데이터를"],[195240,195567,"사용하기"],[195567,195960,"때문이다."],[196230,196400,"이제"],[196400,196700,"이러한"],[196970,197280,"이미지,"],[197310,197620,"텍스트,"],[197620,197820,"정형"],[197820,198120,"데이터"],[198170,198460,"같은"],[198550,199060,"데이터들은"]],"textEdited":"혹은 엑셀 시트로 표현할 수 있는 정형화된 데이터와는 조금 다른 형태의 데이터를 사용하기 때문이다. 이제 이러한 이미지, 텍스트, 정형 데이터 같은 데이터들은"},{"start":199300,"end":206600,"text":"격자 형태로 표현할 수 있다고 얘기하고 이 데이터를 좀 더 일반화된 표현으로는 유클리디언 스페이스에 표현할 수 있다고 합니다.","confidence":0.9867,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[199610,200020,"격자"],[200070,200460,"형태로"],[200460,200760,"표현할"],[200760,200854,"수"],[200854,201140,"있다고"],[201140,201560,"얘기하고"],[202050,202200,"이"],[202200,202540,"데이터를"],[202540,202660,"좀"],[202660,202780,"더"],[202790,203220,"일반화된"],[203270,203700,"표현으로는"],[204010,204600,"유클리디언"],[204610,205380,"스페이스에"],[205430,205760,"표현할"],[205760,205900,"수"],[205900,206147,"있다고"],[206147,206540,"합니다."]],"textEdited":"격자 형태로 표현할 수 있다고 얘기하고 이 데이터를 좀 더 일반화된 표현으로는 유클리디언 스페이스에 표현할 수 있다고 합니다."},{"start":206600,"end":215500,"text":"이 유클리디언 스페이스를 간단하게 설명하자면 우리가 익숙하게 다루는 2차원 평면이나 3차원 공간을 일반화시킨 공간이고요.","confidence":0.982,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[206790,206907,"이"],[206907,207440,"유클리디언"],[207450,207960,"스페이스를"],[208050,208480,"간단하게"],[208530,209100,"설명하자면"],[209830,210140,"우리가"],[210370,210820,"익숙하게"],[210820,211160,"다루는"],[211350,211700,"2차원"],[211830,212280,"평면이나"],[212450,212900,"3차원"],[212970,213380,"공간을"],[213950,214540,"일반화시킨"],[214590,215500,"공간이고요."]],"textEdited":"이 유클리디언 스페이스를 간단하게 설명하자면 우리가 익숙하게 다루는 2차원 평면이나 3차원 공간을 일반화시킨 공간이고요."},{"start":215500,"end":227900,"text":"즉 유한한 실수로 표현할 수 있는 공간이라고 볼 수 있습니다. 그래서 대표적으로 아래 그림과 같이 2차원 평면에 있는 이러한 이미지 데이터는 유클리드 스페이스로 표현할 수 있는 데이터입니다.","confidence":0.9775,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[215790,215940,"즉"],[216070,216520,"유한한"],[216670,217100,"실수로"],[217170,217500,"표현할"],[217510,217614,"수"],[217614,217800,"있는"],[218030,218580,"공간이라고"],[218580,218720,"볼"],[218730,218834,"수"],[218834,219180,"있습니다."],[219330,219520,"그래서"],[219550,220080,"대표적으로"],[220090,220300,"아래"],[220370,220700,"그림과"],[220700,221000,"같이"],[221610,221940,"2차원"],[222030,222420,"평면에"],[222430,222640,"있는"],[223430,223700,"이러한"],[224370,224720,"이미지"],[225010,225480,"데이터는"],[225810,226300,"유클리드"],[226300,226760,"스페이스로"],[226760,227060,"표현할"],[227060,227134,"수"],[227134,227300,"있는"],[227330,227900,"데이터입니다."]],"textEdited":"즉 유한한 실수로 표현할 수 있는 공간이라고 볼 수 있습니다. 그래서 대표적으로 아래 그림과 같이 2차원 평면에 있는 이러한 이미지 데이터는 유클리드 스페이스로 표현할 수 있는 데이터입니다."},{"start":227900,"end":240400,"text":"그러나 이런 소셜 네트워크 데이터와 같은 유저가 서로 연결되어 있는 데이터라든지 혹은 분자 구조와 같은 모양의 데이터는 유클리드 스페이스 즉 어떤 실수형의 형태로 표현할 수는 없는 데이터입니다.","confidence":0.9475,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[228130,228400,"그러나"],[228770,228940,"이런"],[228950,229260,"소셜"],[229260,229660,"네트워크"],[229670,230040,"데이터와"],[230040,230260,"같은"],[230290,230640,"유저가"],[230640,230780,"서로"],[230810,231234,"연결되어"],[231234,231400,"있는"],[231430,232360,"데이터라든지"],[232770,233020,"혹은"],[233530,233820,"분자"],[233870,234520,"구조와"],[234670,234920,"같은"],[235010,235320,"모양의"],[235350,235840,"데이터는"],[236450,236920,"유클리드"],[236920,237280,"스페이스"],[237330,237480,"즉"],[237490,237660,"어떤"],[237770,238320,"실수형의"],[238330,238660,"형태로"],[238670,239000,"표현할"],[239030,239260,"수는"],[239370,239600,"없는"],[239870,240400,"데이터입니다."]],"textEdited":"그러나 이런 소셜 네트워크 데이터와 같은 유저가 서로 연결되어 있는 데이터라든지 혹은 분자 구조와 같은 모양의 데이터는 유클리드 스페이스 즉 어떤 실수형의 형태로 표현할 수는 없는 데이터입니다."},{"start":240400,"end":251500,"text":"오히려 이런 데이터를 그래프를 사용하지 않고 표현한다면 오히려 표현형이 굉장히 복잡해집니다. 그래서 그래프를 사용하여서 데이터의 표현을 단순하게 만드는 것이죠.","confidence":0.9863,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[240670,240980,"오히려"],[241030,241200,"이런"],[241670,242140,"데이터를"],[242390,242800,"그래프를"],[242810,243260,"사용하지"],[243270,243520,"않고"],[243530,244080,"표현한다면"],[244530,244780,"오히려"],[244810,245160,"표현형이"],[245160,245420,"굉장히"],[245550,246280,"복잡해집니다."],[246510,246780,"그래서"],[247230,247680,"그래프를"],[247680,248220,"사용하여서"],[248850,249320,"데이터의"],[249410,249800,"표현을"],[250090,250620,"단순하게"],[250670,251040,"만드는"],[251130,251460,"것이죠."]],"textEdited":"오히려 이런 데이터를 그래프를 사용하지 않고 표현한다면 오히려 표현형이 굉장히 복잡해집니다. 그래서 그래프를 사용하여서 데이터의 표현을 단순하게 만드는 것이죠."},{"start":251500,"end":266500,"text":"그래서 우리가 사용하는 논 유클리디언 스페이스의 데이터들을 그래프로 사용할 수 있다는 것을 언급하였습니다. 네 그렇다면 이 그래프를 신경망에 적용해야 하는데요. 이것을 그래프 신경망 그래프 뉴얼 네트워크라고 합니다. 그렇다면 어떻게","confidence":0.9671,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[251710,251940,"그래서"],[251940,252140,"우리가"],[252230,252620,"사용하는"],[253090,253240,"논"],[253310,253840,"유클리디언"],[253840,254800,"스페이스의"],[254830,255420,"데이터들을"],[255630,256080,"그래프로"],[256110,256440,"사용할"],[256440,256514,"수"],[256514,256800,"있다는"],[256800,257080,"것을"],[257190,257920,"언급하였습니다."],[258490,258640,"네"],[258650,259060,"그렇다면"],[259230,259380,"이"],[259390,260060,"그래프를"],[260610,261100,"신경망에"],[261130,261487,"적용해야"],[261487,261840,"하는데요."],[262310,262600,"이것을"],[262610,262880,"그래프"],[262930,263300,"신경망"],[263610,263867,"그래프"],[263867,264080,"뉴얼"],[264080,264740,"네트워크라고"],[264740,265000,"합니다."],[265270,265620,"그렇다면"],[265650,266040,"어떻게"]],"textEdited":"그래서 우리가 사용하는 논 유클리디언 스페이스의 데이터들을 그래프로 사용할 수 있다는 것을 언급하였습니다. 네 그렇다면 이 그래프를 신경망에 적용해야 하는데요. 이것을 그래프 신경망 그래프 뉴얼 네트워크라고 합니다. 그렇다면 어떻게"},{"start":266500,"end":278400,"text":"이 그래프로 나타낸 데이터를 뉴럴 네트워크에 적용할 수 있을까요? 먼저 왼쪽 아래와 같은 5개의 노드로 된 그래프가 있습니다. 이제 그래프에서 데이터를 설명할 때 제일 중요한 것이 노드","confidence":0.9559,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[266770,266920,"이"],[266920,267340,"그래프로"],[267340,267680,"나타낸"],[267730,268180,"데이터를"],[268530,268820,"뉴럴"],[268820,269360,"네트워크에"],[269360,269700,"적용할"],[269710,269827,"수"],[269827,270240,"있을까요?"],[270610,270840,"먼저"],[270890,271160,"왼쪽"],[271170,271500,"아래와"],[271510,271760,"같은"],[272410,272800,"5개의"],[272800,273074,"노드로"],[273074,273200,"된"],[273310,273700,"그래프가"],[273700,274080,"있습니다."],[274810,274980,"이제"],[274980,275440,"그래프에서"],[275910,276260,"데이터를"],[276270,276600,"설명할"],[276600,276720,"때"],[276720,276900,"제일"],[276910,277200,"중요한"],[277200,277460,"것이"],[277690,277960,"노드"]],"textEdited":"이 그래프로 나타낸 데이터를 뉴럴 네트워크에 적용할 수 있을까요? 먼저 왼쪽 아래와 같은 5개의 노드로 된 그래프가 있습니다. 이제 그래프에서 데이터를 설명할 때 제일 중요한 것이 노드"},{"start":278400,"end":287200,"text":"와 노드 사이에 이어져 있는 엣지들이죠. 그럼 이제 해당 노드에 연결된 이웃 정보들의","confidence":0.8983,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[279230,279380,"와"],[280330,280640,"노드"],[280730,281540,"사이에"],[281670,282000,"이어져"],[282000,282160,"있는"],[282230,282860,"엣지들이죠."],[283910,284080,"그럼"],[284080,284240,"이제"],[284270,284540,"해당"],[284610,284960,"노드에"],[284990,285520,"연결된"],[286030,286240,"이웃"],[286390,286880,"정보들의"]],"textEdited":"와 노드 사이에 이어져 있는 엣지들이죠. 그럼 이제 해당 노드에 연결된 이웃 정보들의"},{"start":287200,"end":296800,"text":"이웃 노드들의 정보를 뉴럴 네트워크 모델이 표현할 수 있어야만 그래프에서 제일 중요한 패턴을 학습할 수 있는 것입니다. 그래서 가장 나이브한 방법은","confidence":0.9777,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[287450,287700,"이웃"],[287770,288200,"노드들의"],[288250,288660,"정보를"],[289150,289400,"뉴럴"],[289430,290020,"네트워크"],[290090,290400,"모델이"],[290450,290800,"표현할"],[290810,290927,"수"],[290927,291360,"있어야만"],[291950,292440,"그래프에서"],[292470,292680,"제일"],[292770,293060,"중요한"],[293130,293460,"패턴을"],[293460,293780,"학습할"],[293780,293874,"수"],[293874,294007,"있는"],[294007,294400,"것입니다."],[294630,294860,"그래서"],[294910,295160,"가장"],[295230,295740,"나이브한"],[295810,296140,"방법은"]],"textEdited":"이웃 노드들의 정보를 뉴럴 네트워크 모델이 표현할 수 있어야만 그래프에서 제일 중요한 패턴을 학습할 수 있는 것입니다. 그래서 가장 나이브한 방법은"},{"start":296800,"end":309300,"text":"이 그림과 같이 그래프를 표현할 수 있는 인접 행렬을 만들어서 그 인접 행렬을 그대로 뉴럴 네트워크 모델의 인풋으로 사용하는 것입니다. 보시면 a와 b가 연결되어 있기 때문에","confidence":0.9538,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[297050,297200,"이"],[297200,297500,"그림과"],[297500,297780,"같이"],[298710,299160,"그래프를"],[299160,299480,"표현할"],[299480,299574,"수"],[299574,299740,"있는"],[300010,300280,"인접"],[300370,300700,"행렬을"],[300700,301120,"만들어서"],[301630,301780,"그"],[301810,302080,"인접"],[302190,302560,"행렬을"],[303090,303440,"그대로"],[303470,303720,"뉴럴"],[303730,304200,"네트워크"],[304200,304540,"모델의"],[304810,305280,"인풋으로"],[305280,305607,"사용하는"],[305607,306040,"것입니다."],[306250,306620,"보시면"],[307210,307560,"a와"],[307570,307820,"b가"],[307850,308340,"연결되어"],[308340,308500,"있기"],[308500,308860,"때문에"]],"textEdited":"이 그림과 같이 그래프를 표현할 수 있는 인접 행렬을 만들어서 그 인접 행렬을 그대로 뉴럴 네트워크 모델의 인풋으로 사용하는 것입니다. 보시면 a와 b가 연결되어 있기 때문에"},{"start":309300,"end":321600,"text":"a와 b의 매트릭스의 원소 1 그리고 또 b와 a의 매트릭스 원소의 1 이렇게 표현되어 있죠. 그래서 이 인접 행렬을 말 그대로 뉴럴 네트워크 인풋 레이어에","confidence":0.9475,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[309650,309980,"a와"],[310090,310780,"b의"],[310830,311760,"매트릭스의"],[312030,312320,"원소"],[312430,312580,"1"],[312770,313000,"그리고"],[313030,313180,"또"],[313430,313720,"b와"],[313810,314160,"a의"],[314390,314760,"매트릭스"],[314760,315080,"원소의"],[315080,315200,"1"],[315370,315560,"이렇게"],[315930,316360,"표현되어"],[316370,316600,"있죠."],[317070,317214,"그래서"],[317214,317340,"이"],[317410,317680,"인접"],[317770,318180,"행렬을"],[318750,318900,"말"],[318970,319320,"그대로"],[319810,320080,"뉴럴"],[320080,320500,"네트워크"],[320570,320840,"인풋"],[320890,321300,"레이어에"]],"textEdited":"a와 b의 매트릭스의 원소 1 그리고 또 b와 a의 매트릭스 원소의 1 이렇게 표현되어 있죠. 그래서 이 인접 행렬을 말 그대로 뉴럴 네트워크 인풋 레이어에"},{"start":321600,"end":334800,"text":"데이터 하나를 뽑아서 그대로 넣어주게 됩니다. 그래서 이 데이터는 아마 이 비에 대한 표현형이겠죠. 그래서 인접 행렬 더하기 다른 기존에 우리가 뉴럴 테크 모델에서 사용하고 있는 다른 피처형을 이렇게 넣어 줄 수 있습니다.","confidence":0.9102,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[321810,322100,"데이터"],[322100,322340,"하나를"],[322340,322620,"뽑아서"],[322670,322980,"그대로"],[323210,323607,"넣어주게"],[323607,323920,"됩니다."],[324050,324280,"그래서"],[324280,324420,"이"],[324430,324840,"데이터는"],[325150,325340,"아마"],[325350,325500,"이"],[325570,325920,"비에"],[325920,326120,"대한"],[326350,327000,"표현형이겠죠."],[327650,327840,"그래서"],[327890,328160,"인접"],[328250,328500,"행렬"],[328870,329360,"더하기"],[329790,330040,"다른"],[330770,331060,"기존에"],[331060,331207,"우리가"],[331207,331440,"뉴럴"],[331470,331747,"테크"],[331747,332027,"모델에서"],[332027,332334,"사용하고"],[332334,332447,"있는"],[332447,332620,"다른"],[332690,333340,"피처형을"],[333570,333840,"이렇게"],[333890,334080,"넣어"],[334080,334220,"줄"],[334250,334367,"수"],[334367,334780,"있습니다."]],"textEdited":"데이터 하나를 뽑아서 그대로 넣어주게 됩니다. 그래서 이 데이터는 아마 이 비에 대한 표현형이겠죠. 그래서 인접 행렬 더하기 다른 기존에 우리가 뉴럴 테크 모델에서 사용하고 있는 다른 피처형을 이렇게 넣어 줄 수 있습니다."},{"start":334800,"end":349100,"text":"하지만 이제 이런 기법의 문제는 무엇일까요? 일단 노드가 계속 증가하게 되면 인접 행렬의 차원은 계속 증가하고 이 행렬의 행을 그대로 인풋 레이어에 사용하게 되면 차원은 계속 증가하게 될 것입니다.","confidence":0.9851,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[335090,335380,"하지만"],[335380,335520,"이제"],[335520,335700,"이런"],[335810,336140,"기법의"],[336150,336500,"문제는"],[336500,337060,"무엇일까요?"],[337770,338040,"일단"],[339070,339460,"노드가"],[339550,339820,"계속"],[339950,340367,"증가하게"],[340367,340640,"되면"],[341690,341980,"인접"],[342050,342340,"행렬의"],[342350,342680,"차원은"],[342690,342900,"계속"],[343010,343480,"증가하고"],[344310,344460,"이"],[344510,344820,"행렬의"],[344820,345060,"행을"],[345090,345400,"그대로"],[345850,346120,"인풋"],[346170,346600,"레이어에"],[346630,347007,"사용하게"],[347007,347240,"되면"],[347430,347760,"차원은"],[347760,347940,"계속"],[348030,348440,"증가하게"],[348440,348560,"될"],[348570,349040,"것입니다."]],"textEdited":"하지만 이제 이런 기법의 문제는 무엇일까요? 일단 노드가 계속 증가하게 되면 인접 행렬의 차원은 계속 증가하고 이 행렬의 행을 그대로 인풋 레이어에 사용하게 되면 차원은 계속 증가하게 될 것입니다."},{"start":349100,"end":362800,"text":"그런 모델의 복잡도는 계속해서 증가하고 연산량은 많아지면서 동시에 입력 데이터는 아주 스파r스해지겠죠. 그렇기 때문에 이 모델이 좋지 않습니다. 또한 여기서 인접 행렬은 a b c d e 순서","confidence":0.9136,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[349330,349500,"그런"],[349530,349900,"모델의"],[349950,350400,"복잡도는"],[350400,350740,"계속해서"],[350850,351400,"증가하고"],[351850,352380,"연산량은"],[352380,352980,"많아지면서"],[353610,353920,"동시에"],[353930,354160,"입력"],[354190,354600,"데이터는"],[354600,354780,"아주"],[354810,355620,"스파r스해지겠죠."],[356370,356567,"그렇기"],[356567,356840,"때문에"],[356840,356960,"이"],[356960,357260,"모델이"],[357260,357520,"좋지"],[357520,357960,"않습니다."],[358290,358500,"또한"],[358530,358780,"여기서"],[358830,359100,"인접"],[359130,359460,"행렬은"],[359990,360140,"a"],[360330,360480,"b"],[360730,360880,"c"],[361090,361240,"d"],[361350,361500,"e"],[362210,362500,"순서"]],"textEdited":"그런 모델의 복잡도는 계속해서 증가하고 연산량은 많아지면서 동시에 입력 데이터는 아주 스파r스해지겠죠. 그렇기 때문에 이 모델이 좋지 않습니다. 또한 여기서 인접 행렬은 a b c d e 순서"},{"start":362800,"end":375600,"text":"대로 구성되어 있는데요. 사실 이 순서는 그래프 자체에서는 아무런 의미가 없습니다. 이 그래프를 살펴보면 abcde의 연결 관계를 모델링 한 것인데 이것을 매트릭스를 표현하기 위해서 우리가 임의로 순서를","confidence":0.9158,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[363130,363380,"대로"],[363380,363760,"구성되어"],[363760,364080,"있는데요."],[364510,364780,"사실"],[364870,365020,"이"],[365110,365480,"순서는"],[365490,365900,"그래프"],[366110,366620,"자체에서는"],[366650,366920,"아무런"],[366950,367207,"의미가"],[367207,367580,"없습니다."],[367870,368020,"이"],[368020,368340,"그래프를"],[368340,368740,"살펴보면"],[369230,370140,"abcde의"],[370490,370780,"연결"],[370830,371220,"관계를"],[371430,371760,"모델링"],[371770,371887,"한"],[371887,372240,"것인데"],[372270,372600,"이것을"],[372610,373040,"매트릭스를"],[373040,373440,"표현하기"],[373440,373740,"위해서"],[374210,374440,"우리가"],[374490,374800,"임의로"],[374950,375340,"순서를"]],"textEdited":"대로 구성되어 있는데요. 사실 이 순서는 그래프 자체에서는 아무런 의미가 없습니다. 이 그래프를 살펴보면 abcde의 연결 관계를 모델링 한 것인데 이것을 매트릭스를 표현하기 위해서 우리가 임의로 순서를"},{"start":375600,"end":389500,"text":"준 것이죠. 근데 이 나이브 어프러치 즉 인접 행렬을 그대로 인플레이어에 사용하는 방법은 노드의 순서가 그대로 입력 값으로 사용되기 때문에 만약에 이 순서가 바뀌면 또 의미가 달라질 수도 있습니다.","confidence":0.9322,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[375830,375980,"준"],[376010,376360,"것이죠."],[377090,377267,"근데"],[377267,377400,"이"],[377400,377700,"나이브"],[377700,378120,"어프러치"],[378270,378420,"즉"],[378850,379120,"인접"],[379150,379460,"행렬을"],[379460,379720,"그대로"],[380130,380680,"인플레이어에"],[380690,381080,"사용하는"],[381170,381560,"방법은"],[382210,382620,"노드의"],[383090,383460,"순서가"],[383470,383720,"그대로"],[383770,384020,"입력"],[384020,384300,"값으로"],[384300,384667,"사용되기"],[384667,385060,"때문에"],[385650,385980,"만약에"],[385980,386100,"이"],[386350,386660,"순서가"],[386710,387060,"바뀌면"],[387170,387320,"또"],[387450,387840,"의미가"],[388190,388580,"달라질"],[388630,388860,"수도"],[388860,389260,"있습니다."]],"textEdited":"준 것이죠. 근데 이 나이브 어프러치 즉 인접 행렬을 그대로 인플레이어에 사용하는 방법은 노드의 순서가 그대로 입력 값으로 사용되기 때문에 만약에 이 순서가 바뀌면 또 의미가 달라질 수도 있습니다."},{"start":389500,"end":400700,"text":"네 그래서 방금 설명한 대로 이 나이브 어프로치 나이브하게 그래프의 인접 행렬을 만들어서 그 인접 행렬을 그대로 뉴럴넷 모델의 입력 값으로 사용하는 방법은 좋지 않습니다.","confidence":0.9579,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[390190,390340,"네"],[390340,390580,"그래서"],[390650,390880,"방금"],[390910,391260,"설명한"],[391260,391500,"대로"],[392110,392260,"이"],[392260,392580,"나이브"],[392580,393060,"어프로치"],[393550,394140,"나이브하게"],[394210,394660,"그래프의"],[394730,395000,"인접"],[395050,395360,"행렬을"],[395360,395780,"만들어서"],[395970,396120,"그"],[396130,396380,"인접"],[396430,396760,"행렬을"],[396810,397180,"그대로"],[397770,398140,"뉴럴넷"],[398170,398480,"모델의"],[398490,398740,"입력"],[398750,399040,"값으로"],[399050,399420,"사용하는"],[399450,399760,"방법은"],[399830,400120,"좋지"],[400120,400640,"않습니다."]],"textEdited":"네 그래서 방금 설명한 대로 이 나이브 어프로치 나이브하게 그래프의 인접 행렬을 만들어서 그 인접 행렬을 그대로 뉴럴넷 모델의 입력 값으로 사용하는 방법은 좋지 않습니다."},{"start":400700,"end":413600,"text":"노드가 많아질수록 연산량은 기하급수적으로 많아지고 또 노드의 순서의 의미가 이상하게 반영될 수 있기 때문이죠. 따라서 이런 나이브한 방법으로는 보통 그래프 데이터를 뉴럴넷 모델링을 하지 않습니다. 대신에","confidence":0.9711,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[400930,401280,"노드가"],[401280,401780,"많아질수록"],[401850,402360,"연산량은"],[402370,403000,"기하급수적으로"],[403000,403480,"많아지고"],[403670,403820,"또"],[403830,404160,"노드의"],[404210,405360,"순서의"],[405390,405740,"의미가"],[406350,406760,"이상하게"],[406790,407100,"반영될"],[407130,407234,"수"],[407234,407387,"있기"],[407387,407800,"때문이죠."],[407870,408160,"따라서"],[408170,408340,"이런"],[408390,408860,"나이브한"],[408930,409440,"방법으로는"],[409570,409840,"보통"],[409870,410180,"그래프"],[410180,410600,"데이터를"],[411130,411500,"뉴럴넷"],[411510,412080,"모델링을"],[412090,412300,"하지"],[412300,412720,"않습니다."],[412890,413260,"대신에"]],"textEdited":"노드가 많아질수록 연산량은 기하급수적으로 많아지고 또 노드의 순서의 의미가 이상하게 반영될 수 있기 때문이죠. 따라서 이런 나이브한 방법으로는 보통 그래프 데이터를 뉴럴넷 모델링을 하지 않습니다. 대신에"},{"start":413600,"end":426500,"text":"이 그래프 컨볼루션 뉴럴 네트워크라는 개념이 등장하죠. 이 컨볼루션이라는 개념 이 단어는 이미지 데이터를 다루는 CNN 계열의 모델에서 아마 자주 들어보셨을 것입니다. 먼저 이 우측에 있는 그림을 보십시다.","confidence":0.844,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[413790,413940,"이"],[413940,414300,"그래프"],[414390,414860,"컨볼루션"],[414890,415140,"뉴럴"],[415150,416060,"네트워크라는"],[416130,416420,"개념이"],[416430,416860,"등장하죠."],[416990,417140,"이"],[417190,417860,"컨볼루션이라는"],[417860,418080,"개념"],[418770,418920,"이"],[418920,419300,"단어는"],[419810,420140,"이미지"],[420140,420560,"데이터를"],[420560,420900,"다루는"],[421010,421380,"CNN"],[421410,421720,"계열의"],[421730,422180,"모델에서"],[422670,422860,"아마"],[422970,423220,"자주"],[423220,423740,"들어보셨을"],[423740,424140,"것입니다."],[424310,424487,"먼저"],[424487,424620,"이"],[424620,424927,"우측에"],[424927,425100,"있는"],[425190,425760,"그림을"],[425790,426220,"보십시다."]],"textEdited":"이 그래프 컨볼루션 뉴럴 네트워크라는 개념이 등장하죠. 이 컨볼루션이라는 개념 이 단어는 이미지 데이터를 다루는 CNN 계열의 모델에서 아마 자주 들어보셨을 것입니다. 먼저 이 우측에 있는 그림을 보십시다."},{"start":426500,"end":439400,"text":"아래와 같은 왼쪽의 이미지 데이터 즉 2차원 공간으로 표현되어 있는 이미지 데이터에서 컨볼루션 레이어를 사용하는 방법은 이 2차원 유클리드 공간에 주어진 빨간색 데이터 포인트","confidence":0.9136,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[426790,427120,"아래와"],[427130,427380,"같은"],[427630,428020,"왼쪽의"],[428050,428360,"이미지"],[428360,428660,"데이터"],[428970,429120,"즉"],[429330,429700,"2차원"],[429750,430940,"공간으로"],[430950,431314,"표현되어"],[431314,431480,"있는"],[431510,431800,"이미지"],[431800,432340,"데이터에서"],[432810,433260,"컨볼루션"],[433270,433680,"레이어를"],[433750,434160,"사용하는"],[434210,434540,"방법은"],[435150,435300,"이"],[435300,435700,"2차원"],[435750,436160,"유클리드"],[436170,436520,"공간에"],[436530,436880,"주어진"],[437510,437940,"빨간색"],[438010,438360,"데이터"],[438750,439140,"포인트"]],"textEdited":"아래와 같은 왼쪽의 이미지 데이터 즉 2차원 공간으로 표현되어 있는 이미지 데이터에서 컨볼루션 레이어를 사용하는 방법은 이 2차원 유클리드 공간에 주어진 빨간색 데이터 포인트"},{"start":439400,"end":453000,"text":"를 포함하여 이 주변에 있는 즉 로컬 커넥티비티가 연결되어 있는 데이터를 모아서 컨볼루션을 진행합니다. 그래서 왼쪽에 있는 것이 2디 컨볼루션의 컨볼루션 레이어를 사용하는 방법이고요.","confidence":0.9222,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[439630,439780,"를"],[439810,440240,"포함하여"],[440570,440720,"이"],[440770,441180,"주변에"],[441180,441360,"있는"],[441830,441980,"즉"],[442410,442700,"로컬"],[443070,443740,"커넥티비티가"],[444030,444620,"연결되어"],[444670,444860,"있는"],[445370,445820,"데이터를"],[445820,446140,"모아서"],[446830,447480,"컨볼루션을"],[447530,448040,"진행합니다."],[448270,448480,"그래서"],[448510,448940,"왼쪽에"],[448970,449180,"있는"],[449270,449560,"것이"],[449750,450040,"2디"],[450070,450600,"컨볼루션의"],[450910,451280,"컨볼루션"],[451290,451660,"레이어를"],[452010,452340,"사용하는"],[452340,452840,"방법이고요."]],"textEdited":"를 포함하여 이 주변에 있는 즉 로컬 커넥티비티가 연결되어 있는 데이터를 모아서 컨볼루션을 진행합니다. 그래서 왼쪽에 있는 것이 2디 컨볼루션의 컨볼루션 레이어를 사용하는 방법이고요."},{"start":453000,"end":465300,"text":"이제 이것을 논 뉴클리디언 공간으로 확장시킨 것이 바로 그래프 뉴럴 네트워크의 그래프 컨볼루션입니다. 그래서 이미 이미지에서 다루는 컨볼루션과 굉장히 유사한데요. 이제 이미지 데이터는 방금 설명했듯이","confidence":0.8474,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[453270,453460,"이제"],[453470,454020,"이것을"],[454250,454400,"논"],[454510,455080,"뉴클리디언"],[455130,455500,"공간으로"],[455550,456280,"확장시킨"],[456280,456520,"것이"],[456520,456740,"바로"],[457250,457640,"그래프"],[457890,458140,"뉴럴"],[458140,458660,"네트워크의"],[458710,459000,"그래프"],[459050,459740,"컨볼루션입니다."],[459970,460160,"그래서"],[460160,460320,"이미"],[460750,461280,"이미지에서"],[461280,461640,"다루는"],[461730,462200,"컨볼루션과"],[462200,462440,"굉장히"],[462440,462960,"유사한데요."],[463470,463640,"이제"],[463640,463867,"이미지"],[463867,464180,"데이터는"],[464180,464360,"방금"],[464370,464960,"설명했듯이"]],"textEdited":"이제 이것을 논 뉴클리디언 공간으로 확장시킨 것이 바로 그래프 뉴럴 네트워크의 그래프 컨볼루션입니다. 그래서 이미 이미지에서 다루는 컨볼루션과 굉장히 유사한데요. 이제 이미지 데이터는 방금 설명했듯이"},{"start":465300,"end":478100,"text":"주어진 데이터와 물리적으로 가까이 있는 데이터들을 가지고 컨볼루션을 수행했습니다. 그럼 그래프 컨볼루션 같은 경우에도 주어진 데이터와 가장 가까이 있다는 의미는 바로 하나의 엣지로 연결되어 있는","confidence":0.9414,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[465530,465900,"주어진"],[465910,466340,"데이터와"],[466370,466860,"물리적으로"],[466950,467360,"가까이"],[467370,467540,"있는"],[467590,468120,"데이터들을"],[468120,468440,"가지고"],[468910,469440,"컨볼루션을"],[469440,470060,"수행했습니다."],[470630,470800,"그럼"],[470800,471120,"그래프"],[471490,471880,"컨볼루션"],[471880,472074,"같은"],[472074,472460,"경우에도"],[472750,473080,"주어진"],[473080,473420,"데이터와"],[473420,473620,"가장"],[473650,473907,"가까이"],[473907,474180,"있다는"],[474210,474560,"의미는"],[475470,475740,"바로"],[476190,476520,"하나의"],[476710,477100,"엣지로"],[477150,477640,"연결되어"],[477640,477820,"있는"]],"textEdited":"주어진 데이터와 물리적으로 가까이 있는 데이터들을 가지고 컨볼루션을 수행했습니다. 그럼 그래프 컨볼루션 같은 경우에도 주어진 데이터와 가장 가까이 있다는 의미는 바로 하나의 엣지로 연결되어 있는"},{"start":478100,"end":490700,"text":"이 그래프를 기준으로 다른 이웃들을 의미하죠. 그래서 이 해당 노드를 포함하여서 연결되어 있는 이웃들, 연결되어 있는 노드들을 한꺼번에 모아서 컨볼루션 연산을 수행합니다.","confidence":0.9795,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[478330,478480,"이"],[478530,478960,"그래프를"],[478960,479300,"기준으로"],[479390,479600,"다른"],[480310,481000,"이웃들을"],[481070,481500,"의미하죠."],[482190,482440,"그래서"],[482950,483100,"이"],[483110,483380,"해당"],[483390,483700,"노드를"],[483730,484300,"포함하여서"],[484650,485220,"연결되어"],[485220,485400,"있는"],[485550,485960,"이웃들,"],[486650,487014,"연결되어"],[487014,487160,"있는"],[487160,487640,"노드들을"],[487850,488240,"한꺼번에"],[488250,488600,"모아서"],[489230,489760,"컨볼루션"],[489790,490160,"연산을"],[490160,490660,"수행합니다."]],"textEdited":"이 그래프를 기준으로 다른 이웃들을 의미하죠. 그래서 이 해당 노드를 포함하여서 연결되어 있는 이웃들, 연결되어 있는 노드들을 한꺼번에 모아서 컨볼루션 연산을 수행합니다."},{"start":490700,"end":505300,"text":"그래서 이런 점이 로컬 커넥티비티 측면에서 이미지의 컨볼루션과 그래프의 컨볼루션이 같은 원리를 사용한다는 것을 알 수 있고요. 또한 둘 다 쉐어드 웨이츠를 사용합니다. 우리가 CNN에서 컨볼루션 연산을 배울 때","confidence":0.8995,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[491010,491187,"그래서"],[491187,491380,"이런"],[491390,491660,"점이"],[492070,492360,"로컬"],[492410,493020,"커넥티비티"],[493110,493500,"측면에서"],[493970,494380,"이미지의"],[494430,495240,"컨볼루션과"],[495270,495640,"그래프의"],[495640,496160,"컨볼루션이"],[496750,497020,"같은"],[497090,497380,"원리를"],[497380,497774,"사용한다는"],[497774,498000,"것을"],[498000,498120,"알"],[498120,498194,"수"],[498194,498480,"있고요."],[498750,498980,"또한"],[499790,499940,"둘"],[499940,500060,"다"],[500130,500480,"쉐어드"],[500480,501740,"웨이츠를"],[501740,502200,"사용합니다."],[502850,503080,"우리가"],[503130,503680,"CNN에서"],[503770,504220,"컨볼루션"],[504250,504587,"연산을"],[504587,504780,"배울"],[504810,504960,"때"]],"textEdited":"그래서 이런 점이 로컬 커넥티비티 측면에서 이미지의 컨볼루션과 그래프의 컨볼루션이 같은 원리를 사용한다는 것을 알 수 있고요. 또한 둘 다 쉐어드 웨이츠를 사용합니다. 우리가 CNN에서 컨볼루션 연산을 배울 때"},{"start":505300,"end":517700,"text":"이거 하나를 필터라고 했는데요. 여기서는 보면 3 바이 3의 필터를 사용한 것이죠. 그런데 이 필터가 가진 파라미터는 동일하고 여기서 사용하는 파라미터와 이 옆에 있는","confidence":0.9511,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[505530,505740,"이거"],[505740,506020,"하나를"],[506070,506700,"필터라고"],[506700,507080,"했는데요."],[507790,508140,"여기서는"],[508140,508340,"보면"],[508610,508760,"3"],[508760,508980,"바이"],[508980,509700,"3의"],[509750,510100,"필터를"],[510100,510347,"사용한"],[510347,510640,"것이죠."],[511010,511167,"그런데"],[511167,511300,"이"],[511350,511780,"필터가"],[511950,512160,"가진"],[512190,512740,"파라미터는"],[512750,513240,"동일하고"],[514070,514480,"여기서"],[515170,515580,"사용하는"],[515750,516280,"파라미터와"],[516690,516840,"이"],[516840,517160,"옆에"],[517170,517380,"있는"]],"textEdited":"이거 하나를 필터라고 했는데요. 여기서는 보면 3 바이 3의 필터를 사용한 것이죠. 그런데 이 필터가 가진 파라미터는 동일하고 여기서 사용하는 파라미터와 이 옆에 있는"},{"start":517700,"end":529600,"text":"파라미터는 모두 같은 웨이트를 공유합니다. 마찬가지로 그래프 컨볼루션에서도 이 데이터를 기준으로 만든 이 웨이트와 혹은 다른 데이터를 기준으로 만든 컨볼루션","confidence":0.9594,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[517930,518500,"파라미터는"],[519090,519320,"모두"],[519410,519660,"같은"],[519730,520120,"웨이트를"],[520150,520660,"공유합니다."],[520910,521580,"마찬가지로"],[522170,522480,"그래프"],[522510,523200,"컨볼루션에서도"],[523850,524000,"이"],[524010,524440,"데이터를"],[524440,524860,"기준으로"],[525390,525680,"만든"],[525730,525880,"이"],[525890,526400,"웨이트와"],[526750,526960,"혹은"],[527010,527300,"다른"],[527370,527780,"데이터를"],[527780,528100,"기준으로"],[528110,528400,"만든"],[528970,529420,"컨볼루션"]],"textEdited":"파라미터는 모두 같은 웨이트를 공유합니다. 마찬가지로 그래프 컨볼루션에서도 이 데이터를 기준으로 만든 이 웨이트와 혹은 다른 데이터를 기준으로 만든 컨볼루션"},{"start":529600,"end":533700,"text":"에서도 같은 파라미터 같은 웨이트를 셰어하게 됩니다.","confidence":0.9801,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[529790,530160,"에서도"],[530650,530940,"같은"],[531030,531480,"파라미터"],[531630,531880,"같은"],[531930,532360,"웨이트를"],[532670,533027,"셰어하게"],[533027,533440,"됩니다."]],"textEdited":"에서도 같은 파라미터 같은 웨이트를 셰어하게 됩니다."},{"start":533700,"end":547900,"text":"네 마지막으로 컨볼루션 레이어를 2개 이상 3개 이상 쌓게 되면은 바로 옆에 있는 데이터 포인트 외에 더 멀리 있는 한 칸 더 멀리 있는 정보까지 많이 참고하여서 이 데이터의 레프레젠테이션을 학습할 수 있는데요.","confidence":0.9391,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[533930,534080,"네"],[534080,534600,"마지막으로"],[535290,535760,"컨볼루션"],[535770,536200,"레이어를"],[536590,536880,"2개"],[536930,537160,"이상"],[537450,537680,"3개"],[537680,537860,"이상"],[537910,538167,"쌓게"],[538167,538520,"되면은"],[539030,539300,"바로"],[539300,539507,"옆에"],[539507,539660,"있는"],[539670,539960,"데이터"],[539990,540400,"포인트"],[540610,540880,"외에"],[541250,541400,"더"],[541490,541740,"멀리"],[541740,541940,"있는"],[542310,542414,"한"],[542414,542540,"칸"],[542540,542660,"더"],[542690,542940,"멀리"],[542940,543120,"있는"],[543270,543880,"정보까지"],[544290,544480,"많이"],[544630,545200,"참고하여서"],[545710,545860,"이"],[545860,546260,"데이터의"],[546260,546980,"레프레젠테이션을"],[546980,547280,"학습할"],[547280,547367,"수"],[547367,547720,"있는데요."]],"textEdited":"네 마지막으로 컨볼루션 레이어를 2개 이상 3개 이상 쌓게 되면은 바로 옆에 있는 데이터 포인트 외에 더 멀리 있는 한 칸 더 멀리 있는 정보까지 많이 참고하여서 이 데이터의 레프레젠테이션을 학습할 수 있는데요."},{"start":547900,"end":560500,"text":"이 지씨앤 그래프 콤블루션도 마찬가지로 바로 옆에 연결되어 있는 다른 이웃 노드가 아니라 하나 더 연결되어 있는 이웃 노드까지 확장시켜서 레이어를 2개 이상 사용할 수 있습니다. 그래서 이렇게 세 가지 측면","confidence":0.8666,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[548170,548320,"이"],[548350,548800,"지씨앤"],[548970,549260,"그래프"],[549290,549780,"콤블루션도"],[549780,550320,"마찬가지로"],[550770,551007,"바로"],[551007,551260,"옆에"],[551290,551714,"연결되어"],[551714,551880,"있는"],[551950,552200,"다른"],[552510,552700,"이웃"],[552710,552987,"노드가"],[552987,553260,"아니라"],[553890,554120,"하나"],[554130,554280,"더"],[554370,554780,"연결되어"],[554780,554940,"있는"],[554970,555160,"이웃"],[555160,555700,"노드까지"],[556030,556720,"확장시켜서"],[557170,557600,"레이어를"],[557790,558040,"2개"],[558040,558240,"이상"],[558290,558580,"사용할"],[558580,558674,"수"],[558674,559020,"있습니다."],[559230,559440,"그래서"],[559440,559600,"이렇게"],[559650,559767,"세"],[559767,559960,"가지"],[560010,560240,"측면"]],"textEdited":"이 지씨앤 그래프 콤블루션도 마찬가지로 바로 옆에 연결되어 있는 다른 이웃 노드가 아니라 하나 더 연결되어 있는 이웃 노드까지 확장시켜서 레이어를 2개 이상 사용할 수 있습니다. 그래서 이렇게 세 가지 측면"},{"start":560500,"end":574600,"text":"바로 옆에 있는 연결되어 있는 데이터를 의 정보를 활용한다. 그리고 그 필터와 같은 웨이트를 셰어한다. 그리고 레이어를 2개 이상 쌓을 수 있다라는 측면에서 이 컴플루션 네트워크라고 볼 수 있고요. 그래서 연산량을 줄이면서","confidence":0.924,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[560730,560960,"바로"],[560960,561220,"옆에"],[561220,561360,"있는"],[561370,561900,"연결되어"],[561900,562080,"있는"],[562310,562800,"데이터를"],[563110,563260,"의"],[563330,563680,"정보를"],[563680,564120,"활용한다."],[564310,564600,"그리고"],[565230,565380,"그"],[565450,565960,"필터와"],[565970,566200,"같은"],[566290,566760,"웨이트를"],[566850,567320,"셰어한다."],[567470,567720,"그리고"],[567770,568180,"레이어를"],[568250,568467,"2개"],[568467,568640,"이상"],[568670,568920,"쌓을"],[568930,569080,"수"],[569080,569860,"있다라는"],[569930,570320,"측면에서"],[570650,570800,"이"],[570810,571200,"컴플루션"],[571200,572120,"네트워크라고"],[572130,572280,"볼"],[572310,572427,"수"],[572427,572740,"있고요."],[573030,573220,"그래서"],[573230,573700,"연산량을"],[573710,574260,"줄이면서"]],"textEdited":"바로 옆에 있는 연결되어 있는 데이터를 의 정보를 활용한다. 그리고 그 필터와 같은 웨이트를 셰어한다. 그리고 레이어를 2개 이상 쌓을 수 있다라는 측면에서 이 컴플루션 네트워크라고 볼 수 있고요. 그래서 연산량을 줄이면서"},{"start":574600,"end":584000,"text":"노드끼리 연결된 관계를 표현할 수 있고 또 그보다 더 멀리 있는 간접 관계를 특징으로 추출할 수 있습니다. 그래서 앞으로 우리가 보통 GNN","confidence":0.9607,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[574830,575800,"노드끼리"],[575810,576240,"연결된"],[576270,576560,"관계를"],[576560,576880,"표현할"],[576880,576987,"수"],[576987,577260,"있고"],[577610,577760,"또"],[578130,578400,"그보다"],[578400,578540,"더"],[578610,578940,"멀리"],[579090,579280,"있는"],[579470,579740,"간접"],[579790,580140,"관계를"],[580670,581100,"특징으로"],[581190,581520,"추출할"],[581520,581607,"수"],[581607,581940,"있습니다."],[582130,582340,"그래서"],[582340,582660,"앞으로"],[582660,582880,"우리가"],[582970,583220,"보통"],[583290,583740,"GNN"]],"textEdited":"노드끼리 연결된 관계를 표현할 수 있고 또 그보다 더 멀리 있는 간접 관계를 특징으로 추출할 수 있습니다. 그래서 앞으로 우리가 보통 GNN"},{"start":584000,"end":596500,"text":"계열의 추천 모델이다라고 하면은 보통 이 그래프 컨볼루션 뉴럴 네트워크를 사용한 모델임을 기억하시길 바랍니다. 그래서 이번 모델은 GNN을 기반으로 유저 아이템의 상호작용 즉","confidence":0.9335,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[584290,584620,"계열의"],[584630,584880,"추천"],[584880,585620,"모델이다라고"],[585620,585900,"하면은"],[585970,586240,"보통"],[586590,586740,"이"],[586740,587060,"그래프"],[587110,587540,"컨볼루션"],[587550,587820,"뉴럴"],[587820,588460,"네트워크를"],[588490,588800,"사용한"],[589290,589780,"모델임을"],[589890,590420,"기억하시길"],[590420,590820,"바랍니다."],[591790,592080,"그래서"],[592310,592520,"이번"],[592590,592960,"모델은"],[593210,593780,"GNN을"],[593810,594280,"기반으로"],[594630,594940,"유저"],[594970,595440,"아이템의"],[595490,595980,"상호작용"],[596170,596320,"즉"]],"textEdited":"계열의 추천 모델이다라고 하면은 보통 이 그래프 컨볼루션 뉴럴 네트워크를 사용한 모델임을 기억하시길 바랍니다. 그래서 이번 모델은 GNN을 기반으로 유저 아이템의 상호작용 즉"},{"start":596500,"end":602800,"text":"이 상호 작용을 콜라베이티브 시그널이라고 본 논문에서 표현하고 있는데요. 이 콜라베이트 시그널을 추출하는 모델인데요.","confidence":0.8872,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[596710,596860,"이"],[596890,597080,"상호"],[597080,597360,"작용을"],[597410,598060,"콜라베이티브"],[598090,598800,"시그널이라고"],[598930,599080,"본"],[599110,599480,"논문에서"],[599510,599847,"표현하고"],[599847,600200,"있는데요."],[600570,600720,"이"],[600790,601240,"콜라베이트"],[601250,601640,"시그널을"],[601730,602100,"추출하는"],[602100,602600,"모델인데요."]],"textEdited":"이 상호 작용을 콜라베이티브 시그널이라고 본 논문에서 표현하고 있는데요. 이 콜라베이트 시그널을 추출하는 모델인데요."},{"start":602800,"end":617100,"text":"그래프가 가진 장점을 사용해서 유저와 아이템 간의 콜라베이트 시그널을 인베딩 단 즉 인베딩 레이어에서 직접 추출한 접근법을 제시한 논문입니다. 이 논문이 발표된 이후 이 뉴럴 글래스 컬라버레이트 필터링이 발표된 이후에","confidence":0.9287,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[603070,603500,"그래프가"],[603500,603680,"가진"],[603810,604380,"장점을"],[604450,604920,"사용해서"],[605270,605660,"유저와"],[605750,606140,"아이템"],[606150,606400,"간의"],[606470,606920,"콜라베이트"],[606970,607400,"시그널을"],[607790,608260,"인베딩"],[608550,608700,"단"],[609030,609180,"즉"],[609270,609640,"인베딩"],[609710,610180,"레이어에서"],[610330,610580,"직접"],[610610,610960,"추출한"],[611250,611740,"접근법을"],[611740,612020,"제시한"],[612030,612500,"논문입니다."],[612790,612940,"이"],[612950,613300,"논문이"],[613350,613720,"발표된"],[613990,614200,"이후"],[614670,614820,"이"],[614820,615060,"뉴럴"],[615060,615340,"글래스"],[615390,615800,"컬라버레이트"],[615830,616280,"필터링이"],[616290,616600,"발표된"],[616600,616880,"이후에"]],"textEdited":"그래프가 가진 장점을 사용해서 유저와 아이템 간의 콜라베이트 시그널을 인베딩 단 즉 인베딩 레이어에서 직접 추출한 접근법을 제시한 논문입니다. 이 논문이 발표된 이후 이 뉴럴 글래스 컬라버레이트 필터링이 발표된 이후에"},{"start":617100,"end":627000,"text":"물론 이보다 더 가볍고 좋은 성능을 가진 GNN 기반 추천 모델이 많이 발표되었지만 이 논문을 통해서 처음 그래프 컨볼루션 뉴럴 네트워크가","confidence":0.9383,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[617370,617620,"물론"],[618050,618340,"이보다"],[618340,618460,"더"],[618470,618860,"가볍고"],[618860,619020,"좋은"],[619130,619440,"성능을"],[619440,619660,"가진"],[620190,620600,"GNN"],[620650,620880,"기반"],[621290,621560,"추천"],[621560,621807,"모델이"],[621807,621980,"많이"],[622030,622660,"발표되었지만"],[623450,623600,"이"],[623610,623980,"논문을"],[623990,624320,"통해서"],[624450,624720,"처음"],[625190,625540,"그래프"],[625610,626060,"컨볼루션"],[626070,626280,"뉴럴"],[626280,626820,"네트워크가"]],"textEdited":"물론 이보다 더 가볍고 좋은 성능을 가진 GNN 기반 추천 모델이 많이 발표되었지만 이 논문을 통해서 처음 그래프 컨볼루션 뉴럴 네트워크가"},{"start":627000,"end":641600,"text":"추천 시스템을 풀기에 좋은 모델이라는 것을 밝혔기 때문에 그런 점에서 이 논문은 굉장히 연구적으로 의미가 있습니다. 논문의 등장 배경은 다음과 같습니다. 그동안 우리가 배웠던 뉴럴 그래프 컬라버레이트 필터링이 아닌","confidence":0.9543,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[627250,627520,"추천"],[627530,628020,"시스템을"],[628190,628580,"풀기에"],[628630,628820,"좋은"],[628890,629367,"모델이라는"],[629367,629640,"것을"],[629690,630060,"밝혔기"],[630060,630380,"때문에"],[630890,631080,"그런"],[631080,631380,"점에서"],[631410,631560,"이"],[631560,631880,"논문은"],[631930,632260,"굉장히"],[633030,633520,"연구적으로"],[633550,633860,"의미가"],[633860,634240,"있습니다."],[634730,635100,"논문의"],[635150,635400,"등장"],[635490,635800,"배경은"],[635870,636180,"다음과"],[636180,636580,"같습니다."],[637570,637860,"그동안"],[637870,638100,"우리가"],[638110,638520,"배웠던"],[639490,639780,"뉴럴"],[639780,640020,"그래프"],[640050,640440,"컬라버레이트"],[640440,640774,"필터링이"],[640774,640960,"아닌"]],"textEdited":"추천 시스템을 풀기에 좋은 모델이라는 것을 밝혔기 때문에 그런 점에서 이 논문은 굉장히 연구적으로 의미가 있습니다. 논문의 등장 배경은 다음과 같습니다. 그동안 우리가 배웠던 뉴럴 그래프 컬라버레이트 필터링이 아닌"},{"start":641600,"end":654600,"text":"3강부터 배웠던 컬래버레이트 필터링, 제일 기본적인 매트리스 팩토라이제이션 같은 모델을 살펴보면 크게 두 가지 특징을 모델이 잘 학습해야 한다는 것을 알 수입니다. 이 추천 시스템 모델이라고 하면은 이 아래 있는 두 가지","confidence":0.9406,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[642070,642547,"3강부터"],[642547,642820,"배웠던"],[642890,643320,"컬래버레이트"],[643320,643680,"필터링,"],[643890,644080,"제일"],[644110,644500,"기본적인"],[644530,644880,"매트리스"],[644880,645360,"팩토라이제이션"],[645360,645580,"같은"],[645580,646120,"모델을"],[646130,646600,"살펴보면"],[647470,647700,"크게"],[647750,647900,"두"],[647900,648160,"가지"],[648290,648680,"특징을"],[649250,649620,"모델이"],[649620,649740,"잘"],[649790,650134,"학습해야"],[650134,650387,"한다는"],[650387,650620,"것을"],[650620,650707,"알"],[650707,651040,"수입니다."],[651530,651680,"이"],[651680,651940,"추천"],[651940,652280,"시스템"],[652280,652740,"모델이라고"],[652770,653060,"하면은"],[653330,653447,"이"],[653447,653640,"아래"],[653730,653900,"있는"],[653930,654080,"두"],[654080,654320,"가지"]],"textEdited":"3강부터 배웠던 컬래버레이트 필터링, 제일 기본적인 매트리스 팩토라이제이션 같은 모델을 살펴보면 크게 두 가지 특징을 모델이 잘 학습해야 한다는 것을 알 수입니다. 이 추천 시스템 모델이라고 하면은 이 아래 있는 두 가지"},{"start":654600,"end":666500,"text":"첫 번째는 유저 아이템의 인베딩을 잘 학습해야 합니다. 유저 아이템은 계속해서 원핫 인코딩으로 처음에 표현된다고 했고, 그것을 우리가 어떤 댄스 한 인베딩으로 학습한다고 했는데요.","confidence":0.9499,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[654950,655100,"첫"],[655110,655480,"번째는"],[655910,656200,"유저"],[656270,656720,"아이템의"],[656770,657300,"인베딩을"],[657310,657460,"잘"],[657490,657860,"학습해야"],[657860,658100,"합니다."],[658530,658800,"유저"],[658830,659300,"아이템은"],[659790,660160,"계속해서"],[660190,660460,"원핫"],[660550,661100,"인코딩으로"],[661130,661420,"처음에"],[661430,661987,"표현된다고"],[661987,662280,"했고,"],[662670,662967,"그것을"],[662967,663200,"우리가"],[663550,663780,"어떤"],[663870,664194,"댄스"],[664194,664320,"한"],[664390,664880,"인베딩으로"],[665570,666027,"학습한다고"],[666027,666400,"했는데요."]],"textEdited":"첫 번째는 유저 아이템의 인베딩을 잘 학습해야 합니다. 유저 아이템은 계속해서 원핫 인코딩으로 처음에 표현된다고 했고, 그것을 우리가 어떤 댄스 한 인베딩으로 학습한다고 했는데요."},{"start":666500,"end":680700,"text":"엠에프부터 시작하여서 유저와 아이템을 어떤 레이턴트 팩터 즉 인베딩으로 표현하는 기법은 딥러닝 모델로 넘어와서도 동일하게 인베딩 레이어를 사용해서 이어져 오고 있습니다. 두 번째로는 유저와 아이템의 상호 작용입니다.","confidence":0.9353,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[667470,668060,"엠에프부터"],[668060,668560,"시작하여서"],[669010,669400,"유저와"],[669450,669880,"아이템을"],[669890,670060,"어떤"],[670130,670580,"레이턴트"],[670590,670880,"팩터"],[671130,671280,"즉"],[671370,671900,"인베딩으로"],[671910,672280,"표현하는"],[672310,672680,"기법은"],[673350,673760,"딥러닝"],[673760,674060,"모델로"],[674060,674560,"넘어와서도"],[674610,675060,"동일하게"],[675390,675720,"인베딩"],[675750,676080,"레이어를"],[676080,676500,"사용해서"],[676690,676967,"이어져"],[676967,677180,"오고"],[677180,677540,"있습니다."],[677930,678080,"두"],[678080,678540,"번째로는"],[678850,679200,"유저와"],[679250,679700,"아이템의"],[679790,680020,"상호"],[680030,680620,"작용입니다."]],"textEdited":"엠에프부터 시작하여서 유저와 아이템을 어떤 레이턴트 팩터 즉 인베딩으로 표현하는 기법은 딥러닝 모델로 넘어와서도 동일하게 인베딩 레이어를 사용해서 이어져 오고 있습니다. 두 번째로는 유저와 아이템의 상호 작용입니다."},{"start":680700,"end":692200,"text":"대표적으로 메트릭스 펙토라이제이션에서는 유저 아이템을 임베딩한 이후에 그 유저 아이템을 내적 닷 프로덕트 하여서 그 유저와 아이템의 상호 작용을 리니어하게 표현했습니다.","confidence":0.8737,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[681270,681720,"대표적으로"],[681720,682047,"메트릭스"],[682047,682960,"펙토라이제이션에서는"],[683470,683740,"유저"],[683790,684180,"아이템을"],[684210,684740,"임베딩한"],[684850,685220,"이후에"],[685450,685600,"그"],[685600,685820,"유저"],[685820,686240,"아이템을"],[686670,686940,"내적"],[687170,687320,"닷"],[687390,687780,"프로덕트"],[687780,688120,"하여서"],[688690,688807,"그"],[688807,689080,"유저와"],[689080,689440,"아이템의"],[689440,689620,"상호"],[689620,689960,"작용을"],[690670,691200,"리니어하게"],[691230,691940,"표현했습니다."]],"textEdited":"대표적으로 메트릭스 펙토라이제이션에서는 유저 아이템을 임베딩한 이후에 그 유저 아이템을 내적 닷 프로덕트 하여서 그 유저와 아이템의 상호 작용을 리니어하게 표현했습니다."},{"start":692200,"end":705700,"text":"문제는 엠프를 포함하여서 기존의 뉴럴넷 기반의 CF 모델 즉 이런 뉴럴 컬라버리티 필터링 계열의 모델들은 유저와 아이템의 상호 작용을 인베딩 단계에서 접근하지 못했습니다.","confidence":0.8562,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[692650,693100,"문제는"],[693630,694140,"엠프를"],[694150,694680,"포함하여서"],[695150,695460,"기존의"],[695460,695840,"뉴럴넷"],[695930,696260,"기반의"],[696270,696580,"CF"],[696580,696800,"모델"],[697050,697200,"즉"],[697290,697460,"이런"],[697490,697760,"뉴럴"],[697830,698340,"컬라버리티"],[698810,699200,"필터링"],[699370,699720,"계열의"],[700110,700600,"모델들은"],[701170,701540,"유저와"],[701590,702080,"아이템의"],[702530,702740,"상호"],[702750,703100,"작용을"],[703730,704080,"인베딩"],[704130,704540,"단계에서"],[704540,704920,"접근하지"],[704920,705560,"못했습니다."]],"textEdited":"문제는 엠프를 포함하여서 기존의 뉴럴넷 기반의 CF 모델 즉 이런 뉴럴 컬라버리티 필터링 계열의 모델들은 유저와 아이템의 상호 작용을 인베딩 단계에서 접근하지 못했습니다."},{"start":705700,"end":716100,"text":"방금 설명했듯이 매트리스 팩터라이제이션에서도 pu 유저의 인베딩과 qi 아이템의 인베딩을 각각 구한 다음에 그 둘의 내적을 통해서","confidence":0.9193,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[706010,706260,"방금"],[706290,706880,"설명했듯이"],[707450,707840,"매트리스"],[707840,708900,"팩터라이제이션에서도"],[709650,710020,"pu"],[710230,710620,"유저의"],[710630,711140,"인베딩과"],[711670,712020,"qi"],[712210,712680,"아이템의"],[712690,713160,"인베딩을"],[713230,713540,"각각"],[713550,713820,"구한"],[713820,714140,"다음에"],[714610,714760,"그"],[714760,715020,"둘의"],[715030,715440,"내적을"],[715450,715780,"통해서"]],"textEdited":"방금 설명했듯이 매트리스 팩터라이제이션에서도 pu 유저의 인베딩과 qi 아이템의 인베딩을 각각 구한 다음에 그 둘의 내적을 통해서"},{"start":716100,"end":729700,"text":"상호 작용을 표현하기 때문에 인베딩과 상호 작용이 분리되어 있습니다. 마찬가지로 뉴럴 컬라버티 필터링 모델도 인베딩이 일어난 이후에 그 인베딩을 컨케이트네이트 하기 때문에 인베딩과 상호 작용이 분리되어 있는 것이죠.","confidence":0.9056,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[716370,716580,"상호"],[716590,716940,"작용을"],[716970,717307,"표현하기"],[717307,717680,"때문에"],[718290,718940,"인베딩과"],[719130,719320,"상호"],[719330,719640,"작용이"],[719670,720060,"분리되어"],[720060,720440,"있습니다."],[720710,721240,"마찬가지로"],[721240,721480,"뉴럴"],[721510,721900,"컬라버티"],[721900,722220,"필터링"],[722220,722640,"모델도"],[723130,723620,"인베딩이"],[723620,723860,"일어난"],[723910,724240,"이후에"],[724430,724580,"그"],[724610,725100,"인베딩을"],[725290,726000,"컨케이트네이트"],[726550,726747,"하기"],[726747,727120,"때문에"],[727770,728280,"인베딩과"],[728350,728540,"상호"],[728540,728820,"작용이"],[728830,729160,"분리되어"],[729160,729287,"있는"],[729287,729600,"것이죠."]],"textEdited":"상호 작용을 표현하기 때문에 인베딩과 상호 작용이 분리되어 있습니다. 마찬가지로 뉴럴 컬라버티 필터링 모델도 인베딩이 일어난 이후에 그 인베딩을 컨케이트네이트 하기 때문에 인베딩과 상호 작용이 분리되어 있는 것이죠."},{"start":729700,"end":743800,"text":"그래서 각각의 인베딩이 상호 작용의 서브 옵티멀하게 학습되기 때문에 그 결과 모델이 더 정확한 표현형을 가지지 못하고 추천의 능력이 조금 떨어지게 되는 것입니다.","confidence":0.9726,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[729970,730240,"그래서"],[730550,730880,"각각의"],[730930,731440,"인베딩이"],[731670,731880,"상호"],[731890,732200,"작용의"],[732270,732520,"서브"],[732520,733060,"옵티멀하게"],[733530,733947,"학습되기"],[733947,734300,"때문에"],[734630,734780,"그"],[734990,735320,"결과"],[735930,736360,"모델이"],[737030,737180,"더"],[737350,737760,"정확한"],[738330,739440,"표현형을"],[739450,739780,"가지지"],[739780,740160,"못하고"],[740510,741420,"추천의"],[742010,742340,"능력이"],[742340,742540,"조금"],[742570,742980,"떨어지게"],[742980,743200,"되는"],[743200,743700,"것입니다."]],"textEdited":"그래서 각각의 인베딩이 상호 작용의 서브 옵티멀하게 학습되기 때문에 그 결과 모델이 더 정확한 표현형을 가지지 못하고 추천의 능력이 조금 떨어지게 되는 것입니다."},{"start":743800,"end":758100,"text":"그래서 이 GNN 추천 모델은 유저와 아이템의 상호 작용이 유저 아이템을 임베딩 시킬 때부터 반영될 수 있도록 모델을 설계하였습니다. 먼저 아래 그림과 같이 유저가 아이템을 소비했다는 데이터","confidence":0.9005,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[744090,744380,"그래서"],[744530,744680,"이"],[744710,745140,"GNN"],[745210,745480,"추천"],[745490,745860,"모델은"],[746510,746920,"유저와"],[746990,747460,"아이템의"],[747530,747740,"상호"],[747750,748100,"작용이"],[748450,748720,"유저"],[748720,749120,"아이템을"],[749170,749520,"임베딩"],[749610,749900,"시킬"],[749950,750400,"때부터"],[750930,751400,"반영될"],[751490,751594,"수"],[751594,751920,"있도록"],[752010,752380,"모델을"],[752430,753200,"설계하였습니다."],[753550,753800,"먼저"],[753850,754060,"아래"],[754130,754480,"그림과"],[754480,754800,"같이"],[755350,755760,"유저가"],[756250,756720,"아이템을"],[756750,757340,"소비했다는"],[757390,757760,"데이터"]],"textEdited":"그래서 이 GNN 추천 모델은 유저와 아이템의 상호 작용이 유저 아이템을 임베딩 시킬 때부터 반영될 수 있도록 모델을 설계하였습니다. 먼저 아래 그림과 같이 유저가 아이템을 소비했다는 데이터"},{"start":758100,"end":770300,"text":"를 다음과 같은 그래프 유저 아이템 인터랙션 그래프라고 표현했습니다. 그래서 왼쪽에 유저 3명이 있고요. 오른쪽에 아이템 5개가 있죠. 그래서 이 edg는 유저 1이 아이템 1을 소비했다.","confidence":0.9111,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[758310,758460,"를"],[758950,759247,"다음과"],[759247,759440,"같은"],[759490,759860,"그래프"],[760150,760460,"유저"],[760530,760860,"아이템"],[760930,761380,"인터랙션"],[761390,761940,"그래프라고"],[762090,762720,"표현했습니다."],[762950,763140,"그래서"],[763410,763860,"왼쪽에"],[763990,764240,"유저"],[764310,764940,"3명이"],[764940,765240,"있고요."],[765290,765680,"오른쪽에"],[765680,765920,"아이템"],[766030,766340,"5개가"],[766350,766600,"있죠."],[767030,767220,"그래서"],[767220,767340,"이"],[767370,767800,"edg는"],[767970,768280,"유저"],[768310,768660,"1이"],[768890,769240,"아이템"],[769250,769540,"1을"],[769550,770080,"소비했다."]],"textEdited":"를 다음과 같은 그래프 유저 아이템 인터랙션 그래프라고 표현했습니다. 그래서 왼쪽에 유저 3명이 있고요. 오른쪽에 아이템 5개가 있죠. 그래서 이 edg는 유저 1이 아이템 1을 소비했다."},{"start":770300,"end":782800,"text":"유저 3이 아이템 4를 소비했다라는 데이터입니다. 단순히 이렇게 유저 1이 아이템 1 2 3를 소비했다. 유저 2가 아이템 2 4 5를 소비했다는 데이터만을 가지고는","confidence":0.9025,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[770530,770820,"유저"],[770820,771100,"3이"],[771570,771940,"아이템"],[772010,772300,"4를"],[772330,773220,"소비했다라는"],[773630,774600,"데이터입니다."],[775010,775327,"단순히"],[775327,775540,"이렇게"],[775610,775880,"유저"],[776050,776360,"1이"],[776450,776780,"아이템"],[776810,776960,"1"],[776970,777120,"2"],[777120,777380,"3를"],[777380,777820,"소비했다."],[778130,778420,"유저"],[778450,778700,"2가"],[779010,779380,"아이템"],[779570,779720,"2"],[779890,780040,"4"],[780190,780620,"5를"],[780630,781260,"소비했다는"],[781310,781920,"데이터만을"],[781920,782360,"가지고는"]],"textEdited":"유저 3이 아이템 4를 소비했다라는 데이터입니다. 단순히 이렇게 유저 1이 아이템 1 2 3를 소비했다. 유저 2가 아이템 2 4 5를 소비했다는 데이터만을 가지고는"},{"start":782800,"end":791000,"text":"이 모든 유저와 아이템 간의 상호작용을 표현할 수 없습니다. 예를 들면 유저 1은 유저 1 2 3랑 연결되어 있긴 하지만 i","confidence":0.892,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[783070,783220,"이"],[783270,783520,"모든"],[783610,783940,"유저와"],[783950,784300,"아이템"],[784610,784880,"간의"],[784950,785500,"상호작용을"],[785530,785860,"표현할"],[785860,785934,"수"],[785934,786320,"없습니다."],[786490,786647,"예를"],[786647,786860,"들면"],[786870,787140,"유저"],[787310,787660,"1은"],[787910,788180,"유저"],[788510,788660,"1"],[788710,788860,"2"],[788860,789180,"3랑"],[789210,789534,"연결되어"],[789534,789680,"있긴"],[789680,789960,"하지만"],[790550,790700,"i"]],"textEdited":"이 모든 유저와 아이템 간의 상호작용을 표현할 수 없습니다. 예를 들면 유저 1은 유저 1 2 3랑 연결되어 있긴 하지만 i"},{"start":791000,"end":805000,"text":"유저 4 아이템 4 아이템 파랑 연결되어 있지 않기 때문에 이 유저 아이템의 개수가 점점 더 많아질수록 유저와 아이템 페어의 컬라버레이트 시그널 즉 어떤 상호작용을 놓치는 경우가 생기게 되죠. 그래서 이러한 문제를","confidence":0.9196,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[791190,791440,"유저"],[791470,791620,"4"],[791710,792060,"아이템"],[792130,792280,"4"],[792310,792600,"아이템"],[792630,793000,"파랑"],[793000,793294,"연결되어"],[793294,793460,"있지"],[793460,793680,"않기"],[793680,794040,"때문에"],[794570,794687,"이"],[794687,794900,"유저"],[794900,795340,"아이템의"],[795650,795960,"개수가"],[795990,796320,"점점"],[796320,796440,"더"],[796440,797040,"많아질수록"],[797430,797880,"유저와"],[798030,798380,"아이템"],[798470,798820,"페어의"],[799350,799820,"컬라버레이트"],[799850,800160,"시그널"],[800270,800420,"즉"],[800550,800720,"어떤"],[800850,801460,"상호작용을"],[801970,802580,"놓치는"],[802630,802940,"경우가"],[802950,803207,"생기게"],[803207,803440,"되죠."],[803750,803940,"그래서"],[803950,804220,"이러한"],[804270,804640,"문제를"]],"textEdited":"유저 4 아이템 4 아이템 파랑 연결되어 있지 않기 때문에 이 유저 아이템의 개수가 점점 더 많아질수록 유저와 아이템 페어의 컬라버레이트 시그널 즉 어떤 상호작용을 놓치는 경우가 생기게 되죠. 그래서 이러한 문제를"},{"start":805000,"end":819100,"text":"지엔엔이 가진 특성을 사용하여서 경로가 1보다 큰 하이워드 커넥티비티를 임베딩하여서 모델에 사용하였습니다. 그래서 이 부분을 이 그림을 통해 좀 자세히 설명해 보겠습니다.","confidence":0.8176,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[805270,805760,"지엔엔이"],[805770,806000,"가진"],[806190,806560,"특성을"],[806870,807440,"사용하여서"],[808210,808660,"경로가"],[809070,809440,"1보다"],[809510,809660,"큰"],[810130,810520,"하이워드"],[810520,811800,"커넥티비티를"],[811910,812660,"임베딩하여서"],[812750,813100,"모델에"],[813390,814100,"사용하였습니다."],[814750,814940,"그래서"],[814940,815060,"이"],[815070,815420,"부분을"],[816710,816860,"이"],[816860,817140,"그림을"],[817140,817340,"통해"],[817340,817460,"좀"],[817490,817820,"자세히"],[817850,818220,"설명해"],[818220,818720,"보겠습니다."]],"textEdited":"지엔엔이 가진 특성을 사용하여서 경로가 1보다 큰 하이워드 커넥티비티를 임베딩하여서 모델에 사용하였습니다. 그래서 이 부분을 이 그림을 통해 좀 자세히 설명해 보겠습니다."},{"start":819100,"end":834000,"text":"먼저 이 유저 1이 소비한 아이템은 총 3개가 되겠죠. 아이템 1 아이템 2 아이템 3입니다. 그래서 패스가 1이라는 뜻은 바로 다이렉트로 연결되어 있는 엣지가 하나라는 것이죠.","confidence":0.8799,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[819930,820200,"먼저"],[820730,820880,"이"],[820890,821220,"유저"],[821230,822860,"1이"],[822870,823240,"소비한"],[823290,823780,"아이템은"],[824210,824360,"총"],[825190,825487,"3개가"],[825487,825820,"되겠죠."],[826310,826660,"아이템"],[826670,826820,"1"],[826930,827320,"아이템"],[827410,827560,"2"],[827630,827920,"아이템"],[827950,828420,"3입니다."],[828770,828960,"그래서"],[829010,829600,"패스가"],[829850,830340,"1이라는"],[830350,830600,"뜻은"],[831170,831480,"바로"],[831970,832400,"다이렉트로"],[832400,832720,"연결되어"],[832720,832880,"있는"],[832890,833200,"엣지가"],[833200,833547,"하나라는"],[833547,833860,"것이죠."]],"textEdited":"먼저 이 유저 1이 소비한 아이템은 총 3개가 되겠죠. 아이템 1 아이템 2 아이템 3입니다. 그래서 패스가 1이라는 뜻은 바로 다이렉트로 연결되어 있는 엣지가 하나라는 것이죠."},{"start":834000,"end":848600,"text":"그래서 여기서 이제 길이가 2인 패스로 확장하게 되면은 아이템 2에서 아이템 2를 소비한 다른 유저인 유저 2로 확장될 수 있고요. 아이템 쓰는 아이템 3를 소비한 다른 유저인 유저 3로 확장될 수 있습니다. 또 이렇게 해서 랭스 3까지","confidence":0.9276,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[834150,834347,"그래서"],[834347,834620,"여기서"],[834630,834800,"이제"],[834830,835140,"길이가"],[835270,835520,"2인"],[835630,835960,"패스로"],[835990,836367,"확장하게"],[836367,836720,"되면은"],[837150,837540,"아이템"],[837610,838020,"2에서"],[838590,838900,"아이템"],[838910,839127,"2를"],[839127,839400,"소비한"],[839410,839620,"다른"],[839630,839960,"유저인"],[839990,840240,"유저"],[840250,840640,"2로"],[840650,841000,"확장될"],[841050,841167,"수"],[841167,841500,"있고요."],[841850,842160,"아이템"],[842210,842560,"쓰는"],[842730,843040,"아이템"],[843050,843360,"3를"],[843360,843640,"소비한"],[843650,843840,"다른"],[843840,844120,"유저인"],[844190,844440,"유저"],[844470,844840,"3로"],[845330,845680,"확장될"],[845680,845787,"수"],[845787,846160,"있습니다."],[846330,846480,"또"],[846480,846667,"이렇게"],[846667,846860,"해서"],[846910,847300,"랭스"],[847530,848080,"3까지"]],"textEdited":"그래서 여기서 이제 길이가 2인 패스로 확장하게 되면은 아이템 2에서 아이템 2를 소비한 다른 유저인 유저 2로 확장될 수 있고요. 아이템 쓰는 아이템 3를 소비한 다른 유저인 유저 3로 확장될 수 있습니다. 또 이렇게 해서 랭스 3까지"},{"start":848600,"end":861600,"text":"확장될 수 있겠죠. 그래서 이 그래프를 통해 세 가지를 알 수 있는데요. 먼저 유저 1과 유저 e는 아이템 2를 가지고 서로 상호작용하기 때문에 이 둘이 유사하다라는 정보","confidence":0.9758,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[849030,849580,"확장될"],[849690,849840,"수"],[849910,850280,"있겠죠."],[850630,850820,"그래서"],[850820,850940,"이"],[850950,851340,"그래프를"],[851340,851580,"통해"],[851750,851867,"세"],[851867,852180,"가지를"],[852190,852340,"알"],[852340,852447,"수"],[852447,852840,"있는데요."],[853630,853940,"먼저"],[854310,854600,"유저"],[854670,855440,"1과"],[855510,855780,"유저"],[855870,856260,"e는"],[856730,857120,"아이템"],[857230,857620,"2를"],[857630,858000,"가지고"],[858230,858480,"서로"],[858530,859167,"상호작용하기"],[859167,859500,"때문에"],[859590,859740,"이"],[859740,859980,"둘이"],[860410,861220,"유사하다라는"],[861290,861580,"정보"]],"textEdited":"확장될 수 있겠죠. 그래서 이 그래프를 통해 세 가지를 알 수 있는데요. 먼저 유저 1과 유저 e는 아이템 2를 가지고 서로 상호작용하기 때문에 이 둘이 유사하다라는 정보"},{"start":861600,"end":871700,"text":"이 그래프에서 볼 수 있습니다. 두 번째는 유저 2가 이미 소비한 아이템 4 아이템 5가 시그널이 전달되어서 다음과 같이 유저 원에게","confidence":0.9412,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[862030,862180,"이"],[862180,863020,"그래프에서"],[863210,863360,"볼"],[863360,863454,"수"],[863454,863820,"있습니다."],[863950,864100,"두"],[864110,864500,"번째는"],[864990,865260,"유저"],[865270,865560,"2가"],[865570,865740,"이미"],[865830,866180,"소비한"],[866250,866620,"아이템"],[866710,866860,"4"],[866930,867300,"아이템"],[867450,868380,"5가"],[868810,869220,"시그널이"],[869250,869880,"전달되어서"],[870230,870560,"다음과"],[870560,870800,"같이"],[870830,871080,"유저"],[871170,871560,"원에게"]],"textEdited":"이 그래프에서 볼 수 있습니다. 두 번째는 유저 2가 이미 소비한 아이템 4 아이템 5가 시그널이 전달되어서 다음과 같이 유저 원에게"},{"start":871700,"end":885100,"text":"전달될 수 있기 때문에 아이템 4 아이템 5가 유저 원에게 추천될 수 있는 확률이 높다는 것을 또 이 그래프에서 의미합니다. 마지막으로 이 레이어 3에 있는 유저 1에서 아이템 2, 유저 2로","confidence":0.9233,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[872010,872380,"전달될"],[872380,872467,"수"],[872467,872627,"있기"],[872627,872980,"때문에"],[873350,873700,"아이템"],[873730,873880,"4"],[873970,874240,"아이템"],[874310,874740,"5가"],[875290,875560,"유저"],[875590,876260,"원에게"],[876330,876740,"추천될"],[876770,876874,"수"],[876874,877040,"있는"],[877150,877480,"확률이"],[877770,878207,"높다는"],[878207,878440,"것을"],[878450,878600,"또"],[878600,878707,"이"],[878707,879120,"그래프에서"],[879130,879620,"의미합니다."],[879890,880380,"마지막으로"],[880490,880640,"이"],[880650,880940,"레이어"],[880970,881360,"3에"],[881360,881560,"있는"],[882090,882380,"유저"],[882790,883160,"1에서"],[883230,883520,"아이템"],[883670,883820,"2,"],[884250,884500,"유저"],[884510,884840,"2로"]],"textEdited":"전달될 수 있기 때문에 아이템 4 아이템 5가 유저 원에게 추천될 수 있는 확률이 높다는 것을 또 이 그래프에서 의미합니다. 마지막으로 이 레이어 3에 있는 유저 1에서 아이템 2, 유저 2로"},{"start":885100,"end":896700,"text":"전달되는 이 레이어 3 아이템 4, 아이템 5가 존재하고 마찬가지로 유저 3로 이어지는 레이어에도 아이템 4가 존재합니다. 따라서 아이템 5보다는 아이템 4가 시그널이","confidence":0.9448,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[885430,885880,"전달되는"],[886230,886380,"이"],[886490,886800,"레이어"],[886830,886980,"3"],[887150,887480,"아이템"],[887550,887700,"4,"],[887730,888000,"아이템"],[888010,888400,"5가"],[888400,888860,"존재하고"],[889250,889780,"마찬가지로"],[889830,890080,"유저"],[890150,890540,"3로"],[890570,890980,"이어지는"],[891010,891440,"레이어에도"],[891440,891720,"아이템"],[891720,891960,"4가"],[892270,892780,"존재합니다."],[892970,893280,"따라서"],[893590,893920,"아이템"],[893990,894600,"5보다는"],[894650,894980,"아이템"],[895010,895280,"4가"],[895710,896160,"시그널이"]],"textEdited":"전달되는 이 레이어 3 아이템 4, 아이템 5가 존재하고 마찬가지로 유저 3로 이어지는 레이어에도 아이템 4가 존재합니다. 따라서 아이템 5보다는 아이템 4가 시그널이"},{"start":896700,"end":910200,"text":"두 번 전달되기 때문에 아이템 4가 추천될 확률이 조금 더 높다는 것을 의미합니다. 이렇게 하나의 노드를 기준으로 경로가 1보다 큰 하이 오더 커넥티비티","confidence":0.9723,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[897330,897480,"두"],[897480,897600,"번"],[897630,898080,"전달되기"],[898080,898440,"때문에"],[898850,899200,"아이템"],[899230,899500,"4가"],[899570,899920,"추천될"],[899950,900220,"확률이"],[900220,900420,"조금"],[900420,900560,"더"],[900590,901060,"높다는"],[901550,901860,"것을"],[902310,902800,"의미합니다."],[903050,903380,"이렇게"],[904250,904620,"하나의"],[904650,905540,"노드를"],[905550,906000,"기준으로"],[907150,907560,"경로가"],[907690,908060,"1보다"],[908150,908300,"큰"],[908870,909120,"하이"],[909150,909400,"오더"],[909410,909960,"커넥티비티"]],"textEdited":"두 번 전달되기 때문에 아이템 4가 추천될 확률이 조금 더 높다는 것을 의미합니다. 이렇게 하나의 노드를 기준으로 경로가 1보다 큰 하이 오더 커넥티비티"},{"start":910200,"end":923000,"text":"를 사용하여서 이 유저에 대한 다양한 표현형을 이 그래프 뉴얼 네트워크는 표현하고 있습니다. 그래서 전체 모델은 다음과 같습니다. 오른쪽에 있는 그림인데요. 이 레이어가 총 3가지로 구성되어 있습니다.","confidence":0.9361,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[910430,910580,"를"],[910590,911140,"사용하여서"],[911530,911680,"이"],[911690,911987,"유저에"],[911987,912180,"대한"],[912310,912620,"다양한"],[912690,913140,"표현형을"],[913510,913660,"이"],[913660,913960,"그래프"],[913960,914160,"뉴얼"],[914160,914720,"네트워크는"],[915070,915427,"표현하고"],[915427,915820,"있습니다."],[916310,916540,"그래서"],[916550,916840,"전체"],[916890,917260,"모델은"],[917770,918180,"다음과"],[918180,918560,"같습니다."],[918710,919120,"오른쪽에"],[919120,919300,"있는"],[919430,919940,"그림인데요."],[920470,920620,"이"],[920630,921000,"레이어가"],[921050,921200,"총"],[921250,921700,"3가지로"],[922150,922540,"구성되어"],[922540,922880,"있습니다."]],"textEdited":"를 사용하여서 이 유저에 대한 다양한 표현형을 이 그래프 뉴얼 네트워크는 표현하고 있습니다. 그래서 전체 모델은 다음과 같습니다. 오른쪽에 있는 그림인데요. 이 레이어가 총 3가지로 구성되어 있습니다."},{"start":923000,"end":934200,"text":"첫 번째 레이어는 유저 아이템을 원핫 인코딩하는 가장 기본적인 레이어로서 기존의 씨프 뉴럴 콜라베이트 필터링 모델 계열과도 동일합니다.","confidence":0.8684,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[923330,923480,"첫"],[923510,923740,"번째"],[923750,924140,"레이어는"],[924650,924960,"유저"],[924990,925760,"아이템을"],[925890,926180,"원핫"],[926290,926860,"인코딩하는"],[927130,927340,"가장"],[927390,927880,"기본적인"],[928490,929020,"레이어로서"],[929550,929880,"기존의"],[929910,930260,"씨프"],[930450,930720,"뉴럴"],[930850,931260,"콜라베이트"],[931290,931660,"필터링"],[931730,931980,"모델"],[932750,933340,"계열과도"],[933390,933980,"동일합니다."]],"textEdited":"첫 번째 레이어는 유저 아이템을 원핫 인코딩하는 가장 기본적인 레이어로서 기존의 씨프 뉴럴 콜라베이트 필터링 모델 계열과도 동일합니다."},{"start":934200,"end":946800,"text":"가장 처음 입력된 원핫 인코딩을 k 차원의 인베딩으로 바꿔주는 것이지요. 그래서 가장 중요한 부분은 사실 이 두 번째 레이어인 인베딩 전파 레이어입니다. 방금 전에 설명했던","confidence":0.9337,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[934810,935060,"가장"],[935590,935800,"처음"],[935890,936240,"입력된"],[936270,936540,"원핫"],[936570,937040,"인코딩을"],[937490,937640,"k"],[937710,938040,"차원의"],[938050,938600,"인베딩으로"],[938650,939060,"바꿔주는"],[939060,939480,"것이지요."],[939670,939860,"그래서"],[940010,940260,"가장"],[940410,940720,"중요한"],[940770,941080,"부분은"],[941110,941360,"사실"],[941570,941720,"이"],[941730,941880,"두"],[941890,942180,"번째"],[942230,942600,"레이어인"],[942770,943200,"인베딩"],[943430,943740,"전파"],[943790,944280,"레이어입니다."],[945410,945647,"방금"],[945647,945860,"전에"],[945870,946340,"설명했던"]],"textEdited":"가장 처음 입력된 원핫 인코딩을 k 차원의 인베딩으로 바꿔주는 것이지요. 그래서 가장 중요한 부분은 사실 이 두 번째 레이어인 인베딩 전파 레이어입니다. 방금 전에 설명했던"},{"start":946800,"end":957600,"text":"그래프 즉 유저와 아이템의 그래프에서 인베딩이 전파되는 이 하이워드 커넥티비티가 바로 이 레이어 인베딩 전파 레이어 인패딩 프로퍼게이션 레이어에서","confidence":0.9566,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[947030,947460,"그래프"],[947770,947920,"즉"],[947950,948280,"유저와"],[948290,948760,"아이템의"],[948790,949420,"그래프에서"],[949970,950380,"인베딩이"],[950390,950880,"전파되는"],[951210,951360,"이"],[951370,951780,"하이워드"],[951780,952420,"커넥티비티가"],[952530,952780,"바로"],[953090,953240,"이"],[953350,953640,"레이어"],[953950,954360,"인베딩"],[954570,954880,"전파"],[954910,955180,"레이어"],[955270,955620,"인패딩"],[956090,956640,"프로퍼게이션"],[956670,957140,"레이어에서"]],"textEdited":"그래프 즉 유저와 아이템의 그래프에서 인베딩이 전파되는 이 하이워드 커넥티비티가 바로 이 레이어 인베딩 전파 레이어 인패딩 프로퍼게이션 레이어에서"},{"start":957600,"end":971400,"text":"학습이 되는 것이죠. 그리고 이 인베딩 전파 레이어를 통해서 출력된 출력 값은 그대로 컨케이트네이트 돼서 마지막 최종 레이어 즉 유저 아이템 선호도 예측 레이어인 프로덕션 레이어에서","confidence":0.9576,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[957930,958227,"학습이"],[958227,958427,"되는"],[958427,958760,"것이죠."],[959190,959420,"그리고"],[959430,959580,"이"],[959790,960160,"인베딩"],[960210,960500,"전파"],[960550,960940,"레이어를"],[960970,961280,"통해서"],[961430,961840,"출력된"],[962130,962420,"출력"],[962490,963720,"값은"],[964270,964600,"그대로"],[964950,965567,"컨케이트네이트"],[965567,965820,"돼서"],[966250,966620,"마지막"],[966730,967020,"최종"],[967070,967400,"레이어"],[967670,967820,"즉"],[968390,968680,"유저"],[968690,969000,"아이템"],[969070,969380,"선호도"],[969410,969640,"예측"],[969670,970000,"레이어인"],[970090,970500,"프로덕션"],[970530,970980,"레이어에서"]],"textEdited":"학습이 되는 것이죠. 그리고 이 인베딩 전파 레이어를 통해서 출력된 출력 값은 그대로 컨케이트네이트 돼서 마지막 최종 레이어 즉 유저 아이템 선호도 예측 레이어인 프로덕션 레이어에서"},{"start":971400,"end":986300,"text":"최종 예측 값을 구하게 됩니다. 네 그리고 그림을 보시면 이 왼쪽이 유저 노드를 기준으로 인베딩이 전파되는 유저 인베딩 레이어고요. 오른쪽은 아이템 노드를 기준으로 전파되는 아이템 레이어입니다.","confidence":0.9692,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[971710,972000,"최종"],[972230,972540,"예측"],[972550,972920,"값을"],[973170,973487,"구하게"],[973487,973820,"됩니다."],[974130,974280,"네"],[974310,974560,"그리고"],[974610,974960,"그림을"],[974970,975340,"보시면"],[976670,976820,"이"],[976870,977400,"왼쪽이"],[978290,978580,"유저"],[978590,978960,"노드를"],[978960,979300,"기준으로"],[979330,979740,"인베딩이"],[979750,980200,"전파되는"],[980550,980820,"유저"],[980830,981120,"인베딩"],[981170,981680,"레이어고요."],[982270,982820,"오른쪽은"],[983750,984200,"아이템"],[984270,984580,"노드를"],[984580,984940,"기준으로"],[985010,985420,"전파되는"],[985450,985760,"아이템"],[985790,986260,"레이어입니다."]],"textEdited":"최종 예측 값을 구하게 됩니다. 네 그리고 그림을 보시면 이 왼쪽이 유저 노드를 기준으로 인베딩이 전파되는 유저 인베딩 레이어고요. 오른쪽은 아이템 노드를 기준으로 전파되는 아이템 레이어입니다."},{"start":986300,"end":997100,"text":"방금 전에 하이 오더 커넥티비티 예시를 설명하면서 유저를 기준으로만 그래프의 연결을 설명했지만 동일하게 아이템 노드를 기준으로도 똑같이 인베딩 전파가 이루어질 수 있습니다.","confidence":0.975,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[986650,986867,"방금"],[986867,987120,"전에"],[987290,987540,"하이"],[987540,987760,"오더"],[987760,988340,"커넥티비티"],[988370,988720,"예시를"],[988730,989380,"설명하면서"],[990030,990440,"유저를"],[990440,990960,"기준으로만"],[991510,991880,"그래프의"],[991880,992140,"연결을"],[992140,992620,"설명했지만"],[992870,993260,"동일하게"],[993310,993660,"아이템"],[993690,994020,"노드를"],[994030,994540,"기준으로도"],[994870,995260,"똑같이"],[995290,995640,"인베딩"],[995710,996080,"전파가"],[996090,996400,"이루어질"],[996410,996514,"수"],[996514,996960,"있습니다."]],"textEdited":"방금 전에 하이 오더 커넥티비티 예시를 설명하면서 유저를 기준으로만 그래프의 연결을 설명했지만 동일하게 아이템 노드를 기준으로도 똑같이 인베딩 전파가 이루어질 수 있습니다."},{"start":997100,"end":1011900,"text":"그래서 각각의 유저 노드와 아이템 노드에 대한 인베딩을 각각 구하고 이 인베딩을 최종적으로 합쳐줘서 마지막 레이어에서 내적을 통해서 최종 예측 값 평점과 같은 선호도를 예측하게 되는 것이죠.","confidence":0.9683,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[997610,997820,"그래서"],[997850,998200,"각각의"],[998210,998460,"유저"],[998470,998840,"노드와"],[998910,999260,"아이템"],[999390,999920,"노드에"],[999920,1000100,"대한"],[1000210,1000660,"인베딩을"],[1000690,1000940,"각각"],[1000940,1001300,"구하고"],[1002490,1002640,"이"],[1002670,1003140,"인베딩을"],[1003190,1003680,"최종적으로"],[1004130,1004680,"합쳐줘서"],[1004790,1005100,"마지막"],[1005150,1005680,"레이어에서"],[1006130,1006560,"내적을"],[1006560,1006900,"통해서"],[1007230,1007520,"최종"],[1008270,1008580,"예측"],[1008610,1008760,"값"],[1009270,1009680,"평점과"],[1009680,1009880,"같은"],[1009890,1010600,"선호도를"],[1010990,1011334,"예측하게"],[1011334,1011507,"되는"],[1011507,1011820,"것이죠."]],"textEdited":"그래서 각각의 유저 노드와 아이템 노드에 대한 인베딩을 각각 구하고 이 인베딩을 최종적으로 합쳐줘서 마지막 레이어에서 내적을 통해서 최종 예측 값 평점과 같은 선호도를 예측하게 되는 것이죠."},{"start":1011900,"end":1024300,"text":"먼저 첫 번째 레이어 유저 아이템에 대한 인베딩 레이어입니다. 지난 4강의 매트리스 팩토라이제이션, 그리고 바로 전 6강에서 배웠던 뉴럴 컬라버레이트 필터링 모델에서","confidence":0.9506,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1012370,1012640,"먼저"],[1012730,1012880,"첫"],[1012890,1013160,"번째"],[1013210,1013500,"레이어"],[1014010,1014280,"유저"],[1014330,1014860,"아이템에"],[1014860,1015080,"대한"],[1015210,1015560,"인베딩"],[1015610,1016120,"레이어입니다."],[1016770,1017020,"지난"],[1017310,1017760,"4강의"],[1017830,1018240,"매트리스"],[1018240,1018860,"팩토라이제이션,"],[1019150,1019440,"그리고"],[1019950,1020200,"바로"],[1020200,1020340,"전"],[1020510,1021040,"6강에서"],[1021070,1021440,"배웠던"],[1022090,1022380,"뉴럴"],[1022470,1022980,"컬라버레이트"],[1023090,1023480,"필터링"],[1023490,1024000,"모델에서"]],"textEdited":"먼저 첫 번째 레이어 유저 아이템에 대한 인베딩 레이어입니다. 지난 4강의 매트리스 팩토라이제이션, 그리고 바로 전 6강에서 배웠던 뉴럴 컬라버레이트 필터링 모델에서"},{"start":1024300,"end":1038700,"text":"원 핫 인코딩 벡터를 인베딩시켰던 그 인베딩이 바로 인터랙션 펑션에 입력되어서 곧바로 선호도를 예측했지만 이 NGCF 즉 GNN을 활용한 추천 모델에서는 이 인베딩이 바로 사용되지 않고","confidence":0.816,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1024850,1025000,"원"],[1025010,1025160,"핫"],[1025210,1025600,"인코딩"],[1025790,1026200,"벡터를"],[1026430,1027120,"인베딩시켰던"],[1027190,1027340,"그"],[1027340,1027760,"인베딩이"],[1028290,1028560,"바로"],[1028650,1029120,"인터랙션"],[1029170,1029540,"펑션에"],[1029540,1030120,"입력되어서"],[1030410,1030740,"곧바로"],[1030750,1031140,"선호도를"],[1031140,1031660,"예측했지만"],[1032190,1032340,"이"],[1032710,1033440,"NGCF"],[1033530,1033680,"즉"],[1034370,1034840,"GNN을"],[1034840,1035120,"활용한"],[1035310,1035560,"추천"],[1035560,1036060,"모델에서는"],[1036250,1036400,"이"],[1036430,1036980,"인베딩이"],[1037250,1037480,"바로"],[1037490,1037920,"사용되지"],[1037930,1038260,"않고"]],"textEdited":"원 핫 인코딩 벡터를 인베딩시켰던 그 인베딩이 바로 인터랙션 펑션에 입력되어서 곧바로 선호도를 예측했지만 이 NGCF 즉 GNN을 활용한 추천 모델에서는 이 인베딩이 바로 사용되지 않고"},{"start":1038700,"end":1053000,"text":"그래프 뉴얼 네트워크 정확히는 그래프 컨볼루션 뉴럴 네트워크 상으로 전파시켜서 이것이 이 GNN 상에서 리파인 되도록 합니다. 그것은 컬래버레이티브 시그널을 명시적으로 인베딩 레이어에","confidence":0.8857,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1038970,1039340,"그래프"],[1039340,1039540,"뉴얼"],[1039540,1039980,"네트워크"],[1040070,1040460,"정확히는"],[1040470,1040760,"그래프"],[1040850,1041247,"컨볼루션"],[1041247,1041460,"뉴럴"],[1041460,1041840,"네트워크"],[1041840,1042120,"상으로"],[1042170,1042980,"전파시켜서"],[1044230,1044580,"이것이"],[1045850,1046000,"이"],[1046000,1046340,"GNN"],[1046340,1046600,"상에서"],[1046630,1046980,"리파인"],[1046980,1047240,"되도록"],[1047250,1047560,"합니다."],[1048430,1048780,"그것은"],[1049350,1050160,"컬래버레이티브"],[1050250,1050740,"시그널을"],[1050890,1051540,"명시적으로"],[1051970,1052360,"인베딩"],[1052390,1052780,"레이어에"]],"textEdited":"그래프 뉴얼 네트워크 정확히는 그래프 컨볼루션 뉴럴 네트워크 상으로 전파시켜서 이것이 이 GNN 상에서 리파인 되도록 합니다. 그것은 컬래버레이티브 시그널을 명시적으로 인베딩 레이어에"},{"start":1053000,"end":1066600,"text":"주입하기 위한 과정으로 볼 수 있습니다. 그래서 첫 번째 인베딩 레이어에서는 임베딩을 생성해 준다. 그 이후 두 번째 레이어에서 생성된 인베딩을 가지고 전파시키는 레이어 가장 중요한 부분입니다.","confidence":0.932,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1053270,1053840,"주입하기"],[1053840,1054020,"위한"],[1054170,1054600,"과정으로"],[1054630,1054780,"볼"],[1054790,1054894,"수"],[1054894,1055240,"있습니다."],[1056030,1056220,"그래서"],[1056230,1056380,"첫"],[1056380,1056620,"번째"],[1056830,1057140,"인베딩"],[1057150,1057700,"레이어에서는"],[1058210,1058660,"임베딩을"],[1058710,1059060,"생성해"],[1059060,1059320,"준다."],[1060090,1060240,"그"],[1060270,1060500,"이후"],[1060950,1061100,"두"],[1061110,1061360,"번째"],[1061390,1061820,"레이어에서"],[1062790,1063100,"생성된"],[1063150,1063600,"인베딩을"],[1063610,1063960,"가지고"],[1064330,1064920,"전파시키는"],[1064930,1065200,"레이어"],[1065350,1065580,"가장"],[1065710,1066060,"중요한"],[1066070,1066600,"부분입니다."]],"textEdited":"주입하기 위한 과정으로 볼 수 있습니다. 그래서 첫 번째 인베딩 레이어에서는 임베딩을 생성해 준다. 그 이후 두 번째 레이어에서 생성된 인베딩을 가지고 전파시키는 레이어 가장 중요한 부분입니다."},{"start":1066600,"end":1077800,"text":"여기서는 유저와 아이템의 컬라버레이트 시그널을 담는 것을 메시지라고 정의하고 이 메시지로 표현을 하였습니다. 일단 여기 아래에 있는 수식들은","confidence":0.9484,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1067050,1067460,"여기서는"],[1067590,1067980,"유저와"],[1067990,1068740,"아이템의"],[1068850,1069400,"컬라버레이트"],[1069470,1070680,"시그널을"],[1070730,1071060,"담는"],[1071060,1071300,"것을"],[1071370,1072260,"메시지라고"],[1072310,1072840,"정의하고"],[1073350,1073500,"이"],[1073500,1073960,"메시지로"],[1074350,1074627,"표현을"],[1074627,1075120,"하였습니다."],[1075810,1076000,"일단"],[1076010,1076240,"여기"],[1076270,1076547,"아래에"],[1076547,1076720,"있는"],[1076810,1077360,"수식들은"]],"textEdited":"여기서는 유저와 아이템의 컬라버레이트 시그널을 담는 것을 메시지라고 정의하고 이 메시지로 표현을 하였습니다. 일단 여기 아래에 있는 수식들은"},{"start":1077800,"end":1090800,"text":"아까와 비슷하게 유저 노드를 기준으로 소식이 설명되고 있는데요. 이제 아이템 아에서 유로 전달되는 메시지를 mui라고 표현하고 있습니다. 그래서 mui는","confidence":0.8254,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1078090,1078400,"아까와"],[1078750,1079120,"비슷하게"],[1079130,1079400,"유저"],[1079410,1080180,"노드를"],[1080190,1080600,"기준으로"],[1080790,1081100,"소식이"],[1081100,1081500,"설명되고"],[1081500,1081820,"있는데요."],[1082690,1082860,"이제"],[1082910,1083320,"아이템"],[1083810,1084360,"아에서"],[1085010,1085380,"유로"],[1086210,1086660,"전달되는"],[1086710,1087180,"메시지를"],[1087290,1088360,"mui라고"],[1088510,1088900,"표현하고"],[1088900,1089280,"있습니다."],[1089430,1089620,"그래서"],[1089630,1090380,"mui는"]],"textEdited":"아까와 비슷하게 유저 노드를 기준으로 소식이 설명되고 있는데요. 이제 아이템 아에서 유로 전달되는 메시지를 mui라고 표현하고 있습니다. 그래서 mui는"},{"start":1090800,"end":1105100,"text":"아이템 아이 자체가 가지고 있는 임베딩 플러스 아이템 아이와 아이템 유저의 상호작용을 표현할 수 있는 이 엘레멘트 와이스 프로덕트 이 두 개의 텀의 합으로 표현됩니다. 그리고 유저 율을 기준으로","confidence":0.8696,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1091150,1091540,"아이템"],[1091830,1092080,"아이"],[1092450,1092820,"자체가"],[1092850,1093140,"가지고"],[1093140,1093300,"있는"],[1093410,1093820,"임베딩"],[1094390,1094820,"플러스"],[1095190,1095580,"아이템"],[1095730,1096100,"아이와"],[1096230,1096560,"아이템"],[1096610,1096960,"유저의"],[1097030,1097600,"상호작용을"],[1098250,1098580,"표현할"],[1098590,1098694,"수"],[1098694,1098840,"있는"],[1098890,1099040,"이"],[1099070,1099467,"엘레멘트"],[1099467,1099780,"와이스"],[1099790,1100180,"프로덕트"],[1100230,1100380,"이"],[1100380,1100500,"두"],[1100500,1100740,"개의"],[1100770,1101080,"텀의"],[1101490,1101860,"합으로"],[1101890,1102400,"표현됩니다."],[1102690,1102960,"그리고"],[1103070,1103380,"유저"],[1103810,1104160,"율을"],[1104160,1104540,"기준으로"]],"textEdited":"아이템 아이 자체가 가지고 있는 임베딩 플러스 아이템 아이와 아이템 유저의 상호작용을 표현할 수 있는 이 엘레멘트 와이스 프로덕트 이 두 개의 텀의 합으로 표현됩니다. 그리고 유저 율을 기준으로"},{"start":1105100,"end":1120000,"text":"연결돼 있는 아이템이 점점 많을수록 이 시그널은 점점 커지기 때문에 그 시그널의 크기를 노말라이제이션 해 주기 위해서 개별 메시지의 크기를 이웃한 노드의 개수로 나눠주는 노멀라이제이션 텀도 앞에 붙어 있습니다.","confidence":0.9346,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1105410,1105867,"연결돼"],[1105867,1106020,"있는"],[1106030,1106500,"아이템이"],[1107030,1107380,"점점"],[1107450,1108040,"많을수록"],[1108230,1108380,"이"],[1108450,1108940,"시그널은"],[1109550,1109840,"점점"],[1109850,1110154,"커지기"],[1110154,1110420,"때문에"],[1110420,1110560,"그"],[1110560,1110920,"시그널의"],[1110950,1111280,"크기를"],[1111450,1112280,"노말라이제이션"],[1112280,1112387,"해"],[1112387,1112620,"주기"],[1112620,1112940,"위해서"],[1113790,1114040,"개별"],[1114110,1114560,"메시지의"],[1114610,1114980,"크기를"],[1115370,1115720,"이웃한"],[1115750,1116720,"노드의"],[1116770,1117180,"개수로"],[1117250,1117720,"나눠주는"],[1118010,1118700,"노멀라이제이션"],[1118710,1119000,"텀도"],[1119010,1119240,"앞에"],[1119250,1119500,"붙어"],[1119500,1119840,"있습니다."]],"textEdited":"연결돼 있는 아이템이 점점 많을수록 이 시그널은 점점 커지기 때문에 그 시그널의 크기를 노말라이제이션 해 주기 위해서 개별 메시지의 크기를 이웃한 노드의 개수로 나눠주는 노멀라이제이션 텀도 앞에 붙어 있습니다."},{"start":1120000,"end":1133300,"text":"이제 하나의 아이템 노드 아에서 유저로 전달되는 유저 메시지를 계산하고 나면은 이 메시지는 하나가 아니라 유저를 기준으로는 여러 개의 아이템이 존재할 수 있겠죠.","confidence":0.9812,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1120210,1120400,"이제"],[1120450,1120760,"하나의"],[1121170,1121520,"아이템"],[1121550,1121820,"노드"],[1121910,1122420,"아에서"],[1123370,1123780,"유저로"],[1123830,1124240,"전달되는"],[1124310,1124560,"유저"],[1124630,1126100,"메시지를"],[1126110,1126540,"계산하고"],[1126540,1126900,"나면은"],[1127970,1128120,"이"],[1128210,1128760,"메시지는"],[1129710,1130014,"하나가"],[1130014,1130280,"아니라"],[1130590,1130980,"유저를"],[1130980,1131460,"기준으로는"],[1131610,1131787,"여러"],[1131787,1131947,"개의"],[1131947,1132300,"아이템이"],[1132300,1132680,"존재할"],[1132680,1132787,"수"],[1132787,1133120,"있겠죠."]],"textEdited":"이제 하나의 아이템 노드 아에서 유저로 전달되는 유저 메시지를 계산하고 나면은 이 메시지는 하나가 아니라 유저를 기준으로는 여러 개의 아이템이 존재할 수 있겠죠."},{"start":1133300,"end":1146500,"text":"그래서 이 밑에서 최종적으로 유저 인베딩을 구해주는 즉 인베딩 전파 레이어를 통해 최종적으로 새로 생성되는 이 이유1 첫 번째 전파 레이어의 최종 유저 인베딩은","confidence":0.9687,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1133630,1133900,"그래서"],[1133970,1134120,"이"],[1134120,1134420,"밑에서"],[1134530,1135040,"최종적으로"],[1135050,1135280,"유저"],[1135290,1135740,"인베딩을"],[1135750,1136200,"구해주는"],[1136670,1136820,"즉"],[1137130,1137500,"인베딩"],[1137550,1137800,"전파"],[1137810,1138160,"레이어를"],[1138160,1138360,"통해"],[1138390,1138940,"최종적으로"],[1139430,1139700,"새로"],[1140070,1140500,"생성되는"],[1140590,1140740,"이"],[1141270,1141840,"이유1"],[1142470,1142620,"첫"],[1142630,1142920,"번째"],[1143050,1143300,"전파"],[1143330,1143740,"레이어의"],[1143970,1144220,"최종"],[1144220,1144440,"유저"],[1144470,1145740,"인베딩은"]],"textEdited":"그래서 이 밑에서 최종적으로 유저 인베딩을 구해주는 즉 인베딩 전파 레이어를 통해 최종적으로 새로 생성되는 이 이유1 첫 번째 전파 레이어의 최종 유저 인베딩은"},{"start":1146500,"end":1160500,"text":"다음과 같은 수식으로 표현할 수 있습니다. 먼저 그 자신으로부터 전달되는 메시지 엠유유와 이 위에서 계산한 엠유아를 주변 인접 노드들에 대해서 모두 더해주게 됩니다.","confidence":0.8197,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1147130,1147640,"다음과"],[1147690,1147940,"같은"],[1147990,1148400,"수식으로"],[1148410,1148760,"표현할"],[1148760,1148867,"수"],[1148867,1149220,"있습니다."],[1149390,1149640,"먼저"],[1149670,1149820,"그"],[1149890,1150480,"자신으로부터"],[1150750,1151220,"전달되는"],[1151290,1151660,"메시지"],[1152450,1153200,"엠유유와"],[1153590,1153740,"이"],[1153740,1154080,"위에서"],[1154090,1154460,"계산한"],[1154890,1156380,"엠유아를"],[1156950,1157180,"주변"],[1157270,1157540,"인접"],[1157550,1158800,"노드들에"],[1158800,1159060,"대해서"],[1159130,1159380,"모두"],[1159670,1160160,"더해주게"],[1160160,1160440,"됩니다."]],"textEdited":"다음과 같은 수식으로 표현할 수 있습니다. 먼저 그 자신으로부터 전달되는 메시지 엠유유와 이 위에서 계산한 엠유아를 주변 인접 노드들에 대해서 모두 더해주게 됩니다."},{"start":1160500,"end":1169900,"text":"그래서 이 더한 값을 리키 밸루라는 액티베이션 펑션을 사용하여서 최종적으로 그 전 레이어로부터 전파된 인베딩들이 다음 레이어","confidence":0.899,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1160630,1160807,"그래서"],[1160807,1160940,"이"],[1160940,1161200,"더한"],[1161290,1161660,"값을"],[1162210,1162480,"리키"],[1162510,1163040,"밸루라는"],[1163210,1163700,"액티베이션"],[1163750,1164180,"펑션을"],[1164190,1164740,"사용하여서"],[1165310,1165920,"최종적으로"],[1166470,1166620,"그"],[1166620,1166760,"전"],[1166890,1167560,"레이어로부터"],[1167650,1168020,"전파된"],[1168090,1168720,"인베딩들이"],[1168990,1169220,"다음"],[1169310,1169640,"레이어"]],"textEdited":"그래서 이 더한 값을 리키 밸루라는 액티베이션 펑션을 사용하여서 최종적으로 그 전 레이어로부터 전파된 인베딩들이 다음 레이어"},{"start":1169900,"end":1180100,"text":"이유원으로 표현되게 됩니다. 이제 이렇게 해서 구해진 이유원이라는 애들은 원호 전파 퍼스트 홈까지 전파된 인베딩이라고 말합니다.","confidence":0.7881,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1170450,1171200,"이유원으로"],[1171510,1171980,"표현되게"],[1171980,1172280,"됩니다."],[1172390,1172560,"이제"],[1172560,1172820,"이렇게"],[1172820,1173040,"해서"],[1173310,1173600,"구해진"],[1173770,1174520,"이유원이라는"],[1174520,1174840,"애들은"],[1175710,1176120,"원호"],[1176230,1176560,"전파"],[1177070,1177434,"퍼스트"],[1177434,1177980,"홈까지"],[1177990,1178340,"전파된"],[1178370,1179220,"인베딩이라고"],[1179510,1179940,"말합니다."]],"textEdited":"이유원으로 표현되게 됩니다. 이제 이렇게 해서 구해진 이유원이라는 애들은 원호 전파 퍼스트 홈까지 전파된 인베딩이라고 말합니다."},{"start":1180100,"end":1192000,"text":"이렇게 인베딩이 전파되는 레이어를 그림과 같이 원 호 퍼스트 톱 즉 하나만 쌓은 것이 아니라 패스를 2개, 3개 확장할 수 있겠죠. 그래서 엘개까지 쌓을 수가 있겠죠.","confidence":0.8676,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1180810,1181100,"이렇게"],[1181190,1181620,"인베딩이"],[1181670,1182160,"전파되는"],[1182230,1182700,"레이어를"],[1182930,1183260,"그림과"],[1183260,1183540,"같이"],[1183950,1184100,"원"],[1184110,1184260,"호"],[1184350,1184620,"퍼스트"],[1184620,1184740,"톱"],[1184810,1184960,"즉"],[1185010,1185340,"하나만"],[1185370,1185620,"쌓은"],[1185620,1185774,"것이"],[1185774,1186060,"아니라"],[1186630,1187040,"패스를"],[1187090,1187340,"2개,"],[1187370,1187600,"3개"],[1189310,1189700,"확장할"],[1189700,1189774,"수"],[1189774,1190060,"있겠죠."],[1190130,1190340,"그래서"],[1190410,1190920,"엘개까지"],[1190920,1191160,"쌓을"],[1191160,1191360,"수가"],[1191370,1191720,"있겠죠."]],"textEdited":"이렇게 인베딩이 전파되는 레이어를 그림과 같이 원 호 퍼스트 톱 즉 하나만 쌓은 것이 아니라 패스를 2개, 3개 확장할 수 있겠죠. 그래서 엘개까지 쌓을 수가 있겠죠."},{"start":1192000,"end":1202000,"text":"그럼 타겟 유저를 기준으로 총 l차 이웃 즉 패스가 엘만큼 떨어져 있는 이웃까지도 인베딩 전파가 이루어집니다. 그래서 이","confidence":0.9081,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1192270,1192440,"그럼"],[1192490,1192740,"타겟"],[1192740,1193080,"유저를"],[1193090,1193440,"기준으로"],[1193610,1193760,"총"],[1194030,1194380,"l차"],[1194470,1194660,"이웃"],[1195010,1195160,"즉"],[1195250,1195640,"패스가"],[1196950,1197340,"엘만큼"],[1197340,1197620,"떨어져"],[1197620,1197800,"있는"],[1198090,1198780,"이웃까지도"],[1199030,1199400,"인베딩"],[1199550,1199960,"전파가"],[1200110,1200740,"이루어집니다."],[1201170,1201420,"그래서"],[1201420,1201560,"이"]],"textEdited":"그럼 타겟 유저를 기준으로 총 l차 이웃 즉 패스가 엘만큼 떨어져 있는 이웃까지도 인베딩 전파가 이루어집니다. 그래서 이"},{"start":1202000,"end":1214300,"text":"3단계의 임베딩을 구하기 위해서는 바로 전 단계인 2단계의 임베딩을 가지고 사용해야겠죠. 그래서 다음과 같은 점화식으로 표현할 수 있겠습니다. 새로운 엘차 인베딩은","confidence":0.9325,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1202490,1203580,"3단계의"],[1203670,1204100,"임베딩을"],[1204100,1204440,"구하기"],[1204440,1204860,"위해서는"],[1204950,1205200,"바로"],[1205210,1205360,"전"],[1205470,1205820,"단계인"],[1205890,1206440,"2단계의"],[1206510,1207540,"임베딩을"],[1207570,1207860,"가지고"],[1207860,1208460,"사용해야겠죠."],[1208730,1208980,"그래서"],[1209010,1209340,"다음과"],[1209340,1209520,"같은"],[1209590,1210920,"점화식으로"],[1210920,1211220,"표현할"],[1211220,1211327,"수"],[1211327,1211820,"있겠습니다."],[1212270,1212580,"새로운"],[1212710,1213040,"엘차"],[1213250,1213760,"인베딩은"]],"textEdited":"3단계의 임베딩을 구하기 위해서는 바로 전 단계인 2단계의 임베딩을 가지고 사용해야겠죠. 그래서 다음과 같은 점화식으로 표현할 수 있겠습니다. 새로운 엘차 인베딩은"},{"start":1214300,"end":1225700,"text":"기존 바로 전에 엘 마이너스 1차 인베딩 전파의 결괏값들로 이루어져서 이 값이 전파되어서 엘차 인베딩의 값을 구하게 되는 것입니다.","confidence":0.9016,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1214610,1214880,"기존"],[1215710,1215980,"바로"],[1215980,1216320,"전에"],[1216850,1217000,"엘"],[1217000,1217400,"마이너스"],[1217490,1217800,"1차"],[1217970,1218340,"인베딩"],[1218390,1218840,"전파의"],[1219230,1220760,"결괏값들로"],[1220770,1221260,"이루어져서"],[1221810,1221960,"이"],[1221990,1222360,"값이"],[1222410,1222960,"전파되어서"],[1223090,1223400,"엘차"],[1223590,1224100,"인베딩의"],[1224370,1224700,"값을"],[1224700,1224954,"구하게"],[1224954,1225127,"되는"],[1225127,1225520,"것입니다."]],"textEdited":"기존 바로 전에 엘 마이너스 1차 인베딩 전파의 결괏값들로 이루어져서 이 값이 전파되어서 엘차 인베딩의 값을 구하게 되는 것입니다."},{"start":1225700,"end":1239000,"text":"여기에 있는 수식은 바로 전에 1차 인베딩을 설명했던 수식과 같은 수식인데 1이 엘로 바뀌었을 뿐입니다. 그래서 이렇게 인베딩을 하나만 쌓는 것이 아니라 2개 3개 4개 쌓는 것이 하이워더 프로퍼게이션","confidence":0.9564,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1226170,1226434,"여기에"],[1226434,1226580,"있는"],[1226610,1226940,"수식은"],[1227030,1227260,"바로"],[1227260,1227540,"전에"],[1227910,1228200,"1차"],[1228250,1228840,"인베딩을"],[1229070,1229500,"설명했던"],[1229530,1229900,"수식과"],[1230550,1230820,"같은"],[1230830,1231260,"수식인데"],[1231370,1231680,"1이"],[1231730,1232040,"엘로"],[1232110,1232500,"바뀌었을"],[1232500,1232880,"뿐입니다."],[1233190,1233420,"그래서"],[1233420,1233700,"이렇게"],[1234250,1234720,"인베딩을"],[1234810,1235180,"하나만"],[1235230,1235480,"쌓는"],[1235480,1235667,"것이"],[1235667,1235940,"아니라"],[1236230,1236500,"2개"],[1236570,1236800,"3개"],[1236950,1237160,"4개"],[1237210,1237480,"쌓는"],[1237480,1237740,"것이"],[1237910,1238340,"하이워더"],[1238340,1238860,"프로퍼게이션"]],"textEdited":"여기에 있는 수식은 바로 전에 1차 인베딩을 설명했던 수식과 같은 수식인데 1이 엘로 바뀌었을 뿐입니다. 그래서 이렇게 인베딩을 하나만 쌓는 것이 아니라 2개 3개 4개 쌓는 것이 하이워더 프로퍼게이션"},{"start":1239000,"end":1250100,"text":"아까 설명했던 패스 하나짜리가 아니라 2개 3개까지의 정보들을 현재 유저 노드를 기준으로 전달시키는 과정을 표현한 것입니다. 자 그래서 우리가 엘게에","confidence":0.9234,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1239290,1239500,"아까"],[1239510,1240000,"설명했던"],[1240370,1240680,"패스"],[1240730,1241334,"하나짜리가"],[1241334,1241580,"아니라"],[1241630,1241860,"2개"],[1241870,1242500,"3개까지의"],[1242650,1243140,"정보들을"],[1243830,1244100,"현재"],[1244110,1244360,"유저"],[1244370,1244980,"노드를"],[1245010,1245420,"기준으로"],[1245710,1246300,"전달시키는"],[1246710,1247200,"과정을"],[1247230,1247540,"표현한"],[1247540,1247940,"것입니다."],[1248470,1248620,"자"],[1248620,1248807,"그래서"],[1248807,1249020,"우리가"],[1249130,1249620,"엘게에"]],"textEdited":"아까 설명했던 패스 하나짜리가 아니라 2개 3개까지의 정보들을 현재 유저 노드를 기준으로 전달시키는 과정을 표현한 것입니다. 자 그래서 우리가 엘게에"},{"start":1250100,"end":1264900,"text":"인베딩 프로파게이션 레이어 인베딩 전파 레이어를 사용했다고 하면은 총 20 처음 인베딩 레이어에서 생산된 인베딩부터 마지막 엘 차 인베딩까지 생성된 인베딩까지 총 엘 플러스 1개의 유저 인베딩","confidence":0.9475,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1250470,1250880,"인베딩"],[1250950,1251520,"프로파게이션"],[1251530,1251780,"레이어"],[1251780,1252060,"인베딩"],[1252110,1252360,"전파"],[1252360,1252680,"레이어를"],[1252680,1253147,"사용했다고"],[1253147,1253480,"하면은"],[1253970,1254120,"총"],[1255350,1255680,"20"],[1256030,1256280,"처음"],[1256630,1256980,"인베딩"],[1257010,1257380,"레이어에서"],[1257410,1257800,"생산된"],[1258050,1258660,"인베딩부터"],[1258850,1259220,"마지막"],[1259510,1259660,"엘"],[1259710,1259860,"차"],[1260110,1260740,"인베딩까지"],[1260810,1261180,"생성된"],[1261570,1262040,"인베딩까지"],[1262050,1262200,"총"],[1262690,1262840,"엘"],[1262910,1263220,"플러스"],[1263250,1263640,"1개의"],[1264110,1264360,"유저"],[1264390,1264700,"인베딩"]],"textEdited":"인베딩 프로파게이션 레이어 인베딩 전파 레이어를 사용했다고 하면은 총 20 처음 인베딩 레이어에서 생산된 인베딩부터 마지막 엘 차 인베딩까지 생성된 인베딩까지 총 엘 플러스 1개의 유저 인베딩"},{"start":1264900,"end":1277400,"text":"그리고 아이템도 지금 전 그림에서 설명하지 않았지만 동일하게 아이템도 생성될 수 있다고 했죠. 아이템도 마찬가지로 엘 플러스 1개의 임베딩 벡터가 생성됩니다. 그럼 요 인베딩 벡터들을 모두 컨택","confidence":0.9218,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1265370,1265660,"그리고"],[1266070,1266620,"아이템도"],[1267230,1267440,"지금"],[1267610,1267760,"전"],[1267870,1268200,"그림에서"],[1268230,1268600,"설명하지"],[1268600,1268900,"않았지만"],[1268970,1269380,"동일하게"],[1269390,1269800,"아이템도"],[1269810,1270140,"생성될"],[1270140,1270234,"수"],[1270234,1270454,"있다고"],[1270454,1270700,"했죠."],[1270830,1271280,"아이템도"],[1271280,1271780,"마찬가지로"],[1272230,1272380,"엘"],[1272470,1272820,"플러스"],[1272830,1273180,"1개의"],[1273230,1273560,"임베딩"],[1273590,1273960,"벡터가"],[1274370,1274960,"생성됩니다."],[1275170,1275340,"그럼"],[1275340,1275460,"요"],[1275510,1275840,"인베딩"],[1275840,1276240,"벡터들을"],[1276270,1276500,"모두"],[1276570,1276900,"컨택"]],"textEdited":"그리고 아이템도 지금 전 그림에서 설명하지 않았지만 동일하게 아이템도 생성될 수 있다고 했죠. 아이템도 마찬가지로 엘 플러스 1개의 임베딩 벡터가 생성됩니다. 그럼 요 인베딩 벡터들을 모두 컨택"},{"start":1277400,"end":1292000,"text":"하게 되고 그러면 이 두 최종 인베딩 이유 스타 이아스타는 차원이 같기 때문에 이 두 값을 내적하여서 최종적으로 우리가 원하는 그 유저가 그 아이템에 갖는 선호도 값을 계산하게 됩니다.","confidence":0.8945,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1277710,1277927,"하게"],[1277927,1278180,"되고"],[1278970,1279200,"그러면"],[1279250,1279400,"이"],[1279450,1279600,"두"],[1280050,1280340,"최종"],[1280410,1280760,"인베딩"],[1280870,1281120,"이유"],[1281170,1281420,"스타"],[1281830,1282700,"이아스타는"],[1282810,1283140,"차원이"],[1283140,1283327,"같기"],[1283327,1283680,"때문에"],[1284090,1284240,"이"],[1284240,1284360,"두"],[1284370,1284700,"값을"],[1285170,1285820,"내적하여서"],[1286170,1286780,"최종적으로"],[1286780,1287020,"우리가"],[1287030,1287360,"원하는"],[1287830,1287980,"그"],[1287990,1288360,"유저가"],[1288410,1288560,"그"],[1288560,1289040,"아이템에"],[1289110,1289320,"갖는"],[1289390,1289740,"선호도"],[1290110,1290460,"값을"],[1291010,1291440,"계산하게"],[1291440,1291760,"됩니다."]],"textEdited":"하게 되고 그러면 이 두 최종 인베딩 이유 스타 이아스타는 차원이 같기 때문에 이 두 값을 내적하여서 최종적으로 우리가 원하는 그 유저가 그 아이템에 갖는 선호도 값을 계산하게 됩니다."},{"start":1292000,"end":1306100,"text":"네 제안한 뉴럴 그래프 컬래버이트 필터링 모델의 성능 및 결과를 보시면 이제 먼저 여기에 있는 1 2 3 4는 우리가 인베딩 전파 레이어에서 인베딩 전파를 한 번 했냐 두 번 했냐 세 번 했냐 이것을 비교한 결과인데요.","confidence":0.8765,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1292590,1292740,"네"],[1292770,1293160,"제안한"],[1293730,1294020,"뉴럴"],[1294020,1294280,"그래프"],[1294310,1294720,"컬래버이트"],[1294730,1295080,"필터링"],[1295080,1295400,"모델의"],[1295510,1295760,"성능"],[1295970,1296120,"및"],[1296190,1296620,"결과를"],[1296730,1297080,"보시면"],[1297630,1297800,"이제"],[1297800,1298080,"먼저"],[1298570,1298834,"여기에"],[1298834,1298980,"있는"],[1299090,1299240,"1"],[1299370,1299520,"2"],[1299570,1299720,"3"],[1299730,1300040,"4는"],[1300210,1300420,"우리가"],[1300450,1300820,"인베딩"],[1300850,1301100,"전파"],[1301130,1301540,"레이어에서"],[1301630,1302000,"인베딩"],[1302130,1302460,"전파를"],[1302530,1302647,"한"],[1302647,1302780,"번"],[1302790,1303020,"했냐"],[1303070,1303187,"두"],[1303187,1303320,"번"],[1303320,1303540,"했냐"],[1303590,1303694,"세"],[1303694,1303820,"번"],[1303820,1304060,"했냐"],[1304450,1304860,"이것을"],[1304870,1305220,"비교한"],[1305430,1306000,"결과인데요."]],"textEdited":"네 제안한 뉴럴 그래프 컬래버이트 필터링 모델의 성능 및 결과를 보시면 이제 먼저 여기에 있는 1 2 3 4는 우리가 인베딩 전파 레이어에서 인베딩 전파를 한 번 했냐 두 번 했냐 세 번 했냐 이것을 비교한 결과인데요."},{"start":1306100,"end":1320400,"text":"인베딩 전파 레이어가 점점 많아질수록 모델의 추천 성능 이 총 3개의 데이터셋을 사용했는데요. 많아지면 많아질수록 점점 좋아지는데 다만 레이어가 너무 많이 쌓일 경우에는 오버피팅이 발생할 수 있습니다.","confidence":0.9782,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1306790,1307160,"인베딩"],[1307210,1307500,"전파"],[1307550,1307920,"레이어가"],[1307990,1308280,"점점"],[1308280,1308860,"많아질수록"],[1309350,1309700,"모델의"],[1309750,1310060,"추천"],[1310570,1310840,"성능"],[1311470,1311620,"이"],[1311630,1311780,"총"],[1311810,1312140,"3개의"],[1312140,1312640,"데이터셋을"],[1312640,1313200,"사용했는데요."],[1313850,1314280,"많아지면"],[1314280,1314780,"많아질수록"],[1314810,1315080,"점점"],[1315090,1315680,"좋아지는데"],[1315990,1316240,"다만"],[1316810,1317180,"레이어가"],[1317180,1317340,"너무"],[1317340,1317540,"많이"],[1317630,1317920,"쌓일"],[1317950,1318420,"경우에는"],[1318990,1319540,"오버피팅이"],[1319550,1319880,"발생할"],[1319880,1319967,"수"],[1319967,1320380,"있습니다."]],"textEdited":"인베딩 전파 레이어가 점점 많아질수록 모델의 추천 성능 이 총 3개의 데이터셋을 사용했는데요. 많아지면 많아질수록 점점 좋아지는데 다만 레이어가 너무 많이 쌓일 경우에는 오버피팅이 발생할 수 있습니다."},{"start":1320400,"end":1333400,"text":"따라서 실험 결과 레이어가 한 3개 정도에서 4개 정도일 때 가장 좋은 성능을 보인다고 밝히고 있습니다. 네 그리고 제안한 모델을 매트리스 팩토라이제이션 모델과 비교한 부분이 있습니다.","confidence":0.9378,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1320770,1321080,"따라서"],[1321110,1321340,"실험"],[1321340,1321580,"결과"],[1321970,1322320,"레이어가"],[1322320,1322440,"한"],[1322510,1322800,"3개"],[1322870,1323320,"정도에서"],[1323330,1323560,"4개"],[1323570,1324000,"정도일"],[1324030,1324180,"때"],[1324250,1324480,"가장"],[1324570,1324740,"좋은"],[1324970,1325300,"성능을"],[1325310,1325820,"보인다고"],[1326710,1327080,"밝히고"],[1327080,1327440,"있습니다."],[1328210,1328360,"네"],[1328390,1328660,"그리고"],[1329230,1329640,"제안한"],[1329650,1330000,"모델을"],[1330210,1330620,"매트리스"],[1330630,1331320,"팩토라이제이션"],[1331370,1331740,"모델과"],[1331810,1332160,"비교한"],[1332160,1332414,"부분이"],[1332414,1332800,"있습니다."]],"textEdited":"따라서 실험 결과 레이어가 한 3개 정도에서 4개 정도일 때 가장 좋은 성능을 보인다고 밝히고 있습니다. 네 그리고 제안한 모델을 매트리스 팩토라이제이션 모델과 비교한 부분이 있습니다."},{"start":1333400,"end":1346000,"text":"아까 이 매트리스 팩토라이제이션 모델은 유저 아이템을 학습시킬 때 그 인베딩 학습과 상호 작용이 분리되어 있다고 말씀드렸는데요. 이 NGCF는 그렇지 않기 때문에 이 인베딩을 더 잘 표현한다는 것을","confidence":0.9024,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1333690,1333880,"아까"],[1333880,1333967,"이"],[1333967,1334300,"매트리스"],[1334300,1334920,"팩토라이제이션"],[1334950,1335340,"모델은"],[1335950,1336220,"유저"],[1336220,1336760,"아이템을"],[1336870,1337360,"학습시킬"],[1337390,1337540,"때"],[1337790,1337940,"그"],[1337940,1338300,"인베딩"],[1338410,1338860,"학습과"],[1339150,1339340,"상호"],[1339350,1339640,"작용이"],[1339690,1340120,"분리되어"],[1340130,1340440,"있다고"],[1340450,1341160,"말씀드렸는데요."],[1341910,1342060,"이"],[1342060,1342660,"NGCF는"],[1342660,1342940,"그렇지"],[1342940,1343127,"않기"],[1343127,1343460,"때문에"],[1343810,1343960,"이"],[1343960,1344380,"인베딩을"],[1344390,1344540,"더"],[1344590,1344740,"잘"],[1345010,1345467,"표현한다는"],[1345467,1345760,"것을"]],"textEdited":"아까 이 매트리스 팩토라이제이션 모델은 유저 아이템을 학습시킬 때 그 인베딩 학습과 상호 작용이 분리되어 있다고 말씀드렸는데요. 이 NGCF는 그렇지 않기 때문에 이 인베딩을 더 잘 표현한다는 것을"},{"start":1346000,"end":1359100,"text":"다음 다음 실험을 통해서 입증하고 싶었습니다. 먼저 어떤 데이터셋을 사용해도 매트리스 팩토라이제이션보다 이 NGCF가 더 모델 성능이 빠르게 수렴하고","confidence":0.8483,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1346330,1346580,"다음"],[1347030,1347280,"다음"],[1347390,1347720,"실험을"],[1347730,1348060,"통해서"],[1348850,1349260,"입증하고"],[1349260,1349760,"싶었습니다."],[1350530,1350780,"먼저"],[1350870,1351120,"어떤"],[1351170,1351780,"데이터셋을"],[1351890,1352360,"사용해도"],[1353610,1354020,"매트리스"],[1354020,1354900,"팩토라이제이션보다"],[1355490,1355640,"이"],[1355670,1356380,"NGCF가"],[1357070,1357220,"더"],[1357220,1357460,"모델"],[1357550,1357840,"성능이"],[1357910,1358220,"빠르게"],[1358250,1358720,"수렴하고"]],"textEdited":"다음 다음 실험을 통해서 입증하고 싶었습니다. 먼저 어떤 데이터셋을 사용해도 매트리스 팩토라이제이션보다 이 NGCF가 더 모델 성능이 빠르게 수렴하고"},{"start":1359100,"end":1371300,"text":"추천 성능인 리콜도 더 높음을 알 수 있습니다. NGCF가 모델의 캐패시티 즉 모델의 표현력과 갖고 있는 파라미터의 개수가 훨씬 크고 또 제일 중요한 이 인베딩 전파를 통해서","confidence":0.9246,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1359650,1359920,"추천"],[1359950,1360200,"성능인"],[1360210,1361080,"리콜도"],[1361080,1361200,"더"],[1361200,1361560,"높음을"],[1361570,1361720,"알"],[1361720,1361794,"수"],[1361794,1362140,"있습니다."],[1363050,1363680,"NGCF가"],[1363690,1364020,"모델의"],[1364050,1364580,"캐패시티"],[1364630,1364780,"즉"],[1364790,1365120,"모델의"],[1365130,1366000,"표현력과"],[1366130,1366314,"갖고"],[1366314,1366460,"있는"],[1366460,1366854,"파라미터의"],[1366854,1367120,"개수가"],[1367120,1367340,"훨씬"],[1367350,1367640,"크고"],[1368330,1368480,"또"],[1368510,1368700,"제일"],[1368770,1369060,"중요한"],[1369090,1369240,"이"],[1369270,1369620,"인베딩"],[1369670,1370400,"전파를"],[1370450,1370800,"통해서"]],"textEdited":"추천 성능인 리콜도 더 높음을 알 수 있습니다. NGCF가 모델의 캐패시티 즉 모델의 표현력과 갖고 있는 파라미터의 개수가 훨씬 크고 또 제일 중요한 이 인베딩 전파를 통해서"},{"start":1371300,"end":1386200,"text":"유저와 아이템에 대한 레프리젠테이션 파워가 더 좋기 때문입니다. 이 레프레젠테이션 파워가 좋다. 즉 유저와 아이템을 더 잘 표현한다라는 것은 시각화를 했을 때도 정성적으로 확인해 볼 수 있습니다.","confidence":0.9643,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1371610,1371980,"유저와"],[1372010,1372394,"아이템에"],[1372394,1372600,"대한"],[1372730,1373580,"레프리젠테이션"],[1373630,1373960,"파워가"],[1374170,1374320,"더"],[1374350,1374700,"좋기"],[1375070,1375600,"때문입니다."],[1376270,1376420,"이"],[1376750,1377440,"레프레젠테이션"],[1377470,1377760,"파워가"],[1377760,1378040,"좋다."],[1378290,1378440,"즉"],[1378610,1378940,"유저와"],[1378940,1379400,"아이템을"],[1379790,1379940,"더"],[1380110,1380260,"잘"],[1380510,1381867,"표현한다라는"],[1381867,1382140,"것은"],[1382890,1383400,"시각화를"],[1383400,1383660,"했을"],[1383660,1383920,"때도"],[1384350,1384900,"정성적으로"],[1384910,1385167,"확인해"],[1385167,1385300,"볼"],[1385370,1385520,"수"],[1385520,1386000,"있습니다."]],"textEdited":"유저와 아이템에 대한 레프리젠테이션 파워가 더 좋기 때문입니다. 이 레프레젠테이션 파워가 좋다. 즉 유저와 아이템을 더 잘 표현한다라는 것은 시각화를 했을 때도 정성적으로 확인해 볼 수 있습니다."},{"start":1386200,"end":1399400,"text":"이제 오른쪽에 있는 그림을 보시면 왼쪽이 MF의 인베딩이고요. 오른쪽이 NGCF 세 번째 레이어의 인베딩입니다. 그래서 이것을 보기 좋게 표현한 것인데요.","confidence":0.8064,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1386790,1386960,"이제"],[1386960,1387380,"오른쪽에"],[1387380,1387540,"있는"],[1387570,1387860,"그림을"],[1387860,1388160,"보시면"],[1388270,1388760,"왼쪽이"],[1389590,1390860,"MF의"],[1390910,1392120,"인베딩이고요."],[1392850,1393360,"오른쪽이"],[1393910,1394500,"NGCF"],[1394570,1394720,"세"],[1394720,1394980,"번째"],[1395050,1395880,"레이어의"],[1395950,1396580,"인베딩입니다."],[1397390,1397580,"그래서"],[1397580,1397860,"이것을"],[1397890,1398100,"보기"],[1398100,1398320,"좋게"],[1398330,1398640,"표현한"],[1398640,1399060,"것인데요."]],"textEdited":"이제 오른쪽에 있는 그림을 보시면 왼쪽이 MF의 인베딩이고요. 오른쪽이 NGCF 세 번째 레이어의 인베딩입니다. 그래서 이것을 보기 좋게 표현한 것인데요."},{"start":1399400,"end":1413500,"text":"여기서 이 별표들은 유저 그리고 아이템은 동그라미를 의미합니다. 그리고 같은 색을 가지는 유저는 그 유저가 해당 아이템을 과거에 소비하였음을 의미합니다.","confidence":0.9465,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1400030,1400360,"여기서"],[1400370,1400520,"이"],[1400610,1402040,"별표들은"],[1402730,1403000,"유저"],[1403570,1403760,"그리고"],[1403790,1404280,"아이템은"],[1404790,1405520,"동그라미를"],[1405550,1406000,"의미합니다."],[1406530,1406740,"그리고"],[1406790,1407100,"같은"],[1407190,1407460,"색을"],[1407460,1407820,"가지는"],[1408110,1409440,"유저는"],[1409830,1409980,"그"],[1409980,1410340,"유저가"],[1410570,1410820,"해당"],[1410850,1411340,"아이템을"],[1411550,1411900,"과거에"],[1412270,1412980,"소비하였음을"],[1412990,1413480,"의미합니다."]],"textEdited":"여기서 이 별표들은 유저 그리고 아이템은 동그라미를 의미합니다. 그리고 같은 색을 가지는 유저는 그 유저가 해당 아이템을 과거에 소비하였음을 의미합니다."},{"start":1413500,"end":1428100,"text":"그 시각화된 결과를 통해서 보시면 오른쪽에 있는 NGCF는 이 유저를 기준으로 그 유저가 과거에 소비한 아이템들이 비슷한 공간에 인베딩이 되어 있죠. 마찬가지로 이 노란색 유저가 과거에 이 아이템들을 소비했고","confidence":0.9763,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1413810,1413960,"그"],[1413970,1414380,"시각화된"],[1414410,1414760,"결과를"],[1414760,1415080,"통해서"],[1415610,1415980,"보시면"],[1416810,1417194,"오른쪽에"],[1417194,1417340,"있는"],[1417340,1418020,"NGCF는"],[1418990,1419140,"이"],[1419140,1419420,"유저를"],[1419420,1419760,"기준으로"],[1419810,1419927,"그"],[1419927,1420240,"유저가"],[1420310,1420620,"과거에"],[1420630,1420960,"소비한"],[1420970,1421560,"아이템들이"],[1422130,1422540,"비슷한"],[1422610,1422940,"공간에"],[1422950,1423314,"인베딩이"],[1423314,1423520,"되어"],[1423520,1423740,"있죠."],[1424470,1425020,"마찬가지로"],[1425020,1425127,"이"],[1425127,1425460,"노란색"],[1425460,1425780,"유저가"],[1426070,1426420,"과거에"],[1426510,1426660,"이"],[1426660,1427240,"아이템들을"],[1427250,1427840,"소비했고"]],"textEdited":"그 시각화된 결과를 통해서 보시면 오른쪽에 있는 NGCF는 이 유저를 기준으로 그 유저가 과거에 소비한 아이템들이 비슷한 공간에 인베딩이 되어 있죠. 마찬가지로 이 노란색 유저가 과거에 이 아이템들을 소비했고"},{"start":1428100,"end":1442600,"text":"그래서 그 인베딩 유저와 아이템의 인베딩이 비슷한 공간에 임베딩이 되어 있습니다. 근데 매트리스 팩토라이제이션 같은 경우에는 그 유저가 소비한 아이템 노란색을 봤을 때 이 유저의 인베딩과 아이템의 인베딩이 다소 떨어져 있는 것을 볼 수 있죠.","confidence":0.9334,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1428310,1428507,"그래서"],[1428507,1428640,"그"],[1428670,1429020,"인베딩"],[1429070,1429400,"유저와"],[1429400,1429740,"아이템의"],[1429740,1430140,"인베딩이"],[1430170,1430580,"비슷한"],[1430630,1430980,"공간에"],[1431050,1431447,"임베딩이"],[1431447,1431640,"되어"],[1431640,1432020,"있습니다."],[1432790,1432960,"근데"],[1432960,1433280,"매트리스"],[1433280,1433820,"팩토라이제이션"],[1433830,1434220,"같은"],[1434220,1434600,"경우에는"],[1435310,1435460,"그"],[1435460,1435780,"유저가"],[1435830,1436160,"소비한"],[1436170,1436480,"아이템"],[1436770,1437220,"노란색을"],[1437220,1437460,"봤을"],[1437470,1437620,"때"],[1437810,1437960,"이"],[1437960,1438320,"유저의"],[1438350,1438860,"인베딩과"],[1439030,1439500,"아이템의"],[1439500,1439960,"인베딩이"],[1440030,1440280,"다소"],[1440280,1440600,"떨어져"],[1441010,1441187,"있는"],[1441187,1441440,"것을"],[1441440,1441560,"볼"],[1441570,1441674,"수"],[1441674,1441900,"있죠."]],"textEdited":"그래서 그 인베딩 유저와 아이템의 인베딩이 비슷한 공간에 임베딩이 되어 있습니다. 근데 매트리스 팩토라이제이션 같은 경우에는 그 유저가 소비한 아이템 노란색을 봤을 때 이 유저의 인베딩과 아이템의 인베딩이 다소 떨어져 있는 것을 볼 수 있죠."},{"start":1442600,"end":1455000,"text":"그 이유는 이 유저 아이템 인베딩을 학습할 때 우리가 유저 아이템의 상호 작용을 직접적으로 그 인베딩에 주입했다. 따라서 그 상호작용이 더 인베딩에 잘 표현된다","confidence":0.9389,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1443250,1443400,"그"],[1443410,1443800,"이유는"],[1445210,1445360,"이"],[1445360,1445660,"유저"],[1445670,1446020,"아이템"],[1446510,1446960,"인베딩을"],[1447010,1447400,"학습할"],[1447400,1447540,"때"],[1447540,1447740,"우리가"],[1448410,1448647,"유저"],[1448647,1448960,"아이템의"],[1448970,1449160,"상호"],[1449170,1449500,"작용을"],[1449690,1450220,"직접적으로"],[1450230,1450380,"그"],[1450380,1450780,"인베딩에"],[1450830,1451340,"주입했다."],[1452250,1452600,"따라서"],[1452990,1453140,"그"],[1453150,1453607,"상호작용이"],[1453607,1453740,"더"],[1453770,1454180,"인베딩에"],[1454190,1454340,"잘"],[1454390,1454840,"표현된다"]],"textEdited":"그 이유는 이 유저 아이템 인베딩을 학습할 때 우리가 유저 아이템의 상호 작용을 직접적으로 그 인베딩에 주입했다. 따라서 그 상호작용이 더 인베딩에 잘 표현된다"},{"start":1455000,"end":1469200,"text":"라고 볼 수 있습니다. 그리고 이 레이어가 점점 많아질수록 더 명확하게 구분되고 있습니다. 여기까지가 neural graf 컬래버이트 필터링에 대한 전반적인 내용이었고요. 이 GNN 기반 추천 시스템에서 제일 중요한 개념","confidence":0.8889,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1455250,1455480,"라고"],[1455480,1455620,"볼"],[1455630,1455734,"수"],[1455734,1456100,"있습니다."],[1456390,1456640,"그리고"],[1456690,1456840,"이"],[1456840,1457260,"레이어가"],[1457650,1457940,"점점"],[1457940,1458460,"많아질수록"],[1458510,1458660,"더"],[1458660,1459060,"명확하게"],[1459090,1459880,"구분되고"],[1459880,1460280,"있습니다."],[1460910,1461440,"여기까지가"],[1461890,1462240,"neural"],[1462240,1462500,"graf"],[1462530,1462960,"컬래버이트"],[1462960,1463560,"필터링에"],[1463560,1463760,"대한"],[1463990,1464420,"전반적인"],[1464430,1465060,"내용이었고요."],[1465590,1465740,"이"],[1465770,1466240,"GNN"],[1466490,1466720,"기반"],[1466790,1467040,"추천"],[1467050,1467580,"시스템에서"],[1468010,1468240,"제일"],[1468370,1468700,"중요한"],[1468770,1469000,"개념"]],"textEdited":"라고 볼 수 있습니다. 그리고 이 레이어가 점점 많아질수록 더 명확하게 구분되고 있습니다. 여기까지가 neural graf 컬래버이트 필터링에 대한 전반적인 내용이었고요. 이 GNN 기반 추천 시스템에서 제일 중요한 개념"},{"start":1469200,"end":1481900,"text":"즉 인베딩 전파되는 레이어 어떻게 인베딩이 계속해서 학습이 되고 그 상호작용이 어떻게 인베딩 단에서 주입되는지를 담고 있는 제일 중요한 개념을 담고 있는 모델이기 때문에","confidence":0.9807,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1469490,1469640,"즉"],[1469710,1470120,"인베딩"],[1470250,1470780,"전파되는"],[1471310,1471620,"레이어"],[1472370,1472700,"어떻게"],[1472750,1473220,"인베딩이"],[1473410,1473800,"계속해서"],[1473870,1474147,"학습이"],[1474147,1474440,"되고"],[1475190,1475340,"그"],[1475370,1475920,"상호작용이"],[1475930,1476240,"어떻게"],[1476290,1476660,"인베딩"],[1476670,1477020,"단에서"],[1477030,1478480,"주입되는지를"],[1479050,1479334,"담고"],[1479334,1479500,"있는"],[1479550,1479740,"제일"],[1479810,1480120,"중요한"],[1480170,1480460,"개념을"],[1480460,1480694,"담고"],[1480694,1480860,"있는"],[1480860,1481240,"모델이기"],[1481240,1481580,"때문에"]],"textEdited":"즉 인베딩 전파되는 레이어 어떻게 인베딩이 계속해서 학습이 되고 그 상호작용이 어떻게 인베딩 단에서 주입되는지를 담고 있는 제일 중요한 개념을 담고 있는 모델이기 때문에"},{"start":1481900,"end":1489600,"text":"자세하게 설명을 했습니다. 이어서는 NGCF보다는 조금 더 간단한 GCN 즉 그래프","confidence":0.8855,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1482390,1482780,"자세하게"],[1482790,1483100,"설명을"],[1483100,1483480,"했습니다."],[1483890,1484380,"이어서는"],[1484870,1485720,"NGCF보다는"],[1485730,1485960,"조금"],[1486010,1486160,"더"],[1486950,1487340,"간단한"],[1487970,1488780,"GCN"],[1488930,1489080,"즉"],[1489230,1489600,"그래프"]],"textEdited":"자세하게 설명을 했습니다. 이어서는 NGCF보다는 조금 더 간단한 GCN 즉 그래프"},{"start":1489600,"end":1502400,"text":"컨볼루션 네트워크의 가장 핵심적인 부분만 사용하여서 더 정확하면서도 파라미터 수 모델 캐파시티는 좀 작은 라이트 지시엔이라는 논문 그 모델을 살펴보도록 하겠습니다.","confidence":0.8747,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1489870,1490260,"컨볼루션"],[1490270,1490860,"네트워크의"],[1490930,1491140,"가장"],[1491250,1491740,"핵심적인"],[1491770,1492100,"부분만"],[1492110,1492620,"사용하여서"],[1493230,1493380,"더"],[1493430,1494120,"정확하면서도"],[1494530,1495000,"파라미터"],[1495050,1495200,"수"],[1495610,1495860,"모델"],[1495890,1497040,"캐파시티는"],[1497110,1497260,"좀"],[1497430,1497700,"작은"],[1498430,1498780,"라이트"],[1498780,1499400,"지시엔이라는"],[1499950,1500180,"논문"],[1500770,1500887,"그"],[1500887,1501200,"모델을"],[1501230,1501700,"살펴보도록"],[1501700,1502300,"하겠습니다."]],"textEdited":"컨볼루션 네트워크의 가장 핵심적인 부분만 사용하여서 더 정확하면서도 파라미터 수 모델 캐파시티는 좀 작은 라이트 지시엔이라는 논문 그 모델을 살펴보도록 하겠습니다."},{"start":1502400,"end":1517100,"text":"라이트 GCN의 아이디어는 먼저 가벼운 모델입니다. 기존의 NGCF 모델은 인베딩 전파 레이어에서 컨볼루션을 할 때 매번 학습 파라미터를 인베딩에 곱하고 거기에","confidence":0.8447,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1503290,1503640,"라이트"],[1503640,1504087,"GCN의"],[1504087,1504620,"아이디어는"],[1505890,1506100,"먼저"],[1506190,1506560,"가벼운"],[1506730,1507240,"모델입니다."],[1508070,1508460,"기존의"],[1509030,1509620,"NGCF"],[1509620,1509940,"모델은"],[1510510,1510860,"인베딩"],[1510910,1511200,"전파"],[1511200,1511660,"레이어에서"],[1512090,1513080,"컨볼루션을"],[1513170,1513320,"할"],[1513350,1513500,"때"],[1513930,1514180,"매번"],[1514270,1514500,"학습"],[1514500,1515040,"파라미터를"],[1515040,1515440,"인베딩에"],[1515490,1515960,"곱하고"],[1516470,1516820,"거기에"]],"textEdited":"라이트 GCN의 아이디어는 먼저 가벼운 모델입니다. 기존의 NGCF 모델은 인베딩 전파 레이어에서 컨볼루션을 할 때 매번 학습 파라미터를 인베딩에 곱하고 거기에"},{"start":1517100,"end":1528800,"text":"노리어 트랜스포메이션 리키렐로를 사용했는데요. 라이트 지시에는 그 인베딩 노드의 파라미터를 곱하지도 않고 단순하게 인베딩을 가중합하는 것이 전부입니다.","confidence":0.8486,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1517390,1517900,"노리어"],[1517910,1518540,"트랜스포메이션"],[1518830,1519440,"리키렐로를"],[1519440,1520040,"사용했는데요."],[1520650,1521000,"라이트"],[1521000,1521480,"지시에는"],[1522050,1522200,"그"],[1522210,1522620,"인베딩"],[1522810,1523180,"노드의"],[1523390,1523920,"파라미터를"],[1523930,1524347,"곱하지도"],[1524347,1524660,"않고"],[1525050,1525580,"단순하게"],[1525990,1526500,"인베딩을"],[1526590,1527280,"가중합하는"],[1527280,1527580,"것이"],[1527970,1528600,"전부입니다."]],"textEdited":"노리어 트랜스포메이션 리키렐로를 사용했는데요. 라이트 지시에는 그 인베딩 노드의 파라미터를 곱하지도 않고 단순하게 인베딩을 가중합하는 것이 전부입니다."},{"start":1528800,"end":1543100,"text":"그래서 학습 파라미터와 연산량이 NGCF에 비해서 훨씬 감소하게 됩니다. 또한 이 레이어의 깊이 즉 인베딩 전파 레이어의 깊이가 계속 깊어질수록 그 강도가 약해질 것이라는 아이디어를 사용하여서 그 라이트 GC에는","confidence":0.9533,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1529370,1529600,"그래서"],[1529610,1529840,"학습"],[1529840,1530380,"파라미터와"],[1530410,1530940,"연산량이"],[1531310,1531934,"NGCF에"],[1531934,1532220,"비해서"],[1532250,1532560,"훨씬"],[1533010,1533427,"감소하게"],[1533427,1533760,"됩니다."],[1534450,1534660,"또한"],[1534660,1534800,"이"],[1534800,1535200,"레이어의"],[1535250,1535520,"깊이"],[1535590,1535740,"즉"],[1535810,1536160,"인베딩"],[1536190,1536460,"전파"],[1536490,1536840,"레이어의"],[1536870,1537180,"깊이가"],[1537180,1537340,"계속"],[1537370,1538000,"깊어질수록"],[1538370,1538520,"그"],[1538590,1538920,"강도가"],[1538930,1539340,"약해질"],[1539370,1539760,"것이라는"],[1539770,1540300,"아이디어를"],[1540300,1540820,"사용하여서"],[1541390,1541540,"그"],[1542090,1542420,"라이트"],[1542420,1542820,"GC에는"]],"textEdited":"그래서 학습 파라미터와 연산량이 NGCF에 비해서 훨씬 감소하게 됩니다. 또한 이 레이어의 깊이 즉 인베딩 전파 레이어의 깊이가 계속 깊어질수록 그 강도가 약해질 것이라는 아이디어를 사용하여서 그 라이트 GC에는"},{"start":1543100,"end":1557600,"text":"인베딩을 합칠 때도 굉장히 단순한 방법을 사용하였습니다. 다음 그림을 통해 조금 더 두 모델의 차이점을 비교해 보겠습니다. 크게 두 가지 부분이 다릅니다. 처음에 인베딩 원핫 인코딩이 인베딩 되는 것은 동일하고요.","confidence":0.9511,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1543370,1543820,"인베딩을"],[1543830,1544100,"합칠"],[1544100,1544340,"때도"],[1544770,1545040,"굉장히"],[1545110,1545460,"단순한"],[1545530,1545860,"방법을"],[1545910,1546640,"사용하였습니다."],[1547070,1547280,"다음"],[1547330,1547620,"그림을"],[1547620,1547860,"통해"],[1548410,1548660,"조금"],[1548660,1548800,"더"],[1549370,1549520,"두"],[1549570,1549880,"모델의"],[1549890,1550300,"차이점을"],[1550330,1550607,"비교해"],[1550607,1551120,"보겠습니다."],[1551530,1551780,"크게"],[1551830,1551980,"두"],[1551980,1552240,"가지"],[1552250,1552514,"부분이"],[1552514,1552900,"다릅니다."],[1553670,1554000,"처음에"],[1554070,1554460,"인베딩"],[1555150,1555440,"원핫"],[1555490,1555960,"인코딩이"],[1555970,1556280,"인베딩"],[1556280,1556467,"되는"],[1556467,1556700,"것은"],[1556710,1557300,"동일하고요."]],"textEdited":"인베딩을 합칠 때도 굉장히 단순한 방법을 사용하였습니다. 다음 그림을 통해 조금 더 두 모델의 차이점을 비교해 보겠습니다. 크게 두 가지 부분이 다릅니다. 처음에 인베딩 원핫 인코딩이 인베딩 되는 것은 동일하고요."},{"start":1557600,"end":1568800,"text":"먼저 이 가운데 있는 인베딩 전파 레이어 라이트 지센 같은 경우는 이 부분인데요. 이 부분이 라이트 지센이 좀 더 간단하게 이루어져 있고요.","confidence":0.8542,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1558290,1558580,"먼저"],[1559230,1559380,"이"],[1559490,1559900,"가운데"],[1559930,1560120,"있는"],[1560330,1560700,"인베딩"],[1560770,1561060,"전파"],[1561090,1561420,"레이어"],[1562690,1563020,"라이트"],[1563020,1563220,"지센"],[1563230,1563427,"같은"],[1563427,1563647,"경우는"],[1563647,1563780,"이"],[1563780,1564240,"부분인데요."],[1564990,1565140,"이"],[1565140,1565460,"부분이"],[1565930,1566260,"라이트"],[1566260,1566600,"지센이"],[1566600,1566740,"좀"],[1566750,1566900,"더"],[1567550,1568000,"간단하게"],[1568000,1568340,"이루어져"],[1568340,1568660,"있고요."]],"textEdited":"먼저 이 가운데 있는 인베딩 전파 레이어 라이트 지센 같은 경우는 이 부분인데요. 이 부분이 라이트 지센이 좀 더 간단하게 이루어져 있고요."},{"start":1568800,"end":1582200,"text":"그리고 이 마지막에 프리딕션 레이어 부분도 기본에 기존에 NGCF는 컴케이트 네이트를 했는데 여기서는 단순하게 가중합으로 더한다고 표현되어 있습니다. 여기 웨이트드 썸이라고 표현되어 있죠","confidence":0.8148,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1569370,1569680,"그리고"],[1570290,1570440,"이"],[1570470,1570940,"마지막에"],[1571010,1571440,"프리딕션"],[1571490,1571820,"레이어"],[1571970,1572400,"부분도"],[1573170,1573500,"기본에"],[1573630,1573940,"기존에"],[1573940,1574640,"NGCF는"],[1574690,1575087,"컴케이트"],[1575087,1575460,"네이트를"],[1575460,1575780,"했는데"],[1576010,1576420,"여기서는"],[1576630,1577020,"단순하게"],[1577070,1577680,"가중합으로"],[1578790,1579260,"더한다고"],[1579270,1579554,"표현되어"],[1579554,1579860,"있습니다."],[1579870,1580040,"여기"],[1580050,1580440,"웨이트드"],[1580440,1580787,"썸이라고"],[1580787,1581140,"표현되어"],[1581140,1581340,"있죠"]],"textEdited":"그리고 이 마지막에 프리딕션 레이어 부분도 기본에 기존에 NGCF는 컴케이트 네이트를 했는데 여기서는 단순하게 가중합으로 더한다고 표현되어 있습니다. 여기 웨이트드 썸이라고 표현되어 있죠"},{"start":1582200,"end":1596400,"text":"이 각각의 차이를 좀 더 수식을 통해서 자세히 비교해 보도록 하겠습니다. 먼저 인베딩 전파 레이어 부분의 차이점을 봅시다. 이 왼쪽에 있는 수식이 방금 전 슬라이드에서 다뤘던 방금 전 파트에서 다뤘던 NGCF","confidence":0.9725,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1582470,1582620,"이"],[1582650,1583000,"각각의"],[1583030,1583380,"차이를"],[1583450,1583567,"좀"],[1583567,1583700,"더"],[1583710,1584080,"수식을"],[1584090,1584400,"통해서"],[1584810,1585160,"자세히"],[1585230,1585494,"비교해"],[1585494,1585780,"보도록"],[1585780,1586240,"하겠습니다."],[1587110,1587380,"먼저"],[1587450,1587780,"인베딩"],[1587810,1588120,"전파"],[1588170,1588480,"레이어"],[1588510,1588840,"부분의"],[1588890,1589340,"차이점을"],[1589340,1589700,"봅시다."],[1590370,1590520,"이"],[1590550,1590960,"왼쪽에"],[1590960,1591120,"있는"],[1591170,1591540,"수식이"],[1592210,1592460,"방금"],[1592460,1592580,"전"],[1592670,1593200,"슬라이드에서"],[1593210,1593580,"다뤘던"],[1593970,1594200,"방금"],[1594210,1594360,"전"],[1594690,1595060,"파트에서"],[1595060,1595460,"다뤘던"],[1595630,1596300,"NGCF"]],"textEdited":"이 각각의 차이를 좀 더 수식을 통해서 자세히 비교해 보도록 하겠습니다. 먼저 인베딩 전파 레이어 부분의 차이점을 봅시다. 이 왼쪽에 있는 수식이 방금 전 슬라이드에서 다뤘던 방금 전 파트에서 다뤘던 NGCF"},{"start":1596400,"end":1607300,"text":"모델의 수식이었고요. 오른쪽이 현재 이야기하고 있는 라이트 GCN의 인베딩 전파 레이어의 계산 수식입니다. NGCF 같은 경우에는","confidence":0.8371,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1596610,1596940,"모델의"],[1596940,1597580,"수식이었고요."],[1598110,1598580,"오른쪽이"],[1598950,1599220,"현재"],[1600390,1600794,"이야기하고"],[1600794,1600960,"있는"],[1600960,1601260,"라이트"],[1601260,1602060,"GCN의"],[1602610,1602920,"인베딩"],[1602930,1603200,"전파"],[1603230,1603660,"레이어의"],[1604070,1604340,"계산"],[1604370,1604880,"수식입니다."],[1605670,1606260,"NGCF"],[1606260,1606467,"같은"],[1606467,1606900,"경우에는"]],"textEdited":"모델의 수식이었고요. 오른쪽이 현재 이야기하고 있는 라이트 GCN의 인베딩 전파 레이어의 계산 수식입니다. NGCF 같은 경우에는"},{"start":1607300,"end":1621100,"text":"케차 인베딩 즉 바로 전 전파되기 전에 인베딩에다가 각각의 학습 파라미터를 곱하고 거기에 액티베이션 펑션까지 통과시켜서 결국 케이 플러스 1차 인베딩을 구합니다.","confidence":0.8982,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1607590,1607960,"케차"],[1608030,1608420,"인베딩"],[1608750,1608900,"즉"],[1609210,1609460,"바로"],[1609510,1609660,"전"],[1610210,1610627,"전파되기"],[1610627,1610900,"전에"],[1610910,1611620,"인베딩에다가"],[1612270,1612700,"각각의"],[1613170,1613440,"학습"],[1613470,1614040,"파라미터를"],[1614050,1614480,"곱하고"],[1615150,1615500,"거기에"],[1617030,1617520,"액티베이션"],[1617570,1618140,"펑션까지"],[1618410,1618900,"통과시켜서"],[1618950,1619180,"결국"],[1619190,1619400,"케이"],[1619410,1619680,"플러스"],[1619680,1619900,"1차"],[1619930,1620360,"인베딩을"],[1620360,1620860,"구합니다."]],"textEdited":"케차 인베딩 즉 바로 전 전파되기 전에 인베딩에다가 각각의 학습 파라미터를 곱하고 거기에 액티베이션 펑션까지 통과시켜서 결국 케이 플러스 1차 인베딩을 구합니다."},{"start":1621100,"end":1633800,"text":"다소 복잡하죠. 그러나 라이트 GCM 같은 경우에는 피처 트랜스포메이션이나 논리뉴 액티베이션 같은 펑션 없이 단순하게 바로 전에 전파된 바로 전에 있는 레이어, 즉","confidence":0.8621,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1621370,1621620,"다소"],[1621650,1622140,"복잡하죠."],[1622770,1623020,"그러나"],[1623070,1623440,"라이트"],[1623440,1623700,"GCM"],[1623700,1623907,"같은"],[1623907,1624320,"경우에는"],[1624830,1625120,"피처"],[1625120,1626000,"트랜스포메이션이나"],[1626310,1626840,"논리뉴"],[1626910,1627380,"액티베이션"],[1627410,1627640,"같은"],[1627690,1627960,"펑션"],[1628030,1628320,"없이"],[1628630,1629120,"단순하게"],[1630390,1630660,"바로"],[1630670,1631160,"전에"],[1631230,1631640,"전파된"],[1631930,1632180,"바로"],[1632180,1632480,"전에"],[1632550,1632740,"있는"],[1632850,1633160,"레이어,"],[1633430,1633580,"즉"]],"textEdited":"다소 복잡하죠. 그러나 라이트 GCM 같은 경우에는 피처 트랜스포메이션이나 논리뉴 액티베이션 같은 펑션 없이 단순하게 바로 전에 전파된 바로 전에 있는 레이어, 즉"},{"start":1633800,"end":1645900,"text":"k차 인베딩 레이어에 있는 인베딩 값을 단순히 가중치를 사용해서 합하는 것이 전부입니다. 또한 연결된 이웃 노드 정보만을 사용했기 때문에 NGCF에서는","confidence":0.9441,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1634070,1634500,"k차"],[1634790,1635120,"인베딩"],[1635190,1635560,"레이어에"],[1635560,1635740,"있는"],[1635950,1636300,"인베딩"],[1636300,1636620,"값을"],[1636730,1637080,"단순히"],[1637850,1638360,"가중치를"],[1638360,1638740,"사용해서"],[1638770,1639127,"합하는"],[1639127,1639360,"것이"],[1639360,1639860,"전부입니다."],[1640630,1640860,"또한"],[1641010,1641560,"연결된"],[1641810,1642040,"이웃"],[1642070,1642340,"노드"],[1642410,1642940,"정보만을"],[1642940,1643307,"사용했기"],[1643307,1643680,"때문에"],[1644390,1645380,"NGCF에서는"]],"textEdited":"k차 인베딩 레이어에 있는 인베딩 값을 단순히 가중치를 사용해서 합하는 것이 전부입니다. 또한 연결된 이웃 노드 정보만을 사용했기 때문에 NGCF에서는"},{"start":1645900,"end":1660900,"text":"다음과 같은 셀프 커넥션 즉 유저에서 유저로 가는 표현도 있는데요. 어 라이트 GCM 같은 경우에는 이 유저에 대한 표현을 자기 자신은 빼고 연결되어 있는 아이템의 인베딩만을 가지고 표현합니다. 그래서 최종적으로 보시면은","confidence":0.9425,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1646190,1646467,"다음과"],[1646467,1646660,"같은"],[1646690,1646960,"셀프"],[1646960,1647300,"커넥션"],[1647510,1647660,"즉"],[1647660,1648100,"유저에서"],[1648100,1648420,"유저로"],[1648430,1648660,"가는"],[1649030,1649347,"표현도"],[1649347,1649720,"있는데요."],[1650130,1650280,"어"],[1650970,1651320,"라이트"],[1651320,1651600,"GCM"],[1651600,1651807,"같은"],[1651807,1652220,"경우에는"],[1652950,1653100,"이"],[1653100,1653367,"유저에"],[1653367,1653560,"대한"],[1653650,1654040,"표현을"],[1654210,1654400,"자기"],[1654410,1654740,"자신은"],[1654770,1655080,"빼고"],[1655310,1655760,"연결되어"],[1655760,1655920,"있는"],[1655990,1656620,"아이템의"],[1656690,1657320,"인베딩만을"],[1657320,1657660,"가지고"],[1658130,1658620,"표현합니다."],[1658910,1659140,"그래서"],[1659410,1659840,"최종적으로"],[1659850,1660340,"보시면은"]],"textEdited":"다음과 같은 셀프 커넥션 즉 유저에서 유저로 가는 표현도 있는데요. 어 라이트 GCM 같은 경우에는 이 유저에 대한 표현을 자기 자신은 빼고 연결되어 있는 아이템의 인베딩만을 가지고 표현합니다. 그래서 최종적으로 보시면은"},{"start":1660900,"end":1675100,"text":"이 베딩이 NGCF 모델은 인베딩 전파 레이어의 학습 파라미터가 존재하지만 라이트 GC에는 이 인베딩 전파 레이어의 학습 파라미터가 1개도 존재하지 않습니다. 처음에 입력될 때 원핫 인코딩이","confidence":0.8617,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1661110,1661260,"이"],[1661470,1662460,"베딩이"],[1662490,1663080,"NGCF"],[1663090,1663480,"모델은"],[1664090,1664440,"인베딩"],[1664490,1664780,"전파"],[1664790,1665160,"레이어의"],[1665210,1665420,"학습"],[1665420,1665920,"파라미터가"],[1665930,1666520,"존재하지만"],[1666990,1667320,"라이트"],[1667320,1667720,"GC에는"],[1667890,1668040,"이"],[1668110,1668520,"인베딩"],[1669170,1669440,"전파"],[1669440,1669740,"레이어의"],[1669830,1670060,"학습"],[1670060,1670500,"파라미터가"],[1670550,1670920,"1개도"],[1671290,1671720,"존재하지"],[1671720,1672040,"않습니다."],[1672230,1672600,"처음에"],[1672670,1673100,"입력될"],[1673150,1673300,"때"],[1673730,1674020,"원핫"],[1674210,1674720,"인코딩이"]],"textEdited":"이 베딩이 NGCF 모델은 인베딩 전파 레이어의 학습 파라미터가 존재하지만 라이트 GC에는 이 인베딩 전파 레이어의 학습 파라미터가 1개도 존재하지 않습니다. 처음에 입력될 때 원핫 인코딩이"},{"start":1675100,"end":1688500,"text":"이 인베딩으로 표현될 때만 그 0번째 인베딩 레이어에서만 학습 파라미터가 존재하고 그 이후에는 파라미터 학습 파라미터의 추가 학습 없이 단순하게 계속 가중 평균을 활용한 전파만","confidence":0.9419,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1675290,1675440,"이"],[1675440,1675920,"인베딩으로"],[1675920,1676300,"표현될"],[1676370,1676640,"때만"],[1677090,1677240,"그"],[1677370,1677940,"0번째"],[1678130,1678480,"인베딩"],[1678510,1679080,"레이어에서만"],[1679090,1679247,"학습"],[1679247,1679680,"파라미터가"],[1679680,1680200,"존재하고"],[1680750,1680900,"그"],[1680930,1681360,"이후에는"],[1682070,1682540,"파라미터"],[1683050,1683280,"학습"],[1683280,1683760,"파라미터의"],[1683790,1684020,"추가"],[1684020,1684260,"학습"],[1684260,1684500,"없이"],[1685350,1685800,"단순하게"],[1685830,1686020,"계속"],[1686090,1686380,"가중"],[1686470,1686840,"평균을"],[1687090,1687400,"활용한"],[1687510,1687920,"전파만"]],"textEdited":"이 인베딩으로 표현될 때만 그 0번째 인베딩 레이어에서만 학습 파라미터가 존재하고 그 이후에는 파라미터 학습 파라미터의 추가 학습 없이 단순하게 계속 가중 평균을 활용한 전파만"},{"start":1688500,"end":1700300,"text":"사용하게 됩니다. 네 마지막은 예측 레이어입니다. 바로 전에 인베딩 전파 레이어에서 구해진 인베딩을 어그리게이트 하여서 최종 예측을 수행하는","confidence":0.9432,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1688830,1689260,"사용하게"],[1689260,1689580,"됩니다."],[1690110,1690260,"네"],[1690290,1690780,"마지막은"],[1690910,1691220,"예측"],[1691330,1691840,"레이어입니다."],[1692410,1692627,"바로"],[1692627,1692900,"전에"],[1693110,1693540,"인베딩"],[1693610,1693880,"전파"],[1693930,1694400,"레이어에서"],[1694490,1694800,"구해진"],[1694930,1695420,"인베딩을"],[1696310,1696847,"어그리게이트"],[1696847,1697160,"하여서"],[1697330,1697580,"최종"],[1697630,1698080,"예측을"],[1698650,1699060,"수행하는"]],"textEdited":"사용하게 됩니다. 네 마지막은 예측 레이어입니다. 바로 전에 인베딩 전파 레이어에서 구해진 인베딩을 어그리게이트 하여서 최종 예측을 수행하는"},{"start":1700300,"end":1713000,"text":"레이어입니다. 이제 이 예측 레이어도 라이트 GCN는 NGCF보다 조금 더 가볍게 다르게 구성하였는데요. NGCF 같은 경우에는 0차 인베딩부터","confidence":0.9441,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1700570,1701100,"레이어입니다."],[1701590,1701760,"이제"],[1701770,1701920,"이"],[1702450,1702800,"예측"],[1702930,1703340,"레이어도"],[1704030,1704400,"라이트"],[1704400,1704860,"GCN는"],[1704990,1705720,"NGCF보다"],[1705720,1705920,"조금"],[1705930,1706080,"더"],[1706530,1706980,"가볍게"],[1707610,1707940,"다르게"],[1707950,1708700,"구성하였는데요."],[1709210,1709740,"NGCF"],[1709740,1709947,"같은"],[1709947,1710380,"경우에는"],[1711250,1711600,"0차"],[1712010,1712660,"인베딩부터"]],"textEdited":"레이어입니다. 이제 이 예측 레이어도 라이트 GCN는 NGCF보다 조금 더 가볍게 다르게 구성하였는데요. NGCF 같은 경우에는 0차 인베딩부터"},{"start":1713000,"end":1725600,"text":"엘차 인베딩까지 모두 가로로 컨케이트네이트에서 유저와 아이템의 임베딩을 구했습니다. 이 라이트 GCM 같은 경우에는 가로로 컨케이트네이트 하지 않고 이 알파 케라는 가중치를 사용하여서","confidence":0.8069,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1713330,1713640,"엘차"],[1713690,1714240,"인베딩까지"],[1714250,1714500,"모두"],[1714850,1715200,"가로로"],[1715250,1716180,"컨케이트네이트에서"],[1716790,1717120,"유저와"],[1717130,1717520,"아이템의"],[1717520,1718000,"임베딩을"],[1718290,1718840,"구했습니다."],[1719230,1719347,"이"],[1719347,1719660,"라이트"],[1719660,1719940,"GCM"],[1719940,1720147,"같은"],[1720147,1720600,"경우에는"],[1721510,1721840,"가로로"],[1721870,1722580,"컨케이트네이트"],[1722580,1722760,"하지"],[1722760,1723040,"않고"],[1723110,1723260,"이"],[1723410,1723720,"알파"],[1723830,1724300,"케라는"],[1724350,1724860,"가중치를"],[1724860,1725360,"사용하여서"]],"textEdited":"엘차 인베딩까지 모두 가로로 컨케이트네이트에서 유저와 아이템의 임베딩을 구했습니다. 이 라이트 GCM 같은 경우에는 가로로 컨케이트네이트 하지 않고 이 알파 케라는 가중치를 사용하여서"},{"start":1725600,"end":1739000,"text":"단순하게 더했는데요. 컨케이트네이트를 하면 계속해서 차원이 옆으로 늘어나지만 이렇게 가중합을 할 경우에는 정보를 조금 더 압축해서 표현하는 것입니다. 따라서 이 가중치를 어떻게 사용하면서 압축하느냐가 중요한 부분인데요.","confidence":0.9056,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1725950,1726380,"단순하게"],[1726410,1727100,"더했는데요."],[1727710,1728427,"컨케이트네이트를"],[1728427,1728620,"하면"],[1728670,1729120,"계속해서"],[1729170,1729434,"차원이"],[1729434,1729700,"옆으로"],[1729700,1730180,"늘어나지만"],[1730650,1730840,"이렇게"],[1730910,1731440,"가중합을"],[1731450,1731600,"할"],[1731610,1732020,"경우에는"],[1732270,1732600,"정보를"],[1732610,1732800,"조금"],[1732800,1732940,"더"],[1733030,1733520,"압축해서"],[1733550,1733887,"표현하는"],[1733887,1734260,"것입니다."],[1734550,1734880,"따라서"],[1734890,1735040,"이"],[1735090,1735740,"가중치를"],[1736030,1736380,"어떻게"],[1736430,1737120,"사용하면서"],[1737170,1737860,"압축하느냐가"],[1737990,1738300,"중요한"],[1738300,1738780,"부분인데요."]],"textEdited":"단순하게 더했는데요. 컨케이트네이트를 하면 계속해서 차원이 옆으로 늘어나지만 이렇게 가중합을 할 경우에는 정보를 조금 더 압축해서 표현하는 것입니다. 따라서 이 가중치를 어떻게 사용하면서 압축하느냐가 중요한 부분인데요."},{"start":1739000,"end":1751800,"text":"본 논문에서는 이 알파 케 가중치를 학습 파라미터로 사용할 수도 있고 하이퍼 파라미터로 사용할 수도 있다고 했습니다. 하지만 둘의 별 차이가 없기 때문에 굳이 학습 파라미터를 늘리지 않고 하이퍼 파라미터를 사용했습니다.","confidence":0.9046,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1739270,1739420,"본"],[1739470,1739960,"논문에서는"],[1739970,1740120,"이"],[1740170,1740460,"알파"],[1740510,1740660,"케"],[1741350,1741880,"가중치를"],[1742170,1742440,"학습"],[1742450,1742960,"파라미터로"],[1742960,1743260,"사용할"],[1743260,1743394,"수도"],[1743394,1743640,"있고"],[1743670,1743960,"하이퍼"],[1743960,1744440,"파라미터로"],[1744440,1744740,"사용할"],[1744740,1744940,"수도"],[1744940,1745240,"있다고"],[1745240,1745640,"했습니다."],[1746070,1746340,"하지만"],[1746730,1747160,"둘의"],[1747190,1747340,"별"],[1747410,1747680,"차이가"],[1747680,1747847,"없기"],[1747847,1748200,"때문에"],[1748610,1748860,"굳이"],[1748870,1749067,"학습"],[1749067,1749500,"파라미터를"],[1749500,1749780,"늘리지"],[1749780,1750080,"않고"],[1750310,1750600,"하이퍼"],[1750600,1751080,"파라미터를"],[1751080,1751660,"사용했습니다."]],"textEdited":"본 논문에서는 이 알파 케 가중치를 학습 파라미터로 사용할 수도 있고 하이퍼 파라미터로 사용할 수도 있다고 했습니다. 하지만 둘의 별 차이가 없기 때문에 굳이 학습 파라미터를 늘리지 않고 하이퍼 파라미터를 사용했습니다."},{"start":1751800,"end":1762800,"text":"그래서 이 하이퍼 파라미터는 케 플러스 1 분의 1을 사용했는데요. 즉 레이어 이 케에가 점점 깊어질수록 가중치는 점점 작아지는 것이죠.","confidence":0.7949,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1752350,1752480,"그래서"],[1752480,1752554,"이"],[1752554,1752800,"하이퍼"],[1752800,1753300,"파라미터는"],[1754050,1754200,"케"],[1754370,1754720,"플러스"],[1754730,1754880,"1"],[1754950,1755220,"분의"],[1755220,1755880,"1을"],[1755910,1756560,"사용했는데요."],[1757310,1757460,"즉"],[1757630,1757960,"레이어"],[1758570,1758720,"이"],[1758730,1759280,"케에가"],[1759330,1759600,"점점"],[1759670,1760320,"깊어질수록"],[1760790,1761280,"가중치는"],[1761310,1761620,"점점"],[1761970,1762420,"작아지는"],[1762420,1762740,"것이죠."]],"textEdited":"그래서 이 하이퍼 파라미터는 케 플러스 1 분의 1을 사용했는데요. 즉 레이어 이 케에가 점점 깊어질수록 가중치는 점점 작아지는 것이죠."},{"start":1762800,"end":1775600,"text":"이 의미는 아까 얘기한 대로 인베딩이 처음보다 점점 전파될수록 그 인베딩의 시그널이 약해질 것이라는 아이디어를 사용하여서 그 아이디어를 수식적으로","confidence":0.9875,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1763170,1763320,"이"],[1763390,1763780,"의미는"],[1763990,1764180,"아까"],[1764230,1764540,"얘기한"],[1764540,1764760,"대로"],[1765390,1765920,"인베딩이"],[1766230,1767060,"처음보다"],[1767170,1767460,"점점"],[1767510,1768140,"전파될수록"],[1768230,1768380,"그"],[1768390,1768800,"인베딩의"],[1768850,1769340,"시그널이"],[1769530,1769920,"약해질"],[1769950,1770340,"것이라는"],[1770350,1770880,"아이디어를"],[1771670,1772180,"사용하여서"],[1772950,1773100,"그"],[1773100,1773620,"아이디어를"],[1774010,1774600,"수식적으로"]],"textEdited":"이 의미는 아까 얘기한 대로 인베딩이 처음보다 점점 전파될수록 그 인베딩의 시그널이 약해질 것이라는 아이디어를 사용하여서 그 아이디어를 수식적으로"},{"start":1775600,"end":1788200,"text":"주입한 것입니다. 최종 결과 및 요약입니다. 라이트 GCM 모델을 계속해서 NGCF와 비교했는데요. 실험 결과도 베이스 라인을 NGCF 모델로 삼고 성능을 비교했습니다.","confidence":0.8954,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1775910,1776320,"주입한"],[1776390,1776800,"것입니다."],[1777430,1777720,"최종"],[1777790,1778100,"결과"],[1778110,1778260,"및"],[1778270,1778800,"요약입니다."],[1780010,1780360,"라이트"],[1780360,1780680,"GCM"],[1780680,1780980,"모델을"],[1781010,1781380,"계속해서"],[1781430,1782160,"NGCF와"],[1782170,1782740,"비교했는데요."],[1783390,1783640,"실험"],[1783670,1784080,"결과도"],[1784690,1784987,"베이스"],[1784987,1785320,"라인을"],[1785390,1786040,"NGCF"],[1786040,1786420,"모델로"],[1786430,1786780,"삼고"],[1787010,1787340,"성능을"],[1787350,1787940,"비교했습니다."]],"textEdited":"주입한 것입니다. 최종 결과 및 요약입니다. 라이트 GCM 모델을 계속해서 NGCF와 비교했는데요. 실험 결과도 베이스 라인을 NGCF 모델로 삼고 성능을 비교했습니다."},{"start":1788200,"end":1798700,"text":"학습을 통한 로스 펑션 손실 함수와 그리고 각 데이터셋 별로 성능인 위코 모두 NGCF보다 뛰어난 성능을 보였습니다.","confidence":0.7368,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1788970,1789300,"학습을"],[1789300,1789500,"통한"],[1789710,1789980,"로스"],[1789990,1790240,"펑션"],[1790390,1790660,"손실"],[1790660,1790980,"함수와"],[1791410,1791600,"그리고"],[1791610,1791760,"각"],[1791830,1792380,"데이터셋"],[1792490,1792840,"별로"],[1793810,1794120,"성능인"],[1794190,1794480,"위코"],[1795290,1795580,"모두"],[1796390,1797080,"NGCF보다"],[1797110,1797440,"뛰어난"],[1797830,1798140,"성능을"],[1798140,1798680,"보였습니다."]],"textEdited":"학습을 통한 로스 펑션 손실 함수와 그리고 각 데이터셋 별로 성능인 위코 모두 NGCF보다 뛰어난 성능을 보였습니다."},{"start":1798700,"end":1813000,"text":"이는 라이트 GCN이 NGCF보다 제너럴라이제이션 파워 즉 일반화 능력이 더 좋다는 것인데요. NGCF 같은 경우에는 아까 언급했듯이 인베딩 전파 레이어 매번 하나하나마다 학습해야 되는 파라미터가 존재했고","confidence":0.831,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1798890,1799160,"이는"],[1799510,1799880,"라이트"],[1799880,1800360,"GCN이"],[1800650,1801320,"NGCF보다"],[1801570,1802260,"제너럴라이제이션"],[1802290,1802520,"파워"],[1802570,1802720,"즉"],[1802770,1803100,"일반화"],[1803130,1803340,"능력이"],[1803340,1803460,"더"],[1803460,1803794,"좋다는"],[1803794,1804220,"것인데요."],[1805230,1805740,"NGCF"],[1805740,1805947,"같은"],[1805947,1806360,"경우에는"],[1806770,1806980,"아까"],[1807010,1807560,"언급했듯이"],[1807790,1808160,"인베딩"],[1808210,1808520,"전파"],[1808930,1809240,"레이어"],[1809290,1809540,"매번"],[1809590,1810280,"하나하나마다"],[1810450,1810827,"학습해야"],[1810827,1811000,"되는"],[1811010,1812160,"파라미터가"],[1812210,1812760,"존재했고"]],"textEdited":"이는 라이트 GCN이 NGCF보다 제너럴라이제이션 파워 즉 일반화 능력이 더 좋다는 것인데요. NGCF 같은 경우에는 아까 언급했듯이 인베딩 전파 레이어 매번 하나하나마다 학습해야 되는 파라미터가 존재했고"},{"start":1813000,"end":1824200,"text":"또 모델 구조가 복잡하기 때문에 오버피팅의 위험이 있는 반면에 라이트 GCN은 모델 구조를 단순하게 구성하되 GCN에서 제일 중요한 컨볼루션을 놓치지 않아서","confidence":0.9605,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1813510,1813660,"또"],[1813690,1813940,"모델"],[1813990,1814280,"구조가"],[1814350,1814767,"복잡하기"],[1814767,1815100,"때문에"],[1815170,1815820,"오버피팅의"],[1815830,1816160,"위험이"],[1816160,1816360,"있는"],[1816390,1816760,"반면에"],[1817370,1817740,"라이트"],[1817750,1818200,"GCN은"],[1818590,1818880,"모델"],[1818990,1819340,"구조를"],[1819390,1819880,"단순하게"],[1819910,1820380,"구성하되"],[1821050,1821560,"GCN에서"],[1821570,1821760,"제일"],[1821830,1822100,"중요한"],[1822230,1823340,"컨볼루션을"],[1823350,1823720,"놓치지"],[1823720,1824020,"않아서"]],"textEdited":"또 모델 구조가 복잡하기 때문에 오버피팅의 위험이 있는 반면에 라이트 GCN은 모델 구조를 단순하게 구성하되 GCN에서 제일 중요한 컨볼루션을 놓치지 않아서"},{"start":1824200,"end":1832300,"text":"최대한 정보를 잘 압축하였고 더 예측력을 뛰어나게 상승시켰습니다. 자 여기까지가 GNN을 활용한","confidence":0.8298,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1824530,1824840,"최대한"],[1824950,1825227,"정보를"],[1825227,1825360,"잘"],[1825410,1826100,"압축하였고"],[1826530,1826680,"더"],[1826690,1827200,"예측력을"],[1827730,1828180,"뛰어나게"],[1828850,1829620,"상승시켰습니다."],[1829950,1830100,"자"],[1830110,1830640,"여기까지가"],[1831210,1831720,"GNN을"],[1831730,1832020,"활용한"]],"textEdited":"최대한 정보를 잘 압축하였고 더 예측력을 뛰어나게 상승시켰습니다. 자 여기까지가 GNN을 활용한"},{"start":1832300,"end":1843600,"text":"정확히는 그래프 컨볼루션 뉴럴 네트워크 GCN을 활용한 추천 모델을 살펴보았습니다. NGCF와 lit GCN이었고요. 번외로 이런 GNN 계열의 모델은","confidence":0.8712,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1832510,1832940,"정확히는"],[1833150,1833460,"그래프"],[1833530,1833960,"컨볼루션"],[1833960,1834180,"뉴럴"],[1834180,1834600,"네트워크"],[1834730,1835340,"GCN을"],[1835340,1835620,"활용한"],[1836270,1836560,"추천"],[1836560,1836880,"모델을"],[1836890,1837580,"살펴보았습니다."],[1838430,1839100,"NGCF와"],[1839110,1839420,"lit"],[1839420,1840760,"GCN이었고요."],[1841490,1841880,"번외로"],[1841910,1842080,"이런"],[1842170,1842560,"GNN"],[1842610,1842940,"계열의"],[1842990,1843360,"모델은"]],"textEdited":"정확히는 그래프 컨볼루션 뉴럴 네트워크 GCN을 활용한 추천 모델을 살펴보았습니다. NGCF와 lit GCN이었고요. 번외로 이런 GNN 계열의 모델은"},{"start":1843600,"end":1857200,"text":"실제로 네이버나 왓챠 같은 이런 실제 서비스에서도 유저 아이템을 잘 표현하는 추천 모델로 활발하게 사용되고 있다고 합니다. 다음은 RNN을 활용한 추천 모델입니다.","confidence":0.9038,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1843990,1844500,"실제로"],[1844670,1845120,"네이버나"],[1845120,1845380,"왓챠"],[1845430,1845680,"같은"],[1845870,1846040,"이런"],[1846330,1846580,"실제"],[1846580,1847280,"서비스에서도"],[1847890,1848180,"유저"],[1848210,1848620,"아이템을"],[1848620,1848760,"잘"],[1848850,1849260,"표현하는"],[1849730,1850020,"추천"],[1850020,1850420,"모델로"],[1850830,1851280,"활발하게"],[1851330,1851760,"사용되고"],[1851760,1852020,"있다고"],[1852020,1852280,"합니다."],[1853330,1853700,"다음은"],[1854690,1855220,"RNN을"],[1855230,1855540,"활용한"],[1855650,1855900,"추천"],[1855900,1856380,"모델입니다."]],"textEdited":"실제로 네이버나 왓챠 같은 이런 실제 서비스에서도 유저 아이템을 잘 표현하는 추천 모델로 활발하게 사용되고 있다고 합니다. 다음은 RNN을 활용한 추천 모델입니다."},{"start":1857200,"end":1869800,"text":"RNN의 개념을 이해하고 이를 적용한 추천 시스템 모델에 대해서 살펴보겠습니다. 먼저 RNN 모델을 간단하게 리캡하고 이 RNN을 사용한 대표적인 추천 모델인 gr 4 LA 모델을","confidence":0.9134,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1857490,1857980,"RNN의"],[1858010,1858380,"개념을"],[1858410,1858840,"이해하고"],[1859190,1859440,"이를"],[1859490,1859840,"적용한"],[1860450,1860700,"추천"],[1860710,1861040,"시스템"],[1861040,1861274,"모델에"],[1861274,1861540,"대해서"],[1861630,1862340,"살펴보겠습니다."],[1863190,1863440,"먼저"],[1863510,1863840,"RNN"],[1863870,1864240,"모델을"],[1864310,1864760,"간단하게"],[1864990,1865560,"리캡하고"],[1866110,1866260,"이"],[1866260,1866880,"RNN을"],[1866890,1867200,"사용한"],[1867310,1867680,"대표적인"],[1867690,1867920,"추천"],[1867920,1868200,"모델인"],[1868310,1868600,"gr"],[1868730,1868880,"4"],[1868890,1869060,"LA"],[1869190,1869560,"모델을"]],"textEdited":"RNN의 개념을 이해하고 이를 적용한 추천 시스템 모델에 대해서 살펴보겠습니다. 먼저 RNN 모델을 간단하게 리캡하고 이 RNN을 사용한 대표적인 추천 모델인 gr 4 LA 모델을"},{"start":1869800,"end":1878000,"text":"다루겠습니다. 먼저 알앤엔을 사용한 추천 모델을 다루기 전에 알앤엔과 그 계열의 모델들을 간단하게 리뷰만 하겠습니다.","confidence":0.7297,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1870050,1870780,"다루겠습니다."],[1871070,1871320,"먼저"],[1871350,1871820,"알앤엔을"],[1872030,1872300,"사용한"],[1872330,1872580,"추천"],[1872580,1872860,"모델을"],[1872860,1873107,"다루기"],[1873107,1873380,"전에"],[1873990,1874560,"알앤엔과"],[1874730,1874880,"그"],[1874910,1875240,"계열의"],[1875250,1876160,"모델들을"],[1876270,1876660,"간단하게"],[1876670,1877020,"리뷰만"],[1877070,1877580,"하겠습니다."]],"textEdited":"다루겠습니다. 먼저 알앤엔을 사용한 추천 모델을 다루기 전에 알앤엔과 그 계열의 모델들을 간단하게 리뷰만 하겠습니다."},{"start":1878000,"end":1892400,"text":"이 RNN 같은 경우에는 시퀀스 데이터의 처리와 이해에 굉장히 좋은 성능을 보이는 모델입니다. 현재의 상태가 그다음 상태 즉 시퀀셜하게 영향을 미치도록","confidence":0.9223,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1879350,1879500,"이"],[1879530,1879900,"RNN"],[1879910,1880160,"같은"],[1880160,1880580,"경우에는"],[1881350,1881760,"시퀀스"],[1881760,1882140,"데이터의"],[1882170,1882520,"처리와"],[1882590,1883460,"이해에"],[1883570,1883840,"굉장히"],[1883870,1884040,"좋은"],[1884190,1884520,"성능을"],[1884870,1885200,"보이는"],[1885210,1885740,"모델입니다."],[1886910,1887240,"현재의"],[1887310,1887680,"상태가"],[1888290,1888680,"그다음"],[1888790,1889100,"상태"],[1889350,1889500,"즉"],[1890190,1890840,"시퀀셜하게"],[1891330,1891640,"영향을"],[1891640,1892080,"미치도록"]],"textEdited":"이 RNN 같은 경우에는 시퀀스 데이터의 처리와 이해에 굉장히 좋은 성능을 보이는 모델입니다. 현재의 상태가 그다음 상태 즉 시퀀셜하게 영향을 미치도록"},{"start":1892400,"end":1906800,"text":"루프 구조를 사용하고 있는데요. 그래서 이 그림을 보시면은 이렇게 루프 형태로 표현할 수 있지만 사실은 시퀀셜한 인풋이 들어왔을 때 계속해서 시퀀셜하게 각각의 셀이 계산돼서","confidence":0.957,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1892990,1893300,"루프"],[1893330,1893680,"구조를"],[1894190,1894560,"사용하고"],[1894560,1894900,"있는데요."],[1896290,1896447,"그래서"],[1896447,1896547,"이"],[1896547,1896860,"그림을"],[1896860,1897340,"보시면은"],[1898010,1898200,"이렇게"],[1898210,1898480,"루프"],[1898490,1898780,"형태로"],[1898780,1899080,"표현할"],[1899080,1899154,"수"],[1899154,1899420,"있지만"],[1899910,1900220,"사실은"],[1900630,1901140,"시퀀셜한"],[1901310,1901800,"인풋이"],[1902170,1902640,"들어왔을"],[1902670,1902820,"때"],[1903650,1904080,"계속해서"],[1904150,1904720,"시퀀셜하게"],[1905030,1905380,"각각의"],[1905390,1905720,"셀이"],[1906070,1906540,"계산돼서"]],"textEdited":"루프 구조를 사용하고 있는데요. 그래서 이 그림을 보시면은 이렇게 루프 형태로 표현할 수 있지만 사실은 시퀀셜한 인풋이 들어왔을 때 계속해서 시퀀셜하게 각각의 셀이 계산돼서"},{"start":1906800,"end":1919300,"text":"이 정보는 다음으로 넘어가게 되고 또 그다음 정보는 그 다음 스텝으로 넘어가는 이런 시퀀셜한 구조를 사용해서 마지막 최종 예측을 수행하게 됩니다. 자 다음은 알의 대표적인 모델인 엘스티엠입니다.","confidence":0.8155,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1907210,1907360,"이"],[1907390,1907760,"정보는"],[1907790,1908100,"다음으로"],[1908100,1908467,"넘어가게"],[1908467,1908760,"되고"],[1909170,1909320,"또"],[1909350,1909680,"그다음"],[1909770,1910160,"정보는"],[1910370,1910520,"그"],[1910520,1910740,"다음"],[1910770,1911180,"스텝으로"],[1911180,1911600,"넘어가는"],[1911990,1912160,"이런"],[1912210,1912680,"시퀀셜한"],[1912690,1913060,"구조를"],[1913210,1913620,"사용해서"],[1914130,1914420,"마지막"],[1914490,1914720,"최종"],[1914730,1915140,"예측을"],[1915230,1915620,"수행하게"],[1915620,1915920,"됩니다."],[1916470,1916620,"자"],[1916620,1916960,"다음은"],[1917070,1917500,"알의"],[1917510,1917920,"대표적인"],[1917950,1918240,"모델인"],[1918390,1919300,"엘스티엠입니다."]],"textEdited":"이 정보는 다음으로 넘어가게 되고 또 그다음 정보는 그 다음 스텝으로 넘어가는 이런 시퀀셜한 구조를 사용해서 마지막 최종 예측을 수행하게 됩니다. 자 다음은 알의 대표적인 모델인 엘스티엠입니다."},{"start":1919300,"end":1934200,"text":"이 LSTM 같은 경우에는 시퀀스가 점점 길어질수록 그 시퀀스의 학습 능력이 현저하게 저하되는 이런 RNN의 한계를 극복하기 위해서 롱 숏텀 메모리라는 개념을 추가한 모델입니다. 그래서 장기 의존성","confidence":0.8037,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1919890,1920007,"이"],[1920007,1920460,"LSTM"],[1920470,1920687,"같은"],[1920687,1921040,"경우에는"],[1921150,1921620,"시퀀스가"],[1921630,1921900,"점점"],[1921930,1922580,"길어질수록"],[1923330,1923480,"그"],[1923490,1923960,"시퀀스의"],[1923990,1924220,"학습"],[1924220,1924540,"능력이"],[1924570,1925000,"현저하게"],[1925550,1926020,"저하되는"],[1926110,1926280,"이런"],[1926410,1926880,"RNN의"],[1926970,1927320,"한계를"],[1927370,1927800,"극복하기"],[1927800,1928140,"위해서"],[1929070,1929220,"롱"],[1929490,1929880,"숏텀"],[1929970,1930540,"메모리라는"],[1930610,1930980,"개념을"],[1931430,1931760,"추가한"],[1931810,1932320,"모델입니다."],[1932950,1933140,"그래서"],[1933190,1933460,"장기"],[1933530,1933960,"의존성"]],"textEdited":"이 LSTM 같은 경우에는 시퀀스가 점점 길어질수록 그 시퀀스의 학습 능력이 현저하게 저하되는 이런 RNN의 한계를 극복하기 위해서 롱 숏텀 메모리라는 개념을 추가한 모델입니다. 그래서 장기 의존성"},{"start":1934200,"end":1947900,"text":"의 문제를 해결하기 위해서 셀 스테이트라는 구조를 고안하였고요. 여기 부분 보시면 게이트는 총 폴 겟 인풋 아웃풋 게이트라는 것을 활용하는데요. 이제 이전 셀에서","confidence":0.9525,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1934410,1934560,"의"],[1934590,1934980,"문제를"],[1935130,1935540,"해결하기"],[1935540,1935820,"위해서"],[1936510,1936660,"셀"],[1936770,1937380,"스테이트라는"],[1937430,1937760,"구조를"],[1937810,1938420,"고안하였고요."],[1939810,1940020,"여기"],[1940030,1940240,"부분"],[1940630,1940980,"보시면"],[1941650,1942140,"게이트는"],[1942170,1942320,"총"],[1942410,1942560,"폴"],[1942590,1942740,"겟"],[1942990,1943300,"인풋"],[1943410,1943780,"아웃풋"],[1943870,1944367,"게이트라는"],[1944367,1944580,"것을"],[1944580,1945120,"활용하는데요."],[1945730,1945900,"이제"],[1945970,1946260,"이전"],[1946970,1947360,"셀에서"]],"textEdited":"의 문제를 해결하기 위해서 셀 스테이트라는 구조를 고안하였고요. 여기 부분 보시면 게이트는 총 폴 겟 인풋 아웃풋 게이트라는 것을 활용하는데요. 이제 이전 셀에서"},{"start":1947900,"end":1960700,"text":"장기 기억이 전달되었을 때 얼마큼 그 기억을 살릴 것인지가 f g 게이트고요. 그리고 이 t 스텝에서 입력된 입력 변수를 얼마큼 사용할 것인지 인풋 게이트가 되겠고요.","confidence":0.9278,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1948190,1948440,"장기"],[1948440,1948780,"기억이"],[1949030,1949660,"전달되었을"],[1949690,1949840,"때"],[1950470,1950980,"얼마큼"],[1951550,1951700,"그"],[1951710,1952080,"기억을"],[1952250,1952500,"살릴"],[1952510,1953220,"것인지가"],[1953250,1953400,"f"],[1953400,1953520,"g"],[1953530,1954040,"게이트고요."],[1954430,1954660,"그리고"],[1954930,1955080,"이"],[1955190,1955340,"t"],[1955410,1955960,"스텝에서"],[1957010,1957400,"입력된"],[1957510,1957760,"입력"],[1957790,1958100,"변수를"],[1958130,1958520,"얼마큼"],[1958530,1958840,"사용할"],[1958840,1959220,"것인지"],[1959370,1959640,"인풋"],[1959690,1960160,"게이트가"],[1960160,1960580,"되겠고요."]],"textEdited":"장기 기억이 전달되었을 때 얼마큼 그 기억을 살릴 것인지가 f g 게이트고요. 그리고 이 t 스텝에서 입력된 입력 변수를 얼마큼 사용할 것인지 인풋 게이트가 되겠고요."},{"start":1960700,"end":1972300,"text":"그리고 그 다음 아웃풋 게이트는 얼마큼 출력할 건지에 대한 내용을 정보를 조절합니다. 그리고 요 셀 스테이트 부분은 계속해서 다음","confidence":0.9522,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1961190,1961480,"그리고"],[1961770,1961920,"그"],[1962270,1962540,"다음"],[1963330,1963740,"아웃풋"],[1963790,1964200,"게이트는"],[1964370,1964720,"얼마큼"],[1964790,1965180,"출력할"],[1965190,1965640,"건지에"],[1965640,1965840,"대한"],[1965930,1966260,"내용을"],[1967430,1967840,"정보를"],[1968090,1968600,"조절합니다."],[1969390,1969587,"그리고"],[1969587,1969720,"요"],[1969810,1969960,"셀"],[1970070,1970540,"스테이트"],[1970690,1971020,"부분은"],[1971210,1971620,"계속해서"],[1971650,1971920,"다음"]],"textEdited":"그리고 그 다음 아웃풋 게이트는 얼마큼 출력할 건지에 대한 내용을 정보를 조절합니다. 그리고 요 셀 스테이트 부분은 계속해서 다음"},{"start":1972300,"end":1983900,"text":"셀로 전달되게 되는데요. 이제 이 부분이 장기 기억을 계속해서 다음 셀로 전달하는 부분이라고 볼 수 있습니다.","confidence":0.9122,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1973750,1974040,"셀로"],[1975570,1975967,"전달되게"],[1975967,1976360,"되는데요."],[1976690,1976860,"이제"],[1976870,1977020,"이"],[1977020,1977340,"부분이"],[1977670,1977940,"장기"],[1977950,1978280,"기억을"],[1978290,1978720,"계속해서"],[1978870,1979120,"다음"],[1979610,1979900,"셀로"],[1979930,1980340,"전달하는"],[1980370,1980840,"부분이라고"],[1980840,1980980,"볼"],[1980980,1981074,"수"],[1981074,1981440,"있습니다."]],"textEdited":"셀로 전달되게 되는데요. 이제 이 부분이 장기 기억을 계속해서 다음 셀로 전달하는 부분이라고 볼 수 있습니다."},{"start":1983900,"end":1997800,"text":"자 다음은 GRU 모델인데요. 사실 이 GRU 모델이 이 다음에 배울 추천 모델 gr 4 r에 사용되는 레이어입니다. 그래서 GRU는 방금 언급했던 LSTM의 변형 중의 하나로","confidence":0.6894,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1984650,1984800,"자"],[1985130,1985500,"다음은"],[1985570,1985940,"GRU"],[1985940,1986460,"모델인데요."],[1986850,1987060,"사실"],[1987060,1987180,"이"],[1987180,1987560,"GRU"],[1987570,1987920,"모델이"],[1988750,1988900,"이"],[1988900,1989220,"다음에"],[1989220,1989420,"배울"],[1989870,1990140,"추천"],[1990140,1990340,"모델"],[1990530,1990820,"gr"],[1990930,1991080,"4"],[1991080,1991340,"r에"],[1991470,1991960,"사용되는"],[1992590,1993280,"레이어입니다."],[1993830,1994020,"그래서"],[1994020,1994480,"GRU는"],[1994970,1995200,"방금"],[1995310,1995680,"언급했던"],[1995750,1996660,"LSTM의"],[1996690,1996960,"변형"],[1996960,1997147,"중의"],[1997147,1997480,"하나로"]],"textEdited":"자 다음은 GRU 모델인데요. 사실 이 GRU 모델이 이 다음에 배울 추천 모델 gr 4 r에 사용되는 레이어입니다. 그래서 GRU는 방금 언급했던 LSTM의 변형 중의 하나로"},{"start":1997800,"end":2011400,"text":"LSTM과는 다르게 출력 게이트가 따로 없어서 LSTM보다 파라미터가 적고 연산량도 훨씬 적은 가벼운 모델입니다. 그래서 모델의 구성을 간단하게 보시면은 어 출력 게이트가","confidence":0.8547,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1998070,1998860,"LSTM과는"],[1998860,1999160,"다르게"],[1999310,1999560,"출력"],[1999590,2000040,"게이트가"],[2000110,2000340,"따로"],[2000340,2000640,"없어서"],[2001030,2001720,"LSTM보다"],[2001750,2002680,"파라미터가"],[2002790,2003140,"적고"],[2003210,2003740,"연산량도"],[2003740,2004020,"훨씬"],[2004150,2004380,"적은"],[2005150,2005460,"가벼운"],[2005490,2006000,"모델입니다."],[2007410,2007600,"그래서"],[2007600,2007920,"모델의"],[2007950,2008240,"구성을"],[2008250,2008640,"간단하게"],[2008650,2009120,"보시면은"],[2010050,2010200,"어"],[2010430,2010680,"출력"],[2010680,2011040,"게이트가"]],"textEdited":"LSTM과는 다르게 출력 게이트가 따로 없어서 LSTM보다 파라미터가 적고 연산량도 훨씬 적은 가벼운 모델입니다. 그래서 모델의 구성을 간단하게 보시면은 어 출력 게이트가"},{"start":2011400,"end":2025900,"text":"있다고 해서 게이트가 총 2개로 구성되어 있는데요. 리셋 게이트와 업데이트 게이트 이 부분이 리셋이고요. 이 부분이 업데이트가 됩니다. 리셋 같은 경우에는 아까 LSTM과 비슷하게 바로 전 셀에서","confidence":0.9641,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2011650,2012080,"있다고"],[2012110,2012340,"해서"],[2012470,2012900,"게이트가"],[2012970,2013120,"총"],[2013210,2013560,"2개로"],[2013560,2013887,"구성되어"],[2013887,2014220,"있는데요."],[2014670,2015000,"리셋"],[2015090,2015500,"게이트와"],[2015570,2016080,"업데이트"],[2016290,2016640,"게이트"],[2017110,2017227,"이"],[2017227,2017480,"부분이"],[2017510,2018140,"리셋이고요."],[2018810,2018960,"이"],[2018960,2019200,"부분이"],[2019230,2019780,"업데이트가"],[2019780,2020060,"됩니다."],[2020870,2021140,"리셋"],[2021170,2021440,"같은"],[2021440,2021860,"경우에는"],[2022670,2022880,"아까"],[2022970,2023580,"LSTM과"],[2023590,2024000,"비슷하게"],[2024310,2024560,"바로"],[2024560,2024700,"전"],[2025050,2025440,"셀에서"]],"textEdited":"있다고 해서 게이트가 총 2개로 구성되어 있는데요. 리셋 게이트와 업데이트 게이트 이 부분이 리셋이고요. 이 부분이 업데이트가 됩니다. 리셋 같은 경우에는 아까 LSTM과 비슷하게 바로 전 셀에서"},{"start":2025900,"end":2036600,"text":"들어오는 정보를 얼마큼 버리고 얼마큼 기억할지에 대한 부분이고요. 업데이트는 이제 인풋과 그전에 정보를 활용해서 얼마큼","confidence":0.9498,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2026270,2026700,"들어오는"],[2026750,2027100,"정보를"],[2027190,2027680,"얼마큼"],[2029030,2029360,"버리고"],[2029410,2029760,"얼마큼"],[2029790,2030567,"기억할지에"],[2030567,2030760,"대한"],[2030810,2031300,"부분이고요."],[2031830,2032440,"업데이트는"],[2032870,2033040,"이제"],[2033110,2033660,"인풋과"],[2034130,2034500,"그전에"],[2034500,2034800,"정보를"],[2034800,2035160,"활용해서"],[2035670,2036120,"얼마큼"]],"textEdited":"들어오는 정보를 얼마큼 버리고 얼마큼 기억할지에 대한 부분이고요. 업데이트는 이제 인풋과 그전에 정보를 활용해서 얼마큼"},{"start":2036600,"end":2047300,"text":"새로 들어오는 입력 정보와 과거 정보를 사용해서 업데이트할지에 대한 부분입니다. 자세한 설명을 다 하진 않겠지만 LSTM과 GRU는","confidence":0.9625,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2037070,2037320,"새로"],[2037330,2037660,"들어오는"],[2037890,2038140,"입력"],[2038170,2038740,"정보와"],[2039050,2039300,"과거"],[2039310,2039680,"정보를"],[2039770,2040120,"사용해서"],[2040130,2041040,"업데이트할지에"],[2041040,2041240,"대한"],[2041650,2042160,"부분입니다."],[2043750,2044060,"자세한"],[2044150,2044460,"설명을"],[2044470,2044620,"다"],[2044620,2044800,"하진"],[2044810,2045260,"않겠지만"],[2045570,2046320,"LSTM과"],[2046490,2047000,"GRU는"]],"textEdited":"새로 들어오는 입력 정보와 과거 정보를 사용해서 업데이트할지에 대한 부분입니다. 자세한 설명을 다 하진 않겠지만 LSTM과 GRU는"},{"start":2047300,"end":2059300,"text":"성능 측면에서는 큰 차이가 없고 그래서 GRU가 훨씬 더 가볍기 때문에 어 지알도 많이 사용을 합니다. 그래서 여러분들이 수행하는 각각의 테스크에 맞게 적합한 모델을 선택하면 되는데요.","confidence":0.7744,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2047590,2047880,"성능"],[2048030,2048500,"측면에서는"],[2048670,2048820,"큰"],[2048890,2049160,"차이가"],[2049160,2049420,"없고"],[2049570,2049760,"그래서"],[2049770,2050280,"GRU가"],[2050310,2050514,"훨씬"],[2050514,2050640,"더"],[2050650,2050987,"가볍기"],[2050987,2051380,"때문에"],[2052010,2052160,"어"],[2052190,2052800,"지알도"],[2053250,2053460,"많이"],[2053530,2053820,"사용을"],[2053820,2054080,"합니다."],[2054550,2054694,"그래서"],[2054694,2055140,"여러분들이"],[2055150,2055520,"수행하는"],[2055550,2055840,"각각의"],[2055870,2056360,"테스크에"],[2056370,2056720,"맞게"],[2057070,2057460,"적합한"],[2057460,2057740,"모델을"],[2057740,2058140,"선택하면"],[2058140,2059300,"되는데요."]],"textEdited":"성능 측면에서는 큰 차이가 없고 그래서 GRU가 훨씬 더 가볍기 때문에 어 지알도 많이 사용을 합니다. 그래서 여러분들이 수행하는 각각의 테스크에 맞게 적합한 모델을 선택하면 되는데요."},{"start":2059300,"end":2070000,"text":"그렇다면 이러한 RNN 모델이 추천 시스템에 어떻게 적용될 수 있을까요? 이 추천 시스템의 시퀀스를 더한 문제가 바로 이 세션 베이스 레코멘데이션 문제입니다.","confidence":0.9018,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2059550,2059920,"그렇다면"],[2059930,2060200,"이러한"],[2060290,2060820,"RNN"],[2060890,2061260,"모델이"],[2061370,2061620,"추천"],[2061630,2062060,"시스템에"],[2062110,2062400,"어떻게"],[2062410,2062760,"적용될"],[2062790,2062940,"수"],[2062940,2063340,"있을까요?"],[2064450,2064600,"이"],[2064600,2064860,"추천"],[2064870,2065280,"시스템의"],[2065370,2065980,"시퀀스를"],[2066830,2067100,"더한"],[2067130,2067460,"문제가"],[2067460,2067660,"바로"],[2067690,2067840,"이"],[2067850,2068160,"세션"],[2068160,2068420,"베이스"],[2068420,2069020,"레코멘데이션"],[2069270,2069760,"문제입니다."]],"textEdited":"그렇다면 이러한 RNN 모델이 추천 시스템에 어떻게 적용될 수 있을까요? 이 추천 시스템의 시퀀스를 더한 문제가 바로 이 세션 베이스 레코멘데이션 문제입니다."},{"start":2070000,"end":2083400,"text":"고객의 선호가 고정된 것이 아니라 시간에 따라서 혹은 지금 무엇을 계속 소비해 왔느냐에 따라서 달라지게 된다는 것인데요. 따라서 세션 베이스 레코멘데이션이 풀고자 하는 문제는","confidence":0.9606,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2070850,2071220,"고객의"],[2071250,2071600,"선호가"],[2072150,2072560,"고정된"],[2072570,2072787,"것이"],[2072787,2073060,"아니라"],[2073670,2074000,"시간에"],[2074000,2074300,"따라서"],[2074850,2075080,"혹은"],[2075310,2075500,"지금"],[2075590,2075980,"무엇을"],[2075990,2076220,"계속"],[2076290,2076740,"소비해"],[2076830,2077400,"왔느냐에"],[2077400,2077720,"따라서"],[2077830,2078240,"달라지게"],[2078240,2078527,"된다는"],[2078527,2078940,"것인데요."],[2079710,2080040,"따라서"],[2080090,2080380,"세션"],[2080380,2080700,"베이스"],[2080790,2081540,"레코멘데이션이"],[2081990,2082294,"풀고자"],[2082294,2082460,"하는"],[2082470,2082820,"문제는"]],"textEdited":"고객의 선호가 고정된 것이 아니라 시간에 따라서 혹은 지금 무엇을 계속 소비해 왔느냐에 따라서 달라지게 된다는 것인데요. 따라서 세션 베이스 레코멘데이션이 풀고자 하는 문제는"},{"start":2083400,"end":2096600,"text":"바로 지금 고객이 좋아할 만한 추천 아이템을 제공해 주는 것입니다. 참고로 유저가 서비스를 이용하는 동안의 행동 데이터를 묶어서 세션","confidence":0.9815,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2083750,2084060,"바로"],[2084130,2084360,"지금"],[2085430,2085800,"고객이"],[2086190,2086547,"좋아할"],[2086547,2086800,"만한"],[2086970,2087320,"추천"],[2087730,2088240,"아이템을"],[2088610,2088940,"제공해"],[2088940,2089127,"주는"],[2089127,2089580,"것입니다."],[2092030,2092360,"참고로"],[2092370,2092720,"유저가"],[2092890,2093440,"서비스를"],[2093490,2093900,"이용하는"],[2094030,2094380,"동안의"],[2094510,2094760,"행동"],[2094770,2095160,"데이터를"],[2095210,2095580,"묶어서"],[2095950,2096280,"세션"]],"textEdited":"바로 지금 고객이 좋아할 만한 추천 아이템을 제공해 주는 것입니다. 참고로 유저가 서비스를 이용하는 동안의 행동 데이터를 묶어서 세션"},{"start":2096600,"end":2108600,"text":"이라고 말하는데요. 예를 들면 브라우저가 종료되기 전까지 그 브라우저 안에서 행동했던 데이터들을 쿠키 형태로 저장하는데 그것을 세션이라고 합니다. 그래서 아래의 그림처럼 사용자가","confidence":0.9816,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2096850,2097127,"이라고"],[2097127,2097600,"말하는데요."],[2097930,2098107,"예를"],[2098107,2098340,"들면"],[2098950,2099480,"브라우저가"],[2099530,2099940,"종료되기"],[2099950,2100460,"전까지"],[2100730,2100880,"그"],[2100880,2101240,"브라우저"],[2101240,2101580,"안에서"],[2101810,2102280,"행동했던"],[2102950,2103480,"데이터들을"],[2103550,2103800,"쿠키"],[2103800,2104060,"형태로"],[2104060,2104520,"저장하는데"],[2104570,2104900,"그것을"],[2104910,2105440,"세션이라고"],[2105440,2105720,"합니다."],[2106530,2106720,"그래서"],[2106720,2107000,"아래의"],[2107010,2107480,"그림처럼"],[2107770,2108240,"사용자가"]],"textEdited":"이라고 말하는데요. 예를 들면 브라우저가 종료되기 전까지 그 브라우저 안에서 행동했던 데이터들을 쿠키 형태로 저장하는데 그것을 세션이라고 합니다. 그래서 아래의 그림처럼 사용자가"},{"start":2108600,"end":2123600,"text":"이런 아이템들을 차례대로 소비했을 때 하나의 세션 안에서 4개의 아이템을 소비했을 때 그다음에 아이템으로 무엇을 소비할지를 예측해서 그 아이템을 추천해 주는 문제가 바로 세션 베이스 레코맨데이 시스템의 문제입니다.","confidence":0.9462,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2108970,2109160,"이런"],[2109210,2109780,"아이템들을"],[2109810,2110180,"차례대로"],[2110230,2110740,"소비했을"],[2110770,2110920,"때"],[2111130,2111440,"하나의"],[2111470,2111740,"세션"],[2111740,2112020,"안에서"],[2112310,2112640,"4개의"],[2112640,2113000,"아이템을"],[2113000,2113440,"소비했을"],[2113440,2113580,"때"],[2114190,2114760,"그다음에"],[2114810,2115320,"아이템으로"],[2115330,2115680,"무엇을"],[2115690,2117140,"소비할지를"],[2117730,2118300,"예측해서"],[2118450,2118600,"그"],[2118600,2118960,"아이템을"],[2118990,2119254,"추천해"],[2119254,2119460,"주는"],[2119460,2119800,"문제가"],[2119850,2120100,"바로"],[2120590,2120900,"세션"],[2120910,2121240,"베이스"],[2121390,2121900,"레코맨데이"],[2122110,2122860,"시스템의"],[2122890,2123520,"문제입니다."]],"textEdited":"이런 아이템들을 차례대로 소비했을 때 하나의 세션 안에서 4개의 아이템을 소비했을 때 그다음에 아이템으로 무엇을 소비할지를 예측해서 그 아이템을 추천해 주는 문제가 바로 세션 베이스 레코맨데이 시스템의 문제입니다."},{"start":2123600,"end":2137500,"text":"그러면 이 세션 베이스 추천 시스템 이 문제를 해결하기 위한 추천 모델인 GRU 4 LA에 대해 살펴봅시다. 아까 얘기했던 대로 이 모델은 GRU 레이어를 사용하여서 시퀀셜하게","confidence":0.9345,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2124090,2124380,"그러면"],[2124430,2124580,"이"],[2124610,2124900,"세션"],[2124910,2125220,"베이스"],[2125220,2125440,"추천"],[2125450,2125800,"시스템"],[2126470,2126620,"이"],[2126620,2126880,"문제를"],[2126880,2127240,"해결하기"],[2127240,2127420,"위한"],[2127550,2127820,"추천"],[2127820,2128180,"모델인"],[2128670,2129040,"GRU"],[2129040,2129180,"4"],[2129180,2129774,"LA에"],[2129774,2129940,"대해"],[2129970,2130520,"살펴봅시다."],[2131510,2131700,"아까"],[2131750,2132247,"얘기했던"],[2132247,2132480,"대로"],[2133590,2133740,"이"],[2133740,2134100,"모델은"],[2134310,2134720,"GRU"],[2134770,2135320,"레이어를"],[2135350,2135900,"사용하여서"],[2136150,2136820,"시퀀셜하게"]],"textEdited":"그러면 이 세션 베이스 추천 시스템 이 문제를 해결하기 위한 추천 모델인 GRU 4 LA에 대해 살펴봅시다. 아까 얘기했던 대로 이 모델은 GRU 레이어를 사용하여서 시퀀셜하게"},{"start":2137500,"end":2151300,"text":"지금 고객이 현재까지 어떤 아이템을 순차적으로 소비해 왔을 때 바로 그다음 아이템이 무엇인지를 추천해 주는 모델입니다. 이 GRF 렉의 모델 구조와 아이디어는 다음과 같습니다.","confidence":0.8892,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2137790,2138040,"지금"],[2138150,2138500,"고객이"],[2139250,2139700,"현재까지"],[2139700,2139880,"어떤"],[2139890,2140360,"아이템을"],[2141110,2141960,"순차적으로"],[2142190,2142487,"소비해"],[2142487,2142760,"왔을"],[2142760,2142900,"때"],[2143210,2143480,"바로"],[2143480,2143740,"그다음"],[2143770,2144160,"아이템이"],[2144170,2144820,"무엇인지를"],[2145290,2145587,"추천해"],[2145587,2145800,"주는"],[2145830,2146480,"모델입니다."],[2147130,2147280,"이"],[2147280,2147780,"GRF"],[2147870,2148160,"렉의"],[2148590,2148840,"모델"],[2148930,2149820,"구조와"],[2149850,2150360,"아이디어는"],[2150370,2150680,"다음과"],[2150680,2151080,"같습니다."]],"textEdited":"지금 고객이 현재까지 어떤 아이템을 순차적으로 소비해 왔을 때 바로 그다음 아이템이 무엇인지를 추천해 주는 모델입니다. 이 GRF 렉의 모델 구조와 아이디어는 다음과 같습니다."},{"start":2151300,"end":2165600,"text":"이 세션이라는 시퀀스 그 세션에서 아이템을 어떤 아이템을 소비해 왔는지라는 시퀀스를 GRU 레이어에 입력하여서 바로 다음에 올 아이템이 무엇인지 가장 확률이 높은 아이템이 무엇인지","confidence":0.8929,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2152290,2152440,"이"],[2152470,2153100,"세션이라는"],[2153190,2153700,"시퀀스"],[2154410,2154560,"그"],[2154560,2155280,"세션에서"],[2155490,2155920,"아이템을"],[2156010,2156240,"어떤"],[2156290,2156680,"아이템을"],[2156690,2157040,"소비해"],[2157040,2157540,"왔는지라는"],[2157570,2158080,"시퀀스를"],[2158590,2158980,"GRU"],[2159010,2159460,"레이어에"],[2159570,2160160,"입력하여서"],[2160870,2161120,"바로"],[2161150,2161540,"다음에"],[2161590,2161740,"올"],[2161770,2163140,"아이템이"],[2163170,2163640,"무엇인지"],[2163750,2163960,"가장"],[2164030,2164287,"확률이"],[2164287,2164500,"높은"],[2164530,2164920,"아이템이"],[2164920,2165400,"무엇인지"]],"textEdited":"이 세션이라는 시퀀스 그 세션에서 아이템을 어떤 아이템을 소비해 왔는지라는 시퀀스를 GRU 레이어에 입력하여서 바로 다음에 올 아이템이 무엇인지 가장 확률이 높은 아이템이 무엇인지"},{"start":2165600,"end":2179900,"text":"를 분류하고 가장 확률이 높은 그 아이템을 추천해 주는 문제입니다. 네 모델 구조를 살펴보겠습니다. 먼저 입력 부분은 원 핫 인코딩 된 아이템으로 이루어진 세션입니다. 여기서 보시면은","confidence":0.94,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2165950,2166100,"를"],[2166130,2166660,"분류하고"],[2167210,2167420,"가장"],[2167470,2167687,"확률이"],[2167687,2167880,"높은"],[2167950,2168100,"그"],[2168100,2168560,"아이템을"],[2169090,2169414,"추천해"],[2169414,2169620,"주는"],[2169670,2170180,"문제입니다."],[2171090,2171240,"네"],[2171450,2171720,"모델"],[2171770,2172060,"구조를"],[2172060,2172700,"살펴보겠습니다."],[2173450,2173660,"먼저"],[2173730,2173980,"입력"],[2173990,2174340,"부분은"],[2174990,2175140,"원"],[2175150,2175300,"핫"],[2175350,2175654,"인코딩"],[2175654,2175780,"된"],[2175810,2176480,"아이템으로"],[2176480,2176820,"이루어진"],[2176890,2177560,"세션입니다."],[2178730,2178960,"여기서"],[2178970,2179520,"보시면은"]],"textEdited":"를 분류하고 가장 확률이 높은 그 아이템을 추천해 주는 문제입니다. 네 모델 구조를 살펴보겠습니다. 먼저 입력 부분은 원 핫 인코딩 된 아이템으로 이루어진 세션입니다. 여기서 보시면은"},{"start":2179900,"end":2193900,"text":"인베딩 레이어가 들어가 있는데 점선으로 들어가 있음을 알 수 있습니다. 본 모델에서는 인베딩 레이어를 사용하지 않았을 때 성능이 더 높다고 했는데요. 사실 이 모델 이후에 RNN 계열의 추천 모델들에서는 인베딩 레이어를 사용한 논문도 많아","confidence":0.9813,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2180170,2180540,"인베딩"],[2180550,2180880,"레이어가"],[2180880,2181107,"들어가"],[2181107,2181400,"있는데"],[2181510,2181980,"점선으로"],[2181980,2182200,"들어가"],[2182200,2182460,"있음을"],[2182460,2182600,"알"],[2182600,2182687,"수"],[2182687,2183060,"있습니다."],[2183950,2184100,"본"],[2184170,2184680,"모델에서는"],[2184810,2185180,"인베딩"],[2185190,2185500,"레이어를"],[2185500,2185860,"사용하지"],[2185870,2186147,"않았을"],[2186147,2186280,"때"],[2186330,2186567,"성능이"],[2186567,2186700,"더"],[2186700,2187080,"높다고"],[2187080,2187440,"했는데요."],[2187950,2188180,"사실"],[2188250,2188400,"이"],[2188400,2188620,"모델"],[2188670,2189000,"이후에"],[2189190,2189540,"RNN"],[2189610,2190080,"계열의"],[2190530,2190800,"추천"],[2190930,2192060,"모델들에서는"],[2192430,2192760,"인베딩"],[2192760,2193040,"레이어를"],[2193040,2193300,"사용한"],[2193300,2193607,"논문도"],[2193607,2193740,"많아"]],"textEdited":"인베딩 레이어가 들어가 있는데 점선으로 들어가 있음을 알 수 있습니다. 본 모델에서는 인베딩 레이어를 사용하지 않았을 때 성능이 더 높다고 했는데요. 사실 이 모델 이후에 RNN 계열의 추천 모델들에서는 인베딩 레이어를 사용한 논문도 많아"},{"start":2193900,"end":2207900,"text":"습니다. 따라서 이 인베딩 레이어를 사용했다 사용하지 않았다는 사실 이 GRU 4 r에서 그렇게 중요한 내용은 아닙니다. 이제 그 이후에 시퀀셜한 아이템들이 GRU 레이어를 통과하게 됩니다. 그래서 이 시퀀스 상에","confidence":0.9059,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2194170,2194520,"습니다."],[2194790,2195080,"따라서"],[2195090,2195207,"이"],[2195207,2195520,"인베딩"],[2195520,2195860,"레이어를"],[2195870,2196360,"사용했다"],[2196430,2196800,"사용하지"],[2196800,2197240,"않았다는"],[2197670,2197880,"사실"],[2197880,2198020,"이"],[2198020,2198340,"GRU"],[2198350,2198500,"4"],[2198500,2198800,"r에서"],[2198810,2199000,"그렇게"],[2199110,2199460,"중요한"],[2199470,2199780,"내용은"],[2199780,2200160,"아닙니다."],[2201490,2201660,"이제"],[2201690,2201840,"그"],[2201850,2202240,"이후에"],[2202690,2203200,"시퀀셜한"],[2203200,2203740,"아이템들이"],[2203810,2204220,"GRU"],[2204250,2204680,"레이어를"],[2204770,2205107,"통과하게"],[2205107,2205420,"됩니다."],[2206290,2206480,"그래서"],[2206710,2206860,"이"],[2206890,2207280,"시퀀스"],[2207280,2207520,"상에"]],"textEdited":"습니다. 따라서 이 인베딩 레이어를 사용했다 사용하지 않았다는 사실 이 GRU 4 r에서 그렇게 중요한 내용은 아닙니다. 이제 그 이후에 시퀀셜한 아이템들이 GRU 레이어를 통과하게 됩니다. 그래서 이 시퀀스 상에"},{"start":2207900,"end":2221500,"text":"어떤 아이템들의 순서와 맥락 그 컨텍스트 정보를 학습하게 됩니다. 그리고 이 마지막에는 이 피드 포워드 유뉴럴 네트워크를 통해 최종 마지막 골라질 아이템에 대한 스코어를 예측하게 됩니다.","confidence":0.936,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2208250,2208440,"어떤"],[2208490,2209120,"아이템들의"],[2209390,2210000,"순서와"],[2210150,2210440,"맥락"],[2210810,2210960,"그"],[2210990,2211520,"컨텍스트"],[2211570,2211980,"정보를"],[2212250,2212647,"학습하게"],[2212647,2212960,"됩니다."],[2214110,2214360,"그리고"],[2214360,2214480,"이"],[2214480,2215060,"마지막에는"],[2215060,2215180,"이"],[2215180,2215420,"피드"],[2215430,2215640,"포워드"],[2215640,2215920,"유뉴럴"],[2215920,2216660,"네트워크를"],[2216710,2216960,"통해"],[2217610,2217880,"최종"],[2217970,2218280,"마지막"],[2218470,2218780,"골라질"],[2218790,2219240,"아이템에"],[2219240,2219420,"대한"],[2219910,2220620,"스코어를"],[2220620,2220967,"예측하게"],[2220967,2221280,"됩니다."]],"textEdited":"어떤 아이템들의 순서와 맥락 그 컨텍스트 정보를 학습하게 됩니다. 그리고 이 마지막에는 이 피드 포워드 유뉴럴 네트워크를 통해 최종 마지막 골라질 아이템에 대한 스코어를 예측하게 됩니다."},{"start":2221500,"end":2234200,"text":"이 부분 피드 포드 유뉴럴 네트워크 부분도 이 모델에서는 선택적으로 사용하였습니다. 그래서 모델의 구조는 굉장히 간단했고요. 이다음에 모델이 학습될 때 사용되었던 몇 가지 테크닉을 추가적으로","confidence":0.9299,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2222270,2222420,"이"],[2222470,2222680,"부분"],[2222770,2223000,"피드"],[2223000,2223180,"포드"],[2223180,2223407,"유뉴럴"],[2223407,2223900,"네트워크"],[2223950,2224380,"부분도"],[2225170,2225320,"이"],[2225330,2225820,"모델에서는"],[2225870,2226420,"선택적으로"],[2226450,2227140,"사용하였습니다."],[2227590,2227820,"그래서"],[2227820,2228140,"모델의"],[2228140,2228400,"구조는"],[2228400,2228620,"굉장히"],[2228670,2229260,"간단했고요."],[2229750,2230240,"이다음에"],[2230290,2230620,"모델이"],[2230650,2231040,"학습될"],[2231070,2231220,"때"],[2231610,2232160,"사용되었던"],[2232170,2232320,"몇"],[2232320,2232500,"가지"],[2232550,2233000,"테크닉을"],[2233130,2233660,"추가적으로"]],"textEdited":"이 부분 피드 포드 유뉴럴 네트워크 부분도 이 모델에서는 선택적으로 사용하였습니다. 그래서 모델의 구조는 굉장히 간단했고요. 이다음에 모델이 학습될 때 사용되었던 몇 가지 테크닉을 추가적으로"},{"start":2234200,"end":2249200,"text":"다루겠습니다. 먼저 세션 패럴러 즉 이 세션의 길이들이 각각의 사용자마다 혹은 어떤 데이터마다 긴 것도 존재하고 짧은 것도 존재하고 모두 길이가 다르다는 것입니다.","confidence":0.9391,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2234570,2235200,"다루겠습니다."],[2236470,2236800,"먼저"],[2237410,2237760,"세션"],[2238490,2238840,"패럴러"],[2239330,2239480,"즉"],[2239590,2239740,"이"],[2239750,2240200,"세션의"],[2240230,2240720,"길이들이"],[2241590,2242060,"각각의"],[2242610,2243200,"사용자마다"],[2243210,2243440,"혹은"],[2243730,2243980,"어떤"],[2244230,2244880,"데이터마다"],[2245370,2245520,"긴"],[2245550,2245820,"것도"],[2245830,2246320,"존재하고"],[2246430,2246647,"짧은"],[2246647,2246920,"것도"],[2246950,2247400,"존재하고"],[2247790,2248020,"모두"],[2248020,2248300,"길이가"],[2248300,2248627,"다르다는"],[2248627,2249020,"것입니다."]],"textEdited":"다루겠습니다. 먼저 세션 패럴러 즉 이 세션의 길이들이 각각의 사용자마다 혹은 어떤 데이터마다 긴 것도 존재하고 짧은 것도 존재하고 모두 길이가 다르다는 것입니다."},{"start":2249200,"end":2263200,"text":"대부분은 짧지만 굉장히 아이템을 많이 소비한 길이가 긴 세션도 존재할 수 있다는 것이죠. 그래서 아래 그림과 같이 세션별로 그 안에 들어있는 아이템들은 모두 다릅니다. 그래서 이 길이가 짧은 세션을 그대로","confidence":0.9854,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2249470,2249920,"대부분은"],[2249930,2250340,"짧지만"],[2250890,2251260,"굉장히"],[2251330,2251740,"아이템을"],[2251740,2251900,"많이"],[2251970,2252300,"소비한"],[2252370,2253320,"길이가"],[2253330,2253480,"긴"],[2253530,2253900,"세션도"],[2253900,2254260,"존재할"],[2254260,2254334,"수"],[2254334,2254567,"있다는"],[2254567,2254880,"것이죠."],[2255910,2256100,"그래서"],[2256110,2256300,"아래"],[2256330,2256660,"그림과"],[2256660,2256940,"같이"],[2257310,2257920,"세션별로"],[2257970,2258087,"그"],[2258087,2258300,"안에"],[2258310,2258700,"들어있는"],[2258730,2259300,"아이템들은"],[2259330,2259560,"모두"],[2259590,2260040,"다릅니다."],[2261150,2261380,"그래서"],[2261380,2261500,"이"],[2261510,2261840,"길이가"],[2261850,2262100,"짧은"],[2262150,2262560,"세션을"],[2262570,2262920,"그대로"]],"textEdited":"대부분은 짧지만 굉장히 아이템을 많이 소비한 길이가 긴 세션도 존재할 수 있다는 것이죠. 그래서 아래 그림과 같이 세션별로 그 안에 들어있는 아이템들은 모두 다릅니다. 그래서 이 길이가 짧은 세션을 그대로"},{"start":2263200,"end":2275300,"text":"학습에 사용했을 때는 학습이 비효율적으로 일어나게 됩니다. 그래서 보시면은 이 세션들 5개를 가지고 미니 배치 학습을 수행할 때 이 세션 4와 세션 5는","confidence":0.9581,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2263590,2263940,"학습에"],[2263940,2264360,"사용했을"],[2264360,2264560,"때는"],[2264650,2264960,"학습이"],[2265070,2265620,"비효율적으로"],[2265620,2265874,"일어나게"],[2265874,2266180,"됩니다."],[2267130,2267320,"그래서"],[2267330,2267800,"보시면은"],[2268630,2268780,"이"],[2268810,2269280,"세션들"],[2270330,2270720,"5개를"],[2270720,2271040,"가지고"],[2271390,2271620,"미니"],[2271620,2271840,"배치"],[2271870,2272220,"학습을"],[2272270,2272640,"수행할"],[2272690,2272840,"때"],[2273390,2273540,"이"],[2273550,2273840,"세션"],[2273930,2274200,"4와"],[2274230,2274480,"세션"],[2274530,2274980,"5는"]],"textEdited":"학습에 사용했을 때는 학습이 비효율적으로 일어나게 됩니다. 그래서 보시면은 이 세션들 5개를 가지고 미니 배치 학습을 수행할 때 이 세션 4와 세션 5는"},{"start":2275300,"end":2288500,"text":"세션의 길이가 비교적 짧은 세션 1과 2에 다음과 같이 붙여서 미니 배치를 구성하고 있습니다. 그래서 하나의 세션을 그대로 사용하는 것이 아니라 세션의 길이가 좀 짧은 것들은","confidence":0.9546,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2275590,2276020,"세션의"],[2276030,2276360,"길이가"],[2276610,2276920,"비교적"],[2276930,2277180,"짧은"],[2277230,2277500,"세션"],[2277590,2277980,"1과"],[2278390,2278700,"2에"],[2279250,2279587,"다음과"],[2279587,2279860,"같이"],[2280310,2280760,"붙여서"],[2281590,2281800,"미니"],[2281800,2282140,"배치를"],[2282470,2282827,"구성하고"],[2282827,2283180,"있습니다."],[2283710,2283900,"그래서"],[2283930,2284260,"하나의"],[2284310,2284720,"세션을"],[2285290,2285560,"그대로"],[2285560,2285907,"사용하는"],[2285907,2286094,"것이"],[2286094,2286340,"아니라"],[2286790,2287140,"세션의"],[2287140,2287360,"길이가"],[2287360,2287480,"좀"],[2287510,2287820,"짧은"],[2287830,2288200,"것들은"]],"textEdited":"세션의 길이가 비교적 짧은 세션 1과 2에 다음과 같이 붙여서 미니 배치를 구성하고 있습니다. 그래서 하나의 세션을 그대로 사용하는 것이 아니라 세션의 길이가 좀 짧은 것들은"},{"start":2288500,"end":2300200,"text":"다른 세션에 뒤에 붙여서 병렬적으로 구성하여서 미니 배치 학습이 좀 더 효율적으로 이루어질 수 있도록 테크닉을 적용하였습니다. 두 번째는 학습 데이터를 샘플링하는 테크닉입니다.","confidence":0.9466,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2288730,2288980,"다른"],[2289030,2289360,"세션에"],[2289360,2289580,"뒤에"],[2289610,2290000,"붙여서"],[2290490,2291120,"병렬적으로"],[2291150,2291680,"구성하여서"],[2291930,2292160,"미니"],[2292170,2292420,"배치"],[2292430,2292760,"학습이"],[2292760,2292900,"좀"],[2292900,2293040,"더"],[2293050,2293560,"효율적으로"],[2293560,2293860,"이루어질"],[2293870,2293974,"수"],[2293974,2294260,"있도록"],[2295370,2295840,"테크닉을"],[2296090,2296820,"적용하였습니다."],[2297450,2297600,"두"],[2297600,2297960,"번째는"],[2298250,2298480,"학습"],[2298480,2298860,"데이터를"],[2298890,2299440,"샘플링하는"],[2299470,2300020,"테크닉입니다."]],"textEdited":"다른 세션에 뒤에 붙여서 병렬적으로 구성하여서 미니 배치 학습이 좀 더 효율적으로 이루어질 수 있도록 테크닉을 적용하였습니다. 두 번째는 학습 데이터를 샘플링하는 테크닉입니다."},{"start":2300200,"end":2315000,"text":"현실에서는 아이템 수가 굉장히 많기 때문에 이 모든 후보의 아이템에 대한 확률을 다 계산할 수 없습니다. 그래서 아이템을 negative 샘플링을 해서 서셋만으로 로스를 계산해야 합니다. 그래서 아이템을 상호작용하는 것들은 우리가 가지고 있지만","confidence":0.9361,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2300610,2301180,"현실에서는"],[2301510,2301940,"아이템"],[2302010,2302260,"수가"],[2302290,2302620,"굉장히"],[2302650,2302940,"많기"],[2302940,2303300,"때문에"],[2303950,2304100,"이"],[2304150,2304380,"모든"],[2304470,2304780,"후보의"],[2304780,2305420,"아이템에"],[2305420,2305580,"대한"],[2305710,2306020,"확률을"],[2306020,2306160,"다"],[2306190,2306560,"계산할"],[2306560,2306667,"수"],[2306667,2307060,"없습니다."],[2307210,2307420,"그래서"],[2307420,2307820,"아이템을"],[2307830,2308260,"negative"],[2308290,2308800,"샘플링을"],[2308810,2309120,"해서"],[2309730,2310400,"서셋만으로"],[2310750,2311080,"로스를"],[2311080,2311387,"계산해야"],[2311387,2311660,"합니다."],[2311810,2312000,"그래서"],[2312050,2312580,"아이템을"],[2312730,2313400,"상호작용하는"],[2313400,2313720,"것들은"],[2313810,2314060,"우리가"],[2314210,2314540,"가지고"],[2314540,2314820,"있지만"]],"textEdited":"현실에서는 아이템 수가 굉장히 많기 때문에 이 모든 후보의 아이템에 대한 확률을 다 계산할 수 없습니다. 그래서 아이템을 negative 샘플링을 해서 서셋만으로 로스를 계산해야 합니다. 그래서 아이템을 상호작용하는 것들은 우리가 가지고 있지만"},{"start":2315000,"end":2324500,"text":"그렇지 않은 그 유저가 그 아이템을 상호 작용하지 않았다는 데이터는 우리가 없죠. 그러면 그 상호작용하지 않은 아이템에 대한","confidence":0.9706,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2315250,2315600,"그렇지"],[2315600,2315780,"않은"],[2316050,2316200,"그"],[2316200,2316480,"유저가"],[2316480,2316567,"그"],[2316567,2316960,"아이템을"],[2317010,2317220,"상호"],[2317570,2318040,"작용하지"],[2318070,2318540,"않았다는"],[2319510,2320000,"데이터는"],[2320170,2320400,"우리가"],[2320430,2320720,"없죠."],[2321950,2322200,"그러면"],[2322230,2322380,"그"],[2322430,2323120,"상호작용하지"],[2323120,2323300,"않은"],[2323390,2323900,"아이템에"],[2323900,2324120,"대한"]],"textEdited":"그렇지 않은 그 유저가 그 아이템을 상호 작용하지 않았다는 데이터는 우리가 없죠. 그러면 그 상호작용하지 않은 아이템에 대한"},{"start":2324500,"end":2338600,"text":"평가는 어떻게 해야 될까요? 그 사용자가 존재를 아예 몰랐을 수도 있고 아니면 반대로 그 아이템을 확인했지만 클릭하지 않았을 수도 있죠. 이제 그러한 어려움이 있는데요. 이제 이 어려움을 극복하기 위해서","confidence":0.9849,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2325030,2325380,"평가는"],[2325380,2325567,"어떻게"],[2325567,2325694,"해야"],[2325694,2326020,"될까요?"],[2326690,2326840,"그"],[2327050,2327460,"사용자가"],[2327490,2327880,"존재를"],[2327880,2328040,"아예"],[2328090,2328500,"몰랐을"],[2328530,2328687,"수도"],[2328687,2328940,"있고"],[2329250,2329560,"아니면"],[2329890,2330220,"반대로"],[2330830,2330980,"그"],[2330980,2331360,"아이템을"],[2331370,2331880,"확인했지만"],[2332350,2332780,"클릭하지"],[2332780,2333080,"않았을"],[2333080,2333234,"수도"],[2333234,2333460,"있죠."],[2334450,2334620,"이제"],[2334630,2334900,"그러한"],[2334990,2335327,"어려움이"],[2335327,2335660,"있는데요."],[2336330,2336500,"이제"],[2336530,2336680,"이"],[2336690,2337100,"어려움을"],[2337630,2338080,"극복하기"],[2338080,2338340,"위해서"]],"textEdited":"평가는 어떻게 해야 될까요? 그 사용자가 존재를 아예 몰랐을 수도 있고 아니면 반대로 그 아이템을 확인했지만 클릭하지 않았을 수도 있죠. 이제 그러한 어려움이 있는데요. 이제 이 어려움을 극복하기 위해서"},{"start":2338600,"end":2352700,"text":"아이템의 인기도가 높았는데 상호 작용이 없었다면 인기도가 높을수록 아무래도 사용자가 관심이 없었을 거다라고 가정하고 인기도에 기반한 negative 샘플링을 제시합니다. 그래서 사용자가","confidence":0.9506,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2338910,2339420,"아이템의"],[2339530,2340040,"인기도가"],[2340650,2341200,"높았는데"],[2341230,2341440,"상호"],[2341440,2341760,"작용이"],[2341760,2342260,"없었다면"],[2343130,2343560,"인기도가"],[2343570,2344140,"높을수록"],[2344310,2344740,"아무래도"],[2344930,2345340,"사용자가"],[2345390,2345780,"관심이"],[2346010,2346460,"없었을"],[2346830,2347360,"거다라고"],[2348030,2348500,"가정하고"],[2348930,2349420,"인기도에"],[2349430,2349780,"기반한"],[2349830,2350240,"negative"],[2350270,2350760,"샘플링을"],[2350810,2351320,"제시합니다."],[2351550,2351780,"그래서"],[2351810,2352320,"사용자가"]],"textEdited":"아이템의 인기도가 높았는데 상호 작용이 없었다면 인기도가 높을수록 아무래도 사용자가 관심이 없었을 거다라고 가정하고 인기도에 기반한 negative 샘플링을 제시합니다. 그래서 사용자가"},{"start":2352700,"end":2361600,"text":"인터랙션 하지 않은 아이템들은 굉장히 많고요. 인터랙션 아이템은 별로 없겠죠. 그럼 이것을 다 학습 데이터를 사용하는 것이 아니라","confidence":0.9029,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2352970,2353420,"인터랙션"],[2353420,2353620,"하지"],[2353620,2353800,"않은"],[2354130,2354740,"아이템들은"],[2354870,2355140,"굉장히"],[2355140,2355540,"많고요."],[2355970,2356400,"인터랙션"],[2356450,2356780,"아이템은"],[2356780,2357020,"별로"],[2357020,2357380,"없겠죠."],[2357930,2358100,"그럼"],[2358100,2358440,"이것을"],[2358510,2358660,"다"],[2359950,2360180,"학습"],[2360180,2360440,"데이터를"],[2360440,2360727,"사용하는"],[2360727,2360914,"것이"],[2360914,2361160,"아니라"]],"textEdited":"인터랙션 하지 않은 아이템들은 굉장히 많고요. 인터랙션 아이템은 별로 없겠죠. 그럼 이것을 다 학습 데이터를 사용하는 것이 아니라"},{"start":2361600,"end":2372700,"text":"이 데이터 가운데 인기도가 높은 것들은 좀 많이 negative 샘플링으로 뽑고 인기도가 낮은 것은 거의 뽑지 않는 방식으로 이 positive 샘플과 negative 샘플 전체를 구성하게 됩니다.","confidence":0.9342,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2361850,2362000,"이"],[2362000,2362320,"데이터"],[2362320,2362660,"가운데"],[2362730,2363140,"인기도가"],[2363140,2363400,"높은"],[2363400,2363720,"것들은"],[2363730,2363880,"좀"],[2363890,2364120,"많이"],[2364390,2364800,"negative"],[2364810,2365320,"샘플링으로"],[2365330,2365660,"뽑고"],[2365950,2366360,"인기도가"],[2366360,2366600,"낮은"],[2366600,2366840,"것은"],[2366850,2367060,"거의"],[2367110,2367480,"뽑지"],[2367510,2367740,"않는"],[2367850,2368300,"방식으로"],[2368890,2369040,"이"],[2369070,2369740,"positive"],[2369750,2370040,"샘플과"],[2370050,2370740,"negative"],[2370750,2371000,"샘플"],[2371070,2371440,"전체를"],[2371770,2372180,"구성하게"],[2372180,2372480,"됩니다."]],"textEdited":"이 데이터 가운데 인기도가 높은 것들은 좀 많이 negative 샘플링으로 뽑고 인기도가 낮은 것은 거의 뽑지 않는 방식으로 이 positive 샘플과 negative 샘플 전체를 구성하게 됩니다."},{"start":2372700,"end":2385600,"text":"네 그래서 GRU 4l의 최종 결과 및 요약 부분입니다. 주어진 데이터셋에 대해서 클래식한 추천 모델인 아이템 KNN 모델과 비교를 했는데요. 이 모델 대비 약 20% 정도의 높은 추천 성능","confidence":0.8847,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2373110,2373260,"네"],[2373510,2373780,"그래서"],[2373850,2374200,"GRU"],[2374210,2374580,"4l의"],[2374630,2374880,"최종"],[2374910,2375220,"결과"],[2375220,2375340,"및"],[2375390,2375660,"요약"],[2375670,2376100,"부분입니다."],[2376170,2376520,"주어진"],[2376570,2377380,"데이터셋에"],[2377380,2377720,"대해서"],[2378190,2378620,"클래식한"],[2378650,2378920,"추천"],[2378930,2379280,"모델인"],[2379350,2379700,"아이템"],[2379750,2380160,"KNN"],[2380550,2381000,"모델과"],[2381030,2381327,"비교를"],[2381327,2381700,"했는데요."],[2382110,2382260,"이"],[2382260,2382480,"모델"],[2382490,2382760,"대비"],[2383130,2383280,"약"],[2383370,2383820,"20%"],[2383950,2384340,"정도의"],[2384650,2384900,"높은"],[2384990,2385240,"추천"],[2385290,2385540,"성능"]],"textEdited":"네 그래서 GRU 4l의 최종 결과 및 요약 부분입니다. 주어진 데이터셋에 대해서 클래식한 추천 모델인 아이템 KNN 모델과 비교를 했는데요. 이 모델 대비 약 20% 정도의 높은 추천 성능"},{"start":2385600,"end":2396800,"text":"리콜과 엠알알을 보였습니다. 본 논문은 모델은 고정시키고 다양한 로스를 사용하기도 했고, 또 모델 안에서 쥐알유 레이어의 히든 유닛도 서로 다르게 실험을 하여서 비교를 했는데요.","confidence":0.8077,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2385850,2386280,"리콜과"],[2386350,2386880,"엠알알을"],[2386880,2387360,"보였습니다."],[2387430,2387580,"본"],[2387610,2388020,"논문은"],[2388530,2388900,"모델은"],[2388910,2389520,"고정시키고"],[2389930,2390280,"다양한"],[2390330,2390840,"로스를"],[2390910,2391360,"사용하기도"],[2391360,2391640,"했고,"],[2391850,2392000,"또"],[2392030,2392280,"모델"],[2392310,2392660,"안에서"],[2392890,2393280,"쥐알유"],[2393290,2393660,"레이어의"],[2393670,2393920,"히든"],[2393920,2394280,"유닛도"],[2394690,2394900,"서로"],[2394900,2395200,"다르게"],[2395270,2395580,"실험을"],[2395730,2396040,"하여서"],[2396070,2396314,"비교를"],[2396314,2396680,"했는데요."]],"textEdited":"리콜과 엠알알을 보였습니다. 본 논문은 모델은 고정시키고 다양한 로스를 사용하기도 했고, 또 모델 안에서 쥐알유 레이어의 히든 유닛도 서로 다르게 실험을 하여서 비교를 했는데요."},{"start":2396800,"end":2408900,"text":"전반적으로는 GRU 레이어의 헤드 유닛이 많아질수록 더 좋은 추천 성능을 보였다고 이야기하고 있습니다. 네 이상 7강 GNN과 RNN을 활용한 추천 모델","confidence":0.9612,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2397030,2397700,"전반적으로는"],[2397930,2398400,"GRU"],[2398750,2399080,"레이어의"],[2399080,2399280,"헤드"],[2399280,2399660,"유닛이"],[2400050,2400700,"많아질수록"],[2401550,2401700,"더"],[2401730,2401900,"좋은"],[2402030,2402280,"추천"],[2402330,2402660,"성능을"],[2403230,2403720,"보였다고"],[2403790,2404220,"이야기하고"],[2404220,2404560,"있습니다."],[2404850,2405000,"네"],[2405150,2405380,"이상"],[2405930,2406200,"7강"],[2406750,2407320,"GNN과"],[2407350,2407820,"RNN을"],[2407830,2408120,"활용한"],[2408230,2408480,"추천"],[2408490,2408760,"모델"]],"textEdited":"전반적으로는 GRU 레이어의 헤드 유닛이 많아질수록 더 좋은 추천 성능을 보였다고 이야기하고 있습니다. 네 이상 7강 GNN과 RNN을 활용한 추천 모델"},{"start":2408900,"end":2420458,"text":"에 대한 강의를 모두 마쳤습니다. 모두 수고하셨습니다.","confidence":0.8374,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2409150,2409267,"에"],[2409267,2409460,"대한"],[2409550,2409860,"강의를"],[2409870,2410100,"모두"],[2410150,2410680,"마쳤습니다."],[2410970,2411180,"모두"],[2411190,2411860,"수고하셨습니다."]],"textEdited":"에 대한 강의를 모두 마쳤습니다. 모두 수고하셨습니다."}],"text":"안녕하세요. 이번 시간 일곱 번째 강의 지난 시간에 이어서 딥러닝 기반 추천 모델에 대해 살펴보겠습니다. 지난 시간에는 뉴럴넷의 가장 기본적인 구조인 MLP 멀티레이어 퍼셉트론 기반 추천 모델과 오토 인코더를 사용한 추천 모델에 대해서 다루었습니다. 오늘은 그보다 조금 더 복잡한 두 가지 딥러닝 기법, 두 가지 딥러닝 모델 아키텍처를 사용한 추천 모델에 대해서 학습하겠습니다. 바로 그래프 뉴얼 네트워크라고 불리는 지엔엔과 시퀀스 모델 등에 주로 사용되는 알앤엔 기반 추천 모델입니다. 먼저 그래프 뉴얼 네트워크라는 다소 생소할 수도 있는 모델에 대해서 학습하겠습니다. 그래프가 무엇이고 어떤 식의 데이터를 표현할 때 이 그래프가 적합한지, 또 그래프 자료 구조를 어떻게 뉴럴넷 추천 모델에 적용하였는지, 뉴럴 그래프 컬래버티 필터링 NGCF라는 모델과 이보다 조금 더 가벼운 라이트 GCN 이 두 가지 모델을 살펴보겠습니다. 그 이후에 RNN 계열의 모델을 간단히 리캡하고 GRU 레이어를 활용한 시퀀스 모델링 추천 기법인 gr 4 r을 살펴보겠습니다. 먼저 그래프 뉴널 네트워크를 활용한 추천 모델입니다. 그래프 신경망 그리고 그래프 컨볼루션 뉴럴 네트워크의 개념을 이해하고 이를 적용한 추천 시스템 모델에 대해서 살펴보겠습니다. 네 먼저 그래프 뉴얼 네트워크를 이야기하기 전에 이 그래프가 무엇인지에 대해서 정의해 봐야겠죠. 그래프의 정의는 굉장히 간단합니다. 이 꼭짓점인 노드와 노드와 노드 사이를 잇는 엣지 로 이루어진 자료 구조를 의미합니다. 따라서 데이터들이 서로 연결되어 있을 때 이러한 데이터를 표현하기에 적합한 자료 구조입니다. 아래에 보이는 제일 기본적인 그래프는 노드의 집합과 엣지들의 집합으로 이루어져 있습니다. 일반적으로 그래프는 노드들의 집합, 엣지들의 집합인 abcd라는 4개의 노드가 존재하고요. 이 abcd 서로서로 연결되어 있는 엣지가 총 4개가 존재하고 이 4개는 다음과 같은 그림으로 나타낼 수 있습니다. 그렇다면 그래프를 왜 사용할까요? 방금 언급했듯이 그래프는 연결되어 있는 데이터를 표현하기에 적합합니다. 따라서 어떤 객체 사이의 관계나 상호 작용 같은 다소 추상적인 개념을 표현할 수 있습니다. 이러한 상호 작용을 테이블 형태의 정형화된 데이터를 표현하면 오히려 표현형이 복잡해지는데요. 그래프를 사용하면 오히려 간단하게 표현할 수 있고 또 다른 관점으로도 표현할 수 있습니다. 아래와 같이 친구 관계, 소셜 네트워크라든지 그 팔로워 팔로잉 관계가 있는 데이터겠죠. 그리고 현재 코로나 바이러스 상황과 같이 바이러스의 확산 같은 상태도 이 그래프를 통해서 표현할 수 있습니다. 그 외에 추천 시스템에서 가장 많이 사용하는 유저가 아이템을 소비했다는 데이터도 그래프로 표현할 수 있는데요. 유저와 아이템을 각각의 노드로 두고 유저가 아이템을 소비했다는 데이터를 엣지로 한다면 이 데이터를 표현할 수 있습니다. 네 그래프를 사용하는 이유는 흔히 우리가 자주 다루는 이미지나 텍스트 혹은 엑셀 시트로 표현할 수 있는 정형화된 데이터와는 조금 다른 형태의 데이터를 사용하기 때문이다. 이제 이러한 이미지, 텍스트, 정형 데이터 같은 데이터들은 격자 형태로 표현할 수 있다고 얘기하고 이 데이터를 좀 더 일반화된 표현으로는 유클리디언 스페이스에 표현할 수 있다고 합니다. 이 유클리디언 스페이스를 간단하게 설명하자면 우리가 익숙하게 다루는 2차원 평면이나 3차원 공간을 일반화시킨 공간이고요. 즉 유한한 실수로 표현할 수 있는 공간이라고 볼 수 있습니다. 그래서 대표적으로 아래 그림과 같이 2차원 평면에 있는 이러한 이미지 데이터는 유클리드 스페이스로 표현할 수 있는 데이터입니다. 그러나 이런 소셜 네트워크 데이터와 같은 유저가 서로 연결되어 있는 데이터라든지 혹은 분자 구조와 같은 모양의 데이터는 유클리드 스페이스 즉 어떤 실수형의 형태로 표현할 수는 없는 데이터입니다. 오히려 이런 데이터를 그래프를 사용하지 않고 표현한다면 오히려 표현형이 굉장히 복잡해집니다. 그래서 그래프를 사용하여서 데이터의 표현을 단순하게 만드는 것이죠. 그래서 우리가 사용하는 논 유클리디언 스페이스의 데이터들을 그래프로 사용할 수 있다는 것을 언급하였습니다. 네 그렇다면 이 그래프를 신경망에 적용해야 하는데요. 이것을 그래프 신경망 그래프 뉴얼 네트워크라고 합니다. 그렇다면 어떻게 이 그래프로 나타낸 데이터를 뉴럴 네트워크에 적용할 수 있을까요? 먼저 왼쪽 아래와 같은 5개의 노드로 된 그래프가 있습니다. 이제 그래프에서 데이터를 설명할 때 제일 중요한 것이 노드 와 노드 사이에 이어져 있는 엣지들이죠. 그럼 이제 해당 노드에 연결된 이웃 정보들의 이웃 노드들의 정보를 뉴럴 네트워크 모델이 표현할 수 있어야만 그래프에서 제일 중요한 패턴을 학습할 수 있는 것입니다. 그래서 가장 나이브한 방법은 이 그림과 같이 그래프를 표현할 수 있는 인접 행렬을 만들어서 그 인접 행렬을 그대로 뉴럴 네트워크 모델의 인풋으로 사용하는 것입니다. 보시면 a와 b가 연결되어 있기 때문에 a와 b의 매트릭스의 원소 1 그리고 또 b와 a의 매트릭스 원소의 1 이렇게 표현되어 있죠. 그래서 이 인접 행렬을 말 그대로 뉴럴 네트워크 인풋 레이어에 데이터 하나를 뽑아서 그대로 넣어주게 됩니다. 그래서 이 데이터는 아마 이 비에 대한 표현형이겠죠. 그래서 인접 행렬 더하기 다른 기존에 우리가 뉴럴 테크 모델에서 사용하고 있는 다른 피처형을 이렇게 넣어 줄 수 있습니다. 하지만 이제 이런 기법의 문제는 무엇일까요? 일단 노드가 계속 증가하게 되면 인접 행렬의 차원은 계속 증가하고 이 행렬의 행을 그대로 인풋 레이어에 사용하게 되면 차원은 계속 증가하게 될 것입니다. 그런 모델의 복잡도는 계속해서 증가하고 연산량은 많아지면서 동시에 입력 데이터는 아주 스파r스해지겠죠. 그렇기 때문에 이 모델이 좋지 않습니다. 또한 여기서 인접 행렬은 a b c d e 순서 대로 구성되어 있는데요. 사실 이 순서는 그래프 자체에서는 아무런 의미가 없습니다. 이 그래프를 살펴보면 abcde의 연결 관계를 모델링 한 것인데 이것을 매트릭스를 표현하기 위해서 우리가 임의로 순서를 준 것이죠. 근데 이 나이브 어프러치 즉 인접 행렬을 그대로 인플레이어에 사용하는 방법은 노드의 순서가 그대로 입력 값으로 사용되기 때문에 만약에 이 순서가 바뀌면 또 의미가 달라질 수도 있습니다. 네 그래서 방금 설명한 대로 이 나이브 어프로치 나이브하게 그래프의 인접 행렬을 만들어서 그 인접 행렬을 그대로 뉴럴넷 모델의 입력 값으로 사용하는 방법은 좋지 않습니다. 노드가 많아질수록 연산량은 기하급수적으로 많아지고 또 노드의 순서의 의미가 이상하게 반영될 수 있기 때문이죠. 따라서 이런 나이브한 방법으로는 보통 그래프 데이터를 뉴럴넷 모델링을 하지 않습니다. 대신에 이 그래프 컨볼루션 뉴럴 네트워크라는 개념이 등장하죠. 이 컨볼루션이라는 개념 이 단어는 이미지 데이터를 다루는 CNN 계열의 모델에서 아마 자주 들어보셨을 것입니다. 먼저 이 우측에 있는 그림을 보십시다. 아래와 같은 왼쪽의 이미지 데이터 즉 2차원 공간으로 표현되어 있는 이미지 데이터에서 컨볼루션 레이어를 사용하는 방법은 이 2차원 유클리드 공간에 주어진 빨간색 데이터 포인트 를 포함하여 이 주변에 있는 즉 로컬 커넥티비티가 연결되어 있는 데이터를 모아서 컨볼루션을 진행합니다. 그래서 왼쪽에 있는 것이 2디 컨볼루션의 컨볼루션 레이어를 사용하는 방법이고요. 이제 이것을 논 뉴클리디언 공간으로 확장시킨 것이 바로 그래프 뉴럴 네트워크의 그래프 컨볼루션입니다. 그래서 이미 이미지에서 다루는 컨볼루션과 굉장히 유사한데요. 이제 이미지 데이터는 방금 설명했듯이 주어진 데이터와 물리적으로 가까이 있는 데이터들을 가지고 컨볼루션을 수행했습니다. 그럼 그래프 컨볼루션 같은 경우에도 주어진 데이터와 가장 가까이 있다는 의미는 바로 하나의 엣지로 연결되어 있는 이 그래프를 기준으로 다른 이웃들을 의미하죠. 그래서 이 해당 노드를 포함하여서 연결되어 있는 이웃들, 연결되어 있는 노드들을 한꺼번에 모아서 컨볼루션 연산을 수행합니다. 그래서 이런 점이 로컬 커넥티비티 측면에서 이미지의 컨볼루션과 그래프의 컨볼루션이 같은 원리를 사용한다는 것을 알 수 있고요. 또한 둘 다 쉐어드 웨이츠를 사용합니다. 우리가 CNN에서 컨볼루션 연산을 배울 때 이거 하나를 필터라고 했는데요. 여기서는 보면 3 바이 3의 필터를 사용한 것이죠. 그런데 이 필터가 가진 파라미터는 동일하고 여기서 사용하는 파라미터와 이 옆에 있는 파라미터는 모두 같은 웨이트를 공유합니다. 마찬가지로 그래프 컨볼루션에서도 이 데이터를 기준으로 만든 이 웨이트와 혹은 다른 데이터를 기준으로 만든 컨볼루션 에서도 같은 파라미터 같은 웨이트를 셰어하게 됩니다. 네 마지막으로 컨볼루션 레이어를 2개 이상 3개 이상 쌓게 되면은 바로 옆에 있는 데이터 포인트 외에 더 멀리 있는 한 칸 더 멀리 있는 정보까지 많이 참고하여서 이 데이터의 레프레젠테이션을 학습할 수 있는데요. 이 지씨앤 그래프 콤블루션도 마찬가지로 바로 옆에 연결되어 있는 다른 이웃 노드가 아니라 하나 더 연결되어 있는 이웃 노드까지 확장시켜서 레이어를 2개 이상 사용할 수 있습니다. 그래서 이렇게 세 가지 측면 바로 옆에 있는 연결되어 있는 데이터를 의 정보를 활용한다. 그리고 그 필터와 같은 웨이트를 셰어한다. 그리고 레이어를 2개 이상 쌓을 수 있다라는 측면에서 이 컴플루션 네트워크라고 볼 수 있고요. 그래서 연산량을 줄이면서 노드끼리 연결된 관계를 표현할 수 있고 또 그보다 더 멀리 있는 간접 관계를 특징으로 추출할 수 있습니다. 그래서 앞으로 우리가 보통 GNN 계열의 추천 모델이다라고 하면은 보통 이 그래프 컨볼루션 뉴럴 네트워크를 사용한 모델임을 기억하시길 바랍니다. 그래서 이번 모델은 GNN을 기반으로 유저 아이템의 상호작용 즉 이 상호 작용을 콜라베이티브 시그널이라고 본 논문에서 표현하고 있는데요. 이 콜라베이트 시그널을 추출하는 모델인데요. 그래프가 가진 장점을 사용해서 유저와 아이템 간의 콜라베이트 시그널을 인베딩 단 즉 인베딩 레이어에서 직접 추출한 접근법을 제시한 논문입니다. 이 논문이 발표된 이후 이 뉴럴 글래스 컬라버레이트 필터링이 발표된 이후에 물론 이보다 더 가볍고 좋은 성능을 가진 GNN 기반 추천 모델이 많이 발표되었지만 이 논문을 통해서 처음 그래프 컨볼루션 뉴럴 네트워크가 추천 시스템을 풀기에 좋은 모델이라는 것을 밝혔기 때문에 그런 점에서 이 논문은 굉장히 연구적으로 의미가 있습니다. 논문의 등장 배경은 다음과 같습니다. 그동안 우리가 배웠던 뉴럴 그래프 컬라버레이트 필터링이 아닌 3강부터 배웠던 컬래버레이트 필터링, 제일 기본적인 매트리스 팩토라이제이션 같은 모델을 살펴보면 크게 두 가지 특징을 모델이 잘 학습해야 한다는 것을 알 수입니다. 이 추천 시스템 모델이라고 하면은 이 아래 있는 두 가지 첫 번째는 유저 아이템의 인베딩을 잘 학습해야 합니다. 유저 아이템은 계속해서 원핫 인코딩으로 처음에 표현된다고 했고, 그것을 우리가 어떤 댄스 한 인베딩으로 학습한다고 했는데요. 엠에프부터 시작하여서 유저와 아이템을 어떤 레이턴트 팩터 즉 인베딩으로 표현하는 기법은 딥러닝 모델로 넘어와서도 동일하게 인베딩 레이어를 사용해서 이어져 오고 있습니다. 두 번째로는 유저와 아이템의 상호 작용입니다. 대표적으로 메트릭스 펙토라이제이션에서는 유저 아이템을 임베딩한 이후에 그 유저 아이템을 내적 닷 프로덕트 하여서 그 유저와 아이템의 상호 작용을 리니어하게 표현했습니다. 문제는 엠프를 포함하여서 기존의 뉴럴넷 기반의 CF 모델 즉 이런 뉴럴 컬라버리티 필터링 계열의 모델들은 유저와 아이템의 상호 작용을 인베딩 단계에서 접근하지 못했습니다. 방금 설명했듯이 매트리스 팩터라이제이션에서도 pu 유저의 인베딩과 qi 아이템의 인베딩을 각각 구한 다음에 그 둘의 내적을 통해서 상호 작용을 표현하기 때문에 인베딩과 상호 작용이 분리되어 있습니다. 마찬가지로 뉴럴 컬라버티 필터링 모델도 인베딩이 일어난 이후에 그 인베딩을 컨케이트네이트 하기 때문에 인베딩과 상호 작용이 분리되어 있는 것이죠. 그래서 각각의 인베딩이 상호 작용의 서브 옵티멀하게 학습되기 때문에 그 결과 모델이 더 정확한 표현형을 가지지 못하고 추천의 능력이 조금 떨어지게 되는 것입니다. 그래서 이 GNN 추천 모델은 유저와 아이템의 상호 작용이 유저 아이템을 임베딩 시킬 때부터 반영될 수 있도록 모델을 설계하였습니다. 먼저 아래 그림과 같이 유저가 아이템을 소비했다는 데이터 를 다음과 같은 그래프 유저 아이템 인터랙션 그래프라고 표현했습니다. 그래서 왼쪽에 유저 3명이 있고요. 오른쪽에 아이템 5개가 있죠. 그래서 이 edg는 유저 1이 아이템 1을 소비했다. 유저 3이 아이템 4를 소비했다라는 데이터입니다. 단순히 이렇게 유저 1이 아이템 1 2 3를 소비했다. 유저 2가 아이템 2 4 5를 소비했다는 데이터만을 가지고는 이 모든 유저와 아이템 간의 상호작용을 표현할 수 없습니다. 예를 들면 유저 1은 유저 1 2 3랑 연결되어 있긴 하지만 i 유저 4 아이템 4 아이템 파랑 연결되어 있지 않기 때문에 이 유저 아이템의 개수가 점점 더 많아질수록 유저와 아이템 페어의 컬라버레이트 시그널 즉 어떤 상호작용을 놓치는 경우가 생기게 되죠. 그래서 이러한 문제를 지엔엔이 가진 특성을 사용하여서 경로가 1보다 큰 하이워드 커넥티비티를 임베딩하여서 모델에 사용하였습니다. 그래서 이 부분을 이 그림을 통해 좀 자세히 설명해 보겠습니다. 먼저 이 유저 1이 소비한 아이템은 총 3개가 되겠죠. 아이템 1 아이템 2 아이템 3입니다. 그래서 패스가 1이라는 뜻은 바로 다이렉트로 연결되어 있는 엣지가 하나라는 것이죠. 그래서 여기서 이제 길이가 2인 패스로 확장하게 되면은 아이템 2에서 아이템 2를 소비한 다른 유저인 유저 2로 확장될 수 있고요. 아이템 쓰는 아이템 3를 소비한 다른 유저인 유저 3로 확장될 수 있습니다. 또 이렇게 해서 랭스 3까지 확장될 수 있겠죠. 그래서 이 그래프를 통해 세 가지를 알 수 있는데요. 먼저 유저 1과 유저 e는 아이템 2를 가지고 서로 상호작용하기 때문에 이 둘이 유사하다라는 정보 이 그래프에서 볼 수 있습니다. 두 번째는 유저 2가 이미 소비한 아이템 4 아이템 5가 시그널이 전달되어서 다음과 같이 유저 원에게 전달될 수 있기 때문에 아이템 4 아이템 5가 유저 원에게 추천될 수 있는 확률이 높다는 것을 또 이 그래프에서 의미합니다. 마지막으로 이 레이어 3에 있는 유저 1에서 아이템 2, 유저 2로 전달되는 이 레이어 3 아이템 4, 아이템 5가 존재하고 마찬가지로 유저 3로 이어지는 레이어에도 아이템 4가 존재합니다. 따라서 아이템 5보다는 아이템 4가 시그널이 두 번 전달되기 때문에 아이템 4가 추천될 확률이 조금 더 높다는 것을 의미합니다. 이렇게 하나의 노드를 기준으로 경로가 1보다 큰 하이 오더 커넥티비티 를 사용하여서 이 유저에 대한 다양한 표현형을 이 그래프 뉴얼 네트워크는 표현하고 있습니다. 그래서 전체 모델은 다음과 같습니다. 오른쪽에 있는 그림인데요. 이 레이어가 총 3가지로 구성되어 있습니다. 첫 번째 레이어는 유저 아이템을 원핫 인코딩하는 가장 기본적인 레이어로서 기존의 씨프 뉴럴 콜라베이트 필터링 모델 계열과도 동일합니다. 가장 처음 입력된 원핫 인코딩을 k 차원의 인베딩으로 바꿔주는 것이지요. 그래서 가장 중요한 부분은 사실 이 두 번째 레이어인 인베딩 전파 레이어입니다. 방금 전에 설명했던 그래프 즉 유저와 아이템의 그래프에서 인베딩이 전파되는 이 하이워드 커넥티비티가 바로 이 레이어 인베딩 전파 레이어 인패딩 프로퍼게이션 레이어에서 학습이 되는 것이죠. 그리고 이 인베딩 전파 레이어를 통해서 출력된 출력 값은 그대로 컨케이트네이트 돼서 마지막 최종 레이어 즉 유저 아이템 선호도 예측 레이어인 프로덕션 레이어에서 최종 예측 값을 구하게 됩니다. 네 그리고 그림을 보시면 이 왼쪽이 유저 노드를 기준으로 인베딩이 전파되는 유저 인베딩 레이어고요. 오른쪽은 아이템 노드를 기준으로 전파되는 아이템 레이어입니다. 방금 전에 하이 오더 커넥티비티 예시를 설명하면서 유저를 기준으로만 그래프의 연결을 설명했지만 동일하게 아이템 노드를 기준으로도 똑같이 인베딩 전파가 이루어질 수 있습니다. 그래서 각각의 유저 노드와 아이템 노드에 대한 인베딩을 각각 구하고 이 인베딩을 최종적으로 합쳐줘서 마지막 레이어에서 내적을 통해서 최종 예측 값 평점과 같은 선호도를 예측하게 되는 것이죠. 먼저 첫 번째 레이어 유저 아이템에 대한 인베딩 레이어입니다. 지난 4강의 매트리스 팩토라이제이션, 그리고 바로 전 6강에서 배웠던 뉴럴 컬라버레이트 필터링 모델에서 원 핫 인코딩 벡터를 인베딩시켰던 그 인베딩이 바로 인터랙션 펑션에 입력되어서 곧바로 선호도를 예측했지만 이 NGCF 즉 GNN을 활용한 추천 모델에서는 이 인베딩이 바로 사용되지 않고 그래프 뉴얼 네트워크 정확히는 그래프 컨볼루션 뉴럴 네트워크 상으로 전파시켜서 이것이 이 GNN 상에서 리파인 되도록 합니다. 그것은 컬래버레이티브 시그널을 명시적으로 인베딩 레이어에 주입하기 위한 과정으로 볼 수 있습니다. 그래서 첫 번째 인베딩 레이어에서는 임베딩을 생성해 준다. 그 이후 두 번째 레이어에서 생성된 인베딩을 가지고 전파시키는 레이어 가장 중요한 부분입니다. 여기서는 유저와 아이템의 컬라버레이트 시그널을 담는 것을 메시지라고 정의하고 이 메시지로 표현을 하였습니다. 일단 여기 아래에 있는 수식들은 아까와 비슷하게 유저 노드를 기준으로 소식이 설명되고 있는데요. 이제 아이템 아에서 유로 전달되는 메시지를 mui라고 표현하고 있습니다. 그래서 mui는 아이템 아이 자체가 가지고 있는 임베딩 플러스 아이템 아이와 아이템 유저의 상호작용을 표현할 수 있는 이 엘레멘트 와이스 프로덕트 이 두 개의 텀의 합으로 표현됩니다. 그리고 유저 율을 기준으로 연결돼 있는 아이템이 점점 많을수록 이 시그널은 점점 커지기 때문에 그 시그널의 크기를 노말라이제이션 해 주기 위해서 개별 메시지의 크기를 이웃한 노드의 개수로 나눠주는 노멀라이제이션 텀도 앞에 붙어 있습니다. 이제 하나의 아이템 노드 아에서 유저로 전달되는 유저 메시지를 계산하고 나면은 이 메시지는 하나가 아니라 유저를 기준으로는 여러 개의 아이템이 존재할 수 있겠죠. 그래서 이 밑에서 최종적으로 유저 인베딩을 구해주는 즉 인베딩 전파 레이어를 통해 최종적으로 새로 생성되는 이 이유1 첫 번째 전파 레이어의 최종 유저 인베딩은 다음과 같은 수식으로 표현할 수 있습니다. 먼저 그 자신으로부터 전달되는 메시지 엠유유와 이 위에서 계산한 엠유아를 주변 인접 노드들에 대해서 모두 더해주게 됩니다. 그래서 이 더한 값을 리키 밸루라는 액티베이션 펑션을 사용하여서 최종적으로 그 전 레이어로부터 전파된 인베딩들이 다음 레이어 이유원으로 표현되게 됩니다. 이제 이렇게 해서 구해진 이유원이라는 애들은 원호 전파 퍼스트 홈까지 전파된 인베딩이라고 말합니다. 이렇게 인베딩이 전파되는 레이어를 그림과 같이 원 호 퍼스트 톱 즉 하나만 쌓은 것이 아니라 패스를 2개, 3개 확장할 수 있겠죠. 그래서 엘개까지 쌓을 수가 있겠죠. 그럼 타겟 유저를 기준으로 총 l차 이웃 즉 패스가 엘만큼 떨어져 있는 이웃까지도 인베딩 전파가 이루어집니다. 그래서 이 3단계의 임베딩을 구하기 위해서는 바로 전 단계인 2단계의 임베딩을 가지고 사용해야겠죠. 그래서 다음과 같은 점화식으로 표현할 수 있겠습니다. 새로운 엘차 인베딩은 기존 바로 전에 엘 마이너스 1차 인베딩 전파의 결괏값들로 이루어져서 이 값이 전파되어서 엘차 인베딩의 값을 구하게 되는 것입니다. 여기에 있는 수식은 바로 전에 1차 인베딩을 설명했던 수식과 같은 수식인데 1이 엘로 바뀌었을 뿐입니다. 그래서 이렇게 인베딩을 하나만 쌓는 것이 아니라 2개 3개 4개 쌓는 것이 하이워더 프로퍼게이션 아까 설명했던 패스 하나짜리가 아니라 2개 3개까지의 정보들을 현재 유저 노드를 기준으로 전달시키는 과정을 표현한 것입니다. 자 그래서 우리가 엘게에 인베딩 프로파게이션 레이어 인베딩 전파 레이어를 사용했다고 하면은 총 20 처음 인베딩 레이어에서 생산된 인베딩부터 마지막 엘 차 인베딩까지 생성된 인베딩까지 총 엘 플러스 1개의 유저 인베딩 그리고 아이템도 지금 전 그림에서 설명하지 않았지만 동일하게 아이템도 생성될 수 있다고 했죠. 아이템도 마찬가지로 엘 플러스 1개의 임베딩 벡터가 생성됩니다. 그럼 요 인베딩 벡터들을 모두 컨택 하게 되고 그러면 이 두 최종 인베딩 이유 스타 이아스타는 차원이 같기 때문에 이 두 값을 내적하여서 최종적으로 우리가 원하는 그 유저가 그 아이템에 갖는 선호도 값을 계산하게 됩니다. 네 제안한 뉴럴 그래프 컬래버이트 필터링 모델의 성능 및 결과를 보시면 이제 먼저 여기에 있는 1 2 3 4는 우리가 인베딩 전파 레이어에서 인베딩 전파를 한 번 했냐 두 번 했냐 세 번 했냐 이것을 비교한 결과인데요. 인베딩 전파 레이어가 점점 많아질수록 모델의 추천 성능 이 총 3개의 데이터셋을 사용했는데요. 많아지면 많아질수록 점점 좋아지는데 다만 레이어가 너무 많이 쌓일 경우에는 오버피팅이 발생할 수 있습니다. 따라서 실험 결과 레이어가 한 3개 정도에서 4개 정도일 때 가장 좋은 성능을 보인다고 밝히고 있습니다. 네 그리고 제안한 모델을 매트리스 팩토라이제이션 모델과 비교한 부분이 있습니다. 아까 이 매트리스 팩토라이제이션 모델은 유저 아이템을 학습시킬 때 그 인베딩 학습과 상호 작용이 분리되어 있다고 말씀드렸는데요. 이 NGCF는 그렇지 않기 때문에 이 인베딩을 더 잘 표현한다는 것을 다음 다음 실험을 통해서 입증하고 싶었습니다. 먼저 어떤 데이터셋을 사용해도 매트리스 팩토라이제이션보다 이 NGCF가 더 모델 성능이 빠르게 수렴하고 추천 성능인 리콜도 더 높음을 알 수 있습니다. NGCF가 모델의 캐패시티 즉 모델의 표현력과 갖고 있는 파라미터의 개수가 훨씬 크고 또 제일 중요한 이 인베딩 전파를 통해서 유저와 아이템에 대한 레프리젠테이션 파워가 더 좋기 때문입니다. 이 레프레젠테이션 파워가 좋다. 즉 유저와 아이템을 더 잘 표현한다라는 것은 시각화를 했을 때도 정성적으로 확인해 볼 수 있습니다. 이제 오른쪽에 있는 그림을 보시면 왼쪽이 MF의 인베딩이고요. 오른쪽이 NGCF 세 번째 레이어의 인베딩입니다. 그래서 이것을 보기 좋게 표현한 것인데요. 여기서 이 별표들은 유저 그리고 아이템은 동그라미를 의미합니다. 그리고 같은 색을 가지는 유저는 그 유저가 해당 아이템을 과거에 소비하였음을 의미합니다. 그 시각화된 결과를 통해서 보시면 오른쪽에 있는 NGCF는 이 유저를 기준으로 그 유저가 과거에 소비한 아이템들이 비슷한 공간에 인베딩이 되어 있죠. 마찬가지로 이 노란색 유저가 과거에 이 아이템들을 소비했고 그래서 그 인베딩 유저와 아이템의 인베딩이 비슷한 공간에 임베딩이 되어 있습니다. 근데 매트리스 팩토라이제이션 같은 경우에는 그 유저가 소비한 아이템 노란색을 봤을 때 이 유저의 인베딩과 아이템의 인베딩이 다소 떨어져 있는 것을 볼 수 있죠. 그 이유는 이 유저 아이템 인베딩을 학습할 때 우리가 유저 아이템의 상호 작용을 직접적으로 그 인베딩에 주입했다. 따라서 그 상호작용이 더 인베딩에 잘 표현된다 라고 볼 수 있습니다. 그리고 이 레이어가 점점 많아질수록 더 명확하게 구분되고 있습니다. 여기까지가 neural graf 컬래버이트 필터링에 대한 전반적인 내용이었고요. 이 GNN 기반 추천 시스템에서 제일 중요한 개념 즉 인베딩 전파되는 레이어 어떻게 인베딩이 계속해서 학습이 되고 그 상호작용이 어떻게 인베딩 단에서 주입되는지를 담고 있는 제일 중요한 개념을 담고 있는 모델이기 때문에 자세하게 설명을 했습니다. 이어서는 NGCF보다는 조금 더 간단한 GCN 즉 그래프 컨볼루션 네트워크의 가장 핵심적인 부분만 사용하여서 더 정확하면서도 파라미터 수 모델 캐파시티는 좀 작은 라이트 지시엔이라는 논문 그 모델을 살펴보도록 하겠습니다. 라이트 GCN의 아이디어는 먼저 가벼운 모델입니다. 기존의 NGCF 모델은 인베딩 전파 레이어에서 컨볼루션을 할 때 매번 학습 파라미터를 인베딩에 곱하고 거기에 노리어 트랜스포메이션 리키렐로를 사용했는데요. 라이트 지시에는 그 인베딩 노드의 파라미터를 곱하지도 않고 단순하게 인베딩을 가중합하는 것이 전부입니다. 그래서 학습 파라미터와 연산량이 NGCF에 비해서 훨씬 감소하게 됩니다. 또한 이 레이어의 깊이 즉 인베딩 전파 레이어의 깊이가 계속 깊어질수록 그 강도가 약해질 것이라는 아이디어를 사용하여서 그 라이트 GC에는 인베딩을 합칠 때도 굉장히 단순한 방법을 사용하였습니다. 다음 그림을 통해 조금 더 두 모델의 차이점을 비교해 보겠습니다. 크게 두 가지 부분이 다릅니다. 처음에 인베딩 원핫 인코딩이 인베딩 되는 것은 동일하고요. 먼저 이 가운데 있는 인베딩 전파 레이어 라이트 지센 같은 경우는 이 부분인데요. 이 부분이 라이트 지센이 좀 더 간단하게 이루어져 있고요. 그리고 이 마지막에 프리딕션 레이어 부분도 기본에 기존에 NGCF는 컴케이트 네이트를 했는데 여기서는 단순하게 가중합으로 더한다고 표현되어 있습니다. 여기 웨이트드 썸이라고 표현되어 있죠 이 각각의 차이를 좀 더 수식을 통해서 자세히 비교해 보도록 하겠습니다. 먼저 인베딩 전파 레이어 부분의 차이점을 봅시다. 이 왼쪽에 있는 수식이 방금 전 슬라이드에서 다뤘던 방금 전 파트에서 다뤘던 NGCF 모델의 수식이었고요. 오른쪽이 현재 이야기하고 있는 라이트 GCN의 인베딩 전파 레이어의 계산 수식입니다. NGCF 같은 경우에는 케차 인베딩 즉 바로 전 전파되기 전에 인베딩에다가 각각의 학습 파라미터를 곱하고 거기에 액티베이션 펑션까지 통과시켜서 결국 케이 플러스 1차 인베딩을 구합니다. 다소 복잡하죠. 그러나 라이트 GCM 같은 경우에는 피처 트랜스포메이션이나 논리뉴 액티베이션 같은 펑션 없이 단순하게 바로 전에 전파된 바로 전에 있는 레이어, 즉 k차 인베딩 레이어에 있는 인베딩 값을 단순히 가중치를 사용해서 합하는 것이 전부입니다. 또한 연결된 이웃 노드 정보만을 사용했기 때문에 NGCF에서는 다음과 같은 셀프 커넥션 즉 유저에서 유저로 가는 표현도 있는데요. 어 라이트 GCM 같은 경우에는 이 유저에 대한 표현을 자기 자신은 빼고 연결되어 있는 아이템의 인베딩만을 가지고 표현합니다. 그래서 최종적으로 보시면은 이 베딩이 NGCF 모델은 인베딩 전파 레이어의 학습 파라미터가 존재하지만 라이트 GC에는 이 인베딩 전파 레이어의 학습 파라미터가 1개도 존재하지 않습니다. 처음에 입력될 때 원핫 인코딩이 이 인베딩으로 표현될 때만 그 0번째 인베딩 레이어에서만 학습 파라미터가 존재하고 그 이후에는 파라미터 학습 파라미터의 추가 학습 없이 단순하게 계속 가중 평균을 활용한 전파만 사용하게 됩니다. 네 마지막은 예측 레이어입니다. 바로 전에 인베딩 전파 레이어에서 구해진 인베딩을 어그리게이트 하여서 최종 예측을 수행하는 레이어입니다. 이제 이 예측 레이어도 라이트 GCN는 NGCF보다 조금 더 가볍게 다르게 구성하였는데요. NGCF 같은 경우에는 0차 인베딩부터 엘차 인베딩까지 모두 가로로 컨케이트네이트에서 유저와 아이템의 임베딩을 구했습니다. 이 라이트 GCM 같은 경우에는 가로로 컨케이트네이트 하지 않고 이 알파 케라는 가중치를 사용하여서 단순하게 더했는데요. 컨케이트네이트를 하면 계속해서 차원이 옆으로 늘어나지만 이렇게 가중합을 할 경우에는 정보를 조금 더 압축해서 표현하는 것입니다. 따라서 이 가중치를 어떻게 사용하면서 압축하느냐가 중요한 부분인데요. 본 논문에서는 이 알파 케 가중치를 학습 파라미터로 사용할 수도 있고 하이퍼 파라미터로 사용할 수도 있다고 했습니다. 하지만 둘의 별 차이가 없기 때문에 굳이 학습 파라미터를 늘리지 않고 하이퍼 파라미터를 사용했습니다. 그래서 이 하이퍼 파라미터는 케 플러스 1 분의 1을 사용했는데요. 즉 레이어 이 케에가 점점 깊어질수록 가중치는 점점 작아지는 것이죠. 이 의미는 아까 얘기한 대로 인베딩이 처음보다 점점 전파될수록 그 인베딩의 시그널이 약해질 것이라는 아이디어를 사용하여서 그 아이디어를 수식적으로 주입한 것입니다. 최종 결과 및 요약입니다. 라이트 GCM 모델을 계속해서 NGCF와 비교했는데요. 실험 결과도 베이스 라인을 NGCF 모델로 삼고 성능을 비교했습니다. 학습을 통한 로스 펑션 손실 함수와 그리고 각 데이터셋 별로 성능인 위코 모두 NGCF보다 뛰어난 성능을 보였습니다. 이는 라이트 GCN이 NGCF보다 제너럴라이제이션 파워 즉 일반화 능력이 더 좋다는 것인데요. NGCF 같은 경우에는 아까 언급했듯이 인베딩 전파 레이어 매번 하나하나마다 학습해야 되는 파라미터가 존재했고 또 모델 구조가 복잡하기 때문에 오버피팅의 위험이 있는 반면에 라이트 GCN은 모델 구조를 단순하게 구성하되 GCN에서 제일 중요한 컨볼루션을 놓치지 않아서 최대한 정보를 잘 압축하였고 더 예측력을 뛰어나게 상승시켰습니다. 자 여기까지가 GNN을 활용한 정확히는 그래프 컨볼루션 뉴럴 네트워크 GCN을 활용한 추천 모델을 살펴보았습니다. NGCF와 lit GCN이었고요. 번외로 이런 GNN 계열의 모델은 실제로 네이버나 왓챠 같은 이런 실제 서비스에서도 유저 아이템을 잘 표현하는 추천 모델로 활발하게 사용되고 있다고 합니다. 다음은 RNN을 활용한 추천 모델입니다. RNN의 개념을 이해하고 이를 적용한 추천 시스템 모델에 대해서 살펴보겠습니다. 먼저 RNN 모델을 간단하게 리캡하고 이 RNN을 사용한 대표적인 추천 모델인 gr 4 LA 모델을 다루겠습니다. 먼저 알앤엔을 사용한 추천 모델을 다루기 전에 알앤엔과 그 계열의 모델들을 간단하게 리뷰만 하겠습니다. 이 RNN 같은 경우에는 시퀀스 데이터의 처리와 이해에 굉장히 좋은 성능을 보이는 모델입니다. 현재의 상태가 그다음 상태 즉 시퀀셜하게 영향을 미치도록 루프 구조를 사용하고 있는데요. 그래서 이 그림을 보시면은 이렇게 루프 형태로 표현할 수 있지만 사실은 시퀀셜한 인풋이 들어왔을 때 계속해서 시퀀셜하게 각각의 셀이 계산돼서 이 정보는 다음으로 넘어가게 되고 또 그다음 정보는 그 다음 스텝으로 넘어가는 이런 시퀀셜한 구조를 사용해서 마지막 최종 예측을 수행하게 됩니다. 자 다음은 알의 대표적인 모델인 엘스티엠입니다. 이 LSTM 같은 경우에는 시퀀스가 점점 길어질수록 그 시퀀스의 학습 능력이 현저하게 저하되는 이런 RNN의 한계를 극복하기 위해서 롱 숏텀 메모리라는 개념을 추가한 모델입니다. 그래서 장기 의존성 의 문제를 해결하기 위해서 셀 스테이트라는 구조를 고안하였고요. 여기 부분 보시면 게이트는 총 폴 겟 인풋 아웃풋 게이트라는 것을 활용하는데요. 이제 이전 셀에서 장기 기억이 전달되었을 때 얼마큼 그 기억을 살릴 것인지가 f g 게이트고요. 그리고 이 t 스텝에서 입력된 입력 변수를 얼마큼 사용할 것인지 인풋 게이트가 되겠고요. 그리고 그 다음 아웃풋 게이트는 얼마큼 출력할 건지에 대한 내용을 정보를 조절합니다. 그리고 요 셀 스테이트 부분은 계속해서 다음 셀로 전달되게 되는데요. 이제 이 부분이 장기 기억을 계속해서 다음 셀로 전달하는 부분이라고 볼 수 있습니다. 자 다음은 GRU 모델인데요. 사실 이 GRU 모델이 이 다음에 배울 추천 모델 gr 4 r에 사용되는 레이어입니다. 그래서 GRU는 방금 언급했던 LSTM의 변형 중의 하나로 LSTM과는 다르게 출력 게이트가 따로 없어서 LSTM보다 파라미터가 적고 연산량도 훨씬 적은 가벼운 모델입니다. 그래서 모델의 구성을 간단하게 보시면은 어 출력 게이트가 있다고 해서 게이트가 총 2개로 구성되어 있는데요. 리셋 게이트와 업데이트 게이트 이 부분이 리셋이고요. 이 부분이 업데이트가 됩니다. 리셋 같은 경우에는 아까 LSTM과 비슷하게 바로 전 셀에서 들어오는 정보를 얼마큼 버리고 얼마큼 기억할지에 대한 부분이고요. 업데이트는 이제 인풋과 그전에 정보를 활용해서 얼마큼 새로 들어오는 입력 정보와 과거 정보를 사용해서 업데이트할지에 대한 부분입니다. 자세한 설명을 다 하진 않겠지만 LSTM과 GRU는 성능 측면에서는 큰 차이가 없고 그래서 GRU가 훨씬 더 가볍기 때문에 어 지알도 많이 사용을 합니다. 그래서 여러분들이 수행하는 각각의 테스크에 맞게 적합한 모델을 선택하면 되는데요. 그렇다면 이러한 RNN 모델이 추천 시스템에 어떻게 적용될 수 있을까요? 이 추천 시스템의 시퀀스를 더한 문제가 바로 이 세션 베이스 레코멘데이션 문제입니다. 고객의 선호가 고정된 것이 아니라 시간에 따라서 혹은 지금 무엇을 계속 소비해 왔느냐에 따라서 달라지게 된다는 것인데요. 따라서 세션 베이스 레코멘데이션이 풀고자 하는 문제는 바로 지금 고객이 좋아할 만한 추천 아이템을 제공해 주는 것입니다. 참고로 유저가 서비스를 이용하는 동안의 행동 데이터를 묶어서 세션 이라고 말하는데요. 예를 들면 브라우저가 종료되기 전까지 그 브라우저 안에서 행동했던 데이터들을 쿠키 형태로 저장하는데 그것을 세션이라고 합니다. 그래서 아래의 그림처럼 사용자가 이런 아이템들을 차례대로 소비했을 때 하나의 세션 안에서 4개의 아이템을 소비했을 때 그다음에 아이템으로 무엇을 소비할지를 예측해서 그 아이템을 추천해 주는 문제가 바로 세션 베이스 레코맨데이 시스템의 문제입니다. 그러면 이 세션 베이스 추천 시스템 이 문제를 해결하기 위한 추천 모델인 GRU 4 LA에 대해 살펴봅시다. 아까 얘기했던 대로 이 모델은 GRU 레이어를 사용하여서 시퀀셜하게 지금 고객이 현재까지 어떤 아이템을 순차적으로 소비해 왔을 때 바로 그다음 아이템이 무엇인지를 추천해 주는 모델입니다. 이 GRF 렉의 모델 구조와 아이디어는 다음과 같습니다. 이 세션이라는 시퀀스 그 세션에서 아이템을 어떤 아이템을 소비해 왔는지라는 시퀀스를 GRU 레이어에 입력하여서 바로 다음에 올 아이템이 무엇인지 가장 확률이 높은 아이템이 무엇인지 를 분류하고 가장 확률이 높은 그 아이템을 추천해 주는 문제입니다. 네 모델 구조를 살펴보겠습니다. 먼저 입력 부분은 원 핫 인코딩 된 아이템으로 이루어진 세션입니다. 여기서 보시면은 인베딩 레이어가 들어가 있는데 점선으로 들어가 있음을 알 수 있습니다. 본 모델에서는 인베딩 레이어를 사용하지 않았을 때 성능이 더 높다고 했는데요. 사실 이 모델 이후에 RNN 계열의 추천 모델들에서는 인베딩 레이어를 사용한 논문도 많아 습니다. 따라서 이 인베딩 레이어를 사용했다 사용하지 않았다는 사실 이 GRU 4 r에서 그렇게 중요한 내용은 아닙니다. 이제 그 이후에 시퀀셜한 아이템들이 GRU 레이어를 통과하게 됩니다. 그래서 이 시퀀스 상에 어떤 아이템들의 순서와 맥락 그 컨텍스트 정보를 학습하게 됩니다. 그리고 이 마지막에는 이 피드 포워드 유뉴럴 네트워크를 통해 최종 마지막 골라질 아이템에 대한 스코어를 예측하게 됩니다. 이 부분 피드 포드 유뉴럴 네트워크 부분도 이 모델에서는 선택적으로 사용하였습니다. 그래서 모델의 구조는 굉장히 간단했고요. 이다음에 모델이 학습될 때 사용되었던 몇 가지 테크닉을 추가적으로 다루겠습니다. 먼저 세션 패럴러 즉 이 세션의 길이들이 각각의 사용자마다 혹은 어떤 데이터마다 긴 것도 존재하고 짧은 것도 존재하고 모두 길이가 다르다는 것입니다. 대부분은 짧지만 굉장히 아이템을 많이 소비한 길이가 긴 세션도 존재할 수 있다는 것이죠. 그래서 아래 그림과 같이 세션별로 그 안에 들어있는 아이템들은 모두 다릅니다. 그래서 이 길이가 짧은 세션을 그대로 학습에 사용했을 때는 학습이 비효율적으로 일어나게 됩니다. 그래서 보시면은 이 세션들 5개를 가지고 미니 배치 학습을 수행할 때 이 세션 4와 세션 5는 세션의 길이가 비교적 짧은 세션 1과 2에 다음과 같이 붙여서 미니 배치를 구성하고 있습니다. 그래서 하나의 세션을 그대로 사용하는 것이 아니라 세션의 길이가 좀 짧은 것들은 다른 세션에 뒤에 붙여서 병렬적으로 구성하여서 미니 배치 학습이 좀 더 효율적으로 이루어질 수 있도록 테크닉을 적용하였습니다. 두 번째는 학습 데이터를 샘플링하는 테크닉입니다. 현실에서는 아이템 수가 굉장히 많기 때문에 이 모든 후보의 아이템에 대한 확률을 다 계산할 수 없습니다. 그래서 아이템을 negative 샘플링을 해서 서셋만으로 로스를 계산해야 합니다. 그래서 아이템을 상호작용하는 것들은 우리가 가지고 있지만 그렇지 않은 그 유저가 그 아이템을 상호 작용하지 않았다는 데이터는 우리가 없죠. 그러면 그 상호작용하지 않은 아이템에 대한 평가는 어떻게 해야 될까요? 그 사용자가 존재를 아예 몰랐을 수도 있고 아니면 반대로 그 아이템을 확인했지만 클릭하지 않았을 수도 있죠. 이제 그러한 어려움이 있는데요. 이제 이 어려움을 극복하기 위해서 아이템의 인기도가 높았는데 상호 작용이 없었다면 인기도가 높을수록 아무래도 사용자가 관심이 없었을 거다라고 가정하고 인기도에 기반한 negative 샘플링을 제시합니다. 그래서 사용자가 인터랙션 하지 않은 아이템들은 굉장히 많고요. 인터랙션 아이템은 별로 없겠죠. 그럼 이것을 다 학습 데이터를 사용하는 것이 아니라 이 데이터 가운데 인기도가 높은 것들은 좀 많이 negative 샘플링으로 뽑고 인기도가 낮은 것은 거의 뽑지 않는 방식으로 이 positive 샘플과 negative 샘플 전체를 구성하게 됩니다. 네 그래서 GRU 4l의 최종 결과 및 요약 부분입니다. 주어진 데이터셋에 대해서 클래식한 추천 모델인 아이템 KNN 모델과 비교를 했는데요. 이 모델 대비 약 20% 정도의 높은 추천 성능 리콜과 엠알알을 보였습니다. 본 논문은 모델은 고정시키고 다양한 로스를 사용하기도 했고, 또 모델 안에서 쥐알유 레이어의 히든 유닛도 서로 다르게 실험을 하여서 비교를 했는데요. 전반적으로는 GRU 레이어의 헤드 유닛이 많아질수록 더 좋은 추천 성능을 보였다고 이야기하고 있습니다. 네 이상 7강 GNN과 RNN을 활용한 추천 모델 에 대한 강의를 모두 마쳤습니다. 모두 수고하셨습니다.","confidence":0.91953725,"speakers":[{"label":"","name":"","edited":false}],"events":[],"eventTypes":[]}