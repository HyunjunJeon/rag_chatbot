{"result":"SUCCEEDED","message":"Succeeded","token":"f1837c248a314f809ac17575e81673c2","version":"ncp_v2_v2.4.6-c00dd1b-20250528__v4.2.20.1_ko_firedepartment_20250923_","params":{"service":"ncp","domain":"general","lang":"ko","completion":"sync","callback":"","diarization":{"enable":false,"speakerCountMin":-1,"speakerCountMax":-1},"sed":{"enable":false},"boostings":[],"forbiddens":"","wordAlignment":true,"fullText":true,"noiseFiltering":true,"priority":0,"userdata":{"_ncp_DomainCode":"tpc-boostcamp","_ncp_DomainId":13807,"_ncp_TaskId":42975751,"_ncp_TraceId":"5ecf8af9cffe44c7855bb766ceb657fa"}},"progress":100,"keywords":{},"segments":[{"start":0,"end":13000,"text":"안녕하세요. 고려대학교 산업경영공학부 강필성 교수입니다. 세 번째 강의로서 텍스트 제너레이션의 두 번째 파트","confidence":0.9614,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[5710,6180,"안녕하세요."],[6370,6920,"고려대학교"],[7010,8000,"산업경영공학부"],[8090,8500,"강필성"],[8500,8940,"교수입니다."],[9290,9440,"세"],[9450,9760,"번째"],[9850,10360,"강의로서"],[11170,11560,"텍스트"],[11610,12200,"제너레이션의"],[12250,12400,"두"],[12400,12660,"번째"],[12730,13000,"파트"]],"textEdited":"안녕하세요. 고려대학교 산업경영공학부 강필성 교수입니다. 세 번째 강의로서 텍스트 제너레이션의 두 번째 파트"},{"start":13000,"end":25100,"text":"파라미터 에피션트 튜닝이라는 주제로 강의를 진행하도록 하겠습니다. 우선 파라미터 에피션트 튜닝이라는 타픽을 가지고 온 이유에 대해서 먼저 설명을 드려야 될 것 같습니다.","confidence":0.9264,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[13270,13820,"파라미터"],[14010,14560,"에피션트"],[14590,15220,"튜닝이라는"],[15570,15940,"주제로"],[16110,16440,"강의를"],[16570,17060,"진행하도록"],[17110,17600,"하겠습니다."],[18870,19080,"우선"],[19810,20260,"파라미터"],[20270,20740,"에피션트"],[20770,21360,"튜닝이라는"],[21590,22080,"타픽을"],[22230,22600,"가지고"],[22600,22740,"온"],[22910,23260,"이유에"],[23260,23560,"대해서"],[23590,23800,"먼저"],[23870,24180,"설명을"],[24180,24387,"드려야"],[24387,24520,"될"],[24530,24680,"것"],[24680,25100,"같습니다."]],"textEdited":"파라미터 에피션트 튜닝이라는 주제로 강의를 진행하도록 하겠습니다. 우선 파라미터 에피션트 튜닝이라는 타픽을 가지고 온 이유에 대해서 먼저 설명을 드려야 될 것 같습니다."},{"start":25100,"end":35700,"text":"LLM의 발전과 파라미터 에피전트 파인 튜닝의 방법론의 중요성을 설명을 드리고요. 이 PFT 파라미터 에피전트 파인 튜닝을 하는 4가지의 방법,","confidence":0.9328,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[25330,25840,"LLM의"],[25930,26400,"발전과"],[26890,27340,"파라미터"],[27390,27900,"에피전트"],[27950,28220,"파인"],[28270,28740,"튜닝의"],[28890,29400,"방법론의"],[29510,30040,"중요성을"],[30150,30460,"설명을"],[30460,30860,"드리고요."],[31270,31420,"이"],[31790,32480,"PFT"],[32730,33140,"파라미터"],[33170,33620,"에피전트"],[33650,33920,"파인"],[33930,34280,"튜닝을"],[34310,34500,"하는"],[34810,35300,"4가지의"],[35390,35660,"방법,"]],"textEdited":"LLM의 발전과 파라미터 에피전트 파인 튜닝의 방법론의 중요성을 설명을 드리고요. 이 PFT 파라미터 에피전트 파인 튜닝을 하는 4가지의 방법,"},{"start":35700,"end":45300,"text":"첫 번째 어댑터 두 번째 프리픽스 튜닝, 세 번째 프롬프트 튜닝, 네 번째 로우 랭크 어댑테이션까지 설명을 드리고 마무리하겠습니다.","confidence":0.9379,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[35970,36120,"첫"],[36170,36480,"번째"],[36610,37100,"어댑터"],[37470,37620,"두"],[37620,37880,"번째"],[38030,38580,"프리픽스"],[38580,38860,"튜닝,"],[39090,39240,"세"],[39240,39520,"번째"],[39750,40220,"프롬프트"],[40230,40520,"튜닝,"],[41050,41200,"네"],[41210,41520,"번째"],[41690,41940,"로우"],[42010,42260,"랭크"],[42260,43200,"어댑테이션까지"],[43530,43880,"설명을"],[43880,44200,"드리고"],[44350,45260,"마무리하겠습니다."]],"textEdited":"첫 번째 어댑터 두 번째 프리픽스 튜닝, 세 번째 프롬프트 튜닝, 네 번째 로우 랭크 어댑테이션까지 설명을 드리고 마무리하겠습니다."},{"start":45300,"end":57500,"text":"LLM의 발전과 파라미터 에피션 파인튜닝 방법론의 중요성을 먼저 말씀을 드리겠습니다. LLM은 트랜스포머 구조 기반의 언어 모델들이","confidence":0.7609,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[46190,46680,"LLM의"],[46890,47540,"발전과"],[47990,48500,"파라미터"],[48570,48920,"에피션"],[49190,49820,"파인튜닝"],[49950,50520,"방법론의"],[50690,51320,"중요성을"],[51390,51660,"먼저"],[51750,52120,"말씀을"],[52130,52740,"드리겠습니다."],[53850,54280,"LLM은"],[54650,55300,"트랜스포머"],[55410,55720,"구조"],[55750,56160,"기반의"],[56290,56540,"언어"],[56610,57180,"모델들이"]],"textEdited":"LLM의 발전과 파라미터 에피션 파인튜닝 방법론의 중요성을 먼저 말씀을 드리겠습니다. LLM은 트랜스포머 구조 기반의 언어 모델들이"},{"start":57500,"end":72400,"text":"다양한 NLP 테스크에서 훌륭한 성능 향상을 보인 이후로 제너럴 퍼포즈를 위한 라즈 랭귀지 모델들이 제한되어 왔습니다. 이를 통해서 학습 데이터와 모델의 파라미터 사이즈가 커질수록","confidence":0.8413,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[57790,58200,"다양한"],[58410,58840,"NLP"],[58930,59460,"테스크에서"],[59790,60120,"훌륭한"],[60370,60740,"성능"],[60870,61240,"향상을"],[61290,61520,"보인"],[61670,62040,"이후로"],[62790,63180,"제너럴"],[63310,64020,"퍼포즈를"],[64030,64260,"위한"],[64570,64920,"라즈"],[64950,65280,"랭귀지"],[65330,65800,"모델들이"],[65850,66300,"제한되어"],[66310,66760,"왔습니다."],[67790,68060,"이를"],[68130,68440,"통해서"],[68830,69120,"학습"],[69170,69640,"데이터와"],[69950,70400,"모델의"],[70530,71040,"파라미터"],[71110,71560,"사이즈가"],[71670,72180,"커질수록"]],"textEdited":"다양한 NLP 테스크에서 훌륭한 성능 향상을 보인 이후로 제너럴 퍼포즈를 위한 라즈 랭귀지 모델들이 제한되어 왔습니다. 이를 통해서 학습 데이터와 모델의 파라미터 사이즈가 커질수록"},{"start":72400,"end":83600,"text":"성능이 점점 더 높아지고 왔다라는 것을 알 수가 있습니다. 그런데 이러한 언어 모델을 활용하기 위해서는 일반적으로 범용 웹 데이터를 기반으로 해서","confidence":0.9868,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[72710,73120,"성능이"],[73230,73600,"점점"],[73650,73800,"더"],[73910,74640,"높아지고"],[74730,75260,"왔다라는"],[75260,75560,"것을"],[75730,75880,"알"],[75950,76160,"수가"],[76250,76680,"있습니다."],[77370,77700,"그런데"],[77850,78120,"이러한"],[78290,78540,"언어"],[78570,78920,"모델을"],[78970,79420,"활용하기"],[79430,79940,"위해서는"],[80170,80840,"일반적으로"],[81090,81480,"범용"],[81630,81780,"웹"],[81930,82380,"데이터를"],[82390,82840,"기반으로"],[82850,83100,"해서"]],"textEdited":"성능이 점점 더 높아지고 왔다라는 것을 알 수가 있습니다. 그런데 이러한 언어 모델을 활용하기 위해서는 일반적으로 범용 웹 데이터를 기반으로 해서"},{"start":83600,"end":97800,"text":"사전 학습을 수행하고 그 이후에 다운스트림 테스크에 맞추어서 파인튜닝을 진행해 왔습니다. 이러한 학습 방법은 특정한 테스크의 성능 향상을 보장한다라는 장점은 있습니다.","confidence":0.9389,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[83870,84160,"사전"],[84250,84640,"학습을"],[84730,85240,"수행하고"],[85770,85920,"그"],[86230,86560,"이후에"],[87090,87760,"다운스트림"],[87850,88340,"테스크에"],[88370,88920,"맞추어서"],[89110,89840,"파인튜닝을"],[89970,90320,"진행해"],[90320,90800,"왔습니다."],[91570,91860,"이러한"],[91970,92220,"학습"],[92290,92660,"방법은"],[93110,93480,"특정한"],[93630,94220,"테스크의"],[94530,94820,"성능"],[94930,95320,"향상을"],[95430,96220,"보장한다라는"],[96430,96840,"장점은"],[96890,97440,"있습니다."]],"textEdited":"사전 학습을 수행하고 그 이후에 다운스트림 테스크에 맞추어서 파인튜닝을 진행해 왔습니다. 이러한 학습 방법은 특정한 테스크의 성능 향상을 보장한다라는 장점은 있습니다."},{"start":97800,"end":109700,"text":"그렇기 때문에 언어 모델을 타겟 테스크에 맞게 어댑팅하거나 파인 튜닝 하는 방법론들이 우수한 성능을 기록해 왔습니다. 그래서 이 전통적인 방식에서는","confidence":0.9463,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[98450,98707,"그렇기"],[98707,99060,"때문에"],[99310,99580,"언어"],[99590,99960,"모델을"],[100250,100560,"타겟"],[100730,101260,"테스크에"],[101270,101580,"맞게"],[101890,102800,"어댑팅하거나"],[103010,103300,"파인"],[103350,103680,"튜닝"],[103710,103920,"하는"],[104050,104680,"방법론들이"],[104850,105180,"우수한"],[105290,105620,"성능을"],[105650,105960,"기록해"],[105960,106400,"왔습니다."],[107030,107300,"그래서"],[107670,107820,"이"],[108050,108640,"전통적인"],[108790,109420,"방식에서는"]],"textEdited":"그렇기 때문에 언어 모델을 타겟 테스크에 맞게 어댑팅하거나 파인 튜닝 하는 방법론들이 우수한 성능을 기록해 왔습니다. 그래서 이 전통적인 방식에서는"},{"start":109700,"end":123400,"text":"세 가지의 방법으로 유형화할 수가 있는데요. 퓨처 베이스 어프로치 같은 경우에는 사전 학습 모델로부터 임베딩을 먼저 추출을 하고 이 추출된 인베딩으로부터 우리가 원하는 어떠한","confidence":0.903,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[110170,110320,"세"],[110350,110720,"가지의"],[110790,111200,"방법으로"],[111210,111920,"유형화할"],[111920,112120,"수가"],[112120,112540,"있는데요."],[113330,113760,"퓨처"],[113870,114180,"베이스"],[114230,114640,"어프로치"],[114640,114854,"같은"],[114854,115260,"경우에는"],[115430,115720,"사전"],[115770,116040,"학습"],[116050,116740,"모델로부터"],[117070,117640,"임베딩을"],[117710,117940,"먼저"],[118070,118440,"추출을"],[118440,118660,"하고"],[119390,119540,"이"],[119770,120200,"추출된"],[120390,121340,"인베딩으로부터"],[121470,121780,"우리가"],[122030,122480,"원하는"],[122650,123020,"어떠한"]],"textEdited":"세 가지의 방법으로 유형화할 수가 있는데요. 퓨처 베이스 어프로치 같은 경우에는 사전 학습 모델로부터 임베딩을 먼저 추출을 하고 이 추출된 인베딩으로부터 우리가 원하는 어떠한"},{"start":123400,"end":134500,"text":"테스크를 수행하기 위한 클래식 파이어를 학습하는 방법이 있습니다. 두 번째는 파인 튜닝의 첫 번째 단계로서 사전 학습된 모형에서부터","confidence":0.9673,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[123630,124100,"테스크를"],[124110,124520,"수행하기"],[124520,124720,"위한"],[124870,125220,"클래식"],[125250,125660,"파이어를"],[125690,126100,"학습하는"],[126170,126487,"방법이"],[126487,126880,"있습니다."],[127870,128020,"두"],[128030,128460,"번째는"],[129130,129420,"파인"],[129490,129900,"튜닝의"],[130010,130160,"첫"],[130190,130460,"번째"],[130510,131060,"단계로서"],[131510,131820,"사전"],[132010,132640,"학습된"],[132950,134060,"모형에서부터"]],"textEdited":"테스크를 수행하기 위한 클래식 파이어를 학습하는 방법이 있습니다. 두 번째는 파인 튜닝의 첫 번째 단계로서 사전 학습된 모형에서부터"},{"start":134500,"end":147800,"text":"가장 마지막 단계에 해당하는 아웃풋 레이어를 업데이트하는 방법이 있고요. 세 번째인 두 번째 파인튜닝 단계에서는 모든 레이어들을 업데이트하는 방식이 있습니다. 그래서 보시면","confidence":0.9608,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[134750,135000,"가장"],[135310,135720,"마지막"],[136130,136580,"단계에"],[136990,137420,"해당하는"],[137650,138060,"아웃풋"],[138250,138780,"레이어를"],[138950,139600,"업데이트하는"],[139690,140027,"방법이"],[140027,140360,"있고요."],[141390,141540,"세"],[141550,141960,"번째인"],[142130,142280,"두"],[142310,142660,"번째"],[142790,143280,"파인튜닝"],[143330,143860,"단계에서는"],[144130,144420,"모든"],[144650,145220,"레이어들을"],[145270,145800,"업데이트하는"],[145870,146167,"방식이"],[146167,146500,"있습니다."],[146690,146900,"그래서"],[146930,147280,"보시면"]],"textEdited":"가장 마지막 단계에 해당하는 아웃풋 레이어를 업데이트하는 방법이 있고요. 세 번째인 두 번째 파인튜닝 단계에서는 모든 레이어들을 업데이트하는 방식이 있습니다. 그래서 보시면"},{"start":147800,"end":160300,"text":"프로즌이라고 되어 있는 거는 사전 학습을 통해서 학습된 웨이트들 파라미터들을 고정을 시키는데 여기에 있는 파인 튜닝의 첫 번째 단계는 후반부를 업데이트를 시킨다면","confidence":0.9652,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[148130,148900,"프로즌이라고"],[148900,149080,"되어"],[149080,149240,"있는"],[149240,149500,"거는"],[149730,150000,"사전"],[150030,150360,"학습을"],[150410,150860,"통해서"],[151730,152120,"학습된"],[152510,153040,"웨이트들"],[153310,153940,"파라미터들을"],[154010,154380,"고정을"],[154390,154880,"시키는데"],[155310,155627,"여기에"],[155627,155820,"있는"],[155990,156240,"파인"],[156250,156600,"튜닝의"],[156670,156820,"첫"],[156830,157120,"번째"],[157170,157540,"단계는"],[158050,158700,"후반부를"],[158910,159420,"업데이트를"],[159420,159940,"시킨다면"]],"textEdited":"프로즌이라고 되어 있는 거는 사전 학습을 통해서 학습된 웨이트들 파라미터들을 고정을 시키는데 여기에 있는 파인 튜닝의 첫 번째 단계는 후반부를 업데이트를 시킨다면"},{"start":160300,"end":173800,"text":"파인 튜닝의 두 번째 단계는 모든 레이어에 대해서 업데이트를 수행하게 됩니다. 이러한 방법을 성능과 학습 효율이라는 관점으로 나누어 볼 때 모든 파라미터를 학습하는 경우가","confidence":0.9727,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[160550,160820,"파인"],[160820,161180,"튜닝의"],[161230,161380,"두"],[161390,161680,"번째"],[161690,162040,"단계는"],[162310,162620,"모든"],[162790,163200,"레이어에"],[163200,163560,"대해서"],[163750,164300,"업데이트를"],[164330,164720,"수행하게"],[164720,165020,"됩니다."],[166170,166480,"이러한"],[166610,166960,"방법을"],[167310,167920,"성능과"],[168250,168560,"학습"],[168690,169280,"효율이라는"],[169430,169940,"관점으로"],[170010,170380,"나누어"],[170410,170560,"볼"],[170610,170760,"때"],[171050,171320,"모든"],[171790,172380,"파라미터를"],[172470,173020,"학습하는"],[173070,173480,"경우가"]],"textEdited":"파인 튜닝의 두 번째 단계는 모든 레이어에 대해서 업데이트를 수행하게 됩니다. 이러한 방법을 성능과 학습 효율이라는 관점으로 나누어 볼 때 모든 파라미터를 학습하는 경우가"},{"start":173800,"end":186300,"text":"가장 높은 성능을 기록하는 것을 확인할 수가 있습니다. 그래서 성능 자체는 가장 높지만 트레이닝 에피션시가 왼쪽이 빠른 거고 오른쪽이 느린 거니까 당연히 모든","confidence":0.9642,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[174050,174300,"가장"],[174530,174900,"높은"],[175190,175580,"성능을"],[175670,176140,"기록하는"],[176140,176420,"것을"],[177110,177460,"확인할"],[177460,177627,"수가"],[177627,177980,"있습니다."],[178230,178500,"그래서"],[179130,179440,"성능"],[179570,180100,"자체는"],[180490,180720,"가장"],[180850,181280,"높지만"],[181950,182340,"트레이닝"],[182350,182920,"에피션시가"],[183210,183620,"왼쪽이"],[183670,183940,"빠른"],[183940,184160,"거고"],[184170,184520,"오른쪽이"],[184520,184687,"느린"],[184687,185000,"거니까"],[185170,185540,"당연히"],[185690,185980,"모든"]],"textEdited":"가장 높은 성능을 기록하는 것을 확인할 수가 있습니다. 그래서 성능 자체는 가장 높지만 트레이닝 에피션시가 왼쪽이 빠른 거고 오른쪽이 느린 거니까 당연히 모든"},{"start":186300,"end":200500,"text":"이 파라미터들을 학습할 때가 속도는 가장 느리겠죠. 이러한 이제 단점을 이제 극복하기 위해서 인 컨텍스트 러닝 icl이라는 개념이 이제 나왔습니다. gpt3가 발표된 이후에는","confidence":0.8476,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[186550,186700,"이"],[186830,187480,"파라미터들을"],[187550,187920,"학습할"],[187920,188160,"때가"],[188830,189240,"속도는"],[189270,189500,"가장"],[189550,190080,"느리겠죠."],[190990,191300,"이러한"],[191390,191560,"이제"],[191850,192620,"단점을"],[192710,192880,"이제"],[193030,193580,"극복하기"],[193610,193980,"위해서"],[194210,194360,"인"],[194630,195180,"컨텍스트"],[195270,195540,"러닝"],[195650,196600,"icl이라는"],[196650,197000,"개념이"],[197000,197160,"이제"],[197210,197800,"나왔습니다."],[198430,199340,"gpt3가"],[199450,199820,"발표된"],[199930,200380,"이후에는"]],"textEdited":"이 파라미터들을 학습할 때가 속도는 가장 느리겠죠. 이러한 이제 단점을 이제 극복하기 위해서 인 컨텍스트 러닝 icl이라는 개념이 이제 나왔습니다. gpt3가 발표된 이후에는"},{"start":200500,"end":210900,"text":"파인 튜닝이 없어도 언어 모델을 쉽게 활용할 수 있게 되었고요. 타겟 테스크에 대해서 몇 가지의 예시를 주어 주고 모델에 입력해 주게 될 경우에는","confidence":0.9595,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[200710,201000,"파인"],[201030,201420,"튜닝이"],[201550,201980,"없어도"],[202230,202460,"언어"],[202490,202840,"모델을"],[202870,203140,"쉽게"],[203190,203540,"활용할"],[203540,203647,"수"],[203647,203840,"있게"],[203840,204300,"되었고요."],[205130,205400,"타겟"],[205470,205927,"테스크에"],[205927,206220,"대해서"],[206410,206560,"몇"],[206650,207020,"가지의"],[207130,207600,"예시를"],[207610,207860,"주어"],[207870,208160,"주고"],[208530,208920,"모델에"],[209130,209600,"입력해"],[209610,209860,"주게"],[209860,209980,"될"],[210090,210540,"경우에는"]],"textEdited":"파인 튜닝이 없어도 언어 모델을 쉽게 활용할 수 있게 되었고요. 타겟 테스크에 대해서 몇 가지의 예시를 주어 주고 모델에 입력해 주게 될 경우에는"},{"start":210900,"end":224500,"text":"모델을 튜닝하지 않고 쉽게 문제를 풀 수 있게 되었다라는 뜻입니다. 이전 강의에서도 잠깐 언급했었던 제로 샷 같은 경우에는 태스크만 주는 것이고요. 원 샷은 이그젬플 하나 주는 것,","confidence":0.9278,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[211170,211620,"모델을"],[211730,212300,"튜닝하지"],[212330,212700,"않고"],[212910,213280,"쉽게"],[213410,213840,"문제를"],[213930,214080,"풀"],[214170,214320,"수"],[214320,214500,"있게"],[214500,215000,"되었다라는"],[215000,215440,"뜻입니다."],[216610,216860,"이전"],[217030,217560,"강의에서도"],[217630,217980,"잠깐"],[218130,218800,"언급했었던"],[218930,219200,"제로"],[219230,219380,"샷"],[219590,219880,"같은"],[219880,220260,"경우에는"],[220650,221180,"태스크만"],[221190,221420,"주는"],[221420,221840,"것이고요."],[222350,222500,"원"],[222690,223040,"샷은"],[223150,223600,"이그젬플"],[223690,223920,"하나"],[223920,224160,"주는"],[224160,224280,"것,"]],"textEdited":"모델을 튜닝하지 않고 쉽게 문제를 풀 수 있게 되었다라는 뜻입니다. 이전 강의에서도 잠깐 언급했었던 제로 샷 같은 경우에는 태스크만 주는 것이고요. 원 샷은 이그젬플 하나 주는 것,"},{"start":224500,"end":236500,"text":"퓨 샷은 여러 개 주는 것을 의미합니다. 이러한 이제 컨벤셔널 어프로치들은 모델을 지속적으로 추가 학습하는 과정 여기서는 시퀀셜 트랜스퍼 러닝 패러다임이라고 표현하는데요.","confidence":0.9452,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[224750,224900,"퓨"],[225070,225400,"샷은"],[225530,225800,"여러"],[225800,225920,"개"],[226010,226260,"주는"],[226260,226540,"것을"],[226630,227180,"의미합니다."],[228170,228480,"이러한"],[228570,228740,"이제"],[228850,229380,"컨벤셔널"],[229430,230120,"어프로치들은"],[230350,230800,"모델을"],[230930,231500,"지속적으로"],[231610,231860,"추가"],[231910,232340,"학습하는"],[232430,232720,"과정"],[232850,233220,"여기서는"],[233410,233940,"시퀀셜"],[234070,234500,"트랜스퍼"],[234530,234800,"러닝"],[234890,235560,"패러다임이라고"],[235570,236300,"표현하는데요."]],"textEdited":"퓨 샷은 여러 개 주는 것을 의미합니다. 이러한 이제 컨벤셔널 어프로치들은 모델을 지속적으로 추가 학습하는 과정 여기서는 시퀀셜 트랜스퍼 러닝 패러다임이라고 표현하는데요."},{"start":236500,"end":244600,"text":"언어 모델이 기존에 학습한 정보를 있는 현상 캐타스트로픽 포게팅이라는 표현을 쓰기도 합니다. 야기합니다.","confidence":0.8657,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[236750,237020,"언어"],[237050,237460,"모델이"],[237810,238220,"기존에"],[238350,238740,"학습한"],[238910,239280,"정보를"],[239430,239700,"있는"],[239930,240220,"현상"],[240790,241560,"캐타스트로픽"],[241690,242400,"포게팅이라는"],[242450,242820,"표현을"],[242870,243160,"쓰기도"],[243160,243400,"합니다."],[243570,244360,"야기합니다."]],"textEdited":"언어 모델이 기존에 학습한 정보를 있는 현상 캐타스트로픽 포게팅이라는 표현을 쓰기도 합니다. 야기합니다."},{"start":244600,"end":253100,"text":"또한 모델이 모든 파라미터를 새로운 데이터에 대해서 항상 학습하는 것이 정답은 아니라는 것도 밝혀져 있습니다.","confidence":0.9964,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[244850,245080,"또한"],[245590,246020,"모델이"],[246170,246440,"모든"],[246570,247200,"파라미터를"],[247290,247580,"새로운"],[247650,248047,"데이터에"],[248047,248360,"대해서"],[248590,248880,"항상"],[249050,249540,"학습하는"],[249540,249840,"것이"],[250050,250520,"정답은"],[250610,251120,"아니라는"],[251130,251520,"것도"],[251930,252380,"밝혀져"],[252410,252940,"있습니다."]],"textEdited":"또한 모델이 모든 파라미터를 새로운 데이터에 대해서 항상 학습하는 것이 정답은 아니라는 것도 밝혀져 있습니다."},{"start":253100,"end":267300,"text":"그리고 컨벤셔널 어프로치는 모델의 크기가 점점 커짐에 따라서 모델의 전체 파라미터를 학습하는 것이 어려워지고요. 다운 스트림 테스크마다 독립적으로 학습된 모델을 저장하고 배포할 때마다","confidence":0.9764,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[254690,254980,"그리고"],[255190,255700,"컨벤셔널"],[255700,256240,"어프로치는"],[256570,256980,"모델의"],[257050,257360,"크기가"],[257430,257740,"점점"],[257830,258220,"커짐에"],[258220,258580,"따라서"],[258750,259160,"모델의"],[259290,259640,"전체"],[259770,260400,"파라미터를"],[260430,260880,"학습하는"],[260880,261120,"것이"],[261130,261780,"어려워지고요."],[262690,262960,"다운"],[263050,263340,"스트림"],[263410,263960,"테스크마다"],[264150,264800,"독립적으로"],[264890,265240,"학습된"],[265290,265640,"모델을"],[265650,266160,"저장하고"],[266250,266700,"배포할"],[266700,267040,"때마다"]],"textEdited":"그리고 컨벤셔널 어프로치는 모델의 크기가 점점 커짐에 따라서 모델의 전체 파라미터를 학습하는 것이 어려워지고요. 다운 스트림 테스크마다 독립적으로 학습된 모델을 저장하고 배포할 때마다"},{"start":267300,"end":275600,"text":"막대한 시간과 컴퓨팅 자원이 필요해진다라는 문제점이 있습니다. 그래서 아까 2절에 설명드렸던","confidence":0.98,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[267650,268060,"막대한"],[268190,268620,"시간과"],[268750,269220,"컴퓨팅"],[269310,269720,"자원이"],[269790,270540,"필요해진다라는"],[270650,271180,"문제점이"],[271180,271560,"있습니다."],[272270,272560,"그래서"],[273130,273360,"아까"],[274050,274400,"2절에"],[274550,275160,"설명드렸던"]],"textEdited":"막대한 시간과 컴퓨팅 자원이 필요해진다라는 문제점이 있습니다. 그래서 아까 2절에 설명드렸던"},{"start":275600,"end":283600,"text":"신뢰성 측면에서 봤을 때 인 컨텍스트 러닝은 모델을 따로 튜닝할 필요가 없이 문제를 효과적으로 풀이할 수 있고요.","confidence":0.9834,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[276070,276540,"신뢰성"],[276670,277060,"측면에서"],[277060,277380,"봤을"],[277390,277540,"때"],[277730,277880,"인"],[278110,278660,"컨텍스트"],[278710,279100,"러닝은"],[279390,279860,"모델을"],[279950,280220,"따로"],[280330,280760,"튜닝할"],[280830,281120,"필요가"],[281150,281400,"없이"],[281610,282040,"문제를"],[282150,282700,"효과적으로"],[282790,283140,"풀이할"],[283140,283234,"수"],[283234,283600,"있고요."]],"textEdited":"신뢰성 측면에서 봤을 때 인 컨텍스트 러닝은 모델을 따로 튜닝할 필요가 없이 문제를 효과적으로 풀이할 수 있고요."},{"start":283600,"end":294100,"text":"인컨텍스트 러닝을 위해서 단순히 데모스트레이션과 테스트 인풋만을 모델의 입력으로 사용을 하게 되면 가능해진다라는 장점이 있습니다.","confidence":0.974,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[284410,285140,"인컨텍스트"],[285210,285560,"러닝을"],[285560,285860,"위해서"],[286150,286540,"단순히"],[286650,287720,"데모스트레이션과"],[287950,288380,"테스트"],[288510,289140,"인풋만을"],[289330,289780,"모델의"],[289950,290380,"입력으로"],[290430,290720,"사용을"],[290720,290940,"하게"],[290940,291200,"되면"],[291770,292680,"가능해진다라는"],[292910,293380,"장점이"],[293380,293860,"있습니다."]],"textEdited":"인컨텍스트 러닝을 위해서 단순히 데모스트레이션과 테스트 인풋만을 모델의 입력으로 사용을 하게 되면 가능해진다라는 장점이 있습니다."},{"start":294100,"end":306500,"text":"하지만 몇몇 경우에는 랜덤한 레이블을 넣어주더라도 문제를 잘 해결한다라는 연구 결과가 존재를 합니다. 이 말이 무슨 얘기냐 하면 데모스트레이션에 대해서","confidence":0.9246,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[294770,295080,"하지만"],[295490,295920,"몇몇"],[296070,296540,"경우에는"],[296790,297220,"랜덤한"],[297410,297900,"레이블을"],[297970,298880,"넣어주더라도"],[299390,299840,"문제를"],[299890,300040,"잘"],[300170,300940,"해결한다라는"],[301250,301680,"연구"],[301790,302220,"결과가"],[302290,302640,"존재를"],[302640,302900,"합니다."],[303010,303160,"이"],[303160,303360,"말이"],[303360,303540,"무슨"],[303550,303860,"얘기냐"],[303860,304060,"하면"],[305090,305927,"데모스트레이션에"],[305927,306280,"대해서"]],"textEdited":"하지만 몇몇 경우에는 랜덤한 레이블을 넣어주더라도 문제를 잘 해결한다라는 연구 결과가 존재를 합니다. 이 말이 무슨 얘기냐 하면 데모스트레이션에 대해서"},{"start":306500,"end":319200,"text":"단순히 어떤 긍정과 부정에 대해서 긍정과 부정도 아닌 이상한 레이블을 넣었음에도 불구하고 이 테스트 인풋을 주어졌을 때 정답을 맞출 수 있다라는 얘기입니다.","confidence":0.9443,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[306830,307280,"단순히"],[307510,307820,"어떤"],[308430,308920,"긍정과"],[309010,309420,"부정에"],[309420,309840,"대해서"],[310370,310820,"긍정과"],[310870,311194,"부정도"],[311194,311340,"아닌"],[311550,311880,"이상한"],[311990,312480,"레이블을"],[312490,313120,"넣었음에도"],[313170,313740,"불구하고"],[314450,314600,"이"],[314850,315260,"테스트"],[315350,315800,"인풋을"],[315810,316320,"주어졌을"],[316350,316500,"때"],[316690,317200,"정답을"],[317270,317620,"맞출"],[317620,317694,"수"],[317694,317967,"있다라는"],[317967,318360,"얘기입니다."]],"textEdited":"단순히 어떤 긍정과 부정에 대해서 긍정과 부정도 아닌 이상한 레이블을 넣었음에도 불구하고 이 테스트 인풋을 주어졌을 때 정답을 맞출 수 있다라는 얘기입니다."},{"start":319200,"end":331100,"text":"바꿔 말하면 수학 문제와 정답 쌍을 주고 공부를 시키는데 문제에 대한 전혀 엉뚱한 답을 주었음에도 정답을 풀어낼 수 있다라는 이러한 연구 결과가 존재한다는 뜻이죠.","confidence":0.9825,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[319590,319814,"바꿔"],[319814,320120,"말하면"],[320310,320580,"수학"],[320690,321100,"문제와"],[321310,321640,"정답"],[321830,322220,"쌍을"],[322290,322600,"주고"],[322730,323080,"공부를"],[323090,323580,"시키는데"],[324230,324760,"문제에"],[324790,325020,"대한"],[325210,325500,"전혀"],[325710,326040,"엉뚱한"],[326070,326360,"답을"],[326370,326960,"주었음에도"],[327110,327540,"정답을"],[327670,328040,"풀어낼"],[328150,328300,"수"],[328350,328840,"있다라는"],[329090,329320,"이러한"],[329450,329700,"연구"],[329710,330100,"결과가"],[330130,330620,"존재한다는"],[330630,330980,"뜻이죠."]],"textEdited":"바꿔 말하면 수학 문제와 정답 쌍을 주고 공부를 시키는데 문제에 대한 전혀 엉뚱한 답을 주었음에도 정답을 풀어낼 수 있다라는 이러한 연구 결과가 존재한다는 뜻이죠."},{"start":331100,"end":341900,"text":"랜덤 레이블과 골드 레이블이라는 거는 실제로 골드 레이블은 정답을 준 것이고요. 데모가 없다라는 거는 데모스트레이션이 존재하지 않는다라는 뜻이고,","confidence":0.9456,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[331690,332060,"랜덤"],[332230,332840,"레이블과"],[333170,333580,"골드"],[333610,334240,"레이블이라는"],[334240,334480,"거는"],[335230,335580,"실제로"],[335630,335920,"골드"],[335920,336280,"레이블은"],[336390,336780,"정답을"],[336780,336920,"준"],[336920,337360,"것이고요."],[338290,338680,"데모가"],[338680,339087,"없다라는"],[339087,339320,"거는"],[339350,340160,"데모스트레이션이"],[340210,340640,"존재하지"],[340640,341100,"않는다라는"],[341100,341460,"뜻이고,"]],"textEdited":"랜덤 레이블과 골드 레이블이라는 거는 실제로 골드 레이블은 정답을 준 것이고요. 데모가 없다라는 거는 데모스트레이션이 존재하지 않는다라는 뜻이고,"},{"start":341900,"end":356200,"text":"랜덤 레이블은 정답이 아닌 레이블을 주었음에도 불구하고 디모스트레이션이 없었을 때보다 훨씬 더 잘하는 것을 보실 수가 있겠습니다. 파라미터가 많은 라즐 랭귀지 모델을 효율적으로 학습할","confidence":0.9502,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[342170,342480,"랜덤"],[342510,342900,"레이블은"],[343110,343540,"정답이"],[343550,343740,"아닌"],[343870,344260,"레이블을"],[344270,344860,"주었음에도"],[344870,345380,"불구하고"],[345570,346440,"디모스트레이션이"],[346590,347040,"없었을"],[347050,347420,"때보다"],[347910,348260,"훨씬"],[348290,348440,"더"],[348590,349080,"잘하는"],[349190,349500,"것을"],[349710,350000,"보실"],[350010,350240,"수가"],[350240,350760,"있겠습니다."],[351730,352420,"파라미터가"],[352890,353160,"많은"],[353570,353880,"라즐"],[353930,354240,"랭귀지"],[354270,354640,"모델을"],[354850,355580,"효율적으로"],[355730,356160,"학습할"]],"textEdited":"랜덤 레이블은 정답이 아닌 레이블을 주었음에도 불구하고 디모스트레이션이 없었을 때보다 훨씬 더 잘하는 것을 보실 수가 있겠습니다. 파라미터가 많은 라즐 랭귀지 모델을 효율적으로 학습할"},{"start":356200,"end":368900,"text":"할 수 있는 방법이 없을까라는 질문이 등장을 하는 거죠. 여기에 대한 답이 바로 파라미터 에피션트 파임 튜닝입니다. 그래서 단어를 하나씩 하나씩 살펴보자면 무엇을 하는 것이냐","confidence":0.8764,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[356210,356360,"할"],[356450,356600,"수"],[356730,356960,"있는"],[357170,357580,"방법이"],[357690,358500,"없을까라는"],[358770,359160,"질문이"],[359190,359500,"등장을"],[359500,359667,"하는"],[359667,359920,"거죠."],[360370,360667,"여기에"],[360667,360840,"대한"],[360910,361180,"답이"],[361180,361420,"바로"],[361990,362600,"파라미터"],[362890,363460,"에피션트"],[363610,363920,"파임"],[364010,364540,"튜닝입니다."],[364830,365020,"그래서"],[365210,365560,"단어를"],[365570,365900,"하나씩"],[365900,366200,"하나씩"],[366390,367040,"살펴보자면"],[367570,367980,"무엇을"],[367980,368180,"하는"],[368180,368500,"것이냐"]],"textEdited":"할 수 있는 방법이 없을까라는 질문이 등장을 하는 거죠. 여기에 대한 답이 바로 파라미터 에피션트 파임 튜닝입니다. 그래서 단어를 하나씩 하나씩 살펴보자면 무엇을 하는 것이냐"},{"start":368900,"end":383400,"text":"언어 모델을 파인 튜닝 하겠다. 미세 조정을 더 하겠다라는 것이고요. 근데 어떻게 할 것이냐 파라미터의 에피션트한 막 하는 게 아니라 최대한 효율적으로 하겠다라는 뜻입니다. 그래서 지금부터는","confidence":0.927,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[369150,369400,"언어"],[369430,369800,"모델을"],[370050,370360,"파인"],[370490,370800,"튜닝"],[370800,371060,"하겠다."],[371270,371620,"미세"],[371670,372080,"조정을"],[372210,372360,"더"],[372490,373340,"하겠다라는"],[373340,373800,"것이고요."],[374450,374760,"근데"],[374890,375200,"어떻게"],[375200,375340,"할"],[375340,375660,"것이냐"],[376290,377020,"파라미터의"],[377290,377960,"에피션트한"],[378470,378620,"막"],[378830,379040,"하는"],[379040,379127,"게"],[379127,379420,"아니라"],[379850,380240,"최대한"],[380470,381180,"효율적으로"],[381210,381820,"하겠다라는"],[381820,382220,"뜻입니다."],[382390,382580,"그래서"],[382650,383220,"지금부터는"]],"textEdited":"언어 모델을 파인 튜닝 하겠다. 미세 조정을 더 하겠다라는 것이고요. 근데 어떻게 할 것이냐 파라미터의 에피션트한 막 하는 게 아니라 최대한 효율적으로 하겠다라는 뜻입니다. 그래서 지금부터는"},{"start":383400,"end":390600,"text":"용어가 길기 때문에 파라미터 에피션 파인튜닝을 줄여서 피프티라고 부르도록 하겠습니다.","confidence":0.8265,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[383690,384080,"용어가"],[384170,384500,"길기"],[384500,384880,"때문에"],[385150,385680,"파라미터"],[385890,386280,"에피션"],[386490,387200,"파인튜닝을"],[387670,388040,"줄여서"],[388290,389240,"피프티라고"],[389330,389740,"부르도록"],[389790,390380,"하겠습니다."]],"textEdited":"용어가 길기 때문에 파라미터 에피션 파인튜닝을 줄여서 피프티라고 부르도록 하겠습니다."},{"start":390600,"end":403800,"text":"이런 peft는 모델의 모든 파라미터를 학습하지 않고 일부 파라미터만 파인튜닝 하는 방법론으로서 2019년도부터 효과적인 학습을 위해서 다양한","confidence":0.9579,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[391810,392060,"이런"],[392310,393060,"peft는"],[393270,393740,"모델의"],[393870,394160,"모든"],[394310,394880,"파라미터를"],[394910,395360,"학습하지"],[395360,395680,"않고"],[395970,396260,"일부"],[396370,397020,"파라미터만"],[397190,397800,"파인튜닝"],[397800,397980,"하는"],[398090,398860,"방법론으로서"],[399550,400940,"2019년도부터"],[401090,401600,"효과적인"],[401790,402160,"학습을"],[402270,402660,"위해서"],[402990,403420,"다양한"]],"textEdited":"이런 peft는 모델의 모든 파라미터를 학습하지 않고 일부 파라미터만 파인튜닝 하는 방법론으로서 2019년도부터 효과적인 학습을 위해서 다양한"},{"start":403800,"end":418300,"text":"방법론들이 제한이 되고 있습니다. 가장 대표적인 방법론들은 아래의 네 가지의 접근 방식입니다. 트랜스폼 모델의 모든 파라미터를 지금 우리가 사용하고 있는 LM 모델은","confidence":0.8766,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[404130,404780,"방법론들이"],[405010,405440,"제한이"],[405550,405880,"되고"],[406070,406560,"있습니다."],[408390,408640,"가장"],[408850,409520,"대표적인"],[409790,410560,"방법론들은"],[410970,411300,"아래의"],[411410,411560,"네"],[411590,412060,"가지의"],[412130,412440,"접근"],[412490,413060,"방식입니다."],[413750,414280,"트랜스폼"],[414370,414780,"모델의"],[414990,415260,"모든"],[415390,415980,"파라미터를"],[416230,416420,"지금"],[416490,416740,"우리가"],[416810,417200,"사용하고"],[417200,417340,"있는"],[417430,417740,"LM"],[417770,418120,"모델은"]],"textEdited":"방법론들이 제한이 되고 있습니다. 가장 대표적인 방법론들은 아래의 네 가지의 접근 방식입니다. 트랜스폼 모델의 모든 파라미터를 지금 우리가 사용하고 있는 LM 모델은"},{"start":418300,"end":432000,"text":"거의 대부분 트랜스포머의 디코더에 기반한 모델이기 때문에 트랜스포머 모델의 모든 파라미터를 업데이트하지 않고 각 방법론 별로 소량의 파라미터를 효과적으로 업데이트하는 방식입니다.","confidence":0.9805,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[418610,418880,"거의"],[418910,419240,"대부분"],[419390,420080,"트랜스포머의"],[420190,420740,"디코더에"],[420750,421120,"기반한"],[421130,421527,"모델이기"],[421527,421900,"때문에"],[422650,423220,"트랜스포머"],[423270,423660,"모델의"],[423790,424040,"모든"],[424190,424800,"파라미터를"],[424850,425480,"업데이트하지"],[425490,425800,"않고"],[426470,426620,"각"],[426950,427580,"방법론"],[427710,428080,"별로"],[428410,428960,"소량의"],[429070,429680,"파라미터를"],[429830,430480,"효과적으로"],[430610,431180,"업데이트하는"],[431250,431920,"방식입니다."]],"textEdited":"거의 대부분 트랜스포머의 디코더에 기반한 모델이기 때문에 트랜스포머 모델의 모든 파라미터를 업데이트하지 않고 각 방법론 별로 소량의 파라미터를 효과적으로 업데이트하는 방식입니다."},{"start":432000,"end":445200,"text":"여기서 mha는 멀티헤드 어텐션이고 FFN은 피드 포드 뉴럴 네트워크이라는 트랜스포머의 인코더 디코더 블록에서의 컴포넌트라고 보시면 어댑터 튜닝은","confidence":0.8512,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[432810,433160,"여기서"],[433370,434080,"mha는"],[434190,434800,"멀티헤드"],[434800,435440,"어텐션이고"],[435790,436380,"FFN은"],[436530,436820,"피드"],[436870,437160,"포드"],[437160,437400,"뉴럴"],[437410,438060,"네트워크이라는"],[439250,440020,"트랜스포머의"],[440210,440680,"인코더"],[440770,441200,"디코더"],[441290,442040,"블록에서의"],[442250,442900,"컴포넌트라고"],[442900,443240,"보시면"],[443930,444480,"어댑터"],[444570,444960,"튜닝은"]],"textEdited":"여기서 mha는 멀티헤드 어텐션이고 FFN은 피드 포드 뉴럴 네트워크이라는 트랜스포머의 인코더 디코더 블록에서의 컴포넌트라고 보시면 어댑터 튜닝은"},{"start":445200,"end":458800,"text":"멀티 헤드 어텐션과 피드 포드 뉴얼 네트워크에다가 어댑터라고 하는 모듈을 붙인 것이고요. 프리픽스 튜닝 같은 경우에는 각각의 레이어에 프리픽스라고 하는 항목을 붙여주는 것,","confidence":0.9142,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[445490,445854,"멀티"],[445854,446060,"헤드"],[446060,446680,"어텐션과"],[447130,447380,"피드"],[447430,447740,"포드"],[447740,447960,"뉴얼"],[447990,448800,"네트워크에다가"],[449050,449980,"어댑터라고"],[450050,450240,"하는"],[450530,450920,"모듈을"],[450970,451280,"붙인"],[451280,451720,"것이고요."],[452610,453220,"프리픽스"],[453220,453520,"튜닝"],[453530,453767,"같은"],[453767,454140,"경우에는"],[454350,454800,"각각의"],[454930,455420,"레이어에"],[455710,456580,"프리픽스라고"],[456630,456840,"하는"],[457070,457440,"항목을"],[457510,457980,"붙여주는"],[457980,458120,"것,"]],"textEdited":"멀티 헤드 어텐션과 피드 포드 뉴얼 네트워크에다가 어댑터라고 하는 모듈을 붙인 것이고요. 프리픽스 튜닝 같은 경우에는 각각의 레이어에 프리픽스라고 하는 항목을 붙여주는 것,"},{"start":458800,"end":469000,"text":"프롬프 튜닝은 인풋의 프롬프트에다가 무엇인가 벡터를 붙여주는 것. 로우 랭크 어댑테이션은 레이어별로 훨씬 더 낮은 랭크로","confidence":0.9357,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[459030,459440,"프롬프"],[459570,459960,"튜닝은"],[460250,460860,"인풋의"],[460950,461800,"프롬프트에다가"],[461930,462380,"무엇인가"],[462470,462840,"벡터를"],[462840,463280,"붙여주는"],[463280,463420,"것."],[464090,464380,"로우"],[464550,464840,"랭크"],[464840,465480,"어댑테이션은"],[465650,466380,"레이어별로"],[466790,467140,"훨씬"],[467150,467300,"더"],[467550,467820,"낮은"],[468090,468660,"랭크로"]],"textEdited":"프롬프 튜닝은 인풋의 프롬프트에다가 무엇인가 벡터를 붙여주는 것. 로우 랭크 어댑테이션은 레이어별로 훨씬 더 낮은 랭크로"},{"start":469000,"end":482100,"text":"벡터를 행렬을 전환을 해서 연산량을 줄여주는 것을 의미합니다. 이 PFT에 대해서 하나씩 구체적으로 설명을 드리겠습니다. 첫 번째 어댑터입니다. 어댑터는","confidence":0.9641,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[469730,470400,"벡터를"],[470610,471000,"행렬을"],[471050,471500,"전환을"],[471500,471700,"해서"],[471950,472580,"연산량을"],[472650,473080,"줄여주는"],[473080,473280,"것을"],[473310,473820,"의미합니다."],[474390,474540,"이"],[474890,476000,"PFT에"],[476010,476400,"대해서"],[476570,477000,"하나씩"],[477230,477800,"구체적으로"],[477950,478320,"설명을"],[478330,478920,"드리겠습니다."],[479630,479780,"첫"],[479830,480120,"번째"],[480310,481080,"어댑터입니다."],[481370,481900,"어댑터는"]],"textEdited":"벡터를 행렬을 전환을 해서 연산량을 줄여주는 것을 의미합니다. 이 PFT에 대해서 하나씩 구체적으로 설명을 드리겠습니다. 첫 번째 어댑터입니다. 어댑터는"},{"start":482100,"end":497100,"text":"기존에 이미 학습이 완료된 모델에 각 레이어의 학습이 가능한 피드 포드 네트워크를 삽입하는 구조입니다. 어댑터 레이어는 트랜스포머의 벡터를 더 작은 차원으로 압축을 하고","confidence":0.9295,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[482390,482800,"기존에"],[483010,483280,"이미"],[483510,483960,"학습이"],[484210,484680,"완료된"],[484810,485220,"모델에"],[485790,485940,"각"],[486210,486660,"레이어의"],[487010,487400,"학습이"],[487490,487860,"가능한"],[488130,488420,"피드"],[488570,488960,"포드"],[488990,489520,"네트워크를"],[489750,490260,"삽입하는"],[490350,490920,"구조입니다."],[491690,492140,"어댑터"],[492190,492640,"레이어는"],[492870,493680,"트랜스포머의"],[493750,494160,"벡터를"],[494310,494460,"더"],[494810,495100,"작은"],[495270,495740,"차원으로"],[495850,496240,"압축을"],[496240,496480,"하고"]],"textEdited":"기존에 이미 학습이 완료된 모델에 각 레이어의 학습이 가능한 피드 포드 네트워크를 삽입하는 구조입니다. 어댑터 레이어는 트랜스포머의 벡터를 더 작은 차원으로 압축을 하고"},{"start":497100,"end":509300,"text":"비선형 변환을 거쳐서 원래 차원으로 복원하는 병목 구조를 가지고 있습니다. 오른쪽에서 이제 보시자면 왼쪽은 트랜스포머라고 본다면 오른쪽이 어댑터 레이어인데","confidence":0.9593,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[497310,497860,"비선형"],[497890,498340,"변환을"],[498410,498860,"거쳐서"],[499410,499740,"원래"],[499870,500300,"차원으로"],[500430,501200,"복원하는"],[501490,501820,"병목"],[501970,502320,"구조를"],[502330,502587,"가지고"],[502587,502960,"있습니다."],[503370,503920,"오른쪽에서"],[503990,504160,"이제"],[504210,504780,"보시자면"],[505350,505800,"왼쪽은"],[506070,506880,"트랜스포머라고"],[506890,507280,"본다면"],[507490,508000,"오른쪽이"],[508030,508440,"어댑터"],[508470,508960,"레이어인데"]],"textEdited":"비선형 변환을 거쳐서 원래 차원으로 복원하는 병목 구조를 가지고 있습니다. 오른쪽에서 이제 보시자면 왼쪽은 트랜스포머라고 본다면 오른쪽이 어댑터 레이어인데"},{"start":509300,"end":523400,"text":"피드 포드 레이어를 통해서 훨씬 더 벡터를 작은 크기로 압축을 하고 비선형 변환을 한번 한 다음에 원래 차원으로 복원하는 겁니다. 이거 굳이 왜 하느냐 결국은 모델의 파라미터를 줄이기 위해서입니다.","confidence":0.9691,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[509590,509860,"피드"],[509950,510260,"포드"],[510290,510680,"레이어를"],[510690,511020,"통해서"],[511190,511560,"훨씬"],[511570,511720,"더"],[511890,512340,"벡터를"],[512470,512720,"작은"],[512830,513140,"크기로"],[513270,513640,"압축을"],[513640,513860,"하고"],[514750,515240,"비선형"],[515290,515660,"변환을"],[515660,515880,"한번"],[515950,516100,"한"],[516110,516520,"다음에"],[516750,517020,"원래"],[517070,517440,"차원으로"],[517470,517847,"복원하는"],[517847,518160,"겁니다."],[518530,518760,"이거"],[518870,519140,"굳이"],[519190,519340,"왜"],[519350,519660,"하느냐"],[519970,520520,"결국은"],[521010,521500,"모델의"],[521630,522260,"파라미터를"],[522350,522740,"줄이기"],[522750,523400,"위해서입니다."]],"textEdited":"피드 포드 레이어를 통해서 훨씬 더 벡터를 작은 크기로 압축을 하고 비선형 변환을 한번 한 다음에 원래 차원으로 복원하는 겁니다. 이거 굳이 왜 하느냐 결국은 모델의 파라미터를 줄이기 위해서입니다."},{"start":523400,"end":537200,"text":"그래서 멀티 헤드 어텐션에 해당하는 부분 피드 FRD 네트워크에 해당하는 부분들은 그대로 고정을 시켜놓고 어댑터 레이어에 해당하는 부분만 학습을 시킨다는 뜻이지요. 어댑터 모듈은 파인 튜닝 단계에서","confidence":0.8355,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[523590,523860,"그래서"],[524390,524767,"멀티"],[524767,524980,"헤드"],[524990,525560,"어텐션에"],[525560,525920,"해당하는"],[525930,526160,"부분"],[526570,526820,"피드"],[526890,527220,"FRD"],[527250,527680,"네트워크에"],[527680,528040,"해당하는"],[528090,528580,"부분들은"],[528610,528880,"그대로"],[528970,529340,"고정을"],[529350,529860,"시켜놓고"],[530410,531000,"어댑터"],[531190,531620,"레이어에"],[531630,532020,"해당하는"],[532070,532460,"부분만"],[532670,533000,"학습을"],[533010,533500,"시킨다는"],[533510,534000,"뜻이지요."],[534770,535220,"어댑터"],[535220,535560,"모듈은"],[535730,536000,"파인"],[536010,536300,"튜닝"],[536390,536880,"단계에서"]],"textEdited":"그래서 멀티 헤드 어텐션에 해당하는 부분 피드 FRD 네트워크에 해당하는 부분들은 그대로 고정을 시켜놓고 어댑터 레이어에 해당하는 부분만 학습을 시킨다는 뜻이지요. 어댑터 모듈은 파인 튜닝 단계에서"},{"start":537200,"end":551100,"text":"특정한 타겟 테스크에 대해서 최적화되는 것이고요. 나머지 트랜스포머 레이어는 모두 고정이 되는 것입니다. 따라서 이를 통해서 훨씬 더 효율적으로 행렬의 차원이 작은 경우만 학습을 함으로써","confidence":0.9759,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[537310,537720,"특정한"],[537910,538180,"타겟"],[538310,538787,"테스크에"],[538787,539100,"대해서"],[539230,539760,"최적화되는"],[539760,540220,"것이고요."],[541010,541440,"나머지"],[541510,542080,"트랜스포머"],[542130,542520,"레이어는"],[542630,542940,"모두"],[543070,543407,"고정이"],[543407,543620,"되는"],[543620,544020,"것입니다."],[544330,544680,"따라서"],[545190,545420,"이를"],[545450,545760,"통해서"],[546410,546667,"훨씬"],[546667,546800,"더"],[546890,547660,"효율적으로"],[548230,548540,"행렬의"],[548570,548900,"차원이"],[549070,549360,"작은"],[549570,549920,"경우만"],[550050,550360,"학습을"],[550370,550900,"함으로써"]],"textEdited":"특정한 타겟 테스크에 대해서 최적화되는 것이고요. 나머지 트랜스포머 레이어는 모두 고정이 되는 것입니다. 따라서 이를 통해서 훨씬 더 효율적으로 행렬의 차원이 작은 경우만 학습을 함으로써"},{"start":551100,"end":566000,"text":"효율적인 학습이 가능해지고 위 어댑터라는 개념을 처음 제안한 논문에서는 글루 벤치마크의 9개 테스크의 성능을 평균을 낸 결과 매우 작은 학습 파라미터만으로도 파인튜닝에 근접한 성능을","confidence":0.9689,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[551370,551900,"효율적인"],[551990,552320,"학습이"],[552370,552940,"가능해지고"],[553630,553780,"위"],[553970,554660,"어댑터라는"],[554730,555100,"개념을"],[555230,555460,"처음"],[555590,555960,"제안한"],[556010,556620,"논문에서는"],[557110,557420,"글루"],[557550,558220,"벤치마크의"],[558390,558760,"9개"],[558930,559380,"테스크의"],[559470,559820,"성능을"],[559970,560360,"평균을"],[560390,560540,"낸"],[560750,561080,"결과"],[561730,562020,"매우"],[562190,562460,"작은"],[562690,562960,"학습"],[563010,563900,"파라미터만으로도"],[564090,564820,"파인튜닝에"],[564950,565400,"근접한"],[565530,565880,"성능을"]],"textEdited":"효율적인 학습이 가능해지고 위 어댑터라는 개념을 처음 제안한 논문에서는 글루 벤치마크의 9개 테스크의 성능을 평균을 낸 결과 매우 작은 학습 파라미터만으로도 파인튜닝에 근접한 성능을"},{"start":566000,"end":576400,"text":"기록했다는 연구 결과가 있습니다. 여기 보시는 것처럼 x축이 파라미터 학습을 해야 되는 파라미터의 수이고요. 오른쪽이 이제 성능 차이라고 했을 때","confidence":0.9556,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[566190,567000,"기록했다는"],[567150,567400,"연구"],[567410,567760,"결과가"],[567760,568080,"있습니다."],[568390,568580,"여기"],[568590,568887,"보시는"],[568887,569240,"것처럼"],[570490,570960,"x축이"],[571370,571940,"파라미터"],[572150,572460,"학습을"],[572460,572554,"해야"],[572554,572740,"되는"],[572790,573260,"파라미터의"],[573260,573680,"수이고요."],[574170,574720,"오른쪽이"],[574730,574900,"이제"],[575090,575340,"성능"],[575390,575780,"차이라고"],[575780,576000,"했을"],[576030,576180,"때"]],"textEdited":"기록했다는 연구 결과가 있습니다. 여기 보시는 것처럼 x축이 파라미터 학습을 해야 되는 파라미터의 수이고요. 오른쪽이 이제 성능 차이라고 했을 때"},{"start":576400,"end":585200,"text":"파인튜닝을 10에 8승 이상만큼을 한 거와 10의 6승 정도에 그러니까 100분의 1 정도만","confidence":0.8838,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[576630,577420,"파인튜닝을"],[577630,577920,"10에"],[578550,578940,"8승"],[579130,580000,"이상만큼을"],[580130,580280,"한"],[580430,580820,"거와"],[581570,581860,"10의"],[581970,582400,"6승"],[582530,582900,"정도에"],[582930,583140,"그러니까"],[583190,583720,"100분의"],[583770,583920,"1"],[584190,584820,"정도만"]],"textEdited":"파인튜닝을 10에 8승 이상만큼을 한 거와 10의 6승 정도에 그러니까 100분의 1 정도만"},{"start":585200,"end":595100,"text":"학습을 파인튜닝을 어댑터를 통해서 했음에도 불구하고 성능은 거의 유사하다라는 것을 의미합니다. 두 번째는 프리픽스 튜닝이라는 겁니다.","confidence":0.9694,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[585530,585920,"학습을"],[586090,586760,"파인튜닝을"],[586830,587320,"어댑터를"],[587330,587660,"통해서"],[587810,588540,"했음에도"],[588630,589180,"불구하고"],[589470,589800,"성능은"],[589870,590100,"거의"],[590150,590840,"유사하다라는"],[590840,591120,"것을"],[591210,591720,"의미합니다."],[592710,592860,"두"],[592870,593260,"번째는"],[593430,594060,"프리픽스"],[594060,594527,"튜닝이라는"],[594527,594940,"겁니다."]],"textEdited":"학습을 파인튜닝을 어댑터를 통해서 했음에도 불구하고 성능은 거의 유사하다라는 것을 의미합니다. 두 번째는 프리픽스 튜닝이라는 겁니다."},{"start":595100,"end":607600,"text":"프리픽스라는 게 바로 어떤 무엇인가의 앞쪽에다가 고정적으로 붙인다라는 것이죠. 각 트랜스포머의 레이어에 프리픽스라는 훈련 가능한 벡터를 추가하는 방식입니다.","confidence":0.9735,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[595470,596280,"프리픽스라는"],[596290,596440,"게"],[596550,596820,"바로"],[597230,597460,"어떤"],[597710,598320,"무엇인가의"],[598450,599100,"앞쪽에다가"],[599210,599840,"고정적으로"],[599910,600500,"붙인다라는"],[600500,600840,"것이죠."],[601610,601760,"각"],[602030,602820,"트랜스포머의"],[603010,603460,"레이어에"],[603830,604620,"프리픽스라는"],[604850,605160,"훈련"],[605350,605720,"가능한"],[605910,606380,"벡터를"],[606490,606900,"추가하는"],[606970,607600,"방식입니다."]],"textEdited":"프리픽스라는 게 바로 어떤 무엇인가의 앞쪽에다가 고정적으로 붙인다라는 것이죠. 각 트랜스포머의 레이어에 프리픽스라는 훈련 가능한 벡터를 추가하는 방식입니다."},{"start":607600,"end":618600,"text":"이거는 가상의 인베딩으로 간주될 수가 있는데요. 원래 파인 튜닝이라면 이 트랜스포머의 모든 이 항목들을 학습을 시켜야 되는데 그게 아니라","confidence":0.9456,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[608230,608560,"이거는"],[608850,609300,"가상의"],[609350,609880,"인베딩으로"],[609930,610280,"간주될"],[610290,610467,"수가"],[610467,610820,"있는데요."],[611410,611800,"원래"],[611950,612220,"파인"],[612220,612880,"튜닝이라면"],[613030,613180,"이"],[613390,614200,"트랜스포머의"],[614830,615260,"모든"],[615490,615640,"이"],[615770,616340,"항목들을"],[616590,616920,"학습을"],[616930,617167,"시켜야"],[617167,617500,"되는데"],[617650,617880,"그게"],[617880,618160,"아니라"]],"textEdited":"이거는 가상의 인베딩으로 간주될 수가 있는데요. 원래 파인 튜닝이라면 이 트랜스포머의 모든 이 항목들을 학습을 시켜야 되는데 그게 아니라"},{"start":618600,"end":630300,"text":"프리트레인드 사전 학습된 트랜스포머의 이 벡터들은 고정을 시켜 놓고 그 앞에다가 프리픽스를 붙여줍니다. 이 분홍색 부분만이 학습을 한다라는 뜻이죠.","confidence":0.9149,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[618870,619500,"프리트레인드"],[619590,619880,"사전"],[619930,620320,"학습된"],[620430,621120,"트랜스포머의"],[621370,621520,"이"],[621670,622140,"벡터들은"],[622210,622560,"고정을"],[622570,622780,"시켜"],[622780,623060,"놓고"],[623690,623840,"그"],[624210,624880,"앞에다가"],[625150,625920,"프리픽스를"],[626110,626700,"붙여줍니다."],[626890,627040,"이"],[627190,627720,"분홍색"],[627790,628380,"부분만이"],[628810,629240,"학습을"],[629370,629800,"한다라는"],[629810,630140,"뜻이죠."]],"textEdited":"프리트레인드 사전 학습된 트랜스포머의 이 벡터들은 고정을 시켜 놓고 그 앞에다가 프리픽스를 붙여줍니다. 이 분홍색 부분만이 학습을 한다라는 뜻이죠."},{"start":630300,"end":639500,"text":"그래서 각 테스크를 더욱더 잘 풀이하기 위해서 벡터를 최적화를 하고 이거를 기존 모델과 병합할 수 있게 됩니다.","confidence":0.9763,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[630590,630880,"그래서"],[631450,631600,"각"],[631870,632400,"테스크를"],[632530,633020,"더욱더"],[633150,633300,"잘"],[633490,634000,"풀이하기"],[634050,634420,"위해서"],[635010,635500,"벡터를"],[635590,636160,"최적화를"],[636170,636420,"하고"],[636690,637040,"이거를"],[637130,637360,"기존"],[637410,637820,"모델과"],[638030,638460,"병합할"],[638490,638607,"수"],[638607,638800,"있게"],[638800,639240,"됩니다."]],"textEdited":"그래서 각 테스크를 더욱더 잘 풀이하기 위해서 벡터를 최적화를 하고 이거를 기존 모델과 병합할 수 있게 됩니다."},{"start":639500,"end":654000,"text":"세 번째 프롬프트 튜닝이라는 거는 프리픽스 튜닝과는 다르게 모델의 입력 레이어에 훈련 가능한 프롬프트 벡터를 통합하는 방법입니다. 그래서 여기서 보시면 레이어는 고정이 되어 있는데 인풋과","confidence":0.9242,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[639850,640000,"세"],[640000,640280,"번째"],[640870,641400,"프롬프트"],[641410,641907,"튜닝이라는"],[641907,642160,"거는"],[642510,643120,"프리픽스"],[643120,643660,"튜닝과는"],[643750,644140,"다르게"],[644530,645020,"모델의"],[645290,645580,"입력"],[645770,646200,"레이어에"],[646470,646760,"훈련"],[646930,647280,"가능한"],[647490,648000,"프롬프트"],[648030,648420,"벡터를"],[648530,648960,"통합하는"],[649010,649460,"방법입니다."],[649830,650060,"그래서"],[650060,650280,"여기서"],[650280,650620,"보시면"],[651290,651780,"레이어는"],[651870,652207,"고정이"],[652207,652420,"되어"],[652450,652800,"있는데"],[653030,653600,"인풋과"]],"textEdited":"세 번째 프롬프트 튜닝이라는 거는 프리픽스 튜닝과는 다르게 모델의 입력 레이어에 훈련 가능한 프롬프트 벡터를 통합하는 방법입니다. 그래서 여기서 보시면 레이어는 고정이 되어 있는데 인풋과"},{"start":654000,"end":665900,"text":"프롬프트에 대한 노란색 부분만이 학습이 되는 파라미터인 거죠. 근데 여기서의 앞에서 설명드렸던 인풋 문장의 자연어 프럼프팅을 덧붙이는 것과는 약간은 다른 개념으로써","confidence":0.9143,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[654000,654447,"프롬프트에"],[654447,654640,"대한"],[654750,655260,"노란색"],[655350,655900,"부분만이"],[656130,656540,"학습이"],[656540,656720,"되는"],[656790,657320,"파라미터인"],[657320,657560,"거죠."],[658330,658560,"근데"],[658650,659160,"여기서의"],[659330,659680,"앞에서"],[659770,660400,"설명드렸던"],[661030,661420,"인풋"],[661610,662040,"문장의"],[662190,662660,"자연어"],[662770,663440,"프럼프팅을"],[663450,663980,"덧붙이는"],[663980,664380,"것과는"],[664470,664880,"약간은"],[664930,665180,"다른"],[665270,665860,"개념으로써"]],"textEdited":"프롬프트에 대한 노란색 부분만이 학습이 되는 파라미터인 거죠. 근데 여기서의 앞에서 설명드렸던 인풋 문장의 자연어 프럼프팅을 덧붙이는 것과는 약간은 다른 개념으로써"},{"start":665900,"end":679600,"text":"인풋의 인베딩 레이어를 최적화하는 방법론이라고 보시면 되겠습니다. 여기서 보시는 것처럼 프롬프트 자체를 어떤 테스크의 배치마다 태스크가 있을 때 앞에 프롬프트를 붙여주고","confidence":0.9676,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[666210,666700,"인풋의"],[666850,667300,"인베딩"],[667410,667880,"레이어를"],[667950,668520,"최적화하는"],[668830,669720,"방법론이라고"],[669810,670180,"보시면"],[670190,670720,"되겠습니다."],[671110,671360,"여기서"],[671390,671720,"보시는"],[671720,672120,"것처럼"],[672810,673300,"프롬프트"],[673370,673820,"자체를"],[673890,674100,"어떤"],[674350,675040,"테스크의"],[675130,675700,"배치마다"],[675790,676280,"태스크가"],[676310,676540,"있을"],[676590,676740,"때"],[677270,677640,"앞에"],[677870,678600,"프롬프트를"],[678690,679260,"붙여주고"]],"textEdited":"인풋의 인베딩 레이어를 최적화하는 방법론이라고 보시면 되겠습니다. 여기서 보시는 것처럼 프롬프트 자체를 어떤 테스크의 배치마다 태스크가 있을 때 앞에 프롬프트를 붙여주고"},{"start":679600,"end":689700,"text":"해당하는 프롬프트는 여기서 보시는 것처럼 여기 이 테스크 프롬프트에서 이렇게 어 딕셔너리처럼 사용을 할 수 있는 끼워 넣을 수 있는 부분이 되겠습니다.","confidence":0.9423,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[679810,680260,"해당하는"],[680310,681020,"프롬프트는"],[681210,681540,"여기서"],[681550,681880,"보시는"],[681880,682260,"것처럼"],[682670,682840,"여기"],[683090,683240,"이"],[683470,683820,"테스크"],[683870,684520,"프롬프트에서"],[684710,684900,"이렇게"],[685370,685520,"어"],[685790,686500,"딕셔너리처럼"],[686630,686960,"사용을"],[686970,687120,"할"],[687130,687247,"수"],[687247,687420,"있는"],[687810,688080,"끼워"],[688080,688280,"넣을"],[688330,688434,"수"],[688434,688600,"있는"],[688650,688967,"부분이"],[688967,689620,"되겠습니다."]],"textEdited":"해당하는 프롬프트는 여기서 보시는 것처럼 여기 이 테스크 프롬프트에서 이렇게 어 딕셔너리처럼 사용을 할 수 있는 끼워 넣을 수 있는 부분이 되겠습니다."},{"start":689700,"end":700800,"text":"마지막으로 로우 랭크 어댑테이션 로라 같은 경우에는 사전 학습된 모델의 파라미터를 고정하고 학습 가능한 랭크의 디컴포지션 행렬을 삽입하는 방법입니다.","confidence":0.9651,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[690390,690980,"마지막으로"],[691090,691360,"로우"],[691430,691720,"랭크"],[691720,692200,"어댑테이션"],[692350,692700,"로라"],[692730,693000,"같은"],[693000,693400,"경우에는"],[693610,693900,"사전"],[693970,694380,"학습된"],[694490,694860,"모델의"],[694890,695440,"파라미터를"],[695490,695980,"고정하고"],[696630,696940,"학습"],[697010,697340,"가능한"],[697510,697980,"랭크의"],[698070,698820,"디컴포지션"],[698930,699280,"행렬을"],[699350,699880,"삽입하는"],[700150,700800,"방법입니다."]],"textEdited":"마지막으로 로우 랭크 어댑테이션 로라 같은 경우에는 사전 학습된 모델의 파라미터를 고정하고 학습 가능한 랭크의 디컴포지션 행렬을 삽입하는 방법입니다."},{"start":700800,"end":712000,"text":"행렬의 차원을 랭크만큼 줄이는 행렬과 다시 원래의 차원을 크기로 바꿔주는 행렬로 구성이 되고요. 이거를 롤 랭크 키 콤포지션이라고 표현을 합니다.","confidence":0.8984,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[701390,701780,"행렬의"],[701790,702140,"차원을"],[702370,703020,"랭크만큼"],[703130,703480,"줄이는"],[703590,704020,"행렬과"],[704330,704620,"다시"],[704710,705080,"원래의"],[705130,705560,"차원을"],[706090,706440,"크기로"],[706710,707180,"바꿔주는"],[707270,707640,"행렬로"],[707730,708120,"구성이"],[708120,708500,"되고요."],[709130,709560,"이거를"],[709750,709900,"롤"],[710150,710420,"랭크"],[710430,710580,"키"],[710650,711360,"콤포지션이라고"],[711370,711594,"표현을"],[711594,711980,"합니다."]],"textEdited":"행렬의 차원을 랭크만큼 줄이는 행렬과 다시 원래의 차원을 크기로 바꿔주는 행렬로 구성이 되고요. 이거를 롤 랭크 키 콤포지션이라고 표현을 합니다."},{"start":712000,"end":724400,"text":"레이어마다 히든 스테이트의 로라 파라미터를 더해서 파인 튜닝을 하게 됩니다. 최근에 이제 그 PFT 방법론 중에서 가장 널리 쓰이는 것이고요. 여기 보시는 것처럼","confidence":0.954,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[712610,713240,"레이어마다"],[713390,713660,"히든"],[713690,714300,"스테이트의"],[714530,714880,"로라"],[714990,715620,"파라미터를"],[715630,716100,"더해서"],[716430,716700,"파인"],[716750,717180,"튜닝을"],[717430,717640,"하게"],[717640,717960,"됩니다."],[718870,719220,"최근에"],[719410,719580,"이제"],[719870,720020,"그"],[720390,720980,"PFT"],[721110,721520,"방법론"],[721530,721880,"중에서"],[722030,722260,"가장"],[722350,722620,"널리"],[722630,722920,"쓰이는"],[722920,723320,"것이고요."],[723430,723620,"여기"],[723650,723947,"보시는"],[723947,724320,"것처럼"]],"textEdited":"레이어마다 히든 스테이트의 로라 파라미터를 더해서 파인 튜닝을 하게 됩니다. 최근에 이제 그 PFT 방법론 중에서 가장 널리 쓰이는 것이고요. 여기 보시는 것처럼"},{"start":724400,"end":735600,"text":"왼쪽에 있는 분홍색의 사전 학습된 웨이트 같은 경우에는 이게 굉장히 사이즈가 큽니다. 랭크 자체가 d 모델 d","confidence":0.9779,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[725350,726160,"왼쪽에"],[726430,726740,"있는"],[727290,727960,"분홍색의"],[728190,728460,"사전"],[728530,728980,"학습된"],[729210,729600,"웨이트"],[729650,729940,"같은"],[729950,730460,"경우에는"],[731150,731400,"이게"],[731650,732000,"굉장히"],[732130,732660,"사이즈가"],[732790,733120,"큽니다."],[733550,733860,"랭크"],[733870,734220,"자체가"],[734330,734480,"d"],[734670,734980,"모델"],[735190,735340,"d"]],"textEdited":"왼쪽에 있는 분홍색의 사전 학습된 웨이트 같은 경우에는 이게 굉장히 사이즈가 큽니다. 랭크 자체가 d 모델 d"},{"start":735600,"end":748100,"text":"FID f 웨이트라고 했을 때 모델의 사이즈가 더 작지만 여전히 가로 길이가 크죠. 아주 완벽한 개념은 아닙니다마는 예시 비유로서 들어드리면 이 다각형의 면적이 우리가 학습","confidence":0.8859,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[735810,736060,"FID"],[736110,736260,"f"],[736530,737047,"웨이트라고"],[737047,737260,"했을"],[737260,737400,"때"],[737490,737900,"모델의"],[737910,738280,"사이즈가"],[738280,738400,"더"],[738430,738800,"작지만"],[738850,739200,"여전히"],[739250,739520,"가로"],[739530,739900,"길이가"],[740810,741080,"크죠."],[741670,741880,"아주"],[741950,742400,"완벽한"],[742570,742880,"개념은"],[742880,743580,"아닙니다마는"],[743810,744100,"예시"],[744150,744680,"비유로서"],[744680,745240,"들어드리면"],[745690,745840,"이"],[746130,746680,"다각형의"],[746770,747360,"면적이"],[747450,747720,"우리가"],[747830,748080,"학습"]],"textEdited":"FID f 웨이트라고 했을 때 모델의 사이즈가 더 작지만 여전히 가로 길이가 크죠. 아주 완벽한 개념은 아닙니다마는 예시 비유로서 들어드리면 이 다각형의 면적이 우리가 학습"},{"start":748100,"end":762100,"text":"시켜야 되는 파라미터의 개수에 비례한다라고 보면 로우 랭크 어댑테이션 같은 경우에는 이 중간에 스몰알이라고 하는 훨씬 더 작은 그 수의 벡터로 압축을 한번 한 뒤에 풀어내는 과정입니다.","confidence":0.9326,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[748170,748520,"시켜야"],[748520,748700,"되는"],[748770,749300,"파라미터의"],[749330,749720,"개수에"],[749750,750440,"비례한다라고"],[750450,750720,"보면"],[751190,751420,"로우"],[751470,751740,"랭크"],[751750,752260,"어댑테이션"],[752290,752560,"같은"],[752560,752980,"경우에는"],[753370,753520,"이"],[753910,754380,"중간에"],[754850,755980,"스몰알이라고"],[755980,756160,"하는"],[756350,756700,"훨씬"],[756700,756820,"더"],[756990,757280,"작은"],[757730,757880,"그"],[757910,758220,"수의"],[758350,758820,"벡터로"],[759290,759800,"압축을"],[759850,760100,"한번"],[760210,760360,"한"],[760430,760700,"뒤에"],[760830,761240,"풀어내는"],[761290,761860,"과정입니다."]],"textEdited":"시켜야 되는 파라미터의 개수에 비례한다라고 보면 로우 랭크 어댑테이션 같은 경우에는 이 중간에 스몰알이라고 하는 훨씬 더 작은 그 수의 벡터로 압축을 한번 한 뒤에 풀어내는 과정입니다."},{"start":762100,"end":774200,"text":"이 노란색 부분만 학습을 해서 원래 프리트레이드 된 웨이트에다가 더해주면 이 파인 튜닝이 가능하다라는 것이고요. 가장 널리 쓰이는 방법론이라고 좀 전에 말씀드렸습니다.","confidence":0.9042,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[762350,762500,"이"],[762630,763100,"노란색"],[763170,763560,"부분만"],[763730,764060,"학습을"],[764060,764260,"해서"],[764550,764840,"원래"],[764990,765614,"프리트레이드"],[765614,765740,"된"],[765930,766600,"웨이트에다가"],[766710,767280,"더해주면"],[767910,768060,"이"],[768230,768500,"파인"],[768500,768900,"튜닝이"],[769010,769680,"가능하다라는"],[769680,770100,"것이고요."],[770450,770680,"가장"],[770770,771020,"널리"],[771070,771340,"쓰이는"],[771450,772200,"방법론이라고"],[772670,772820,"좀"],[772820,773040,"전에"],[773110,774020,"말씀드렸습니다."]],"textEdited":"이 노란색 부분만 학습을 해서 원래 프리트레이드 된 웨이트에다가 더해주면 이 파인 튜닝이 가능하다라는 것이고요. 가장 널리 쓰이는 방법론이라고 좀 전에 말씀드렸습니다."},{"start":774200,"end":787400,"text":"여기서 보시는 것처럼 로우 랭크 디컴 포지션을 통해서 사용한 파라미터를 델타w라고 봤을 때 원래 기존의 분홍색에 해당하는 이 웨이트에다가","confidence":0.9426,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[775070,775380,"여기서"],[775410,775687,"보시는"],[775687,776020,"것처럼"],[776290,776560,"로우"],[776650,776920,"랭크"],[776970,777260,"디컴"],[777290,777780,"포지션을"],[777810,778180,"통해서"],[778430,778720,"사용한"],[778810,779840,"파라미터를"],[780050,781560,"델타w라고"],[781650,782020,"봤을"],[782070,782220,"때"],[782790,783120,"원래"],[783330,783800,"기존의"],[784150,784880,"분홍색에"],[784950,785500,"해당하는"],[785730,785880,"이"],[786310,787100,"웨이트에다가"]],"textEdited":"여기서 보시는 것처럼 로우 랭크 디컴 포지션을 통해서 사용한 파라미터를 델타w라고 봤을 때 원래 기존의 분홍색에 해당하는 이 웨이트에다가"},{"start":787400,"end":802200,"text":"이 로우 랭크 어댑테이션을 통해서 사전 파인 튜닝이 된 웨이트를 단순히 합쳐주는 겁니다. 이거를 가능하게 하는 이유는 뭐냐 하면 여기에 있는 디프블라고 하는 핑크색에 해당하는 요 윗면의 가로 길이와","confidence":0.8488,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[787770,787920,"이"],[788050,788300,"로우"],[788300,788560,"랭크"],[788560,789100,"어댑테이션을"],[789110,789500,"통해서"],[789890,790100,"사전"],[790250,790520,"파인"],[790530,790887,"튜닝이"],[790887,791020,"된"],[791310,791800,"웨이트를"],[791930,792320,"단순히"],[792330,792727,"합쳐주는"],[792727,793020,"겁니다."],[793230,793620,"이거를"],[793790,794220,"가능하게"],[794220,794400,"하는"],[794450,794780,"이유는"],[794780,794940,"뭐냐"],[794940,795140,"하면"],[795890,796300,"여기에"],[796390,796640,"있는"],[796930,798420,"디프블라고"],[798420,798620,"하는"],[799230,799840,"핑크색에"],[799840,800300,"해당하는"],[800470,800620,"요"],[800810,801320,"윗면의"],[801350,801600,"가로"],[801600,801920,"길이와"]],"textEdited":"이 로우 랭크 어댑테이션을 통해서 사전 파인 튜닝이 된 웨이트를 단순히 합쳐주는 겁니다. 이거를 가능하게 하는 이유는 뭐냐 하면 여기에 있는 디프블라고 하는 핑크색에 해당하는 요 윗면의 가로 길이와"},{"start":802200,"end":817200,"text":"노란색에 해당하는 윗면의 가로 길이가 같기 때문에 웨이트를 1 대 1로 대응을 시킬 수가 있고요. 그럼으로 인해서 단순히 더해지는 것만으로써 효과를 볼 수 있다라는 겁니다. 그리고 이 스몰알은 원래 데이터인","confidence":0.9373,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[802470,803067,"노란색에"],[803067,803400,"해당하는"],[803490,803880,"윗면의"],[803930,804180,"가로"],[804180,804520,"길이가"],[805010,805340,"같기"],[805340,805720,"때문에"],[806150,806660,"웨이트를"],[806870,807020,"1"],[807070,807174,"대"],[807174,807440,"1로"],[807530,807900,"대응을"],[807930,808180,"시킬"],[808190,808420,"수가"],[808420,808720,"있고요."],[808910,809320,"그럼으로"],[809320,809620,"인해서"],[809770,810140,"단순히"],[810210,810720,"더해지는"],[810720,811380,"것만으로써"],[811870,812260,"효과를"],[812270,812420,"볼"],[812430,812534,"수"],[812534,812900,"있다라는"],[812910,813260,"겁니다."],[813890,814180,"그리고"],[814390,814540,"이"],[814690,815340,"스몰알은"],[815630,815960,"원래"],[816450,816960,"데이터인"]],"textEdited":"노란색에 해당하는 윗면의 가로 길이가 같기 때문에 웨이트를 1 대 1로 대응을 시킬 수가 있고요. 그럼으로 인해서 단순히 더해지는 것만으로써 효과를 볼 수 있다라는 겁니다. 그리고 이 스몰알은 원래 데이터인"},{"start":817200,"end":828300,"text":"행렬인 에하고 비가 가지고 있는 랭크의 엠과 엔이라는 랭크보다 훨씬 더 적은 수의 랭크를 가짐으로써 학습 파라미터의 개수를 줄여준다는 뜻입니다.","confidence":0.8504,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[817870,818220,"행렬인"],[818390,818880,"에하고"],[818950,819280,"비가"],[819390,819780,"가지고"],[819790,820000,"있는"],[820230,820720,"랭크의"],[820930,821340,"엠과"],[821470,821940,"엔이라는"],[822010,822480,"랭크보다"],[822890,823260,"훨씬"],[823260,823400,"더"],[823550,823880,"적은"],[823910,824160,"수의"],[824230,824620,"랭크를"],[824670,825280,"가짐으로써"],[825790,826080,"학습"],[826110,826580,"파라미터의"],[826580,826940,"개수를"],[827010,827640,"줄여준다는"],[827650,828200,"뜻입니다."]],"textEdited":"행렬인 에하고 비가 가지고 있는 랭크의 엠과 엔이라는 랭크보다 훨씬 더 적은 수의 랭크를 가짐으로써 학습 파라미터의 개수를 줄여준다는 뜻입니다."},{"start":828300,"end":840700,"text":"로라는 어댑터 레이어에 추가하는 방법은 작은 바틀랙 레이어만 추가하는 경우에도 인퍼런스 레이턴시 다시 말하면 뭔가를 제공 입력을 제공했을 때","confidence":0.9417,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[829730,830180,"로라는"],[830730,831360,"어댑터"],[831530,832020,"레이어에"],[832150,832560,"추가하는"],[832650,833020,"방법은"],[833310,833600,"작은"],[833990,834460,"바틀랙"],[834570,834980,"레이어만"],[835070,835460,"추가하는"],[835470,835940,"경우에도"],[836530,837120,"인퍼런스"],[837130,837620,"레이턴시"],[837710,837907,"다시"],[837907,838240,"말하면"],[838350,838880,"뭔가를"],[839210,839460,"제공"],[839590,839900,"입력을"],[839910,840380,"제공했을"],[840390,840540,"때"]],"textEdited":"로라는 어댑터 레이어에 추가하는 방법은 작은 바틀랙 레이어만 추가하는 경우에도 인퍼런스 레이턴시 다시 말하면 뭔가를 제공 입력을 제공했을 때"},{"start":840700,"end":853500,"text":"출력이 만들어지는 레이턴시가 매우 증가하는 모습을 보였기 때문에 사용하기가 어렵습니다. 반면 로라를 이용하는 경우에는 새롭게 학습한 파라미터를 기존 모델을 단순히 합쳐 줌으로써","confidence":0.9667,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[841010,841400,"출력이"],[841450,841980,"만들어지는"],[842090,842740,"레이턴시가"],[842790,843020,"매우"],[843190,843640,"증가하는"],[843690,844040,"모습을"],[844070,844440,"보였기"],[844440,844800,"때문에"],[844950,845460,"사용하기가"],[845530,846080,"어렵습니다."],[846610,846900,"반면"],[847490,847900,"로라를"],[847910,848260,"이용하는"],[848270,848740,"경우에는"],[849190,849640,"새롭게"],[849810,850220,"학습한"],[850330,850920,"파라미터를"],[851330,851580,"기존"],[851650,852020,"모델을"],[852110,852440,"단순히"],[852530,852860,"합쳐"],[852860,853340,"줌으로써"]],"textEdited":"출력이 만들어지는 레이턴시가 매우 증가하는 모습을 보였기 때문에 사용하기가 어렵습니다. 반면 로라를 이용하는 경우에는 새롭게 학습한 파라미터를 기존 모델을 단순히 합쳐 줌으로써"},{"start":853500,"end":867800,"text":"추가적인 연산이 필요하지 않고 인퍼런스의 스피드도 유지하면서 모델의 아키텍처를 변형하지 않고 활용할 수 있다는 뜻입니다. 로라는 보시는 바와 같이 기존 방법론 대비 월등하게 높은 성능을 보이고 있고요.","confidence":0.9746,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[853790,854320,"추가적인"],[854470,854960,"연산이"],[855110,855580,"필요하지"],[855650,856040,"않고"],[856610,857240,"인퍼런스의"],[857310,857800,"스피드도"],[857850,858480,"유지하면서"],[858650,859040,"모델의"],[859130,859820,"아키텍처를"],[859910,860440,"변형하지"],[860440,860680,"않고"],[860710,861060,"활용할"],[861060,861134,"수"],[861134,861314,"있다는"],[861314,861680,"뜻입니다."],[862250,862660,"로라는"],[862790,863087,"보시는"],[863087,863300,"바와"],[863310,863580,"같이"],[863790,864060,"기존"],[864370,864780,"방법론"],[864830,865080,"대비"],[865270,865860,"월등하게"],[866170,866440,"높은"],[866630,866920,"성능을"],[866950,867280,"보이고"],[867280,867780,"있고요."]],"textEdited":"추가적인 연산이 필요하지 않고 인퍼런스의 스피드도 유지하면서 모델의 아키텍처를 변형하지 않고 활용할 수 있다는 뜻입니다. 로라는 보시는 바와 같이 기존 방법론 대비 월등하게 높은 성능을 보이고 있고요."},{"start":867800,"end":879400,"text":"로보타 또는 디볼타와 같은 인코더 모델 뿐만이 아니라 지피티와 같은 디코더 모델에서도 가장 우수한 성능을 기록하고 있다라는 것을 보실 수가 있겠습니다.","confidence":0.8781,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[868110,868640,"로보타"],[868810,869060,"또는"],[869350,869940,"디볼타와"],[869950,870180,"같은"],[870270,870680,"인코더"],[870690,870940,"모델"],[871010,871354,"뿐만이"],[871354,871640,"아니라"],[872170,872820,"지피티와"],[872870,873220,"같은"],[873450,873880,"디코더"],[873910,874520,"모델에서도"],[874910,875160,"가장"],[875430,875740,"우수한"],[875850,876180,"성능을"],[876230,876680,"기록하고"],[876680,877100,"있다라는"],[877100,877380,"것을"],[877490,877780,"보실"],[877790,878020,"수가"],[878030,878720,"있겠습니다."]],"textEdited":"로보타 또는 디볼타와 같은 인코더 모델 뿐만이 아니라 지피티와 같은 디코더 모델에서도 가장 우수한 성능을 기록하고 있다라는 것을 보실 수가 있겠습니다."},{"start":879400,"end":892800,"text":"또한 175 빌리언 파라미터를 가지고 있는 지피티3에서도 로라가 여러 가지 데이터셋에 대해서도 로라 튜닝을 했을 때 단순히 직접 파인튜닝을 하는 거 프리픽스 인베딩을 하는 거,","confidence":0.8521,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[879630,879860,"또한"],[880370,881100,"175"],[881190,881580,"빌리언"],[881650,882180,"파라미터를"],[882210,882620,"가지고"],[882650,882860,"있는"],[883110,884440,"지피티3에서도"],[884630,885060,"로라가"],[885510,885760,"여러"],[885760,885980,"가지"],[886050,886707,"데이터셋에"],[886707,887160,"대해서도"],[887270,887600,"로라"],[887650,888020,"튜닝을"],[888020,888280,"했을"],[888310,888460,"때"],[888790,889220,"단순히"],[889370,889660,"직접"],[889690,890280,"파인튜닝을"],[890280,890480,"하는"],[890480,890620,"거"],[891170,891720,"프리픽스"],[891720,892160,"인베딩을"],[892160,892360,"하는"],[892370,892520,"거,"]],"textEdited":"또한 175 빌리언 파라미터를 가지고 있는 지피티3에서도 로라가 여러 가지 데이터셋에 대해서도 로라 튜닝을 했을 때 단순히 직접 파인튜닝을 하는 거 프리픽스 인베딩을 하는 거,"},{"start":892800,"end":901200,"text":"레이어에 대해서 인베딩을 하는 거 어댑터를 쓰는 것보다도 성능이 높게 나타나는 것을 이 그림으로써 확인하실 수 있습니다.","confidence":0.9727,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[893170,893680,"레이어에"],[893680,894020,"대해서"],[894070,894500,"인베딩을"],[894500,894680,"하는"],[894690,894840,"거"],[895050,895660,"어댑터를"],[895670,895880,"쓰는"],[895880,896400,"것보다도"],[896610,896940,"성능이"],[896990,897340,"높게"],[897370,897820,"나타나는"],[897820,898100,"것을"],[898510,898660,"이"],[898730,899320,"그림으로써"],[899570,900060,"확인하실"],[900070,900220,"수"],[900220,900800,"있습니다."]],"textEdited":"레이어에 대해서 인베딩을 하는 거 어댑터를 쓰는 것보다도 성능이 높게 나타나는 것을 이 그림으로써 확인하실 수 있습니다."},{"start":901200,"end":914100,"text":"또한 이 파라미터 이피션 파인튜닝 같은 경우에는 아래와 같은 방식으로 굉장히 간단하게 적용이 될 수가 있습니다. 어댑터의 경우 그다음에 프리픽스 튜닝의 경우 프롬프트 튜닝의 경우 로라의 경우","confidence":0.9004,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[901510,901760,"또한"],[902150,902300,"이"],[902470,902920,"파라미터"],[902920,903220,"이피션"],[903330,903840,"파인튜닝"],[903850,904140,"같은"],[904140,904560,"경우에는"],[904810,905140,"아래와"],[905170,905440,"같은"],[905570,906040,"방식으로"],[906170,906540,"굉장히"],[906670,907160,"간단하게"],[907210,907540,"적용이"],[907540,907660,"될"],[907670,907880,"수가"],[907880,908240,"있습니다."],[908630,909220,"어댑터의"],[909350,909640,"경우"],[910310,910620,"그다음에"],[910690,911320,"프리픽스"],[911320,911660,"튜닝의"],[911660,911920,"경우"],[912390,912860,"프롬프트"],[912860,913200,"튜닝의"],[913200,913400,"경우"],[913530,913920,"로라의"],[913920,914100,"경우"]],"textEdited":"또한 이 파라미터 이피션 파인튜닝 같은 경우에는 아래와 같은 방식으로 굉장히 간단하게 적용이 될 수가 있습니다. 어댑터의 경우 그다음에 프리픽스 튜닝의 경우 프롬프트 튜닝의 경우 로라의 경우"},{"start":914100,"end":927700,"text":"이제 수도 코드로서 표현을 한 것이고, 이거는 어떤 모델 구조를 많이 바꾸는 것이 아니라 몇 가지의 이 몇 라인의 함수만으로써 이 피프티를 활용할 수 있기 때문에 여러분들이 오픈 소스 모델을","confidence":0.871,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[914330,914500,"이제"],[914570,914920,"수도"],[914920,915280,"코드로서"],[915280,915507,"표현을"],[915507,915640,"한"],[915640,915960,"것이고,"],[916030,916360,"이거는"],[916830,917080,"어떤"],[917190,917460,"모델"],[917510,917840,"구조를"],[918810,919080,"많이"],[919250,919600,"바꾸는"],[919600,919780,"것이"],[919780,920060,"아니라"],[920510,920660,"몇"],[920910,921320,"가지의"],[921450,921600,"이"],[921630,921780,"몇"],[921890,922280,"라인의"],[922470,923280,"함수만으로써"],[923690,923840,"이"],[924210,924840,"피프티를"],[924870,925220,"활용할"],[925220,925307,"수"],[925307,925467,"있기"],[925467,925820,"때문에"],[925850,926340,"여러분들이"],[926470,926720,"오픈"],[926770,927080,"소스"],[927090,927480,"모델을"]],"textEdited":"이제 수도 코드로서 표현을 한 것이고, 이거는 어떤 모델 구조를 많이 바꾸는 것이 아니라 몇 가지의 이 몇 라인의 함수만으로써 이 피프티를 활용할 수 있기 때문에 여러분들이 오픈 소스 모델을"},{"start":927700,"end":941400,"text":"여러분들의 태스크에 맞게 파인튜닝을 할 때는 적극적으로 활용해 보시는 것을 권장합니다. 또한 이 피프티 모든 방법론 소개해 드렸던 방법론들은 허깅 페이스에서의 라이브러리로 구현이 되어 있기 때문에","confidence":0.8718,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[927930,928540,"여러분들의"],[928690,929200,"태스크에"],[929250,929540,"맞게"],[929670,930200,"파인튜닝을"],[930200,930340,"할"],[930340,930560,"때는"],[930710,931320,"적극적으로"],[931410,931760,"활용해"],[931760,932100,"보시는"],[932100,932380,"것을"],[932530,933080,"권장합니다."],[933890,934120,"또한"],[934550,934700,"이"],[934830,935280,"피프티"],[935390,935640,"모든"],[935750,936120,"방법론"],[936810,937034,"소개해"],[937034,937400,"드렸던"],[937550,938140,"방법론들은"],[938330,938600,"허깅"],[938710,939340,"페이스에서의"],[939450,940120,"라이브러리로"],[940190,940527,"구현이"],[940527,940740,"되어"],[940750,940960,"있기"],[940960,941280,"때문에"]],"textEdited":"여러분들의 태스크에 맞게 파인튜닝을 할 때는 적극적으로 활용해 보시는 것을 권장합니다. 또한 이 피프티 모든 방법론 소개해 드렸던 방법론들은 허깅 페이스에서의 라이브러리로 구현이 되어 있기 때문에"},{"start":941400,"end":952200,"text":"실제 학습에서는 굉장히 간편하게 사용될 수 있다라는 팁도 마지막으로 드리겠습니다. 여기까지 해서 세 번째 강의를 마치도록 하겠습니다. 감사합니다.","confidence":0.9305,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[941730,942040,"실제"],[942090,942660,"학습에서는"],[942830,943160,"굉장히"],[943290,943760,"간편하게"],[943850,944240,"사용될"],[944290,944407,"수"],[944407,944840,"있다라는"],[944990,945320,"팁도"],[945770,946260,"마지막으로"],[946290,946900,"드리겠습니다."],[947790,948320,"여기까지"],[948320,948540,"해서"],[948870,949020,"세"],[949030,949340,"번째"],[949430,949780,"강의를"],[949890,950340,"마치도록"],[950350,950860,"하겠습니다."],[951210,951660,"감사합니다."]],"textEdited":"실제 학습에서는 굉장히 간편하게 사용될 수 있다라는 팁도 마지막으로 드리겠습니다. 여기까지 해서 세 번째 강의를 마치도록 하겠습니다. 감사합니다."}],"text":"안녕하세요. 고려대학교 산업경영공학부 강필성 교수입니다. 세 번째 강의로서 텍스트 제너레이션의 두 번째 파트 파라미터 에피션트 튜닝이라는 주제로 강의를 진행하도록 하겠습니다. 우선 파라미터 에피션트 튜닝이라는 타픽을 가지고 온 이유에 대해서 먼저 설명을 드려야 될 것 같습니다. LLM의 발전과 파라미터 에피전트 파인 튜닝의 방법론의 중요성을 설명을 드리고요. 이 PFT 파라미터 에피전트 파인 튜닝을 하는 4가지의 방법, 첫 번째 어댑터 두 번째 프리픽스 튜닝, 세 번째 프롬프트 튜닝, 네 번째 로우 랭크 어댑테이션까지 설명을 드리고 마무리하겠습니다. LLM의 발전과 파라미터 에피션 파인튜닝 방법론의 중요성을 먼저 말씀을 드리겠습니다. LLM은 트랜스포머 구조 기반의 언어 모델들이 다양한 NLP 테스크에서 훌륭한 성능 향상을 보인 이후로 제너럴 퍼포즈를 위한 라즈 랭귀지 모델들이 제한되어 왔습니다. 이를 통해서 학습 데이터와 모델의 파라미터 사이즈가 커질수록 성능이 점점 더 높아지고 왔다라는 것을 알 수가 있습니다. 그런데 이러한 언어 모델을 활용하기 위해서는 일반적으로 범용 웹 데이터를 기반으로 해서 사전 학습을 수행하고 그 이후에 다운스트림 테스크에 맞추어서 파인튜닝을 진행해 왔습니다. 이러한 학습 방법은 특정한 테스크의 성능 향상을 보장한다라는 장점은 있습니다. 그렇기 때문에 언어 모델을 타겟 테스크에 맞게 어댑팅하거나 파인 튜닝 하는 방법론들이 우수한 성능을 기록해 왔습니다. 그래서 이 전통적인 방식에서는 세 가지의 방법으로 유형화할 수가 있는데요. 퓨처 베이스 어프로치 같은 경우에는 사전 학습 모델로부터 임베딩을 먼저 추출을 하고 이 추출된 인베딩으로부터 우리가 원하는 어떠한 테스크를 수행하기 위한 클래식 파이어를 학습하는 방법이 있습니다. 두 번째는 파인 튜닝의 첫 번째 단계로서 사전 학습된 모형에서부터 가장 마지막 단계에 해당하는 아웃풋 레이어를 업데이트하는 방법이 있고요. 세 번째인 두 번째 파인튜닝 단계에서는 모든 레이어들을 업데이트하는 방식이 있습니다. 그래서 보시면 프로즌이라고 되어 있는 거는 사전 학습을 통해서 학습된 웨이트들 파라미터들을 고정을 시키는데 여기에 있는 파인 튜닝의 첫 번째 단계는 후반부를 업데이트를 시킨다면 파인 튜닝의 두 번째 단계는 모든 레이어에 대해서 업데이트를 수행하게 됩니다. 이러한 방법을 성능과 학습 효율이라는 관점으로 나누어 볼 때 모든 파라미터를 학습하는 경우가 가장 높은 성능을 기록하는 것을 확인할 수가 있습니다. 그래서 성능 자체는 가장 높지만 트레이닝 에피션시가 왼쪽이 빠른 거고 오른쪽이 느린 거니까 당연히 모든 이 파라미터들을 학습할 때가 속도는 가장 느리겠죠. 이러한 이제 단점을 이제 극복하기 위해서 인 컨텍스트 러닝 icl이라는 개념이 이제 나왔습니다. gpt3가 발표된 이후에는 파인 튜닝이 없어도 언어 모델을 쉽게 활용할 수 있게 되었고요. 타겟 테스크에 대해서 몇 가지의 예시를 주어 주고 모델에 입력해 주게 될 경우에는 모델을 튜닝하지 않고 쉽게 문제를 풀 수 있게 되었다라는 뜻입니다. 이전 강의에서도 잠깐 언급했었던 제로 샷 같은 경우에는 태스크만 주는 것이고요. 원 샷은 이그젬플 하나 주는 것, 퓨 샷은 여러 개 주는 것을 의미합니다. 이러한 이제 컨벤셔널 어프로치들은 모델을 지속적으로 추가 학습하는 과정 여기서는 시퀀셜 트랜스퍼 러닝 패러다임이라고 표현하는데요. 언어 모델이 기존에 학습한 정보를 있는 현상 캐타스트로픽 포게팅이라는 표현을 쓰기도 합니다. 야기합니다. 또한 모델이 모든 파라미터를 새로운 데이터에 대해서 항상 학습하는 것이 정답은 아니라는 것도 밝혀져 있습니다. 그리고 컨벤셔널 어프로치는 모델의 크기가 점점 커짐에 따라서 모델의 전체 파라미터를 학습하는 것이 어려워지고요. 다운 스트림 테스크마다 독립적으로 학습된 모델을 저장하고 배포할 때마다 막대한 시간과 컴퓨팅 자원이 필요해진다라는 문제점이 있습니다. 그래서 아까 2절에 설명드렸던 신뢰성 측면에서 봤을 때 인 컨텍스트 러닝은 모델을 따로 튜닝할 필요가 없이 문제를 효과적으로 풀이할 수 있고요. 인컨텍스트 러닝을 위해서 단순히 데모스트레이션과 테스트 인풋만을 모델의 입력으로 사용을 하게 되면 가능해진다라는 장점이 있습니다. 하지만 몇몇 경우에는 랜덤한 레이블을 넣어주더라도 문제를 잘 해결한다라는 연구 결과가 존재를 합니다. 이 말이 무슨 얘기냐 하면 데모스트레이션에 대해서 단순히 어떤 긍정과 부정에 대해서 긍정과 부정도 아닌 이상한 레이블을 넣었음에도 불구하고 이 테스트 인풋을 주어졌을 때 정답을 맞출 수 있다라는 얘기입니다. 바꿔 말하면 수학 문제와 정답 쌍을 주고 공부를 시키는데 문제에 대한 전혀 엉뚱한 답을 주었음에도 정답을 풀어낼 수 있다라는 이러한 연구 결과가 존재한다는 뜻이죠. 랜덤 레이블과 골드 레이블이라는 거는 실제로 골드 레이블은 정답을 준 것이고요. 데모가 없다라는 거는 데모스트레이션이 존재하지 않는다라는 뜻이고, 랜덤 레이블은 정답이 아닌 레이블을 주었음에도 불구하고 디모스트레이션이 없었을 때보다 훨씬 더 잘하는 것을 보실 수가 있겠습니다. 파라미터가 많은 라즐 랭귀지 모델을 효율적으로 학습할 할 수 있는 방법이 없을까라는 질문이 등장을 하는 거죠. 여기에 대한 답이 바로 파라미터 에피션트 파임 튜닝입니다. 그래서 단어를 하나씩 하나씩 살펴보자면 무엇을 하는 것이냐 언어 모델을 파인 튜닝 하겠다. 미세 조정을 더 하겠다라는 것이고요. 근데 어떻게 할 것이냐 파라미터의 에피션트한 막 하는 게 아니라 최대한 효율적으로 하겠다라는 뜻입니다. 그래서 지금부터는 용어가 길기 때문에 파라미터 에피션 파인튜닝을 줄여서 피프티라고 부르도록 하겠습니다. 이런 peft는 모델의 모든 파라미터를 학습하지 않고 일부 파라미터만 파인튜닝 하는 방법론으로서 2019년도부터 효과적인 학습을 위해서 다양한 방법론들이 제한이 되고 있습니다. 가장 대표적인 방법론들은 아래의 네 가지의 접근 방식입니다. 트랜스폼 모델의 모든 파라미터를 지금 우리가 사용하고 있는 LM 모델은 거의 대부분 트랜스포머의 디코더에 기반한 모델이기 때문에 트랜스포머 모델의 모든 파라미터를 업데이트하지 않고 각 방법론 별로 소량의 파라미터를 효과적으로 업데이트하는 방식입니다. 여기서 mha는 멀티헤드 어텐션이고 FFN은 피드 포드 뉴럴 네트워크이라는 트랜스포머의 인코더 디코더 블록에서의 컴포넌트라고 보시면 어댑터 튜닝은 멀티 헤드 어텐션과 피드 포드 뉴얼 네트워크에다가 어댑터라고 하는 모듈을 붙인 것이고요. 프리픽스 튜닝 같은 경우에는 각각의 레이어에 프리픽스라고 하는 항목을 붙여주는 것, 프롬프 튜닝은 인풋의 프롬프트에다가 무엇인가 벡터를 붙여주는 것. 로우 랭크 어댑테이션은 레이어별로 훨씬 더 낮은 랭크로 벡터를 행렬을 전환을 해서 연산량을 줄여주는 것을 의미합니다. 이 PFT에 대해서 하나씩 구체적으로 설명을 드리겠습니다. 첫 번째 어댑터입니다. 어댑터는 기존에 이미 학습이 완료된 모델에 각 레이어의 학습이 가능한 피드 포드 네트워크를 삽입하는 구조입니다. 어댑터 레이어는 트랜스포머의 벡터를 더 작은 차원으로 압축을 하고 비선형 변환을 거쳐서 원래 차원으로 복원하는 병목 구조를 가지고 있습니다. 오른쪽에서 이제 보시자면 왼쪽은 트랜스포머라고 본다면 오른쪽이 어댑터 레이어인데 피드 포드 레이어를 통해서 훨씬 더 벡터를 작은 크기로 압축을 하고 비선형 변환을 한번 한 다음에 원래 차원으로 복원하는 겁니다. 이거 굳이 왜 하느냐 결국은 모델의 파라미터를 줄이기 위해서입니다. 그래서 멀티 헤드 어텐션에 해당하는 부분 피드 FRD 네트워크에 해당하는 부분들은 그대로 고정을 시켜놓고 어댑터 레이어에 해당하는 부분만 학습을 시킨다는 뜻이지요. 어댑터 모듈은 파인 튜닝 단계에서 특정한 타겟 테스크에 대해서 최적화되는 것이고요. 나머지 트랜스포머 레이어는 모두 고정이 되는 것입니다. 따라서 이를 통해서 훨씬 더 효율적으로 행렬의 차원이 작은 경우만 학습을 함으로써 효율적인 학습이 가능해지고 위 어댑터라는 개념을 처음 제안한 논문에서는 글루 벤치마크의 9개 테스크의 성능을 평균을 낸 결과 매우 작은 학습 파라미터만으로도 파인튜닝에 근접한 성능을 기록했다는 연구 결과가 있습니다. 여기 보시는 것처럼 x축이 파라미터 학습을 해야 되는 파라미터의 수이고요. 오른쪽이 이제 성능 차이라고 했을 때 파인튜닝을 10에 8승 이상만큼을 한 거와 10의 6승 정도에 그러니까 100분의 1 정도만 학습을 파인튜닝을 어댑터를 통해서 했음에도 불구하고 성능은 거의 유사하다라는 것을 의미합니다. 두 번째는 프리픽스 튜닝이라는 겁니다. 프리픽스라는 게 바로 어떤 무엇인가의 앞쪽에다가 고정적으로 붙인다라는 것이죠. 각 트랜스포머의 레이어에 프리픽스라는 훈련 가능한 벡터를 추가하는 방식입니다. 이거는 가상의 인베딩으로 간주될 수가 있는데요. 원래 파인 튜닝이라면 이 트랜스포머의 모든 이 항목들을 학습을 시켜야 되는데 그게 아니라 프리트레인드 사전 학습된 트랜스포머의 이 벡터들은 고정을 시켜 놓고 그 앞에다가 프리픽스를 붙여줍니다. 이 분홍색 부분만이 학습을 한다라는 뜻이죠. 그래서 각 테스크를 더욱더 잘 풀이하기 위해서 벡터를 최적화를 하고 이거를 기존 모델과 병합할 수 있게 됩니다. 세 번째 프롬프트 튜닝이라는 거는 프리픽스 튜닝과는 다르게 모델의 입력 레이어에 훈련 가능한 프롬프트 벡터를 통합하는 방법입니다. 그래서 여기서 보시면 레이어는 고정이 되어 있는데 인풋과 프롬프트에 대한 노란색 부분만이 학습이 되는 파라미터인 거죠. 근데 여기서의 앞에서 설명드렸던 인풋 문장의 자연어 프럼프팅을 덧붙이는 것과는 약간은 다른 개념으로써 인풋의 인베딩 레이어를 최적화하는 방법론이라고 보시면 되겠습니다. 여기서 보시는 것처럼 프롬프트 자체를 어떤 테스크의 배치마다 태스크가 있을 때 앞에 프롬프트를 붙여주고 해당하는 프롬프트는 여기서 보시는 것처럼 여기 이 테스크 프롬프트에서 이렇게 어 딕셔너리처럼 사용을 할 수 있는 끼워 넣을 수 있는 부분이 되겠습니다. 마지막으로 로우 랭크 어댑테이션 로라 같은 경우에는 사전 학습된 모델의 파라미터를 고정하고 학습 가능한 랭크의 디컴포지션 행렬을 삽입하는 방법입니다. 행렬의 차원을 랭크만큼 줄이는 행렬과 다시 원래의 차원을 크기로 바꿔주는 행렬로 구성이 되고요. 이거를 롤 랭크 키 콤포지션이라고 표현을 합니다. 레이어마다 히든 스테이트의 로라 파라미터를 더해서 파인 튜닝을 하게 됩니다. 최근에 이제 그 PFT 방법론 중에서 가장 널리 쓰이는 것이고요. 여기 보시는 것처럼 왼쪽에 있는 분홍색의 사전 학습된 웨이트 같은 경우에는 이게 굉장히 사이즈가 큽니다. 랭크 자체가 d 모델 d FID f 웨이트라고 했을 때 모델의 사이즈가 더 작지만 여전히 가로 길이가 크죠. 아주 완벽한 개념은 아닙니다마는 예시 비유로서 들어드리면 이 다각형의 면적이 우리가 학습 시켜야 되는 파라미터의 개수에 비례한다라고 보면 로우 랭크 어댑테이션 같은 경우에는 이 중간에 스몰알이라고 하는 훨씬 더 작은 그 수의 벡터로 압축을 한번 한 뒤에 풀어내는 과정입니다. 이 노란색 부분만 학습을 해서 원래 프리트레이드 된 웨이트에다가 더해주면 이 파인 튜닝이 가능하다라는 것이고요. 가장 널리 쓰이는 방법론이라고 좀 전에 말씀드렸습니다. 여기서 보시는 것처럼 로우 랭크 디컴 포지션을 통해서 사용한 파라미터를 델타w라고 봤을 때 원래 기존의 분홍색에 해당하는 이 웨이트에다가 이 로우 랭크 어댑테이션을 통해서 사전 파인 튜닝이 된 웨이트를 단순히 합쳐주는 겁니다. 이거를 가능하게 하는 이유는 뭐냐 하면 여기에 있는 디프블라고 하는 핑크색에 해당하는 요 윗면의 가로 길이와 노란색에 해당하는 윗면의 가로 길이가 같기 때문에 웨이트를 1 대 1로 대응을 시킬 수가 있고요. 그럼으로 인해서 단순히 더해지는 것만으로써 효과를 볼 수 있다라는 겁니다. 그리고 이 스몰알은 원래 데이터인 행렬인 에하고 비가 가지고 있는 랭크의 엠과 엔이라는 랭크보다 훨씬 더 적은 수의 랭크를 가짐으로써 학습 파라미터의 개수를 줄여준다는 뜻입니다. 로라는 어댑터 레이어에 추가하는 방법은 작은 바틀랙 레이어만 추가하는 경우에도 인퍼런스 레이턴시 다시 말하면 뭔가를 제공 입력을 제공했을 때 출력이 만들어지는 레이턴시가 매우 증가하는 모습을 보였기 때문에 사용하기가 어렵습니다. 반면 로라를 이용하는 경우에는 새롭게 학습한 파라미터를 기존 모델을 단순히 합쳐 줌으로써 추가적인 연산이 필요하지 않고 인퍼런스의 스피드도 유지하면서 모델의 아키텍처를 변형하지 않고 활용할 수 있다는 뜻입니다. 로라는 보시는 바와 같이 기존 방법론 대비 월등하게 높은 성능을 보이고 있고요. 로보타 또는 디볼타와 같은 인코더 모델 뿐만이 아니라 지피티와 같은 디코더 모델에서도 가장 우수한 성능을 기록하고 있다라는 것을 보실 수가 있겠습니다. 또한 175 빌리언 파라미터를 가지고 있는 지피티3에서도 로라가 여러 가지 데이터셋에 대해서도 로라 튜닝을 했을 때 단순히 직접 파인튜닝을 하는 거 프리픽스 인베딩을 하는 거, 레이어에 대해서 인베딩을 하는 거 어댑터를 쓰는 것보다도 성능이 높게 나타나는 것을 이 그림으로써 확인하실 수 있습니다. 또한 이 파라미터 이피션 파인튜닝 같은 경우에는 아래와 같은 방식으로 굉장히 간단하게 적용이 될 수가 있습니다. 어댑터의 경우 그다음에 프리픽스 튜닝의 경우 프롬프트 튜닝의 경우 로라의 경우 이제 수도 코드로서 표현을 한 것이고, 이거는 어떤 모델 구조를 많이 바꾸는 것이 아니라 몇 가지의 이 몇 라인의 함수만으로써 이 피프티를 활용할 수 있기 때문에 여러분들이 오픈 소스 모델을 여러분들의 태스크에 맞게 파인튜닝을 할 때는 적극적으로 활용해 보시는 것을 권장합니다. 또한 이 피프티 모든 방법론 소개해 드렸던 방법론들은 허깅 페이스에서의 라이브러리로 구현이 되어 있기 때문에 실제 학습에서는 굉장히 간편하게 사용될 수 있다라는 팁도 마지막으로 드리겠습니다. 여기까지 해서 세 번째 강의를 마치도록 하겠습니다. 감사합니다.","confidence":0.9294926,"speakers":[{"label":"","name":"","edited":false}],"events":[],"eventTypes":[]}