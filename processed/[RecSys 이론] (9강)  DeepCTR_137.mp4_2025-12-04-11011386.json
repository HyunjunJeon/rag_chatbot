{"result":"SUCCEEDED","message":"Succeeded","token":"e2b59e0ff8f5488381f913c9165772af","version":"ncp_v2_v2.4.6-c00dd1b-20250528__v4.2.20.1_ko_firedepartment_20250923_","params":{"service":"ncp","domain":"general","lang":"ko","completion":"sync","callback":"","diarization":{"enable":false,"speakerCountMin":-1,"speakerCountMax":-1},"sed":{"enable":false},"boostings":[],"forbiddens":"","wordAlignment":true,"fullText":true,"noiseFiltering":true,"priority":0,"userdata":{"_ncp_DomainCode":"tpc-boostcamp","_ncp_DomainId":13807,"_ncp_TaskId":42975814,"_ncp_TraceId":"149e02ca860b4e44963f8f3502792c69"}},"progress":100,"keywords":{},"segments":[{"start":0,"end":13700,"text":"안녕하세요. 추천 시스템 강의를 맡은 강사 이준원입니다. 이번 시간에는 지난 8강에 이어서","confidence":0.9864,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[5810,6280,"안녕하세요."],[7310,7600,"추천"],[7630,7940,"시스템"],[8030,8360,"강의를"],[8370,8640,"맡은"],[8890,9160,"강사"],[9330,10340,"이준원입니다."],[11410,11620,"이번"],[11690,12080,"시간에는"],[12350,12560,"지난"],[12710,13120,"8강에"],[13120,13420,"이어서"]],"textEdited":"안녕하세요. 추천 시스템 강의를 맡은 강사 이준원입니다. 이번 시간에는 지난 8강에 이어서"},{"start":13700,"end":24700,"text":"컨텍스트 어 레커멘데이션 모델에 대해서 학습합니다. 이 car 테스크의 대표적인 씨티알 예측 테스크 관련된 모델을 오늘 강의 동안 학습할 것인데요.","confidence":0.7597,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[13990,14507,"컨텍스트"],[14507,14640,"어"],[14710,15340,"레커멘데이션"],[15410,15707,"모델에"],[15707,15980,"대해서"],[16010,16480,"학습합니다."],[17410,17560,"이"],[17610,18020,"car"],[18110,18560,"테스크의"],[18610,19160,"대표적인"],[20250,20620,"씨티알"],[20650,20900,"예측"],[20950,21320,"테스크"],[22210,22560,"관련된"],[22590,22940,"모델을"],[23010,23200,"오늘"],[23330,23540,"강의"],[23540,23740,"동안"],[23770,24080,"학습할"],[24080,24500,"것인데요."]],"textEdited":"컨텍스트 어 레커멘데이션 모델에 대해서 학습합니다. 이 car 테스크의 대표적인 씨티알 예측 테스크 관련된 모델을 오늘 강의 동안 학습할 것인데요."},{"start":24700,"end":39600,"text":"지난 강의에서 로지스틱 리게이션부터 시작하여서 프엠과 프프엠 등의 모델을 다루었고 바로 이 모델들이 씨티알을 예측하는 테스크에서 좋은 성능을 보였다고 말씀드렸습니다. 이번 시간에는 딥러닝의 모델로 넘어옵니다.","confidence":0.8204,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[25410,25640,"지난"],[25710,26100,"강의에서"],[26310,26760,"로지스틱"],[26770,27420,"리게이션부터"],[27420,27880,"시작하여서"],[28310,28700,"프엠과"],[28850,29300,"프프엠"],[29330,29560,"등의"],[29560,29880,"모델을"],[29880,30340,"다루었고"],[30890,31100,"바로"],[31100,31220,"이"],[31220,31740,"모델들이"],[32210,32680,"씨티알을"],[32680,33020,"예측하는"],[33050,33580,"테스크에서"],[33850,34020,"좋은"],[34250,34600,"성능을"],[34790,35300,"보였다고"],[35470,36180,"말씀드렸습니다."],[37310,37520,"이번"],[37550,37940,"시간에는"],[38090,38580,"딥러닝의"],[38580,38880,"모델로"],[38880,39340,"넘어옵니다."]],"textEdited":"지난 강의에서 로지스틱 리게이션부터 시작하여서 프엠과 프프엠 등의 모델을 다루었고 바로 이 모델들이 씨티알을 예측하는 테스크에서 좋은 성능을 보였다고 말씀드렸습니다. 이번 시간에는 딥러닝의 모델로 넘어옵니다."},{"start":39600,"end":53500,"text":"딥러닝 모델을 이용한 CTR 예측 모델들을 차례로 배워보고 어떻게 모델의 표현력과 성능이 발전했는지를 살펴봅시다. 네 오늘 다루게 될 모델은 총 4가지입니다. 먼저 CTR 예측 문제에서","confidence":0.9489,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[40190,40600,"딥러닝"],[40670,41060,"모델을"],[41230,41480,"이용한"],[41570,41860,"CTR"],[41860,42080,"예측"],[42080,42500,"모델들을"],[42530,42820,"차례로"],[42830,43280,"배워보고"],[43990,44320,"어떻게"],[44370,44700,"모델의"],[44730,45220,"표현력과"],[45270,45580,"성능이"],[45670,46440,"발전했는지를"],[46950,47540,"살펴봅시다."],[48930,49080,"네"],[49110,49320,"오늘"],[49410,49687,"다루게"],[49687,49820,"될"],[49830,50140,"모델은"],[50230,50380,"총"],[50470,51120,"4가지입니다."],[51730,52000,"먼저"],[52090,52400,"CTR"],[52400,52620,"예측"],[52630,53100,"문제에서"]],"textEdited":"딥러닝 모델을 이용한 CTR 예측 모델들을 차례로 배워보고 어떻게 모델의 표현력과 성능이 발전했는지를 살펴봅시다. 네 오늘 다루게 될 모델은 총 4가지입니다. 먼저 CTR 예측 문제에서"},{"start":53500,"end":66500,"text":"왜 딥러닝 모델이 효과적인지를 간단하게 살펴본 뒤에 딥러닝 추천 모델 가운데 가장 많이 알려진 와이드 앤 딥과 딥 FM 모델을 학습합니다. 그다음에 유저의 행동 데이터를 인풋 피처로 사용한","confidence":0.9375,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[53710,53860,"왜"],[53950,54360,"딥러닝"],[54360,54620,"모델이"],[54620,55260,"효과적인지를"],[55310,55740,"간단하게"],[55790,56160,"살펴본"],[56190,56420,"뒤에"],[57190,57580,"딥러닝"],[57670,57940,"추천"],[57940,58140,"모델"],[58150,58480,"가운데"],[58550,58800,"가장"],[58830,59020,"많이"],[59070,59420,"알려진"],[59670,60020,"와이드"],[60020,60140,"앤"],[60190,60580,"딥과"],[60810,60960,"딥"],[61350,61600,"FM"],[61610,61980,"모델을"],[62070,62580,"학습합니다."],[63270,63740,"그다음에"],[63890,64220,"유저의"],[64310,64560,"행동"],[64570,65060,"데이터를"],[65270,65520,"인풋"],[65610,65940,"피처로"],[65940,66240,"사용한"]],"textEdited":"왜 딥러닝 모델이 효과적인지를 간단하게 살펴본 뒤에 딥러닝 추천 모델 가운데 가장 많이 알려진 와이드 앤 딥과 딥 FM 모델을 학습합니다. 그다음에 유저의 행동 데이터를 인풋 피처로 사용한"},{"start":66500,"end":80600,"text":"딥 인터레스 네트워크와 트랜스포머를 이용한 비스티 모델까지 모델이 연구를 통해 공개된 시간 순서대로 그리고 모델의 표현력과 복잡도 성능이 향상된 순서대로 차례대로 다뤄보도록 하겠습니다.","confidence":0.9286,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[66710,66860,"딥"],[66970,67400,"인터레스"],[67410,68200,"네트워크와"],[68890,69600,"트랜스포머를"],[69610,69860,"이용한"],[70070,70540,"비스티"],[70570,71120,"모델까지"],[71710,72080,"모델이"],[72090,72460,"연구를"],[72460,72640,"통해"],[72690,73000,"공개된"],[73090,73300,"시간"],[73350,73800,"순서대로"],[74210,74480,"그리고"],[74590,74920,"모델의"],[74950,75420,"표현력과"],[75710,76220,"복잡도"],[76770,77120,"성능이"],[77390,77820,"향상된"],[77950,78420,"순서대로"],[78950,79340,"차례대로"],[79340,79940,"다뤄보도록"],[79940,80480,"하겠습니다."]],"textEdited":"딥 인터레스 네트워크와 트랜스포머를 이용한 비스티 모델까지 모델이 연구를 통해 공개된 시간 순서대로 그리고 모델의 표현력과 복잡도 성능이 향상된 순서대로 차례대로 다뤄보도록 하겠습니다."},{"start":80600,"end":90400,"text":"네 먼저 CTR 예측 문제에서 딥러닝을 활용하는 이유를 간단하게 짚고 넘어가겠습니다. 지난 강의에서도 CTR 예측 문제의 정의와 중요성에 대해서 언급했는데요.","confidence":0.869,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[80810,80960,"네"],[80970,81240,"먼저"],[81410,81760,"CTR"],[81760,81980,"예측"],[81980,82400,"문제에서"],[82550,83120,"딥러닝을"],[83230,83640,"활용하는"],[83730,84060,"이유를"],[84150,84580,"간단하게"],[84710,84980,"짚고"],[84980,85640,"넘어가겠습니다."],[86250,86460,"지난"],[86530,87060,"강의에서도"],[87190,87520,"CTR"],[87520,87720,"예측"],[87730,88080,"문제의"],[88110,88480,"정의와"],[88690,89127,"중요성에"],[89127,89420,"대해서"],[89530,90180,"언급했는데요."]],"textEdited":"네 먼저 CTR 예측 문제에서 딥러닝을 활용하는 이유를 간단하게 짚고 넘어가겠습니다. 지난 강의에서도 CTR 예측 문제의 정의와 중요성에 대해서 언급했는데요."},{"start":90400,"end":103500,"text":"네 시티알 예측 문제란 주어진 아이템을 클릭할 확률을 구하는 문제라고 말씀드렸죠. 그래서 이 시티 예측에 딥러닝이 필요한 이유는 현실의 씨티알 데이터 주로 광고 추천 데이터가 되는데요.","confidence":0.7659,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[91110,91260,"네"],[91290,91600,"시티알"],[91600,91800,"예측"],[91800,92120,"문제란"],[92350,92720,"주어진"],[92850,93260,"아이템을"],[93330,93680,"클릭할"],[93770,94100,"확률을"],[94100,94400,"구하는"],[94490,94900,"문제라고"],[94900,95480,"말씀드렸죠."],[96310,96560,"그래서"],[96710,96860,"이"],[96860,97080,"시티"],[97090,97380,"예측에"],[97390,97860,"딥러닝이"],[97910,98240,"필요한"],[98450,98760,"이유는"],[99810,100180,"현실의"],[100190,100520,"씨티알"],[100520,100800,"데이터"],[101050,101280,"주로"],[101350,101660,"광고"],[102130,102420,"추천"],[102420,102767,"데이터가"],[102767,103380,"되는데요."]],"textEdited":"네 시티알 예측 문제란 주어진 아이템을 클릭할 확률을 구하는 문제라고 말씀드렸죠. 그래서 이 시티 예측에 딥러닝이 필요한 이유는 현실의 씨티알 데이터 주로 광고 추천 데이터가 되는데요."},{"start":103500,"end":116000,"text":"이 데이터는 기존의 선형 모델로 예측하는 데에는 한계가 있습니다. 굉장히 스파스한 그리고 차원이 굉장히 높은 데이터 높은 피처로 이루어져 있고요. 그리고 그 피처 간의 어떠한 관계가","confidence":0.9772,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[103750,103900,"이"],[103900,104300,"데이터는"],[104350,104660,"기존의"],[104660,104920,"선형"],[104930,105340,"모델로"],[105410,105920,"예측하는"],[105920,106200,"데에는"],[106270,106580,"한계가"],[106580,106960,"있습니다."],[107450,107780,"굉장히"],[107810,108460,"스파스한"],[108870,109120,"그리고"],[109410,109740,"차원이"],[109740,109980,"굉장히"],[109990,110260,"높은"],[110330,110680,"데이터"],[111190,111420,"높은"],[111490,111820,"피처로"],[111820,112140,"이루어져"],[112140,112440,"있고요."],[112970,113220,"그리고"],[113250,113400,"그"],[113450,113720,"피처"],[113720,114000,"간의"],[114490,114780,"어떠한"],[115050,115540,"관계가"]],"textEdited":"이 데이터는 기존의 선형 모델로 예측하는 데에는 한계가 있습니다. 굉장히 스파스한 그리고 차원이 굉장히 높은 데이터 높은 피처로 이루어져 있고요. 그리고 그 피처 간의 어떠한 관계가"},{"start":116000,"end":125800,"text":"굉장히 높은 비선형성을 가지고 있기 때문에 기본적인 단순한 선형 모델로 이 문제를 풀기에는 다소 어려움이 있습니다.","confidence":0.9946,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[116370,116740,"굉장히"],[116830,117140,"높은"],[117390,118020,"비선형성을"],[118050,118400,"가지고"],[118400,118567,"있기"],[118567,118940,"때문에"],[119590,120140,"기본적인"],[120750,121160,"단순한"],[121230,121520,"선형"],[121520,121920,"모델로"],[122390,122540,"이"],[122590,123020,"문제를"],[123230,123720,"풀기에는"],[124290,124580,"다소"],[124580,124940,"어려움이"],[124940,125420,"있습니다."]],"textEdited":"굉장히 높은 비선형성을 가지고 있기 때문에 기본적인 단순한 선형 모델로 이 문제를 풀기에는 다소 어려움이 있습니다."},{"start":125800,"end":140700,"text":"그래서 이러한 데이터의 효과적인 딥러닝 모델들이 시티r 예측 문제에 적용되기 시작하였습니다. 첫 번째로 와이드 앤 딥 모델의 등장 배경과 장점을 살펴보고 논문의 제목에 나타나는 와이드 컴포넌트와 딥 컴포넌트를 각각 이해해 봅시다.","confidence":0.9203,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[126070,126320,"그래서"],[126330,126600,"이러한"],[126650,127060,"데이터의"],[127070,127540,"효과적인"],[127790,128200,"딥러닝"],[128230,128720,"모델들이"],[128910,129240,"시티r"],[129240,129480,"예측"],[129490,129860,"문제에"],[129970,130440,"적용되기"],[130450,131120,"시작하였습니다."],[131950,132100,"첫"],[132100,132440,"번째로"],[132710,133027,"와이드"],[133027,133160,"앤"],[133210,133360,"딥"],[133450,133820,"모델의"],[133870,134120,"등장"],[134210,134580,"배경과"],[134670,135060,"장점을"],[135090,135540,"살펴보고"],[136170,136520,"논문의"],[136530,136860,"제목에"],[136860,137240,"나타나는"],[137330,137700,"와이드"],[137850,138400,"컴포넌트와"],[138470,138620,"딥"],[138710,139300,"컴포넌트를"],[139430,139780,"각각"],[139890,140180,"이해해"],[140180,140500,"봅시다."]],"textEdited":"그래서 이러한 데이터의 효과적인 딥러닝 모델들이 시티r 예측 문제에 적용되기 시작하였습니다. 첫 번째로 와이드 앤 딥 모델의 등장 배경과 장점을 살펴보고 논문의 제목에 나타나는 와이드 컴포넌트와 딥 컴포넌트를 각각 이해해 봅시다."},{"start":140700,"end":155700,"text":"네 와이드앤딥 레코멘데이션은 선형 모델과 비선형 모델의 기존이 가지고 있는 장점들을 결합하여서 각각의 모델들이 가지고 있는 장점을 모두 취하고자 한 논문입니다. 본 논문은 구글에서 발표한 논문인데요.","confidence":0.9356,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[141430,141580,"네"],[141590,142140,"와이드앤딥"],[142190,143400,"레코멘데이션은"],[143910,144260,"선형"],[144290,144680,"모델과"],[144730,145100,"비선형"],[145100,145460,"모델의"],[145850,146180,"기존이"],[146190,146447,"가지고"],[146447,146600,"있는"],[146710,147240,"장점들을"],[147240,147820,"결합하여서"],[148510,148880,"각각의"],[148880,149320,"모델들이"],[149350,149627,"가지고"],[149627,149780,"있는"],[149870,150240,"장점을"],[150250,150460,"모두"],[150530,150960,"취하고자"],[150970,151120,"한"],[151250,151760,"논문입니다."],[152770,152920,"본"],[152990,153380,"논문은"],[153810,154240,"구글에서"],[154290,154640,"발표한"],[154730,155240,"논문인데요."]],"textEdited":"네 와이드앤딥 레코멘데이션은 선형 모델과 비선형 모델의 기존이 가지고 있는 장점들을 결합하여서 각각의 모델들이 가지고 있는 장점을 모두 취하고자 한 논문입니다. 본 논문은 구글에서 발표한 논문인데요."},{"start":155700,"end":170300,"text":"플레이 스토어에서 사용자가 검색한 쿼리에 대해서 앱을 추천해 주는 테스크에도 실제로 사용하고 있다고 밝혔습니다. 이 와이드 앤 딥 논문에서는 추천 시스템에서 해결해야 하는 두 가지 과제가 있다고 말하는데요. 바로","confidence":0.9502,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[155930,156240,"플레이"],[156250,156780,"스토어에서"],[156890,157340,"사용자가"],[157410,157800,"검색한"],[157930,158320,"쿼리에"],[158320,158640,"대해서"],[158990,159260,"앱을"],[159610,159914,"추천해"],[159914,160140,"주는"],[160230,160800,"테스크에도"],[161370,161660,"실제로"],[161670,162060,"사용하고"],[162060,162380,"있다고"],[162510,163080,"밝혔습니다."],[163970,164120,"이"],[164120,164440,"와이드"],[164440,164560,"앤"],[164690,164840,"딥"],[165510,166060,"논문에서는"],[166230,166500,"추천"],[166550,167080,"시스템에서"],[167150,167560,"해결해야"],[167560,167720,"하는"],[167770,167887,"두"],[167887,168120,"가지"],[168150,168480,"과제가"],[168480,168820,"있다고"],[168850,169440,"말하는데요."],[169690,169960,"바로"]],"textEdited":"플레이 스토어에서 사용자가 검색한 쿼리에 대해서 앱을 추천해 주는 테스크에도 실제로 사용하고 있다고 밝혔습니다. 이 와이드 앤 딥 논문에서는 추천 시스템에서 해결해야 하는 두 가지 과제가 있다고 말하는데요. 바로"},{"start":170300,"end":181700,"text":"메모라이제이션과 제너럴라이제이션입니다. 먼저 메모라이제이션을 해석하면 말 그대로 암기인데요. 우리가 이미 가지고 있는 학습 데이터에 자주 등장했던 패턴은","confidence":0.947,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[170550,171560,"메모라이제이션과"],[171850,173020,"제너럴라이제이션입니다."],[173390,173620,"먼저"],[173650,174440,"메모라이제이션을"],[175070,175520,"해석하면"],[175590,175740,"말"],[175790,176060,"그대로"],[176150,176780,"암기인데요."],[177610,177880,"우리가"],[177950,178160,"이미"],[178490,178820,"가지고"],[178820,178980,"있는"],[179110,179380,"학습"],[179390,179840,"데이터에"],[179930,180220,"자주"],[180270,180760,"등장했던"],[180830,181220,"패턴은"]],"textEdited":"메모라이제이션과 제너럴라이제이션입니다. 먼저 메모라이제이션을 해석하면 말 그대로 암기인데요. 우리가 이미 가지고 있는 학습 데이터에 자주 등장했던 패턴은"},{"start":181700,"end":196400,"text":"그대로 모델이 암기하여서 예측에 활용해야 한다는 것입니다. 먼저 어떤 남자라는 피처를 가진 사람이 컴퓨터에 해당하는 CTR 데이터가 많이 있을 때 이때 이 y 데이터 클릭 데이터가","confidence":0.982,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[181930,182260,"그대로"],[182310,182660,"모델이"],[182750,183400,"암기하여서"],[183670,184060,"예측에"],[184250,184627,"활용해야"],[184627,184960,"한다는"],[184970,185400,"것입니다."],[185930,186200,"먼저"],[186200,186380,"어떤"],[186450,188180,"남자라는"],[188470,188920,"피처를"],[188930,189160,"가진"],[189190,189540,"사람이"],[190330,191520,"컴퓨터에"],[191520,191880,"해당하는"],[191950,192280,"CTR"],[192650,193060,"데이터가"],[193060,193280,"많이"],[193330,193580,"있을"],[193610,193760,"때"],[194250,194480,"이때"],[194510,194660,"이"],[194660,194780,"y"],[194870,195160,"데이터"],[195310,195540,"클릭"],[195570,195980,"데이터가"]],"textEdited":"그대로 모델이 암기하여서 예측에 활용해야 한다는 것입니다. 먼저 어떤 남자라는 피처를 가진 사람이 컴퓨터에 해당하는 CTR 데이터가 많이 있을 때 이때 이 y 데이터 클릭 데이터가"},{"start":196400,"end":208600,"text":"1로 많이 존재한다고 합시다. 즉 남자 사용자가 컴퓨터에 대한 아이템에 대한 CTR이 높다는 것이죠. 이 피처 관계를 모델이 그대로 학습해줘야 한다는 것이 메모라이제이션입니다.","confidence":0.9674,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[196610,197040,"1로"],[197070,197260,"많이"],[197310,197900,"존재한다고"],[197900,198200,"합시다."],[198850,199000,"즉"],[199110,199400,"남자"],[200770,201220,"사용자가"],[201270,201700,"컴퓨터에"],[201700,201860,"대한"],[201870,202254,"아이템에"],[202254,202420,"대한"],[202470,202940,"CTR이"],[203290,203720,"높다는"],[203910,204240,"것이죠."],[204690,204840,"이"],[204850,205100,"피처"],[205130,205460,"관계를"],[205490,205840,"모델이"],[205850,206160,"그대로"],[206350,206980,"학습해줘야"],[207130,207427,"한다는"],[207427,207660,"것이"],[207670,208560,"메모라이제이션입니다."]],"textEdited":"1로 많이 존재한다고 합시다. 즉 남자 사용자가 컴퓨터에 대한 아이템에 대한 CTR이 높다는 것이죠. 이 피처 관계를 모델이 그대로 학습해줘야 한다는 것이 메모라이제이션입니다."},{"start":208600,"end":214000,"text":"그래서 이 메모라이제이션은 로지스틱 리그레이션과 같은 선형 모델에 딱 맞는 설명인데요.","confidence":0.8763,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[208790,208967,"그래서"],[208967,209100,"이"],[209100,209900,"메모라이제이션은"],[210470,210900,"로지스틱"],[210900,211327,"리그레이션과"],[211327,211540,"같은"],[211610,211880,"선형"],[211880,212180,"모델에"],[212230,212380,"딱"],[212430,212680,"맞는"],[213330,213860,"설명인데요."]],"textEdited":"그래서 이 메모라이제이션은 로지스틱 리그레이션과 같은 선형 모델에 딱 맞는 설명인데요."},{"start":214000,"end":228700,"text":"네 모델이 단순한 만큼 학습을 통한 파라미터의 수렴이 상대적으로 빠르고 모델에 사용되는 피처가 추가되더라도 모델의 확장성과 해석이 용이하다는 장점이 있습니다. 그러나 이러한 학습 모델이 가지는 바로 단점이 있는데요.","confidence":0.9637,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[214230,214380,"네"],[214380,214720,"모델이"],[214770,215160,"단순한"],[215160,215500,"만큼"],[216130,216600,"학습을"],[216610,216820,"통한"],[216910,217500,"파라미터의"],[217510,217840,"수렴이"],[217890,218340,"상대적으로"],[218370,218780,"빠르고"],[219450,219800,"모델에"],[219800,220240,"사용되는"],[220310,220640,"피처가"],[220670,221280,"추가되더라도"],[221730,222060,"모델의"],[222130,222800,"확장성과"],[222810,223160,"해석이"],[223510,224020,"용이하다는"],[224090,224460,"장점이"],[224460,224840,"있습니다."],[224970,225220,"그러나"],[225570,225840,"이러한"],[226030,226320,"학습"],[226320,226620,"모델이"],[226690,227120,"가지는"],[227150,227380,"바로"],[227450,227900,"단점이"],[227900,228280,"있는데요."]],"textEdited":"네 모델이 단순한 만큼 학습을 통한 파라미터의 수렴이 상대적으로 빠르고 모델에 사용되는 피처가 추가되더라도 모델의 확장성과 해석이 용이하다는 장점이 있습니다. 그러나 이러한 학습 모델이 가지는 바로 단점이 있는데요."},{"start":228700,"end":240000,"text":"만약에 어떤 남자에 대해서 기존의 컴퓨터 아이템이 아니라 화장품이라는 아이템을 클릭할 확률을 예측해야 한다고 가정합시다.","confidence":0.9657,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[228970,229300,"만약에"],[229710,229920,"어떤"],[229990,230780,"남자에"],[230780,231120,"대해서"],[231890,232200,"기존의"],[232230,232600,"컴퓨터"],[232610,232994,"아이템이"],[232994,233260,"아니라"],[233510,235620,"화장품이라는"],[237110,237620,"아이템을"],[237730,238060,"클릭할"],[238110,238420,"확률을"],[238430,238860,"예측해야"],[238860,239180,"한다고"],[239350,239880,"가정합시다."]],"textEdited":"만약에 어떤 남자에 대해서 기존의 컴퓨터 아이템이 아니라 화장품이라는 아이템을 클릭할 확률을 예측해야 한다고 가정합시다."},{"start":240000,"end":255000,"text":"이제 기존 학습 데이터를 살펴보니 남자가 화장품을 노출하고 클릭했던 데이터가 굉장히 부족하거나 혹은 아예 없을 경우에 이 메모라이제이션 모델 즉 로지스트 리액션과 같은 단순한 모델은","confidence":0.9546,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[240730,240940,"이제"],[241410,241640,"기존"],[241690,241900,"학습"],[241900,242260,"데이터를"],[242260,242700,"살펴보니"],[242970,243400,"남자가"],[244130,244660,"화장품을"],[245770,246220,"노출하고"],[246250,246660,"클릭했던"],[246670,247040,"데이터가"],[247070,247420,"굉장히"],[247590,248120,"부족하거나"],[248120,248300,"혹은"],[248390,248600,"아예"],[248690,248980,"없을"],[249050,249380,"경우에"],[250050,250200,"이"],[250370,251060,"메모라이제이션"],[251130,251380,"모델"],[251510,251660,"즉"],[252330,252740,"로지스트"],[252750,253094,"리액션과"],[253094,253280,"같은"],[253330,253720,"단순한"],[253730,254100,"모델은"]],"textEdited":"이제 기존 학습 데이터를 살펴보니 남자가 화장품을 노출하고 클릭했던 데이터가 굉장히 부족하거나 혹은 아예 없을 경우에 이 메모라이제이션 모델 즉 로지스트 리액션과 같은 단순한 모델은"},{"start":255000,"end":267300,"text":"일반화 능력이 떨어지기 때문에 이런 남자가 화장품에 대해서 예측할 확률, 클릭할 확률을 제대로 구할 수 없습니다. 그래서 이러한 단점을 극복하기 위한 방법이 바로","confidence":0.9937,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[255310,255660,"일반화"],[255670,255927,"능력이"],[255927,256340,"떨어지기"],[256340,256680,"때문에"],[256990,257160,"이런"],[257270,257820,"남자가"],[257930,258500,"화장품에"],[258500,258840,"대해서"],[259510,259920,"예측할"],[260470,260700,"확률,"],[260890,261240,"클릭할"],[261290,261640,"확률을"],[262050,262420,"제대로"],[263350,263620,"구할"],[263630,263747,"수"],[263747,264180,"없습니다."],[264590,264820,"그래서"],[264820,265060,"이러한"],[265190,265600,"단점을"],[265670,266140,"극복하기"],[266140,266320,"위한"],[266450,266820,"방법이"],[266820,267080,"바로"]],"textEdited":"일반화 능력이 떨어지기 때문에 이런 남자가 화장품에 대해서 예측할 확률, 클릭할 확률을 제대로 구할 수 없습니다. 그래서 이러한 단점을 극복하기 위한 방법이 바로"},{"start":267300,"end":279000,"text":"제너럴라이제이션 일반화입니다. 방금 얘기한 남자와 화장품의 피처 조합은 실제 학습 데이터에 거의 발생하지 않았을지라도 각각 남자와 화장품에 대한 변수","confidence":0.9774,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[267590,268380,"제너럴라이제이션"],[268510,269120,"일반화입니다."],[269590,269840,"방금"],[269850,270160,"얘기한"],[271290,271800,"남자와"],[271810,272260,"화장품의"],[272270,272520,"피처"],[272520,272860,"조합은"],[273130,273360,"실제"],[273410,273640,"학습"],[273640,274040,"데이터에"],[274050,274260,"거의"],[274310,274800,"발생하지"],[274800,275540,"않았을지라도"],[276130,276460,"각각"],[276590,277120,"남자와"],[277610,278200,"화장품에"],[278210,278420,"대한"],[278530,278860,"변수"]],"textEdited":"제너럴라이제이션 일반화입니다. 방금 얘기한 남자와 화장품의 피처 조합은 실제 학습 데이터에 거의 발생하지 않았을지라도 각각 남자와 화장품에 대한 변수"},{"start":279000,"end":290300,"text":"특징을 다른 관계 데이터로부터 발견하여서 이를 적절하게 표현할 수 있습니다. 보통 인베딩을 사용하여서 각각의 피처를 적절하게 표현하는데요.","confidence":0.976,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[279610,279940,"특징을"],[280310,280560,"다른"],[280710,280960,"관계"],[280970,281680,"데이터로부터"],[282150,282700,"발견하여서"],[282710,282900,"이를"],[282950,283400,"적절하게"],[283510,283820,"표현할"],[283820,283927,"수"],[283927,284280,"있습니다."],[284650,284940,"보통"],[285190,286400,"인베딩을"],[286430,287000,"사용하여서"],[287690,288080,"각각의"],[288150,288560,"피처를"],[288670,289100,"적절하게"],[289150,289700,"표현하는데요."]],"textEdited":"특징을 다른 관계 데이터로부터 발견하여서 이를 적절하게 표현할 수 있습니다. 보통 인베딩을 사용하여서 각각의 피처를 적절하게 표현하는데요."},{"start":290300,"end":304200,"text":"두 피처의 상호 작용은 인베딩이 곱해지면서 계산이 되겠죠. 그래서 이러한 일반화 능력은 FM 같은 지난 시간에 배운 모델도 해당이 되지만 이번 시간에 다룰 DNN 기반의 딥러닝 CTR 예측 모델은","confidence":0.8591,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[290490,290640,"두"],[290730,291080,"피처의"],[291090,291300,"상호"],[291300,291620,"작용은"],[291850,292320,"인베딩이"],[292350,293020,"곱해지면서"],[293210,293620,"계산이"],[293620,293940,"되겠죠."],[294950,295160,"그래서"],[295190,295460,"이러한"],[295590,295940,"일반화"],[295950,296300,"능력은"],[297030,297300,"FM"],[297330,297580,"같은"],[297590,297800,"지난"],[297830,298140,"시간에"],[298150,298360,"배운"],[298390,298800,"모델도"],[298800,299120,"해당이"],[299120,299460,"되지만"],[300150,300360,"이번"],[300450,300740,"시간에"],[300740,301040,"다룰"],[301470,301840,"DNN"],[301850,302200,"기반의"],[302270,302680,"딥러닝"],[302910,303240,"CTR"],[303240,303460,"예측"],[303460,303780,"모델은"]],"textEdited":"두 피처의 상호 작용은 인베딩이 곱해지면서 계산이 되겠죠. 그래서 이러한 일반화 능력은 FM 같은 지난 시간에 배운 모델도 해당이 되지만 이번 시간에 다룰 DNN 기반의 딥러닝 CTR 예측 모델은"},{"start":304200,"end":318000,"text":"제너럴라이제이션에 훨씬 더 강한 특징을 가지고 있습니다. 그래서 이 메모라이제이션과 제너럴라이제이션이 가지고 있는 각각의 특징을 결합한 모델이 바로 와이드앤디입니다. 그래서 이 두 가지 과제를 모두 커버한다면은","confidence":0.8747,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[304470,305400,"제너럴라이제이션에"],[305430,305687,"훨씬"],[305687,305820,"더"],[305910,306120,"강한"],[306990,307300,"특징을"],[307300,307527,"가지고"],[307527,307900,"있습니다."],[308690,308847,"그래서"],[308847,308980,"이"],[308980,309840,"메모라이제이션과"],[310050,311480,"제너럴라이제이션이"],[311490,311727,"가지고"],[311727,311880,"있는"],[311930,312260,"각각의"],[312410,312820,"특징을"],[313050,313500,"결합한"],[313870,314200,"모델이"],[314200,314420,"바로"],[314420,315220,"와이드앤디입니다."],[315390,315534,"그래서"],[315534,315627,"이"],[315627,315727,"두"],[315727,315940,"가지"],[315970,316280,"과제를"],[316290,316520,"모두"],[316690,317580,"커버한다면은"]],"textEdited":"제너럴라이제이션에 훨씬 더 강한 특징을 가지고 있습니다. 그래서 이 메모라이제이션과 제너럴라이제이션이 가지고 있는 각각의 특징을 결합한 모델이 바로 와이드앤디입니다. 그래서 이 두 가지 과제를 모두 커버한다면은"},{"start":318000,"end":332900,"text":"사용자의 어떤 검색 쿼리에 맞는 앱을 추천하는 추천 성능이 더 향상되게 됩니다. 먼저 와이드 컴포넌트를 살펴봅시다. 이 부분은 가장 기본적인 선형 모델 로지스틱 리그레션과 거의 비슷한 모델인데요.","confidence":0.9656,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[318370,318960,"사용자의"],[319150,319360,"어떤"],[319510,319780,"검색"],[319830,320180,"쿼리에"],[320180,320400,"맞는"],[320490,320760,"앱을"],[320810,321220,"추천하는"],[321650,321900,"추천"],[321950,322260,"성능이"],[322270,322420,"더"],[322490,322980,"향상되게"],[323590,323940,"됩니다."],[324690,324960,"먼저"],[325150,325540,"와이드"],[325630,326380,"컴포넌트를"],[326410,326940,"살펴봅시다."],[327190,327340,"이"],[327340,327660,"부분은"],[328110,328360,"가장"],[328470,328900,"기본적인"],[328930,329220,"선형"],[329220,329460,"모델"],[330210,330620,"로지스틱"],[330620,331120,"리그레션과"],[331150,331320,"거의"],[331370,331700,"비슷한"],[331710,332240,"모델인데요."]],"textEdited":"사용자의 어떤 검색 쿼리에 맞는 앱을 추천하는 추천 성능이 더 향상되게 됩니다. 먼저 와이드 컴포넌트를 살펴봅시다. 이 부분은 가장 기본적인 선형 모델 로지스틱 리그레션과 거의 비슷한 모델인데요."},{"start":332900,"end":344600,"text":"입력 변수가 n개가 있고 이 엔 개에 해당하는 n개의 파라미터로 이루어져 있습니다. 각각의 변수는 모두 선형 결합으로 이루어져 있고 글로벌 바이오스 도 학습 파라미터로 존재합니다.","confidence":0.9105,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[333170,333440,"입력"],[333510,333860,"변수가"],[334170,334580,"n개가"],[334590,334880,"있고"],[335010,335160,"이"],[335230,335380,"엔"],[335380,335640,"개에"],[335640,336040,"해당하는"],[336290,336780,"n개의"],[336870,338140,"파라미터로"],[338150,338480,"이루어져"],[338480,338840,"있습니다."],[339130,339460,"각각의"],[339470,339820,"변수는"],[339820,340040,"모두"],[340090,340380,"선형"],[340550,340920,"결합으로"],[340920,341220,"이루어져"],[341220,341460,"있고"],[341690,341980,"글로벌"],[342010,342440,"바이오스"],[342950,343100,"도"],[343110,343287,"학습"],[343287,343780,"파라미터로"],[343970,344480,"존재합니다."]],"textEdited":"입력 변수가 n개가 있고 이 엔 개에 해당하는 n개의 파라미터로 이루어져 있습니다. 각각의 변수는 모두 선형 결합으로 이루어져 있고 글로벌 바이오스 도 학습 파라미터로 존재합니다."},{"start":344600,"end":358400,"text":"그러나 이렇게 모델링을 할 경우 이 두 개의 변수 즉 서로 다른 두 개의 변수의 인터랙션 그 관계를 전혀 이 모델을 학습할 수 없는데요. 그래서 와이드 파트에서도 변수 사이의 인터랙션을 표현하기 위해서","confidence":0.9589,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[344850,345100,"그러나"],[345100,345360,"이렇게"],[345410,345860,"모델링을"],[345860,346000,"할"],[346010,346240,"경우"],[346790,346940,"이"],[347050,347200,"두"],[347210,347520,"개의"],[347910,348200,"변수"],[348250,348400,"즉"],[348400,348587,"서로"],[348587,348760,"다른"],[348790,348894,"두"],[348894,349120,"개의"],[349210,349600,"변수의"],[349600,350080,"인터랙션"],[350470,350620,"그"],[350690,351120,"관계를"],[351270,351540,"전혀"],[352130,352280,"이"],[352280,352600,"모델을"],[352610,352960,"학습할"],[352960,353067,"수"],[353067,353440,"없는데요."],[354110,354340,"그래서"],[354370,354780,"와이드"],[354850,355420,"파트에서도"],[355910,356260,"변수"],[356290,356660,"사이의"],[356730,357240,"인터랙션을"],[357250,357640,"표현하기"],[357640,357960,"위해서"]],"textEdited":"그러나 이렇게 모델링을 할 경우 이 두 개의 변수 즉 서로 다른 두 개의 변수의 인터랙션 그 관계를 전혀 이 모델을 학습할 수 없는데요. 그래서 와이드 파트에서도 변수 사이의 인터랙션을 표현하기 위해서"},{"start":358400,"end":369000,"text":"다음과 같은 크로스 프로덕트 트랜스포메이션을 사용합니다. 예를 들면 아래와 같이 성별이 여자라는 변수와 그 사용자의 주요 사용","confidence":0.9913,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[358650,358947,"다음과"],[358947,359160,"같은"],[359250,359560,"크로스"],[359570,360000,"프로덕트"],[360030,360860,"트랜스포메이션을"],[360860,361320,"사용합니다."],[361710,361894,"예를"],[361894,362120,"들면"],[362130,362440,"아래와"],[362450,362760,"같이"],[363510,363920,"성별이"],[364230,364800,"여자라는"],[365350,365740,"변수와"],[366410,366560,"그"],[366590,367160,"사용자의"],[367790,368100,"주요"],[368470,368720,"사용"]],"textEdited":"다음과 같은 크로스 프로덕트 트랜스포메이션을 사용합니다. 예를 들면 아래와 같이 성별이 여자라는 변수와 그 사용자의 주요 사용"},{"start":369000,"end":380400,"text":"랭귀즈가 영어라는 입력 변수가 동시에 1일 경우 그 두 변수의 프로덕트 텀, 크로스 프로덕트 텀 즉 성별이 여자면서 랭귀즈가 영어라는 변수를 모델에 하나 더 추가해 주게 됩니다.","confidence":0.9792,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[369350,369820,"랭귀즈가"],[369870,370460,"영어라는"],[370490,370700,"입력"],[370710,371040,"변수가"],[371070,371420,"동시에"],[371420,371660,"1일"],[371660,371880,"경우"],[372510,372660,"그"],[372670,372820,"두"],[372910,373320,"변수의"],[373330,373760,"프로덕트"],[374090,374240,"텀,"],[374650,374920,"크로스"],[374920,375207,"프로덕트"],[375207,375340,"텀"],[375510,375660,"즉"],[376350,376720,"성별이"],[376720,377220,"여자면서"],[377310,377740,"랭귀즈가"],[377770,378160,"영어라는"],[378250,378680,"변수를"],[378850,379147,"모델에"],[379147,379287,"하나"],[379287,379420,"더"],[379490,379827,"추가해"],[379827,380040,"주게"],[380040,380320,"됩니다."]],"textEdited":"랭귀즈가 영어라는 입력 변수가 동시에 1일 경우 그 두 변수의 프로덕트 텀, 크로스 프로덕트 텀 즉 성별이 여자면서 랭귀즈가 영어라는 변수를 모델에 하나 더 추가해 주게 됩니다."},{"start":380400,"end":389900,"text":"이러한 크로스 프로덕트 텀을 일반화하여서 이렇게 파케스라고 표현을 할 수 있는데요. 이 파이케이에 해당하는 웨이트도 모델에 추가되어 학습에 반영됩니다.","confidence":0.854,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[380630,380900,"이러한"],[381010,381340,"크로스"],[381350,381700,"프로덕트"],[381700,381940,"텀을"],[381950,382620,"일반화하여서"],[382620,382800,"이렇게"],[382890,384300,"파케스라고"],[384310,384534,"표현을"],[384534,384627,"할"],[384627,384727,"수"],[384727,385060,"있는데요."],[385550,385700,"이"],[385750,386367,"파이케이에"],[386367,386760,"해당하는"],[386790,387960,"웨이트도"],[388010,388340,"모델에"],[388390,388800,"추가되어"],[388870,389160,"학습에"],[389190,389820,"반영됩니다."]],"textEdited":"이러한 크로스 프로덕트 텀을 일반화하여서 이렇게 파케스라고 표현을 할 수 있는데요. 이 파이케이에 해당하는 웨이트도 모델에 추가되어 학습에 반영됩니다."},{"start":389900,"end":404300,"text":"다만 가능한 모든 변수들 사이에 크로스 프로덕트를 표현하게 된다면은 학습해야 되는 웨이트가 기하급수적으로 증가하게 됩니다. 그래서 본 논문에서는 주요 피처 2개에 대한 세컨 오더 크로스 프로덕트만을 사용하였습니다.","confidence":0.935,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[390730,390980,"다만"],[391150,391500,"가능한"],[391850,392120,"모든"],[392310,392740,"변수들"],[392740,393020,"사이에"],[393030,393300,"크로스"],[393300,393760,"프로덕트를"],[393770,394320,"표현하게"],[394320,394860,"된다면은"],[395090,395527,"학습해야"],[395527,395720,"되는"],[395730,396220,"웨이트가"],[396670,397400,"기하급수적으로"],[397470,397860,"증가하게"],[397860,398140,"됩니다."],[398270,398500,"그래서"],[398630,398780,"본"],[398810,399320,"논문에서는"],[399390,399640,"주요"],[399730,399980,"피처"],[400050,400460,"2개에"],[400460,400680,"대한"],[401770,402040,"세컨"],[402040,402300,"오더"],[402630,402900,"크로스"],[402900,403460,"프로덕트만을"],[403460,404100,"사용하였습니다."]],"textEdited":"다만 가능한 모든 변수들 사이에 크로스 프로덕트를 표현하게 된다면은 학습해야 되는 웨이트가 기하급수적으로 증가하게 됩니다. 그래서 본 논문에서는 주요 피처 2개에 대한 세컨 오더 크로스 프로덕트만을 사용하였습니다."},{"start":404300,"end":417000,"text":"이 피처는 뒤에 등장하는 모델의 아키텍처에도 표현되어 있습니다. 방금까지 설명한 이 크로스 프로덕트 트랜스포메이션. 사실 이 부분의 모델링은 우리가 지난 시간에 배웠던 이 로지스틱 리그레션에다가","confidence":0.9436,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[404550,404700,"이"],[404710,405080,"피처는"],[405610,405820,"뒤에"],[405850,406260,"등장하는"],[406350,406680,"모델의"],[406680,407460,"아키텍처에도"],[407490,407860,"표현되어"],[407860,408220,"있습니다."],[408630,409040,"방금까지"],[409070,409460,"설명한"],[409550,409700,"이"],[409700,409980,"크로스"],[409980,410380,"프로덕트"],[410970,411580,"트랜스포메이션."],[412030,412240,"사실"],[412310,412460,"이"],[412460,412720,"부분의"],[412720,413160,"모델링은"],[413690,413880,"우리가"],[413880,414100,"지난"],[414130,414440,"시간에"],[414440,414780,"배웠던"],[415230,415380,"이"],[415380,415780,"로지스틱"],[415790,416600,"리그레션에다가"]],"textEdited":"이 피처는 뒤에 등장하는 모델의 아키텍처에도 표현되어 있습니다. 방금까지 설명한 이 크로스 프로덕트 트랜스포메이션. 사실 이 부분의 모델링은 우리가 지난 시간에 배웠던 이 로지스틱 리그레션에다가"},{"start":417000,"end":430000,"text":"2차 폴리노미얼 항을 추가한 것과 거의 동일한 수식입니다. 이 수식에서는 두 개의 서로 다른 변수에 해당하는 학습 파라미터를 정의하고 이 학습 파라미터를 가지고","confidence":0.951,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[417330,417620,"2차"],[417770,418260,"폴리노미얼"],[418290,418580,"항을"],[418990,419340,"추가한"],[419340,419580,"것과"],[419830,420060,"거의"],[420110,420440,"동일한"],[420590,421060,"수식입니다."],[421230,421380,"이"],[421380,421940,"수식에서는"],[423010,423160,"두"],[423170,423440,"개의"],[423470,423627,"서로"],[423627,423820,"다른"],[423890,424300,"변수에"],[424390,424820,"해당하는"],[425730,425980,"학습"],[425980,426580,"파라미터를"],[427050,427560,"정의하고"],[428050,428200,"이"],[428230,428460,"학습"],[428460,428960,"파라미터를"],[428960,429300,"가지고"]],"textEdited":"2차 폴리노미얼 항을 추가한 것과 거의 동일한 수식입니다. 이 수식에서는 두 개의 서로 다른 변수에 해당하는 학습 파라미터를 정의하고 이 학습 파라미터를 가지고"},{"start":430000,"end":438600,"text":"두 피처의 인터랙션을 표현하였는데요. 보시다시피 엔 곱하기 n만큼 즉 n의 제곱 배로 학습 웨이트가 늘어나게 됩니다.","confidence":0.9193,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[430210,430360,"두"],[430510,431000,"피처의"],[431390,431920,"인터랙션을"],[431920,432560,"표현하였는데요."],[433170,433740,"보시다시피"],[434270,434420,"엔"],[434450,434820,"곱하기"],[434830,435340,"n만큼"],[435410,435560,"즉"],[435590,435880,"n의"],[435930,436200,"제곱"],[436200,436460,"배로"],[436650,436920,"학습"],[436920,437340,"웨이트가"],[437710,438087,"늘어나게"],[438087,438380,"됩니다."]],"textEdited":"두 피처의 인터랙션을 표현하였는데요. 보시다시피 엔 곱하기 n만큼 즉 n의 제곱 배로 학습 웨이트가 늘어나게 됩니다."},{"start":438600,"end":447900,"text":"즉 표현할 수 있는 인터랙션의 한계가 명확한 모델이 바로 이 와이드 컴포넌트 구분입니다. 네 딥 컴포넌트는 다음과 같습니다.","confidence":0.9536,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[438870,439020,"즉"],[439710,440080,"표현할"],[440170,440274,"수"],[440274,440420,"있는"],[440470,440980,"인터랙션의"],[440990,441280,"한계가"],[441470,441920,"명확한"],[442210,442540,"모델이"],[442540,442780,"바로"],[442950,443100,"이"],[443100,443460,"와이드"],[443510,444040,"컴포넌트"],[444350,444840,"구분입니다."],[445830,445980,"네"],[446030,446180,"딥"],[446250,446760,"컴포넌트는"],[446760,447060,"다음과"],[447060,447420,"같습니다."]],"textEdited":"즉 표현할 수 있는 인터랙션의 한계가 명확한 모델이 바로 이 와이드 컴포넌트 구분입니다. 네 딥 컴포넌트는 다음과 같습니다."},{"start":447900,"end":457800,"text":"이 팁 컴포넌트는 피드 포워드 뉴럴 네트워크를 사용하는 것 외에는 큰 특징은 없습니다. 아주 단순한 구조인데요. 먼저 이 팁 컴포넌트는 오른쪽에 있는 이 부분입니다.","confidence":0.8866,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[448230,448347,"이"],[448347,448480,"팁"],[448510,449000,"컴포넌트는"],[449010,449280,"피드"],[449290,449567,"포워드"],[449567,449780,"뉴럴"],[449780,450260,"네트워크를"],[450270,450647,"사용하는"],[450647,450780,"것"],[450790,451160,"외에는"],[451730,451880,"큰"],[451990,452320,"특징은"],[452320,452700,"없습니다."],[452750,452940,"아주"],[452990,453320,"단순한"],[453330,453820,"구조인데요."],[455170,455380,"먼저"],[455380,455487,"이"],[455487,455620,"팁"],[455650,456160,"컴포넌트는"],[456250,456680,"오른쪽에"],[456680,456820,"있는"],[456850,457000,"이"],[457090,457620,"부분입니다."]],"textEdited":"이 팁 컴포넌트는 피드 포워드 뉴럴 네트워크를 사용하는 것 외에는 큰 특징은 없습니다. 아주 단순한 구조인데요. 먼저 이 팁 컴포넌트는 오른쪽에 있는 이 부분입니다."},{"start":457800,"end":471100,"text":"총 3 레이어로 구성하였으며 그 각각의 레이어에 대해서는 옐로 함수를 사용했는데요. 연속형 변수는 그대로 이 MLP 레이어에 넣어주고 카테고리형 변수는 피처 인베딩을 한 뒤 전체를 컨택하여서","confidence":0.8533,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[458430,458580,"총"],[458670,458820,"3"],[458910,459300,"레이어로"],[459300,459860,"구성하였으며"],[460110,460260,"그"],[460260,460580,"각각의"],[460590,460914,"레이어에"],[460914,461240,"대해서는"],[461250,461500,"옐로"],[461500,461767,"함수를"],[461767,462320,"사용했는데요."],[463030,463440,"연속형"],[463450,463780,"변수는"],[463790,464160,"그대로"],[465410,465560,"이"],[465590,465980,"MLP"],[465990,466327,"레이어에"],[466327,466780,"넣어주고"],[467190,467840,"카테고리형"],[467910,468220,"변수는"],[468270,468540,"피처"],[468550,468960,"인베딩을"],[468960,469080,"한"],[469080,469220,"뒤"],[469730,470140,"전체를"],[470150,470800,"컨택하여서"]],"textEdited":"총 3 레이어로 구성하였으며 그 각각의 레이어에 대해서는 옐로 함수를 사용했는데요. 연속형 변수는 그대로 이 MLP 레이어에 넣어주고 카테고리형 변수는 피처 인베딩을 한 뒤 전체를 컨택하여서"},{"start":471100,"end":485200,"text":"최종 클릭 여부를 예측하고 있습니다. 그래서 이 두 개의 와이드 모델과 딥 모델을 합쳐서 가운데에 있는 와이드 앤 딥 모델이 완성됩니다. 그래서 두 모델을 합쳐서 다음과 같은 모델을","confidence":0.9613,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[471750,472040,"최종"],[472430,472680,"클릭"],[472710,473080,"여부를"],[473130,473567,"예측하고"],[473567,473960,"있습니다."],[474550,474780,"그래서"],[474780,474900,"이"],[474930,475080,"두"],[475230,475500,"개의"],[475570,475960,"와이드"],[475990,476400,"모델과"],[476610,476760,"딥"],[476810,477280,"모델을"],[477910,478300,"합쳐서"],[478410,478767,"가운데에"],[478767,478920,"있는"],[478990,479267,"와이드"],[479267,479400,"앤"],[479450,479600,"딥"],[480110,480480,"모델이"],[480610,481280,"완성됩니다."],[481930,482120,"그래서"],[482120,482240,"두"],[482290,482580,"모델을"],[482580,482940,"합쳐서"],[483210,483600,"다음과"],[483600,483840,"같은"],[484230,484660,"모델을"]],"textEdited":"최종 클릭 여부를 예측하고 있습니다. 그래서 이 두 개의 와이드 모델과 딥 모델을 합쳐서 가운데에 있는 와이드 앤 딥 모델이 완성됩니다. 그래서 두 모델을 합쳐서 다음과 같은 모델을"},{"start":485200,"end":499700,"text":"이 와이드 앤 딥을 사용하고 있다고 하는데요. 이 수식을 보시면 이쪽 부분이 와이드 파트고요. 이쪽 부분이 딥 파트입니다. 이 와이드 컴포넌트에는 x와 파스가 있는데요. 이 x는 주어진 n개의 변수를 의미하고요.","confidence":0.88,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[485510,485660,"이"],[485660,485914,"와이드"],[485914,486040,"앤"],[486040,486300,"딥을"],[486310,486680,"사용하고"],[486680,486947,"있다고"],[486947,487320,"하는데요."],[488010,488160,"이"],[488160,488520,"수식을"],[488520,488840,"보시면"],[489430,489700,"이쪽"],[489710,490020,"부분이"],[490020,490420,"와이드"],[490470,490940,"파트고요."],[491470,491740,"이쪽"],[491770,492080,"부분이"],[492190,492340,"딥"],[492530,493020,"파트입니다."],[493470,493620,"이"],[493620,493960,"와이드"],[493970,494640,"컴포넌트에는"],[495010,495340,"x와"],[495550,496300,"파스가"],[496300,496660,"있는데요."],[497170,497320,"이"],[497390,497660,"x는"],[497710,498080,"주어진"],[498190,498600,"n개의"],[498630,499020,"변수를"],[499050,499540,"의미하고요."]],"textEdited":"이 와이드 앤 딥을 사용하고 있다고 하는데요. 이 수식을 보시면 이쪽 부분이 와이드 파트고요. 이쪽 부분이 딥 파트입니다. 이 와이드 컴포넌트에는 x와 파스가 있는데요. 이 x는 주어진 n개의 변수를 의미하고요."},{"start":499700,"end":514000,"text":"파스는 아까 설명했던 것처럼 n개의 변수 사이의 크로스 프로덕트 트랜스포메이션을 의미합니다. 이 모델에서는 앞서 언급했듯이 n개의 모든 변수 사이에 가능한 크로스 프로덕트를 정의하지 않았고요. 이 두 개의 피처","confidence":0.8698,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[500110,500680,"파스는"],[501110,501300,"아까"],[501330,501727,"설명했던"],[501727,502020,"것처럼"],[502570,502960,"n개의"],[502970,503207,"변수"],[503207,503500,"사이의"],[503530,503800,"크로스"],[503800,504220,"프로덕트"],[504670,505460,"트랜스포메이션을"],[505460,505900,"의미합니다."],[506210,506360,"이"],[506360,506820,"모델에서는"],[506820,507080,"앞서"],[507150,507820,"언급했듯이"],[508390,508820,"n개의"],[508890,509140,"모든"],[509250,509540,"변수"],[509540,509900,"사이에"],[510030,510360,"가능한"],[510530,510800,"크로스"],[510800,511340,"프로덕트를"],[511450,511880,"정의하지"],[511890,512320,"않았고요."],[512810,512960,"이"],[512970,513120,"두"],[513120,513360,"개의"],[513390,513660,"피처"]],"textEdited":"파스는 아까 설명했던 것처럼 n개의 변수 사이의 크로스 프로덕트 트랜스포메이션을 의미합니다. 이 모델에서는 앞서 언급했듯이 n개의 모든 변수 사이에 가능한 크로스 프로덕트를 정의하지 않았고요. 이 두 개의 피처"},{"start":514000,"end":523600,"text":"사용자가 과거에 설치한 앱과 지금 현재 CTR을 예측할 그 앱의 크로스 프로덕트만을 이 변수에 추가하였습니다.","confidence":0.9129,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[514390,514860,"사용자가"],[514930,515280,"과거에"],[515790,516200,"설치한"],[516250,516580,"앱과"],[516930,517120,"지금"],[517190,517500,"현재"],[517970,518420,"CTR을"],[518420,518800,"예측할"],[519210,519360,"그"],[519410,519700,"앱의"],[520070,520380,"크로스"],[520380,521880,"프로덕트만을"],[522130,522280,"이"],[522310,522700,"변수에"],[522730,523440,"추가하였습니다."]],"textEdited":"사용자가 과거에 설치한 앱과 지금 현재 CTR을 예측할 그 앱의 크로스 프로덕트만을 이 변수에 추가하였습니다."},{"start":523600,"end":537900,"text":"그리고 이 뒷부분은 딥 컴포넌트에 있는 MLP 레이어를 일반화해서 표현한 것입니다. 그래서 와이드 프로덕 와이드 컴포넌트와 딥 컴포넌트의 최종 출력 값을 더한 뒤","confidence":0.9087,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[524450,524594,"그리고"],[524594,524720,"이"],[524770,525300,"뒷부분은"],[526290,526440,"딥"],[526550,527087,"컴포넌트에"],[527087,527260,"있는"],[527310,527680,"MLP"],[527690,528140,"레이어를"],[528190,528780,"일반화해서"],[528790,529120,"표현한"],[529120,529500,"것입니다."],[530030,530280,"그래서"],[530590,531000,"와이드"],[532070,532360,"프로덕"],[532890,533280,"와이드"],[533770,534300,"컴포넌트와"],[534590,534740,"딥"],[534850,535460,"컴포넌트의"],[535670,535940,"최종"],[536030,536280,"출력"],[536310,536660,"값을"],[537170,537540,"더한"],[537570,537720,"뒤"]],"textEdited":"그리고 이 뒷부분은 딥 컴포넌트에 있는 MLP 레이어를 일반화해서 표현한 것입니다. 그래서 와이드 프로덕 와이드 컴포넌트와 딥 컴포넌트의 최종 출력 값을 더한 뒤"},{"start":537900,"end":552300,"text":"글로벌 바이러스를 추가하여서 최종적으로 시그모이드를 취하게 되면은 우리가 예측할 클릭 여부가 완성이 됩니다. 네 마지막으로 본 논문에서 와이드 앤 딥 모델의 성능을 온라인 매트릭과 오프라인 매트릭으로 이용하여서 비교하였습니다.","confidence":0.9285,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[538150,538460,"글로벌"],[538490,538920,"바이러스를"],[538950,539460,"추가하여서"],[540010,540420,"최종적으로"],[540470,541100,"시그모이드를"],[541330,541627,"취하게"],[541627,541960,"되면은"],[542170,542420,"우리가"],[542550,542960,"예측할"],[543130,543360,"클릭"],[543410,544140,"여부가"],[544330,544720,"완성이"],[544750,545080,"됩니다."],[545870,546020,"네"],[546020,546500,"마지막으로"],[546710,546860,"본"],[546970,547440,"논문에서"],[547550,547907,"와이드"],[547907,548040,"앤"],[548070,548220,"딥"],[548290,548600,"모델의"],[548670,549000,"성능을"],[549530,549860,"온라인"],[549910,550440,"매트릭과"],[550470,550820,"오프라인"],[550820,551260,"매트릭으로"],[551260,551660,"이용하여서"],[551730,552260,"비교하였습니다."]],"textEdited":"글로벌 바이러스를 추가하여서 최종적으로 시그모이드를 취하게 되면은 우리가 예측할 클릭 여부가 완성이 됩니다. 네 마지막으로 본 논문에서 와이드 앤 딥 모델의 성능을 온라인 매트릭과 오프라인 매트릭으로 이용하여서 비교하였습니다."},{"start":552300,"end":565900,"text":"이 비교 대상은 기본적인 제너럴 라이즈 리니어 모델 크로스 프로덕트를 추가한 이 와이드 모델이 되는데요. 그리고 추가적으로 딥 모델만 사용한 성능도 같이 비교하였습니다. 이 베이스 라인인 와이드 모델과","confidence":0.9041,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[552570,552720,"이"],[552720,552940,"비교"],[552940,553260,"대상은"],[554010,554560,"기본적인"],[554750,555060,"제너럴"],[555060,555360,"라이즈"],[555370,555680,"리니어"],[555680,555900,"모델"],[556370,556660,"크로스"],[556660,557100,"프로덕트를"],[557110,557420,"추가한"],[557770,557920,"이"],[557920,558260,"와이드"],[559030,559320,"모델이"],[559320,559700,"되는데요."],[560110,560340,"그리고"],[560370,560820,"추가적으로"],[560890,561040,"딥"],[561150,561600,"모델만"],[561770,562080,"사용한"],[562630,562960,"성능도"],[562970,563240,"같이"],[563310,563960,"비교하였습니다."],[564090,564240,"이"],[564240,564514,"베이스"],[564514,564820,"라인인"],[564830,565160,"와이드"],[565170,565560,"모델과"]],"textEdited":"이 비교 대상은 기본적인 제너럴 라이즈 리니어 모델 크로스 프로덕트를 추가한 이 와이드 모델이 되는데요. 그리고 추가적으로 딥 모델만 사용한 성능도 같이 비교하였습니다. 이 베이스 라인인 와이드 모델과"},{"start":565900,"end":578800,"text":"딥 모델 각각은 오프라인에서는 와이드 모델이 좀 더 좋은 예식 성능을 보이고 온라인 서빙을 했을 때는 실제로 얻는 개인는 딥 모델이 컸는데요. 이 두 개의 모델을 합친 와이드 앤 딥은 온라인 성능","confidence":0.9274,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[566150,566300,"딥"],[566350,566600,"모델"],[567130,567520,"각각은"],[568110,568780,"오프라인에서는"],[568780,569080,"와이드"],[569110,569440,"모델이"],[569550,569667,"좀"],[569667,569800,"더"],[569850,570020,"좋은"],[570130,570400,"예식"],[570410,570667,"성능을"],[570667,571000,"보이고"],[571570,571900,"온라인"],[571930,572280,"서빙을"],[572280,572500,"했을"],[572500,572740,"때는"],[572850,573160,"실제로"],[573160,573360,"얻는"],[573390,573700,"개인는"],[573710,573860,"딥"],[573870,574180,"모델이"],[574180,574620,"컸는데요."],[575390,575540,"이"],[575550,575667,"두"],[575667,575880,"개의"],[575880,576200,"모델을"],[576230,576520,"합친"],[576630,576987,"와이드"],[576987,577120,"앤"],[577150,577420,"딥은"],[577930,578260,"온라인"],[578370,578620,"성능"]],"textEdited":"딥 모델 각각은 오프라인에서는 와이드 모델이 좀 더 좋은 예식 성능을 보이고 온라인 서빙을 했을 때는 실제로 얻는 개인는 딥 모델이 컸는데요. 이 두 개의 모델을 합친 와이드 앤 딥은 온라인 성능"},{"start":578800,"end":592300,"text":"과 오프라인 성능 모두 좋은 성능을 보이고 있다고 말합니다. 이상 와이드앤딥 모델에 대한 내용이었습니다. 다음은 딥fm 모델입니다. 딥fm도 와이드앤딥과 굉장히 비슷한 구조를 가지고 있습니다.","confidence":0.9225,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[579050,579200,"과"],[579350,579740,"오프라인"],[579810,580060,"성능"],[580110,580360,"모두"],[580490,580660,"좋은"],[580810,581140,"성능을"],[581210,581487,"보이고"],[581487,581820,"있다고"],[581930,582360,"말합니다."],[583410,583620,"이상"],[583790,584360,"와이드앤딥"],[584370,584634,"모델에"],[584634,584820,"대한"],[584890,585580,"내용이었습니다."],[586110,586460,"다음은"],[586570,587140,"딥fm"],[587140,587600,"모델입니다."],[588430,589100,"딥fm도"],[589190,590000,"와이드앤딥과"],[590110,590440,"굉장히"],[590530,590900,"비슷한"],[591030,591340,"구조를"],[591350,591680,"가지고"],[591680,592080,"있습니다."]],"textEdited":"과 오프라인 성능 모두 좋은 성능을 보이고 있다고 말합니다. 이상 와이드앤딥 모델에 대한 내용이었습니다. 다음은 딥fm 모델입니다. 딥fm도 와이드앤딥과 굉장히 비슷한 구조를 가지고 있습니다."},{"start":592300,"end":600600,"text":"마찬가지로 두 개의 컴포넌트 프엠 컴포넌트와 딥 컴포넌트로 이루어져 있고 이 둘을 합쳐서 딥프엠이라는 이름을 명명하였습니다.","confidence":0.8815,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[592570,593140,"마찬가지로"],[593230,593347,"두"],[593347,593580,"개의"],[593610,594040,"컴포넌트"],[594430,594700,"프엠"],[594790,595340,"컴포넌트와"],[595450,595600,"딥"],[595710,596200,"컴포넌트로"],[596350,596720,"이루어져"],[596720,596960,"있고"],[597390,597540,"이"],[597540,597760,"둘을"],[597760,598080,"합쳐서"],[598450,599240,"딥프엠이라는"],[599350,599640,"이름을"],[599750,600460,"명명하였습니다."]],"textEdited":"마찬가지로 두 개의 컴포넌트 프엠 컴포넌트와 딥 컴포넌트로 이루어져 있고 이 둘을 합쳐서 딥프엠이라는 이름을 명명하였습니다."},{"start":600600,"end":613600,"text":"그래서 딥프엠에는 어떤 컴포넌트가 있는지 그리고 와이드앤 딥에 비해서 어떠한 장점이 있는지도 같이 살펴봅시다. 네 딥프엠 모델은 지난 시간에 배운 팩토라이제이션 모델의","confidence":0.8311,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[601170,601380,"그래서"],[601410,602660,"딥프엠에는"],[602910,603180,"어떤"],[603350,603840,"컴포넌트가"],[603840,604140,"있는지"],[604470,604700,"그리고"],[604710,605160,"와이드앤"],[605190,605447,"딥에"],[605447,605800,"비해서"],[605890,606220,"어떠한"],[606330,606720,"장점이"],[606720,607100,"있는지도"],[607510,607780,"같이"],[607830,608420,"살펴봅시다."],[609150,609300,"네"],[609350,609880,"딥프엠"],[609890,610260,"모델은"],[610930,611160,"지난"],[611230,611540,"시간에"],[611540,611740,"배운"],[611930,612740,"팩토라이제이션"],[612790,613160,"모델의"]],"textEdited":"그래서 딥프엠에는 어떤 컴포넌트가 있는지 그리고 와이드앤 딥에 비해서 어떠한 장점이 있는지도 같이 살펴봅시다. 네 딥프엠 모델은 지난 시간에 배운 팩토라이제이션 모델의"},{"start":613600,"end":627500,"text":"딥러닝 구조를 추가한 모델입니다. CTR 예측에서 뛰어난 성능을 보인 팩토라이제이션 모델 프엠 모델에다가 딥 컴포넌트를 추가하여서 좀 더 복잡한 상호 작용을 캡처하기 위해서 이러한 모델 구조를 설계하였습니다.","confidence":0.8779,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[613850,614280,"딥러닝"],[614290,614620,"구조를"],[614670,615020,"추가한"],[615170,615680,"모델입니다."],[616090,616440,"CTR"],[616450,617200,"예측에서"],[617250,617580,"뛰어난"],[617630,617960,"성능을"],[617960,618140,"보인"],[618310,618860,"팩토라이제이션"],[618860,619060,"모델"],[619370,619620,"프엠"],[619620,620080,"모델에다가"],[620590,620740,"딥"],[620830,621360,"컴포넌트를"],[621410,622000,"추가하여서"],[622570,622720,"좀"],[622730,622880,"더"],[623010,623420,"복잡한"],[623550,623800,"상호"],[623830,624180,"작용을"],[624530,625020,"캡처하기"],[625020,625300,"위해서"],[625410,625660,"이러한"],[625690,625920,"모델"],[625970,626320,"구조를"],[626750,627420,"설계하였습니다."]],"textEdited":"딥러닝 구조를 추가한 모델입니다. CTR 예측에서 뛰어난 성능을 보인 팩토라이제이션 모델 프엠 모델에다가 딥 컴포넌트를 추가하여서 좀 더 복잡한 상호 작용을 캡처하기 위해서 이러한 모델 구조를 설계하였습니다."},{"start":627500,"end":640600,"text":"그래서 와이드 앤 딥 모델과는 달리 이 와이드에 해당하는 프엠 컴포넌트와 딥 컴포넌트가 모두 하나의 입력 값으로 공유하고 있어서 따로 피처 엔지니어링이 필요 없는","confidence":0.911,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[627670,627880,"그래서"],[627880,628147,"와이드"],[628147,628280,"앤"],[628290,628440,"딥"],[628450,628920,"모델과는"],[628930,629200,"달리"],[629730,629880,"이"],[629910,630460,"와이드에"],[630470,630900,"해당하는"],[632230,632460,"프엠"],[632510,633020,"컴포넌트와"],[633610,633760,"딥"],[633870,634660,"컴포넌트가"],[634690,634900,"모두"],[634970,635280,"하나의"],[635390,635660,"입력"],[635730,636640,"값으로"],[637070,637487,"공유하고"],[637487,637780,"있어서"],[638290,638540,"따로"],[638610,638900,"피처"],[638900,639720,"엔지니어링이"],[639750,639980,"필요"],[639980,640180,"없는"]],"textEdited":"그래서 와이드 앤 딥 모델과는 달리 이 와이드에 해당하는 프엠 컴포넌트와 딥 컴포넌트가 모두 하나의 입력 값으로 공유하고 있어서 따로 피처 엔지니어링이 필요 없는"},{"start":640600,"end":651800,"text":"앤드 투 엔드 방식의 모델을 설계하고 있습니다. 네 딥프엠의 등장 배경을 살펴보겠습니다. 추천 시스템에서 피처 인터랙션 계속 다른 모델에서도 언급했던 내용인데요.","confidence":0.8663,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[640890,641140,"앤드"],[641170,641320,"투"],[641320,641600,"엔드"],[641730,642080,"방식의"],[642090,642460,"모델을"],[642590,643060,"설계하고"],[643060,643460,"있습니다."],[644090,644240,"네"],[644450,645060,"딥프엠의"],[645110,645360,"등장"],[645410,645720,"배경을"],[645730,646400,"살펴보겠습니다."],[646750,647020,"추천"],[647050,647540,"시스템에서"],[647610,647900,"피처"],[647970,648440,"인터랙션"],[648950,649180,"계속"],[649190,649420,"다른"],[649430,649940,"모델에서도"],[650090,650900,"언급했던"],[650930,651440,"내용인데요."]],"textEdited":"앤드 투 엔드 방식의 모델을 설계하고 있습니다. 네 딥프엠의 등장 배경을 살펴보겠습니다. 추천 시스템에서 피처 인터랙션 계속 다른 모델에서도 언급했던 내용인데요."},{"start":651800,"end":665000,"text":"그만큼 추천 시스템에서는 이 인플리시트한 피처의 인터랙션을 학습하는 것이 굉장히 중요합니다. 예를 들면 식사 시간 시간이라는 피처와 배달 앱이라는 피처가 두 개가 동시에 만났을 때","confidence":0.9574,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[652110,652500,"그만큼"],[652950,653200,"추천"],[653200,653720,"시스템에서는"],[653720,653860,"이"],[653950,654560,"인플리시트한"],[654610,654980,"피처의"],[654980,655500,"인터랙션을"],[655510,655900,"학습하는"],[655900,656160,"것이"],[656610,656900,"굉장히"],[656950,657460,"중요합니다."],[658570,658734,"예를"],[658734,658920,"들면"],[658990,659260,"식사"],[659260,659460,"시간"],[659810,660260,"시간이라는"],[660330,661040,"피처와"],[661210,661480,"배달"],[661490,661900,"앱이라는"],[662010,663060,"피처가"],[663370,663487,"두"],[663487,663720,"개가"],[663810,664120,"동시에"],[664130,664540,"만났을"],[664570,664720,"때"]],"textEdited":"그만큼 추천 시스템에서는 이 인플리시트한 피처의 인터랙션을 학습하는 것이 굉장히 중요합니다. 예를 들면 식사 시간 시간이라는 피처와 배달 앱이라는 피처가 두 개가 동시에 만났을 때"},{"start":665000,"end":676300,"text":"이 시간에 다운로드 수가 증가한다는 것을 모델링하기 위해서는 세컨 오더 인터랙션이 필요하고요. 10대 남성이 이러한 게임을 좋아한다라는 것을 모델링하기 위해서는","confidence":0.9643,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[665230,665347,"이"],[665347,665680,"시간에"],[666050,666407,"다운로드"],[666407,666640,"수가"],[666910,667520,"증가한다는"],[667520,667840,"것을"],[667870,668420,"모델링하기"],[668420,668820,"위해서는"],[668950,669220,"세컨"],[669220,669420,"오더"],[669420,669860,"인터랙션이"],[669860,670300,"필요하고요."],[671490,671940,"10대"],[672070,672580,"남성이"],[673130,673380,"이러한"],[673470,673740,"게임을"],[673740,674920,"좋아한다라는"],[674920,675160,"것을"],[675160,675680,"모델링하기"],[675680,676100,"위해서는"]],"textEdited":"이 시간에 다운로드 수가 증가한다는 것을 모델링하기 위해서는 세컨 오더 인터랙션이 필요하고요. 10대 남성이 이러한 게임을 좋아한다라는 것을 모델링하기 위해서는"},{"start":676300,"end":688400,"text":"10대라는 피처와 남성이라는 피처 그리고 슈팅 RPG 카테고리라는 피처 3개의 피처가 동시에 인터랙션 했을 때 CTR이 높아진다는 것을 모델링해야 합니다. 근데 이제 기존의 CTR 예측 모델들은","confidence":0.8244,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[676610,677100,"10대라는"],[677190,677560,"피처와"],[677590,678100,"남성이라는"],[678150,678420,"피처"],[678550,678800,"그리고"],[678990,679280,"슈팅"],[679310,679640,"RPG"],[679670,680180,"카테고리라는"],[680210,680460,"피처"],[680990,681340,"3개의"],[681370,681700,"피처가"],[681730,682120,"동시에"],[682130,682560,"인터랙션"],[682560,682800,"했을"],[682830,682980,"때"],[683470,683827,"CTR이"],[683827,684387,"높아진다는"],[684387,684680,"것을"],[685050,685600,"모델링해야"],[685600,685880,"합니다."],[686090,686254,"근데"],[686254,686400,"이제"],[686410,686780,"기존의"],[687090,687380,"CTR"],[687380,687600,"예측"],[687600,688020,"모델들은"]],"textEdited":"10대라는 피처와 남성이라는 피처 그리고 슈팅 RPG 카테고리라는 피처 3개의 피처가 동시에 인터랙션 했을 때 CTR이 높아진다는 것을 모델링해야 합니다. 근데 이제 기존의 CTR 예측 모델들은"},{"start":688400,"end":700100,"text":"로우 오더 혹은 하이 오더 한쪽에만 강한 특징을 보여왔는데요. 이 와이드앤틴 모델 바로 전 슬라이드에서 배운 모델은 이 둘을 통합하여서 문제를 해결했지만","confidence":0.9525,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[688650,688900,"로우"],[688900,689140,"오더"],[689270,689480,"혹은"],[689570,689800,"하이"],[689800,690000,"오더"],[690050,690840,"한쪽에만"],[690970,691200,"강한"],[691830,692140,"특징을"],[692140,692740,"보여왔는데요."],[693610,693760,"이"],[693760,694320,"와이드앤틴"],[694350,694600,"모델"],[694870,695160,"바로"],[695190,695340,"전"],[695470,696040,"슬라이드에서"],[696090,696320,"배운"],[696330,696680,"모델은"],[697370,697520,"이"],[697530,697820,"둘을"],[697870,698400,"통합하여서"],[698830,699160,"문제를"],[699160,699640,"해결했지만"]],"textEdited":"로우 오더 혹은 하이 오더 한쪽에만 강한 특징을 보여왔는데요. 이 와이드앤틴 모델 바로 전 슬라이드에서 배운 모델은 이 둘을 통합하여서 문제를 해결했지만"},{"start":700100,"end":712500,"text":"와이드 컴포넌트의 크로스 프로덕트 트랜스포메이션 같은 어떤 피처 엔지니어링이 추가적으로 필요하다는 단점이 있습니다. 그래서 이 와이드 앤 디이 가지고 있던 피처 엔지니어링 파트, 즉 와이드 파트 대신에","confidence":0.9443,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[700370,700700,"와이드"],[700710,701260,"컴포넌트의"],[701810,702100,"크로스"],[702100,702520,"프로덕트"],[702630,703220,"트랜스포메이션"],[703230,703500,"같은"],[703650,703820,"어떤"],[703950,704240,"피처"],[704290,705000,"엔지니어링이"],[705130,705600,"추가적으로"],[705650,706080,"필요하다는"],[706170,706587,"단점이"],[706587,706960,"있습니다."],[707510,707740,"그래서"],[707740,707860,"이"],[707860,708167,"와이드"],[708167,708300,"앤"],[708310,708540,"디이"],[708590,708900,"가지고"],[708900,709120,"있던"],[709630,709920,"피처"],[709950,710400,"엔지니어링"],[710450,710740,"파트,"],[710950,711100,"즉"],[711100,711400,"와이드"],[711450,711740,"파트"],[711770,712200,"대신에"]],"textEdited":"와이드 컴포넌트의 크로스 프로덕트 트랜스포메이션 같은 어떤 피처 엔지니어링이 추가적으로 필요하다는 단점이 있습니다. 그래서 이 와이드 앤 디이 가지고 있던 피처 엔지니어링 파트, 즉 와이드 파트 대신에"},{"start":712500,"end":725600,"text":"지난 시간에 배운 프엠 모델을 그대로 와이드 컴포넌트로 대체하여서 사용하고 있는 것이 바로 딥프엠입니다. 프엠에는 두 변수 사이에 세컨오더 인터랙션이 그대로 표현되어 있기 때문에","confidence":0.8818,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[712770,713080,"지난"],[713150,713480,"시간에"],[713490,713700,"배운"],[714370,714640,"프엠"],[714650,715020,"모델을"],[715590,715980,"그대로"],[716130,716480,"와이드"],[716530,717100,"컴포넌트로"],[717170,717900,"대체하여서"],[718810,719200,"사용하고"],[719200,719360,"있는"],[719360,719547,"것이"],[719547,719740,"바로"],[719750,720480,"딥프엠입니다."],[720970,721440,"프엠에는"],[721730,721880,"두"],[721970,722260,"변수"],[722260,722600,"사이에"],[722850,723227,"세컨오더"],[723227,723760,"인터랙션이"],[724130,724400,"그대로"],[724450,724860,"표현되어"],[724860,725007,"있기"],[725007,725340,"때문에"]],"textEdited":"지난 시간에 배운 프엠 모델을 그대로 와이드 컴포넌트로 대체하여서 사용하고 있는 것이 바로 딥프엠입니다. 프엠에는 두 변수 사이에 세컨오더 인터랙션이 그대로 표현되어 있기 때문에"},{"start":725600,"end":739100,"text":"강제로 피처 엔지니어링을 통해 크로스 프로덕트 트랜스포메이션을 해줄 필요 없이 그대로 사용하면 됩니다. 그래서 와이드 파트에는 FM을 사용하고 d 파트에는 기본적인 DNN 피드 포워드 n 네트워크를 사용하기 때문에 이 둘을 합쳐서","confidence":0.9068,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[725970,726360,"강제로"],[726450,726700,"피처"],[726700,727220,"엔지니어링을"],[727220,727420,"통해"],[727850,728140,"크로스"],[728140,728480,"프로덕트"],[728480,729014,"트랜스포메이션을"],[729014,729220,"해줄"],[729250,729460,"필요"],[729470,729760,"없이"],[730230,730560,"그대로"],[730650,731080,"사용하면"],[731130,731480,"됩니다."],[731850,732080,"그래서"],[732080,732420,"와이드"],[732450,732860,"파트에는"],[733070,733460,"FM을"],[733460,733900,"사용하고"],[734270,734420,"d"],[734510,734980,"파트에는"],[735250,735700,"기본적인"],[735730,736080,"DNN"],[736430,736680,"피드"],[736680,736907,"포워드"],[736907,737040,"n"],[737040,737420,"네트워크를"],[737420,737760,"사용하기"],[737760,738080,"때문에"],[738230,738347,"이"],[738347,738580,"둘을"],[738580,738920,"합쳐서"]],"textEdited":"강제로 피처 엔지니어링을 통해 크로스 프로덕트 트랜스포메이션을 해줄 필요 없이 그대로 사용하면 됩니다. 그래서 와이드 파트에는 FM을 사용하고 d 파트에는 기본적인 DNN 피드 포워드 n 네트워크를 사용하기 때문에 이 둘을 합쳐서"},{"start":739100,"end":750600,"text":"딥 FM이라고 명명하고 있습니다. 다음 그림과 수식을 통해 좀 더 자세히 살펴보겠습니다. 먼저 FM 컴포넌트인데요. FM은 지난 8강에서 배웠던 이 FM 포뮬라와","confidence":0.9265,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[739310,739460,"딥"],[739670,740200,"FM이라고"],[740290,740760,"명명하고"],[740760,741160,"있습니다."],[742310,742540,"다음"],[742670,743040,"그림과"],[743070,743400,"수식을"],[743400,743600,"통해"],[743610,743760,"좀"],[743760,743900,"더"],[744190,744500,"자세히"],[744510,745160,"살펴보겠습니다."],[745310,745560,"먼저"],[745650,745900,"FM"],[745970,746660,"컴포넌트인데요."],[747390,747760,"FM은"],[747930,748160,"지난"],[748410,748880,"8강에서"],[748880,749180,"배웠던"],[749190,749340,"이"],[749450,749720,"FM"],[749790,750300,"포뮬라와"]],"textEdited":"딥 FM이라고 명명하고 있습니다. 다음 그림과 수식을 통해 좀 더 자세히 살펴보겠습니다. 먼저 FM 컴포넌트인데요. FM은 지난 8강에서 배웠던 이 FM 포뮬라와"},{"start":750600,"end":764900,"text":"수식이 완전히 일치합니다. 그래서 이 FM 모델 수식의 변수들을 살펴보면은 x1부터 xn에 피처가 있고요. 그다음에 두 개의 피처 인터랙션을 표현하는 팩토라이제이션 파라미터","confidence":0.8039,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[750870,751220,"수식이"],[751250,751680,"완전히"],[751730,752240,"일치합니다."],[752530,752707,"그래서"],[752707,752840,"이"],[752930,753200,"FM"],[753610,753880,"모델"],[754250,754620,"수식의"],[755610,756040,"변수들을"],[756040,756580,"살펴보면은"],[757970,758380,"x1부터"],[758470,758960,"xn에"],[759230,759580,"피처가"],[759580,759900,"있고요."],[760270,760660,"그다음에"],[761210,761360,"두"],[761410,761700,"개의"],[761790,762060,"피처"],[762130,762780,"인터랙션을"],[762810,763180,"표현하는"],[763630,764240,"팩토라이제이션"],[764270,764680,"파라미터"]],"textEdited":"수식이 완전히 일치합니다. 그래서 이 FM 모델 수식의 변수들을 살펴보면은 x1부터 xn에 피처가 있고요. 그다음에 두 개의 피처 인터랙션을 표현하는 팩토라이제이션 파라미터"},{"start":764900,"end":776400,"text":"가 있죠. 그래서 이것을 기억하시면서 왼쪽에 있는 그림을 살펴봅시다. 먼저 각각의 필드가 하나하나의 피처를 의미하고 이 디프엠에서는 모두 스파스한 피처로 구성하고 있습니다.","confidence":0.9053,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[765230,765380,"가"],[765390,765620,"있죠."],[766190,766380,"그래서"],[766380,766780,"이것을"],[767510,768160,"기억하시면서"],[768290,768640,"왼쪽에"],[768640,768780,"있는"],[768830,769140,"그림을"],[769210,769720,"살펴봅시다."],[770070,770300,"먼저"],[770350,770720,"각각의"],[770790,771180,"필드가"],[771290,771780,"하나하나의"],[771830,772240,"피처를"],[772240,772640,"의미하고"],[773090,773240,"이"],[773240,773940,"디프엠에서는"],[773940,774140,"모두"],[774210,774740,"스파스한"],[774770,775160,"피처로"],[775610,775987,"구성하고"],[775987,776360,"있습니다."]],"textEdited":"가 있죠. 그래서 이것을 기억하시면서 왼쪽에 있는 그림을 살펴봅시다. 먼저 각각의 필드가 하나하나의 피처를 의미하고 이 디프엠에서는 모두 스파스한 피처로 구성하고 있습니다."},{"start":776400,"end":787500,"text":"먼저 바로 이 어디션이라고 되어 있는 이 부분으로 연결되어 있는 선은 이 1차 텀을 의미합니다. 각각의 피처가 갖는","confidence":0.9532,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[776690,776960,"먼저"],[777090,777400,"바로"],[777490,777640,"이"],[777710,778520,"어디션이라고"],[779150,779360,"되어"],[779360,779540,"있는"],[779570,779687,"이"],[779687,780060,"부분으로"],[780930,781354,"연결되어"],[781354,781500,"있는"],[781500,781820,"선은"],[782770,782920,"이"],[783370,783720,"1차"],[783750,784040,"텀을"],[784040,784460,"의미합니다."],[785130,785640,"각각의"],[786410,786840,"피처가"],[786910,787160,"갖는"]],"textEdited":"먼저 바로 이 어디션이라고 되어 있는 이 부분으로 연결되어 있는 선은 이 1차 텀을 의미합니다. 각각의 피처가 갖는"},{"start":787500,"end":792700,"text":"그에 대응되는 웨이트가 학습되는 것이고요. 이제 그다음에","confidence":0.964,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[787750,788400,"그에"],[788430,788940,"대응되는"],[789350,790220,"웨이트가"],[790290,790680,"학습되는"],[790680,791060,"것이고요."],[791570,791740,"이제"],[791750,792280,"그다음에"]],"textEdited":"그에 대응되는 웨이트가 학습되는 것이고요. 이제 그다음에"},{"start":792700,"end":805700,"text":"세컨오더 피처 인터랙션 FM에서 팩토라이제이션 파라미터 텀에 해당하는 부분인데요. 각각의 피처는 동일한 차원의 인베딩 여기서는 5개의 차원인데요. 5차원으로","confidence":0.9582,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[793130,793600,"세컨오더"],[793600,793860,"피처"],[795130,795580,"인터랙션"],[796310,796760,"FM에서"],[797390,797960,"팩토라이제이션"],[797960,798340,"파라미터"],[798350,798680,"텀에"],[798680,799040,"해당하는"],[799040,799500,"부분인데요."],[800010,800400,"각각의"],[800410,800800,"피처는"],[801050,801380,"동일한"],[801470,801820,"차원의"],[802210,802560,"인베딩"],[802650,803040,"여기서는"],[803310,803660,"5개의"],[803670,804180,"차원인데요."],[804650,805280,"5차원으로"]],"textEdited":"세컨오더 피처 인터랙션 FM에서 팩토라이제이션 파라미터 텀에 해당하는 부분인데요. 각각의 피처는 동일한 차원의 인베딩 여기서는 5개의 차원인데요. 5차원으로"},{"start":805700,"end":820500,"text":"인베딩 즉 팩토라이제이션 된 다음에 이들끼리 서로 내적으로 인해서 서로 피처 간의 인터랙션을 학습합니다. 그리고 이 모든 것이 더해지면 다음과 같은 에프엠 포뮬라와 동일한 모델을 이렇게 비주얼라이즈 할 수 있습니다.","confidence":0.9145,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[806030,806400,"인베딩"],[806430,806580,"즉"],[806630,807240,"팩토라이제이션"],[807240,807380,"된"],[807410,807740,"다음에"],[807790,808147,"이들끼리"],[808147,808400,"서로"],[808790,809300,"내적으로"],[809300,809580,"인해서"],[810090,810360,"서로"],[810430,810700,"피처"],[810700,810940,"간의"],[810950,811540,"인터랙션을"],[811750,812260,"학습합니다."],[812930,813140,"그리고"],[813140,813280,"이"],[813280,813500,"모든"],[813500,813800,"것이"],[814090,814580,"더해지면"],[815150,815447,"다음과"],[815447,815640,"같은"],[815670,816000,"에프엠"],[816050,816500,"포뮬라와"],[816650,817000,"동일한"],[817670,818040,"모델을"],[818350,818600,"이렇게"],[818810,819440,"비주얼라이즈"],[819590,819740,"할"],[819740,819847,"수"],[819847,820200,"있습니다."]],"textEdited":"인베딩 즉 팩토라이제이션 된 다음에 이들끼리 서로 내적으로 인해서 서로 피처 간의 인터랙션을 학습합니다. 그리고 이 모든 것이 더해지면 다음과 같은 에프엠 포뮬라와 동일한 모델을 이렇게 비주얼라이즈 할 수 있습니다."},{"start":820500,"end":830700,"text":"다음은 딥 컴포넌트입니다. 딥 컴포넌트는 좀 더 고차원 즉 하이오더 피처 인터랙션을 모델링 해 줍니다. 여기서 모든 피처는 각각 다","confidence":0.937,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[821270,821620,"다음은"],[821750,821900,"딥"],[821990,822680,"컴포넌트입니다."],[823430,823580,"딥"],[823670,824260,"컴포넌트는"],[824850,825000,"좀"],[825010,825160,"더"],[825350,825760,"고차원"],[825870,826020,"즉"],[826090,826540,"하이오더"],[826630,826880,"피처"],[826890,827400,"인터랙션을"],[827400,827654,"모델링"],[827654,827747,"해"],[827747,828060,"줍니다."],[828450,828740,"여기서"],[828810,829040,"모든"],[829150,829540,"피처는"],[829870,830220,"각각"],[830330,830480,"다"]],"textEdited":"다음은 딥 컴포넌트입니다. 딥 컴포넌트는 좀 더 고차원 즉 하이오더 피처 인터랙션을 모델링 해 줍니다. 여기서 모든 피처는 각각 다"},{"start":830700,"end":842900,"text":"동일한 차원, 여기서는 5차원인데요. k 차원으로 인베딩 되는데요. 여기서 이 인베딩 되는 이 인베딩 파라미터는 FM에서 사용하는 가중치와 동일하게 사용합니다. 그래서","confidence":0.9566,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[831010,831340,"동일한"],[831390,831620,"차원,"],[831990,832320,"여기서는"],[832320,832940,"5차원인데요."],[833010,833160,"k"],[833250,833620,"차원으로"],[833670,834040,"인베딩"],[834040,834440,"되는데요."],[834990,835280,"여기서"],[835430,835580,"이"],[835650,836080,"인베딩"],[836090,836360,"되는"],[836810,836960,"이"],[837130,837500,"인베딩"],[837530,838200,"파라미터는"],[838910,839360,"FM에서"],[839360,839740,"사용하는"],[839810,840300,"가중치와"],[840410,841160,"동일하게"],[841190,841640,"사용합니다."],[842030,842300,"그래서"]],"textEdited":"동일한 차원, 여기서는 5차원인데요. k 차원으로 인베딩 되는데요. 여기서 이 인베딩 되는 이 인베딩 파라미터는 FM에서 사용하는 가중치와 동일하게 사용합니다. 그래서"},{"start":842900,"end":857500,"text":"이 딥 레이어의 인베딩과 FM 레이어의 인베딩이 따로따로 학습되지 않고 한꺼번에 엔드 투 엔드로 학습되는 것이죠. 각각의 인베딩은 모두 컨케이트네이트 돼서 쭉 가로로 붙게 됩니다. 그래서 이 컨케이트네이트 된 전체 임베딩","confidence":0.8664,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[843190,843340,"이"],[843410,843560,"딥"],[843770,844240,"레이어의"],[844290,844800,"인베딩과"],[844950,845200,"FM"],[845230,845520,"레이어의"],[845520,845960,"인베딩이"],[845960,846340,"따로따로"],[846340,846700,"학습되지"],[846700,847000,"않고"],[847170,847640,"한꺼번에"],[847670,847867,"엔드"],[847867,847967,"투"],[847967,848300,"엔드로"],[848590,848987,"학습되는"],[848987,849320,"것이죠."],[849630,849960,"각각의"],[849960,850420,"인베딩은"],[850690,850940,"모두"],[851070,851687,"컨케이트네이트"],[851687,851920,"돼서"],[852550,852700,"쭉"],[852850,853200,"가로로"],[853870,854167,"붙게"],[854167,854460,"됩니다."],[855070,855240,"그래서"],[855240,855360,"이"],[855370,856014,"컨케이트네이트"],[856014,856140,"된"],[856230,856520,"전체"],[856590,856980,"임베딩"]],"textEdited":"이 딥 레이어의 인베딩과 FM 레이어의 인베딩이 따로따로 학습되지 않고 한꺼번에 엔드 투 엔드로 학습되는 것이죠. 각각의 인베딩은 모두 컨케이트네이트 돼서 쭉 가로로 붙게 됩니다. 그래서 이 컨케이트네이트 된 전체 임베딩"},{"start":857500,"end":871000,"text":"벡터가 map 레이어에 처음에 인플 레이어가 되고 이 이후에 엘게의 피디 4 디뉴 네트워크를 쌓게 되면 마지막 레이어에서 최종적으로 클릭 여부를 예측할 수 있게 됩니다.","confidence":0.8106,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[858330,858700,"벡터가"],[859130,859520,"map"],[859520,859880,"레이어에"],[859930,860240,"처음에"],[860250,860500,"인플"],[860510,860860,"레이어가"],[860860,861100,"되고"],[861950,862100,"이"],[862230,862580,"이후에"],[862810,863440,"엘게의"],[863670,863920,"피디"],[863930,864080,"4"],[864110,864340,"디뉴"],[864350,864980,"네트워크를"],[865510,865767,"쌓게"],[865767,865980,"되면"],[866070,866400,"마지막"],[866910,867340,"레이어에서"],[867730,868300,"최종적으로"],[868610,868840,"클릭"],[868850,869740,"여부를"],[869790,870200,"예측할"],[870210,870314,"수"],[870314,870467,"있게"],[870467,870780,"됩니다."]],"textEdited":"벡터가 map 레이어에 처음에 인플 레이어가 되고 이 이후에 엘게의 피디 4 디뉴 네트워크를 쌓게 되면 마지막 레이어에서 최종적으로 클릭 여부를 예측할 수 있게 됩니다."},{"start":871000,"end":873300,"text":"그래서 전체 구조를 보시면","confidence":0.9975,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[871270,871560,"그래서"],[871690,872020,"전체"],[872030,872320,"구조를"],[872320,872640,"보시면"]],"textEdited":"그래서 전체 구조를 보시면"},{"start":873300,"end":887100,"text":"FM과 딥 컴포넌트 각각의 임베딩을 공유한다는 것을 더 쉽게 이해할 수 있습니다. 각각의 필드 각각의 피처가 인베딩 된 이후에 FM 컴포넌트 쪽으로 팩토라이제이션 즉 세컨오더","confidence":0.8753,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[873710,874160,"FM과"],[874350,874500,"딥"],[874890,875340,"컴포넌트"],[875590,875980,"각각의"],[876490,876940,"임베딩을"],[877010,877547,"공유한다는"],[877547,877820,"것을"],[877820,877960,"더"],[878030,878360,"쉽게"],[878430,878740,"이해할"],[878740,878834,"수"],[878834,879200,"있습니다."],[879550,879960,"각각의"],[880170,880460,"필드"],[880510,880820,"각각의"],[880870,881280,"피처가"],[881670,882060,"인베딩"],[882110,882260,"된"],[882450,882840,"이후에"],[883550,883800,"FM"],[883870,884260,"컴포넌트"],[884270,884640,"쪽으로"],[885370,886060,"팩토라이제이션"],[886110,886260,"즉"],[886290,886780,"세컨오더"]],"textEdited":"FM과 딥 컴포넌트 각각의 임베딩을 공유한다는 것을 더 쉽게 이해할 수 있습니다. 각각의 필드 각각의 피처가 인베딩 된 이후에 FM 컴포넌트 쪽으로 팩토라이제이션 즉 세컨오더"},{"start":887100,"end":898800,"text":"인터랙션이 이루어지는 부분이 있고요. 그리고 이 전체가 컨케이트네이트 돼서 딥 컴포넌트의 인풋으로 사용되는 것이죠. 그래서 정리하면은 딥fm은 와이드 앤 딥과 비슷하게","confidence":0.9009,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[887410,887840,"인터랙션이"],[887840,888240,"이루어지는"],[888250,888507,"부분이"],[888507,888800,"있고요."],[889210,889400,"그리고"],[889400,889520,"이"],[889530,890120,"전체가"],[890450,891027,"컨케이트네이트"],[891027,891260,"돼서"],[892010,892160,"딥"],[892270,893040,"컴포넌트의"],[893070,893540,"인풋으로"],[893550,893980,"사용되는"],[893980,894280,"것이죠."],[894970,895160,"그래서"],[895170,895740,"정리하면은"],[895870,896460,"딥fm은"],[896910,897234,"와이드"],[897234,897360,"앤"],[897390,897720,"딥과"],[897790,898300,"비슷하게"]],"textEdited":"인터랙션이 이루어지는 부분이 있고요. 그리고 이 전체가 컨케이트네이트 돼서 딥 컴포넌트의 인풋으로 사용되는 것이죠. 그래서 정리하면은 딥fm은 와이드 앤 딥과 비슷하게"},{"start":898800,"end":913700,"text":"FM 컴포넌트와 딥 컴포넌트가 가지고 있는 각각의 장점, 아까 이야기했던 메모라이제이션과 제널라이제이션의 장점을 모두 활용하여서 좋은 예측 성능을 보입니다. 다음 부분은 딥fm을 비슷한 시기에 발표했던 다른 딥 CTR 모델","confidence":0.9288,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[899170,899400,"FM"],[899470,900120,"컴포넌트와"],[900230,900380,"딥"],[900490,901000,"컴포넌트가"],[901050,901287,"가지고"],[901287,901460,"있는"],[901530,901860,"각각의"],[901930,902260,"장점,"],[902710,902900,"아까"],[902930,903340,"이야기했던"],[903410,904300,"메모라이제이션과"],[904790,905620,"제널라이제이션의"],[905690,906100,"장점을"],[906130,906340,"모두"],[906370,906920,"활용하여서"],[907430,907620,"좋은"],[907750,908000,"예측"],[908050,908380,"성능을"],[908380,908760,"보입니다."],[909230,909460,"다음"],[909490,909820,"부분은"],[909990,910660,"딥fm을"],[910770,911140,"비슷한"],[911210,911500,"시기에"],[911570,912020,"발표했던"],[912330,912560,"다른"],[912670,912820,"딥"],[912870,913240,"CTR"],[913250,913480,"모델"]],"textEdited":"FM 컴포넌트와 딥 컴포넌트가 가지고 있는 각각의 장점, 아까 이야기했던 메모라이제이션과 제널라이제이션의 장점을 모두 활용하여서 좋은 예측 성능을 보입니다. 다음 부분은 딥fm을 비슷한 시기에 발표했던 다른 딥 CTR 모델"},{"start":913700,"end":928200,"text":"들과의 성능과 특징을 비교한 부분입니다. 이 DFM 외에 위에 있는 FNN, PNN, ydnt 같은 다른 CTR 예측 논문들도 이 DFM이 등장하기 전에 발표가 되었는데요. 먼저 FNN 같은 경우에는","confidence":0.8458,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[913890,914600,"들과의"],[914970,915680,"성능과"],[915830,916160,"특징을"],[916230,916520,"비교한"],[916520,916960,"부분입니다."],[917210,917360,"이"],[917360,917860,"DFM"],[917890,918140,"외에"],[918530,918780,"위에"],[918790,918980,"있는"],[919490,919860,"FNN,"],[919990,920360,"PNN,"],[920450,921020,"ydnt"],[921070,921340,"같은"],[921430,921620,"다른"],[921710,921954,"CTR"],[921954,922140,"예측"],[922140,922640,"논문들도"],[923230,923380,"이"],[923430,923940,"DFM이"],[923950,924440,"등장하기"],[924590,924960,"전에"],[925170,925487,"발표가"],[925487,926000,"되었는데요."],[926590,926820,"먼저"],[926930,927280,"FNN"],[927290,927527,"같은"],[927527,927940,"경우에는"]],"textEdited":"들과의 성능과 특징을 비교한 부분입니다. 이 DFM 외에 위에 있는 FNN, PNN, ydnt 같은 다른 CTR 예측 논문들도 이 DFM이 등장하기 전에 발표가 되었는데요. 먼저 FNN 같은 경우에는"},{"start":928200,"end":941900,"text":"에프엠 모델을 사용하지만 엔드 투 엔드로 학습하지 않고 에프엠 모델을 학습한 이후에 그 인베딩을 가지고 와서 다시 딥러닝 모델을 사용합니다. 그래서 프리트레이닝이 반드시 필요한 부분이 있죠.","confidence":0.804,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[928470,928840,"에프엠"],[928840,929160,"모델을"],[929160,929680,"사용하지만"],[930210,930480,"엔드"],[930480,930620,"투"],[930620,930920,"엔드로"],[931330,931740,"학습하지"],[931740,932020,"않고"],[932250,932620,"에프엠"],[932630,932940,"모델을"],[932970,933320,"학습한"],[933430,933820,"이후에"],[934330,934480,"그"],[934490,934960,"인베딩을"],[935030,935380,"가지고"],[935380,935640,"와서"],[935990,936300,"다시"],[936330,936700,"딥러닝"],[936700,937040,"모델을"],[937230,937720,"사용합니다."],[938290,938540,"그래서"],[939350,940080,"프리트레이닝이"],[940410,940760,"반드시"],[940850,941140,"필요한"],[941150,941440,"부분이"],[941440,941640,"있죠."]],"textEdited":"에프엠 모델을 사용하지만 엔드 투 엔드로 학습하지 않고 에프엠 모델을 학습한 이후에 그 인베딩을 가지고 와서 다시 딥러닝 모델을 사용합니다. 그래서 프리트레이닝이 반드시 필요한 부분이 있죠."},{"start":941900,"end":955900,"text":"또한 이 PNN 같은 경우에는 DFM과 굉장히 비슷한 부분이 있지만 로우 오더 인터랙션 즉 메모라이제이션 부분에 학습 파라미터가 빠져 있습니다. 그리고 바로 전 파트에서 설명했던 와이드 앤 딥은","confidence":0.9186,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[942170,942400,"또한"],[942430,942580,"이"],[942590,942960,"PNN"],[942970,943174,"같은"],[943174,943600,"경우에는"],[944750,945300,"DFM과"],[945310,945600,"굉장히"],[945650,946020,"비슷한"],[946020,946320,"부분이"],[946320,946600,"있지만"],[947250,947500,"로우"],[947500,947740,"오더"],[948650,949080,"인터랙션"],[949110,949260,"즉"],[949290,950000,"메모라이제이션"],[950030,950380,"부분에"],[950630,950880,"학습"],[950880,951300,"파라미터가"],[951300,951540,"빠져"],[951540,951880,"있습니다."],[952170,952440,"그리고"],[952690,952980,"바로"],[953070,953220,"전"],[953370,953760,"파트에서"],[953770,954160,"설명했던"],[954230,954600,"와이드"],[954600,954740,"앤"],[954750,955040,"딥은"]],"textEdited":"또한 이 PNN 같은 경우에는 DFM과 굉장히 비슷한 부분이 있지만 로우 오더 인터랙션 즉 메모라이제이션 부분에 학습 파라미터가 빠져 있습니다. 그리고 바로 전 파트에서 설명했던 와이드 앤 딥은"},{"start":955900,"end":969400,"text":"와이드와 딥 컴포넌트로 나누어서 각각의 장점을 활용하였지만 이 와이드 컴포넌트 쪽에 크로스 프로덕트 트랜스포메이션 같은 피처 엔지니어링이 필요하기 때문에 모델을 구성할 때마다 상당히 불편한","confidence":0.9862,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[956130,956560,"와이드와"],[956630,956780,"딥"],[957230,957647,"컴포넌트로"],[957647,958100,"나누어서"],[958250,958560,"각각의"],[958590,958900,"장점을"],[958910,959500,"활용하였지만"],[959610,959760,"이"],[959770,960100,"와이드"],[960130,960620,"컴포넌트"],[960630,960900,"쪽에"],[961530,961840,"크로스"],[961840,962280,"프로덕트"],[962390,962980,"트랜스포메이션"],[962990,963260,"같은"],[963630,963920,"피처"],[963930,964560,"엔지니어링이"],[964790,965140,"필요하기"],[965140,965460,"때문에"],[966050,966460,"모델을"],[966530,966880,"구성할"],[966880,967200,"때마다"],[968150,968420,"상당히"],[968490,968900,"불편한"]],"textEdited":"와이드와 딥 컴포넌트로 나누어서 각각의 장점을 활용하였지만 이 와이드 컴포넌트 쪽에 크로스 프로덕트 트랜스포메이션 같은 피처 엔지니어링이 필요하기 때문에 모델을 구성할 때마다 상당히 불편한"},{"start":969400,"end":984100,"text":"프리 프로세싱 과정을 거쳐야 한다는 단점이 있습니다. 그래서 DFM은 이와 같은 모델들이 표현할 수 없는 일부 특징이나 혹은 모델이 가진 단점을 보완한 모델입니다. 네 그래서 최종적으로 모델의 예측 성능을 비교해 보았는데요.","confidence":0.9472,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[969650,969860,"프리"],[969860,970240,"프로세싱"],[970270,970600,"과정을"],[970650,970934,"거쳐야"],[970934,971220,"한다는"],[971270,971647,"단점이"],[971647,972020,"있습니다."],[972830,973080,"그래서"],[973090,973640,"DFM은"],[973650,973880,"이와"],[973890,974140,"같은"],[974390,975020,"모델들이"],[975470,975820,"표현할"],[975830,975947,"수"],[975947,976120,"없는"],[976250,976440,"일부"],[976550,976960,"특징이나"],[977110,977320,"혹은"],[977370,977700,"모델이"],[977710,977920,"가진"],[978030,978520,"단점을"],[978990,979340,"보완한"],[979570,980100,"모델입니다."],[980630,980780,"네"],[980810,981100,"그래서"],[981270,981860,"최종적으로"],[981890,982240,"모델의"],[982240,982480,"예측"],[982550,982900,"성능을"],[982950,983234,"비교해"],[983234,983740,"보았는데요."]],"textEdited":"프리 프로세싱 과정을 거쳐야 한다는 단점이 있습니다. 그래서 DFM은 이와 같은 모델들이 표현할 수 없는 일부 특징이나 혹은 모델이 가진 단점을 보완한 모델입니다. 네 그래서 최종적으로 모델의 예측 성능을 비교해 보았는데요."},{"start":984100,"end":997700,"text":"이 표에는 우리가 지난 강의부터 다루었던 많은 CTR 예측 모델들을 확인할 수 있습니다. 가장 기본적인 로지스틱 리그레션과 프엠프엔엔 그리고 여기에서는 이렇게 표현하였지만 이 모델들은 모두 와이드 앤 딥","confidence":0.7631,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[984310,984460,"이"],[984490,984900,"표에는"],[985350,985600,"우리가"],[985690,985920,"지난"],[986050,986520,"강의부터"],[986550,987100,"다루었던"],[987530,987740,"많은"],[987890,988240,"CTR"],[988490,988760,"예측"],[988760,989180,"모델들을"],[989210,989520,"확인할"],[989520,989614,"수"],[989614,989980,"있습니다."],[990090,990320,"가장"],[990370,990860,"기본적인"],[990910,991300,"로지스틱"],[991310,991880,"리그레션과"],[992350,993120,"프엠프엔엔"],[993750,993907,"그리고"],[993907,994420,"여기에서는"],[994970,995220,"이렇게"],[995220,995740,"표현하였지만"],[995870,996020,"이"],[996020,996420,"모델들은"],[996430,996620,"모두"],[996630,996980,"와이드"],[996980,997120,"앤"],[997130,997280,"딥"]],"textEdited":"이 표에는 우리가 지난 강의부터 다루었던 많은 CTR 예측 모델들을 확인할 수 있습니다. 가장 기본적인 로지스틱 리그레션과 프엠프엔엔 그리고 여기에서는 이렇게 표현하였지만 이 모델들은 모두 와이드 앤 딥"},{"start":997700,"end":1011200,"text":"에 해당하는 모델입니다. 그래서 먼저 이 AUC라는 예측 성능은 보통 바이너리 클래시피케이션에서 쓰이는 성능 지표로 높을수록 좋고요. 이 로그로스 같은 경우에는 CTR 예측 모델의 바이너리 크로스 엔트로피 손실 함수를 의미합니다.","confidence":0.971,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[998030,998180,"에"],[998180,998540,"해당하는"],[998540,998980,"모델입니다."],[999290,999480,"그래서"],[999480,999680,"먼저"],[999690,999840,"이"],[999890,1000640,"AUC라는"],[1001310,1001560,"예측"],[1001630,1001980,"성능은"],[1002190,1002440,"보통"],[1002490,1002860,"바이너리"],[1002860,1003640,"클래시피케이션에서"],[1003640,1003900,"쓰이는"],[1003930,1004140,"성능"],[1004140,1004460,"지표로"],[1004590,1005060,"높을수록"],[1005060,1005380,"좋고요."],[1005910,1006060,"이"],[1006060,1006480,"로그로스"],[1006480,1006700,"같은"],[1006700,1007120,"경우에는"],[1007310,1007640,"CTR"],[1007640,1007880,"예측"],[1007880,1008180,"모델의"],[1008590,1008940,"바이너리"],[1008940,1009180,"크로스"],[1009180,1009580,"엔트로피"],[1009850,1010120,"손실"],[1010120,1010660,"함수를"],[1010690,1011140,"의미합니다."]],"textEdited":"에 해당하는 모델입니다. 그래서 먼저 이 AUC라는 예측 성능은 보통 바이너리 클래시피케이션에서 쓰이는 성능 지표로 높을수록 좋고요. 이 로그로스 같은 경우에는 CTR 예측 모델의 바이너리 크로스 엔트로피 손실 함수를 의미합니다."},{"start":1011200,"end":1022700,"text":"그래서 이 모델을 발표한 화웨이의 데이터셋과 그리고 오픈 시티알 데이터셋으로 유명한 크리테오에 대해서 모두 딥프엠이 가장 우수한 성능을 보이고 있는 것을 알 수 있습니다.","confidence":0.8839,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1011410,1011640,"그래서"],[1011640,1011780,"이"],[1011890,1012300,"모델을"],[1012410,1012800,"발표한"],[1012870,1014020,"화웨이의"],[1014050,1014640,"데이터셋과"],[1015170,1015420,"그리고"],[1015570,1015780,"오픈"],[1015870,1016220,"시티알"],[1016230,1016920,"데이터셋으로"],[1016920,1017200,"유명한"],[1017330,1018180,"크리테오에"],[1018180,1018460,"대해서"],[1018510,1018740,"모두"],[1019130,1019700,"딥프엠이"],[1019730,1019960,"가장"],[1020530,1020840,"우수한"],[1020950,1021300,"성능을"],[1021490,1021800,"보이고"],[1021800,1021960,"있는"],[1021960,1022200,"것을"],[1022200,1022340,"알"],[1022340,1022400,"수"],[1022400,1022700,"있습니다."]],"textEdited":"그래서 이 모델을 발표한 화웨이의 데이터셋과 그리고 오픈 시티알 데이터셋으로 유명한 크리테오에 대해서 모두 딥프엠이 가장 우수한 성능을 보이고 있는 것을 알 수 있습니다."},{"start":1022700,"end":1036300,"text":"네 여기까지가 딥fm에 대한 내용이었습니다. 다음은 디아n 딥 인터레스 네트워크입니다. 이 디아에서 사용된 특징은 기존에 사용하지 않았던 유저의 행동 피처, 유저의 비에이비어 피처를 사용했다는 것인데요.","confidence":0.8643,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1022930,1023080,"네"],[1023110,1023600,"여기까지가"],[1023670,1024207,"딥fm에"],[1024207,1024420,"대한"],[1024870,1025560,"내용이었습니다."],[1025770,1026100,"다음은"],[1026390,1026800,"디아n"],[1027010,1027160,"딥"],[1027270,1027680,"인터레스"],[1027690,1028760,"네트워크입니다."],[1029210,1029360,"이"],[1029360,1029920,"디아에서"],[1029930,1030320,"사용된"],[1030710,1031020,"특징은"],[1031610,1031940,"기존에"],[1031970,1032360,"사용하지"],[1032360,1032620,"않았던"],[1032730,1033080,"유저의"],[1033130,1033380,"행동"],[1033410,1033660,"피처,"],[1033810,1034140,"유저의"],[1034190,1034700,"비에이비어"],[1034730,1035200,"피처를"],[1035200,1035607,"사용했다는"],[1035607,1036000,"것인데요."]],"textEdited":"네 여기까지가 딥fm에 대한 내용이었습니다. 다음은 디아n 딥 인터레스 네트워크입니다. 이 디아에서 사용된 특징은 기존에 사용하지 않았던 유저의 행동 피처, 유저의 비에이비어 피처를 사용했다는 것인데요."},{"start":1036300,"end":1049500,"text":"다음을 통해 모델의 전체 구조를 이해해 봅시다. 이 모델에서는 유저가 과거에 행동했던 기록인 유저 비에뷰어 피처를 인풋 피처로 사용하여서 좀 더 정확한 CTR을 예측","confidence":0.8864,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1037290,1037640,"다음을"],[1037640,1037840,"통해"],[1037870,1038240,"모델의"],[1038290,1038560,"전체"],[1038590,1038940,"구조를"],[1039050,1039307,"이해해"],[1039307,1039640,"봅시다."],[1040570,1040720,"이"],[1040730,1041340,"모델에서는"],[1042010,1042360,"유저가"],[1042470,1042840,"과거에"],[1042970,1043400,"행동했던"],[1043470,1043800,"기록인"],[1044030,1044320,"유저"],[1044370,1044860,"비에뷰어"],[1044860,1045280,"피처를"],[1045550,1045880,"인풋"],[1046050,1046380,"피처로"],[1046380,1046900,"사용하여서"],[1047470,1047620,"좀"],[1047630,1047780,"더"],[1047870,1048300,"정확한"],[1048390,1048860,"CTR을"],[1048870,1049140,"예측"]],"textEdited":"다음을 통해 모델의 전체 구조를 이해해 봅시다. 이 모델에서는 유저가 과거에 행동했던 기록인 유저 비에뷰어 피처를 인풋 피처로 사용하여서 좀 더 정확한 CTR을 예측"},{"start":1049500,"end":1061400,"text":"하고 있습니다. 이 피처를 어떻게 사용할 수 있도록 본 모델을 설계했는지 전체 구조를 이해해 봅시다. 먼저 dim 모델 이 딥 인터레스트 네트워크 모델은","confidence":0.9435,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1049750,1049960,"하고"],[1049960,1050320,"있습니다."],[1050990,1051140,"이"],[1051170,1051560,"피처를"],[1051590,1051940,"어떻게"],[1051990,1052360,"사용할"],[1052370,1052487,"수"],[1052487,1052800,"있도록"],[1053290,1053440,"본"],[1053490,1053820,"모델을"],[1053830,1054420,"설계했는지"],[1054750,1055060,"전체"],[1055090,1055440,"구조를"],[1055810,1056047,"이해해"],[1056047,1056380,"봅시다."],[1057010,1057260,"먼저"],[1057330,1057760,"dim"],[1057810,1058040,"모델"],[1058450,1058567,"이"],[1058567,1058700,"딥"],[1058750,1059220,"인터레스트"],[1059220,1059680,"네트워크"],[1060190,1060540,"모델은"]],"textEdited":"하고 있습니다. 이 피처를 어떻게 사용할 수 있도록 본 모델을 설계했는지 전체 구조를 이해해 봅시다. 먼저 dim 모델 이 딥 인터레스트 네트워크 모델은"},{"start":1061400,"end":1070500,"text":"더 많은 유저 정보 더 많은 유저의 과거 행동 정보와 같은 다양한 피처를 모델에 사용하고 싶다는 니즈에서 출발하였습니다.","confidence":0.9327,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1062010,1062160,"더"],[1062290,1062480,"많은"],[1062570,1062780,"유저"],[1062790,1063060,"정보"],[1063230,1063380,"더"],[1063430,1063620,"많은"],[1063670,1063980,"유저의"],[1064030,1064280,"과거"],[1064350,1064620,"행동"],[1064690,1065080,"정보와"],[1065090,1065320,"같은"],[1065810,1066140,"다양한"],[1066190,1066600,"피처를"],[1067070,1067460,"모델에"],[1068630,1069020,"사용하고"],[1069020,1069280,"싶다는"],[1069290,1069660,"니즈에서"],[1069710,1070400,"출발하였습니다."]],"textEdited":"더 많은 유저 정보 더 많은 유저의 과거 행동 정보와 같은 다양한 피처를 모델에 사용하고 싶다는 니즈에서 출발하였습니다."},{"start":1070500,"end":1080300,"text":"기존의 딥러닝 기반 모델 우리가 방금 전에 배웠던 와이드 앤 딥과 딥fm 같은 경우에는 모두 인베딩 이후에 멀티 레이얼 퍼셉트론 즉 피드 포워드 뉴럴 네트워크를","confidence":0.8658,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1070830,1071160,"기존의"],[1071160,1071520,"딥러닝"],[1071520,1071700,"기반"],[1071730,1071940,"모델"],[1072110,1072360,"우리가"],[1072470,1072740,"방금"],[1072740,1072960,"전에"],[1072960,1073260,"배웠던"],[1073310,1073634,"와이드"],[1073634,1073760,"앤"],[1073810,1074180,"딥과"],[1074490,1074980,"딥fm"],[1074990,1075247,"같은"],[1075247,1075620,"경우에는"],[1075690,1075920,"모두"],[1076410,1076780,"인베딩"],[1076810,1077140,"이후에"],[1077670,1077967,"멀티"],[1077967,1078200,"레이얼"],[1078210,1078600,"퍼셉트론"],[1078610,1078760,"즉"],[1078770,1079000,"피드"],[1079000,1079227,"포워드"],[1079227,1079420,"뉴럴"],[1079420,1079980,"네트워크를"]],"textEdited":"기존의 딥러닝 기반 모델 우리가 방금 전에 배웠던 와이드 앤 딥과 딥fm 같은 경우에는 모두 인베딩 이후에 멀티 레이얼 퍼셉트론 즉 피드 포워드 뉴럴 네트워크를"},{"start":1080300,"end":1094600,"text":"통과시키는 이러한 패러다임을 계속해서 다뤘습니다. 그래서 스프러스한 피처들을 인베딩 변환한 이후에 컨택해서 이 풀 커넥티드 레이어인 MLP의 입력으로 사용하는 방식으로 계속 모델 구조를 설계하였는데요.","confidence":0.8562,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1080850,1081320,"통과시키는"],[1081550,1081780,"이러한"],[1081810,1082400,"패러다임을"],[1082470,1082860,"계속해서"],[1082860,1083380,"다뤘습니다."],[1083690,1083880,"그래서"],[1083880,1084360,"스프러스한"],[1084390,1084880,"피처들을"],[1085290,1085660,"인베딩"],[1085660,1086040,"변환한"],[1086130,1086460,"이후에"],[1086750,1087260,"컨택해서"],[1087770,1087920,"이"],[1087990,1088140,"풀"],[1088270,1088680,"커넥티드"],[1088970,1089340,"레이어인"],[1089430,1089960,"MLP의"],[1090050,1090440,"입력으로"],[1090470,1090880,"사용하는"],[1091090,1091540,"방식으로"],[1091970,1092220,"계속"],[1092630,1092880,"모델"],[1092950,1093580,"구조를"],[1093730,1094420,"설계하였는데요."]],"textEdited":"통과시키는 이러한 패러다임을 계속해서 다뤘습니다. 그래서 스프러스한 피처들을 인베딩 변환한 이후에 컨택해서 이 풀 커넥티드 레이어인 MLP의 입력으로 사용하는 방식으로 계속 모델 구조를 설계하였는데요."},{"start":1094600,"end":1109200,"text":"이제 이러한 기본 기존의 방식들은 사용자의 다양한 관심사를 반영할 수 없는 모델의 구조였습니다. 예를 들면 사용자는 여러 종류의 식재료와 생필품 같은 서로 다른 카테고리에 관심사가 동시에 존재할 수도 있고요.","confidence":0.9708,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1095050,1095240,"이제"],[1095240,1095500,"이러한"],[1095650,1095880,"기본"],[1096130,1096440,"기존의"],[1096510,1097020,"방식들은"],[1097250,1097780,"사용자의"],[1097910,1098340,"다양한"],[1098550,1099400,"관심사를"],[1099450,1099840,"반영할"],[1099870,1100020,"수"],[1100020,1100220,"없는"],[1100310,1100660,"모델의"],[1100670,1101280,"구조였습니다."],[1101610,1101807,"예를"],[1101807,1102020,"들면"],[1102050,1102600,"사용자는"],[1103390,1103640,"여러"],[1103690,1104000,"종류의"],[1104130,1104620,"식재료와"],[1104730,1105260,"생필품"],[1105270,1105500,"같은"],[1105500,1105720,"서로"],[1105730,1105980,"다른"],[1106110,1106840,"카테고리에"],[1107430,1107900,"관심사가"],[1107930,1108240,"동시에"],[1108240,1108600,"존재할"],[1108600,1108740,"수도"],[1108740,1109020,"있고요."]],"textEdited":"이제 이러한 기본 기존의 방식들은 사용자의 다양한 관심사를 반영할 수 없는 모델의 구조였습니다. 예를 들면 사용자는 여러 종류의 식재료와 생필품 같은 서로 다른 카테고리에 관심사가 동시에 존재할 수도 있고요."},{"start":1109200,"end":1122700,"text":"그리고 원래 어떤 특정 카테고리의 상품을 검색하던 도중에 중간에 추천 목록에 상품이 떠 있을 때 그 클릭한 데이터를 살펴보면 사실 그 사용자는 원래 특정 카테고리의 상품에 관심이 있다는","confidence":0.9719,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1109670,1109940,"그리고"],[1110250,1110480,"원래"],[1110490,1110680,"어떤"],[1110790,1111040,"특정"],[1111090,1111660,"카테고리의"],[1111970,1112280,"상품을"],[1112310,1112800,"검색하던"],[1112810,1113180,"도중에"],[1113310,1113680,"중간에"],[1114170,1114460,"추천"],[1114460,1114740,"목록에"],[1114740,1115060,"상품이"],[1115190,1115340,"떠"],[1115370,1115580,"있을"],[1115580,1115700,"때"],[1115700,1115840,"그"],[1115910,1116220,"클릭한"],[1116250,1117840,"데이터를"],[1117870,1118280,"살펴보면"],[1118280,1118500,"사실"],[1118530,1118680,"그"],[1118680,1119160,"사용자는"],[1119610,1119860,"원래"],[1119970,1120240,"특정"],[1120270,1120780,"카테고리의"],[1120810,1121260,"상품에"],[1121630,1122020,"관심이"],[1122020,1122340,"있다는"]],"textEdited":"그리고 원래 어떤 특정 카테고리의 상품을 검색하던 도중에 중간에 추천 목록에 상품이 떠 있을 때 그 클릭한 데이터를 살펴보면 사실 그 사용자는 원래 특정 카테고리의 상품에 관심이 있다는"},{"start":1122700,"end":1135700,"text":"컨텍스트가 있는 것이죠. 그래서 이러한 피처를 사용하기 위해서는 좀 더 사용자의 행동이나 관심을 더 잘 담을 수 있는 모델을 설계했어야 합니다. 그래서 사용자가 기존에 소비한 아이템을 리스트","confidence":0.9863,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1123490,1123967,"컨텍스트가"],[1123967,1124127,"있는"],[1124127,1124420,"것이죠."],[1124750,1125000,"그래서"],[1125430,1125700,"이러한"],[1125790,1126120,"피처를"],[1126120,1126520,"사용하기"],[1126520,1126880,"위해서는"],[1126910,1127060,"좀"],[1127090,1127240,"더"],[1127650,1128220,"사용자의"],[1128530,1128940,"행동이나"],[1129270,1129660,"관심을"],[1129690,1129840,"더"],[1129990,1130140,"잘"],[1130270,1130520,"담을"],[1130570,1130687,"수"],[1130687,1130860,"있는"],[1130970,1131300,"모델을"],[1131330,1132107,"설계했어야"],[1132107,1132380,"합니다."],[1132770,1133000,"그래서"],[1133050,1133480,"사용자가"],[1133510,1133840,"기존에"],[1134310,1134620,"소비한"],[1134650,1135120,"아이템을"],[1135230,1135600,"리스트"]],"textEdited":"컨텍스트가 있는 것이죠. 그래서 이러한 피처를 사용하기 위해서는 좀 더 사용자의 행동이나 관심을 더 잘 담을 수 있는 모델을 설계했어야 합니다. 그래서 사용자가 기존에 소비한 아이템을 리스트"},{"start":1135700,"end":1150500,"text":"즉 유저 비에이비어 피처로 만들어서 사용자의 관심사를 더 반영하고 그래서 예측 대상 아이템과 내가 과거에 소비한 아이템의 관련성을 더 학습할 수 있도록 그래서 씨티알 예측을 더 정확하게 할 수 있도록 디아의 모델을 설계하였습니다.","confidence":0.8361,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1135970,1136120,"즉"],[1136270,1136540,"유저"],[1136550,1137080,"비에이비어"],[1137090,1137520,"피처로"],[1137650,1138100,"만들어서"],[1138230,1138660,"사용자의"],[1138710,1139140,"관심사를"],[1139140,1139260,"더"],[1139370,1140320,"반영하고"],[1140890,1141140,"그래서"],[1141190,1141460,"예측"],[1141460,1141700,"대상"],[1141700,1142160,"아이템과"],[1142490,1142700,"내가"],[1142770,1143080,"과거에"],[1143130,1143460,"소비한"],[1143470,1143900,"아이템의"],[1143990,1144520,"관련성을"],[1144990,1145140,"더"],[1145190,1145540,"학습할"],[1145550,1145654,"수"],[1145654,1145940,"있도록"],[1146530,1146760,"그래서"],[1146770,1147047,"씨티알"],[1147047,1147287,"예측을"],[1147287,1147420,"더"],[1147450,1147820,"정확하게"],[1147820,1147940,"할"],[1147940,1148014,"수"],[1148014,1148280,"있도록"],[1148790,1149200,"디아의"],[1149200,1149540,"모델을"],[1149610,1150360,"설계하였습니다."]],"textEdited":"즉 유저 비에이비어 피처로 만들어서 사용자의 관심사를 더 반영하고 그래서 예측 대상 아이템과 내가 과거에 소비한 아이템의 관련성을 더 학습할 수 있도록 그래서 씨티알 예측을 더 정확하게 할 수 있도록 디아의 모델을 설계하였습니다."},{"start":1150500,"end":1161600,"text":"네 그래서 이 피처 목록은 din에서 사용한 피처들인데요. dim 모델에서는 지금까지 배웠던 CTR 예측 모델의 인풋에서는 없던 처음 등장하는 피처가 바로","confidence":0.95,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1150830,1150980,"네"],[1151050,1151300,"그래서"],[1151510,1151660,"이"],[1151890,1152160,"피처"],[1152230,1152560,"목록은"],[1152670,1153320,"din에서"],[1153350,1153680,"사용한"],[1153970,1154580,"피처들인데요."],[1155330,1155740,"dim"],[1155740,1156220,"모델에서는"],[1156270,1156860,"지금까지"],[1156950,1157380,"배웠던"],[1157550,1157900,"CTR"],[1157910,1158180,"예측"],[1158190,1158480,"모델의"],[1158510,1159220,"인풋에서는"],[1159510,1159780,"없던"],[1160050,1160300,"처음"],[1160430,1160820,"등장하는"],[1160850,1161180,"피처가"],[1161180,1161420,"바로"]],"textEdited":"네 그래서 이 피처 목록은 din에서 사용한 피처들인데요. dim 모델에서는 지금까지 배웠던 CTR 예측 모델의 인풋에서는 없던 처음 등장하는 피처가 바로"},{"start":1161600,"end":1175700,"text":"이 유저 baby 피처입니다. 물론 dim 모델에서도 과거의 모델에서 사용하였던 워낫 인코딩 피처 유저 프로파일이나 우리가 지금 예측하려고 하는 그 아이템 여기서는 광고를 의미하는데요. 이런 피처들이 다 원핫 인코딩의 형태로","confidence":0.8774,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1161830,1161980,"이"],[1161990,1162260,"유저"],[1162390,1162900,"baby"],[1162990,1163520,"피처입니다."],[1163750,1163960,"물론"],[1163990,1164320,"dim"],[1164320,1164800,"모델에서도"],[1165290,1165620,"과거의"],[1165630,1165980,"모델에서"],[1165980,1166420,"사용하였던"],[1166470,1166720,"워낫"],[1166830,1167160,"인코딩"],[1167210,1167460,"피처"],[1167750,1167980,"유저"],[1168010,1168640,"프로파일이나"],[1169370,1169600,"우리가"],[1169600,1169740,"지금"],[1169740,1170280,"예측하려고"],[1170290,1170500,"하는"],[1170630,1170780,"그"],[1170970,1171340,"아이템"],[1171450,1171820,"여기서는"],[1171890,1172260,"광고를"],[1172260,1172760,"의미하는데요."],[1173450,1173620,"이런"],[1173650,1174000,"피처들이"],[1174000,1174120,"다"],[1174130,1174400,"원핫"],[1174590,1175100,"인코딩의"],[1175110,1175560,"형태로"]],"textEdited":"이 유저 baby 피처입니다. 물론 dim 모델에서도 과거의 모델에서 사용하였던 워낫 인코딩 피처 유저 프로파일이나 우리가 지금 예측하려고 하는 그 아이템 여기서는 광고를 의미하는데요. 이런 피처들이 다 원핫 인코딩의 형태로"},{"start":1175700,"end":1187300,"text":"표현되어 있지만 가운데 있는 유저 BA비어는 특이하게 멀티아 인코딩으로 표현된다는 점이 있습니다. 그 이유는 유저가 과거에 소비했던 아이템이 하나가 아니라 2개 이상","confidence":0.9188,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1175970,1176420,"표현되어"],[1176430,1176740,"있지만"],[1177470,1177840,"가운데"],[1177840,1177980,"있는"],[1178010,1178260,"유저"],[1178290,1178900,"BA비어는"],[1179250,1179660,"특이하게"],[1179730,1180160,"멀티아"],[1180230,1180840,"인코딩으로"],[1180930,1181500,"표현된다는"],[1181500,1181720,"점이"],[1181720,1182100,"있습니다."],[1183010,1183160,"그"],[1183160,1183460,"이유는"],[1183930,1184260,"유저가"],[1184310,1184620,"과거에"],[1184630,1185020,"소비했던"],[1185050,1185480,"아이템이"],[1185550,1185900,"하나가"],[1185900,1186160,"아니라"],[1186650,1186920,"2개"],[1186930,1187140,"이상"]],"textEdited":"표현되어 있지만 가운데 있는 유저 BA비어는 특이하게 멀티아 인코딩으로 표현된다는 점이 있습니다. 그 이유는 유저가 과거에 소비했던 아이템이 하나가 아니라 2개 이상"},{"start":1187300,"end":1198300,"text":"즉 n 개까지 있을 수 있고 이를 사용하기 위해서는 이를 표현하기 위해서는 단순한 원핫 인코딩이 아니라 멀티샷 인코딩의 방식으로 모델 인풋에 넣어야 하기 때문입니다.","confidence":0.891,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1187510,1187660,"즉"],[1187770,1187920,"n"],[1187920,1188320,"개까지"],[1188320,1188520,"있을"],[1188520,1188594,"수"],[1188594,1188860,"있고"],[1189570,1189820,"이를"],[1189830,1190280,"사용하기"],[1190280,1190660,"위해서는"],[1190810,1191020,"이를"],[1191050,1191440,"표현하기"],[1191440,1191820,"위해서는"],[1192150,1192460,"단순한"],[1192490,1192740,"원핫"],[1192740,1193114,"인코딩이"],[1193114,1193380,"아니라"],[1193890,1194300,"멀티샷"],[1194370,1194880,"인코딩의"],[1194970,1195420,"방식으로"],[1195670,1195940,"모델"],[1195970,1196320,"인풋에"],[1196320,1196600,"넣어야"],[1196600,1196780,"하기"],[1196780,1197300,"때문입니다."]],"textEdited":"즉 n 개까지 있을 수 있고 이를 사용하기 위해서는 이를 표현하기 위해서는 단순한 원핫 인코딩이 아니라 멀티샷 인코딩의 방식으로 모델 인풋에 넣어야 하기 때문입니다."},{"start":1198300,"end":1213000,"text":"그럼 이러한 멀티엣 인코딩 피처를 어떻게 모델이 담고 있는지 다음 슬라이드를 통해 살펴봅시다. 네 모델의 구조는 다음과 같습니다. 크게 3개의 레이어로 구성되어 있는데요. 첫 번째 레이어는 따로 언급하진 않았지만","confidence":0.9617,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1198530,1198700,"그럼"],[1198700,1198920,"이러한"],[1199010,1199380,"멀티엣"],[1199430,1199840,"인코딩"],[1200290,1200680,"피처를"],[1200770,1201100,"어떻게"],[1201150,1201480,"모델이"],[1201480,1201780,"담고"],[1201780,1202140,"있는지"],[1202610,1202820,"다음"],[1202910,1203380,"슬라이드를"],[1203380,1203600,"통해"],[1203830,1204380,"살펴봅시다."],[1205550,1205700,"네"],[1205770,1206140,"모델의"],[1206150,1206440,"구조는"],[1206450,1206800,"다음과"],[1206800,1207160,"같습니다."],[1207650,1207880,"크게"],[1208450,1208820,"3개의"],[1208990,1209340,"레이어로"],[1209340,1209647,"구성되어"],[1209647,1209980,"있는데요."],[1210450,1210600,"첫"],[1210600,1210840,"번째"],[1210850,1211240,"레이어는"],[1211470,1211700,"따로"],[1211710,1212120,"언급하진"],[1212130,1212540,"않았지만"]],"textEdited":"그럼 이러한 멀티엣 인코딩 피처를 어떻게 모델이 담고 있는지 다음 슬라이드를 통해 살펴봅시다. 네 모델의 구조는 다음과 같습니다. 크게 3개의 레이어로 구성되어 있는데요. 첫 번째 레이어는 따로 언급하진 않았지만"},{"start":1213000,"end":1224700,"text":"각각의 모든 스팟 피처를 임베딩하는 레이어이고요. 중요한 부분은 이 두 번째 로컬 액티베이션 레이어입니다. 그리고 마지막 부분은 여러분들이 이미 익숙한","confidence":0.9289,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1213550,1213880,"각각의"],[1213880,1214080,"모든"],[1214110,1214360,"스팟"],[1214510,1214940,"피처를"],[1215090,1215640,"임베딩하는"],[1215650,1216440,"레이어이고요."],[1217510,1217840,"중요한"],[1217850,1218140,"부분은"],[1218140,1218280,"이"],[1218280,1218420,"두"],[1218450,1218760,"번째"],[1219510,1219800,"로컬"],[1219870,1220380,"액티베이션"],[1220430,1220980,"레이어입니다."],[1221710,1221900,"그리고"],[1221900,1222200,"마지막"],[1222230,1222560,"부분은"],[1223050,1223500,"여러분들이"],[1223650,1223820,"이미"],[1223930,1224280,"익숙한"]],"textEdited":"각각의 모든 스팟 피처를 임베딩하는 레이어이고요. 중요한 부분은 이 두 번째 로컬 액티베이션 레이어입니다. 그리고 마지막 부분은 여러분들이 이미 익숙한"},{"start":1224700,"end":1237100,"text":"피디 포드 뉴럴 네트워크로 이루어져 있는 엠엘피 레이어입니다. 다른 레이어에는 특별한 특징은 없고 이 두 번째 로컬 액티베이션 레이어에서 바로 유저 비에뷰 피처와 지금 노출하려고 하는 아이템의 관련성을 학습하게 되는데요.","confidence":0.8301,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1224910,1225160,"피디"],[1225160,1225407,"포드"],[1225407,1225587,"뉴럴"],[1225587,1225967,"네트워크로"],[1225967,1226240,"이루어져"],[1226240,1226380,"있는"],[1226430,1226800,"엠엘피"],[1226810,1227300,"레이어입니다."],[1227510,1227780,"다른"],[1227780,1228140,"레이어에는"],[1228190,1228540,"특별한"],[1228570,1228840,"특징은"],[1228840,1229100,"없고"],[1229290,1229440,"이"],[1229470,1229620,"두"],[1229650,1229940,"번째"],[1230010,1230260,"로컬"],[1230330,1230860,"액티베이션"],[1230930,1231480,"레이어에서"],[1231790,1232060,"바로"],[1232190,1232460,"유저"],[1232510,1232900,"비에뷰"],[1233010,1233660,"피처와"],[1234070,1234260,"지금"],[1234290,1234840,"노출하려고"],[1234840,1235000,"하는"],[1235010,1235500,"아이템의"],[1235710,1236200,"관련성을"],[1236230,1236587,"학습하게"],[1236587,1236980,"되는데요."]],"textEdited":"피디 포드 뉴럴 네트워크로 이루어져 있는 엠엘피 레이어입니다. 다른 레이어에는 특별한 특징은 없고 이 두 번째 로컬 액티베이션 레이어에서 바로 유저 비에뷰 피처와 지금 노출하려고 하는 아이템의 관련성을 학습하게 되는데요."},{"start":1237100,"end":1248000,"text":"여기서 사용하는 것이 바로 이 액티베이션 유닛입니다. 이 두 번째 레이어가 본 모델의 핵심적인 부분입니다. 그래서 이 로컬 액티베이션 레이어를 통해","confidence":0.964,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1238150,1238480,"여기서"],[1238490,1238847,"사용하는"],[1238847,1239100,"것이"],[1239100,1239320,"바로"],[1239370,1239520,"이"],[1239710,1240300,"액티베이션"],[1240350,1241000,"유닛입니다."],[1241790,1241940,"이"],[1241940,1242080,"두"],[1242080,1242320,"번째"],[1242330,1242720,"레이어가"],[1243290,1243440,"본"],[1243490,1243840,"모델의"],[1244130,1244660,"핵심적인"],[1244770,1245280,"부분입니다."],[1245890,1246120,"그래서"],[1246120,1246240,"이"],[1246240,1246460,"로컬"],[1246490,1246920,"액티베이션"],[1246950,1247380,"레이어를"],[1247390,1247620,"통해"]],"textEdited":"여기서 사용하는 것이 바로 이 액티베이션 유닛입니다. 이 두 번째 레이어가 본 모델의 핵심적인 부분입니다. 그래서 이 로컬 액티베이션 레이어를 통해"},{"start":1248000,"end":1262500,"text":"좀 더 자세하게 유저 비에이비어 피처에 대해서 살펴봅시다. 먼저 이 우측에 보이는 아까 언급했던 로컬 액티베이션 유닛을 통해서 우리가 지금 노출하려고 하는 즉 CTR을 예측하려고 하는 후보 광고와 과거 유저 비에이뷰어,","confidence":0.8655,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1248250,1248400,"좀"],[1248400,1248540,"더"],[1248690,1249120,"자세하게"],[1249290,1249580,"유저"],[1249590,1250020,"비에이비어"],[1250030,1250387,"피처에"],[1250387,1250680,"대해서"],[1250710,1251240,"살펴봅시다."],[1251690,1251940,"먼저"],[1251940,1252080,"이"],[1252110,1252520,"우측에"],[1253150,1253460,"보이는"],[1253810,1254000,"아까"],[1254030,1254440,"언급했던"],[1254590,1254860,"로컬"],[1254950,1255440,"액티베이션"],[1255440,1255780,"유닛을"],[1255780,1256100,"통해서"],[1256830,1257080,"우리가"],[1257130,1257300,"지금"],[1257330,1257947,"노출하려고"],[1257947,1258140,"하는"],[1258230,1258380,"즉"],[1258450,1258900,"CTR을"],[1258900,1259420,"예측하려고"],[1259420,1259600,"하는"],[1259750,1260020,"후보"],[1260090,1260460,"광고와"],[1261130,1261420,"과거"],[1261490,1261780,"유저"],[1261830,1262360,"비에이뷰어,"]],"textEdited":"좀 더 자세하게 유저 비에이비어 피처에 대해서 살펴봅시다. 먼저 이 우측에 보이는 아까 언급했던 로컬 액티베이션 유닛을 통해서 우리가 지금 노출하려고 하는 즉 CTR을 예측하려고 하는 후보 광고와 과거 유저 비에이뷰어,"},{"start":1262500,"end":1273600,"text":"과거 유저가 어떤 아이템을 소비했는지의 관련성을 계산하게 됩니다. 그래서 이 예측 아이템과 과거에 소비했던 아이템들을 각각 페어로","confidence":0.9885,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1262750,1262980,"과거"],[1262980,1263280,"유저가"],[1263390,1263600,"어떤"],[1263630,1264140,"아이템을"],[1264250,1265900,"소비했는지의"],[1266490,1267080,"관련성을"],[1267190,1267720,"계산하게"],[1267720,1268040,"됩니다."],[1268470,1268680,"그래서"],[1268710,1268860,"이"],[1269070,1269400,"예측"],[1269470,1270480,"아이템과"],[1270730,1271140,"과거에"],[1271370,1271760,"소비했던"],[1271770,1272340,"아이템들을"],[1272470,1272780,"각각"],[1272890,1273280,"페어로"]],"textEdited":"과거 유저가 어떤 아이템을 소비했는지의 관련성을 계산하게 됩니다. 그래서 이 예측 아이템과 과거에 소비했던 아이템들을 각각 페어로"},{"start":1273600,"end":1286500,"text":"계산하여서 이 액티베이션 유닛에 넣게 되면 최종적으로 각각의 인베딩이 하나의 리니어한 즉 스칼라 값으로 출력되게 되고 이 값이 바로 액티베이션 웨이트","confidence":0.974,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1274370,1274920,"계산하여서"],[1275470,1275620,"이"],[1275690,1276220,"액티베이션"],[1276230,1276600,"유닛에"],[1276600,1276867,"넣게"],[1276867,1277080,"되면"],[1277670,1278300,"최종적으로"],[1279130,1279600,"각각의"],[1279750,1280300,"인베딩이"],[1280850,1281160,"하나의"],[1281230,1281700,"리니어한"],[1281930,1282080,"즉"],[1282130,1282520,"스칼라"],[1282530,1282900,"값으로"],[1283490,1283854,"출력되게"],[1283854,1284080,"되고"],[1284150,1284300,"이"],[1284310,1284660,"값이"],[1284890,1285120,"바로"],[1285270,1285860,"액티베이션"],[1285930,1286300,"웨이트"]],"textEdited":"계산하여서 이 액티베이션 유닛에 넣게 되면 최종적으로 각각의 인베딩이 하나의 리니어한 즉 스칼라 값으로 출력되게 되고 이 값이 바로 액티베이션 웨이트"},{"start":1286500,"end":1298900,"text":"입니다. 이 액티베이션 웨이트는 내가 지금 예측하려고 하는 아이템과 과거에 내가 소비했던 아이템이 얼마나 연관이 있는지를 말하는 것이고요. 그래서 그 웨이트가","confidence":0.9935,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1286730,1287040,"입니다."],[1288150,1288300,"이"],[1288350,1288880,"액티베이션"],[1288910,1289380,"웨이트는"],[1289990,1290220,"내가"],[1290330,1290560,"지금"],[1291690,1292247,"예측하려고"],[1292247,1292420,"하는"],[1292450,1292980,"아이템과"],[1293230,1293600,"과거에"],[1293600,1293780,"내가"],[1293830,1294240,"소비했던"],[1294270,1294720,"아이템이"],[1294770,1295120,"얼마나"],[1295250,1295680,"연관이"],[1295680,1296700,"있는지를"],[1296700,1296987,"말하는"],[1296987,1297400,"것이고요."],[1297770,1297927,"그래서"],[1297927,1298060,"그"],[1298060,1298540,"웨이트가"]],"textEdited":"입니다. 이 액티베이션 웨이트는 내가 지금 예측하려고 하는 아이템과 과거에 내가 소비했던 아이템이 얼마나 연관이 있는지를 말하는 것이고요. 그래서 그 웨이트가"},{"start":1298900,"end":1307700,"text":"높다는 것은 연관성이 높아서 이 정보를 더 많이 활용하겠다는 것이고, 웨이트가 낮다는 것은 최근에 소비한 이 아이템의 경우에는 현재","confidence":0.9931,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1299110,1299580,"높다는"],[1299580,1299820,"것은"],[1299930,1300420,"연관성이"],[1300420,1300820,"높아서"],[1301170,1301320,"이"],[1301370,1301647,"정보를"],[1301647,1301780,"더"],[1301780,1301960,"많이"],[1302030,1302660,"활용하겠다는"],[1302660,1302980,"것이고,"],[1303930,1304340,"웨이트가"],[1304340,1304687,"낮다는"],[1304687,1304960,"것은"],[1305410,1305700,"최근에"],[1305700,1305980,"소비한"],[1305990,1306140,"이"],[1306150,1306640,"아이템의"],[1306640,1307020,"경우에는"],[1307190,1307460,"현재"]],"textEdited":"높다는 것은 연관성이 높아서 이 정보를 더 많이 활용하겠다는 것이고, 웨이트가 낮다는 것은 최근에 소비한 이 아이템의 경우에는 현재"},{"start":1307700,"end":1321900,"text":"노출하려고 하는 아이템과 연관성이 낮기 때문에 최대한 덜 반영하겠다는 것이죠. 그래서 여기에 있는 이 n개의 인베딩이 웨이트가 곱해지고 나서는 선플링 즉 이 전체 인베딩을 다 더해서 그 차원이 늘어나지 않고","confidence":0.9255,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1307910,1308407,"노출하려고"],[1308407,1308527,"하는"],[1308527,1308960,"아이템과"],[1308960,1309420,"연관성이"],[1309420,1309720,"낮기"],[1309720,1310080,"때문에"],[1310590,1310900,"최대한"],[1310950,1311100,"덜"],[1311210,1311794,"반영하겠다는"],[1311794,1312120,"것이죠."],[1312350,1312507,"그래서"],[1312507,1312754,"여기에"],[1312754,1312900,"있는"],[1312910,1313060,"이"],[1313110,1313640,"n개의"],[1314430,1314900,"인베딩이"],[1315070,1315500,"웨이트가"],[1315530,1316040,"곱해지고"],[1316040,1316400,"나서는"],[1317390,1317900,"선플링"],[1318030,1318180,"즉"],[1318370,1318520,"이"],[1318590,1318880,"전체"],[1319110,1319600,"인베딩을"],[1319610,1319760,"다"],[1319790,1320220,"더해서"],[1320570,1320720,"그"],[1320730,1321047,"차원이"],[1321047,1321420,"늘어나지"],[1321420,1321740,"않고"]],"textEdited":"노출하려고 하는 아이템과 연관성이 낮기 때문에 최대한 덜 반영하겠다는 것이죠. 그래서 여기에 있는 이 n개의 인베딩이 웨이트가 곱해지고 나서는 선플링 즉 이 전체 인베딩을 다 더해서 그 차원이 늘어나지 않고"},{"start":1321900,"end":1336600,"text":"계속해서 같은 차원으로 유지시키도록 만듭니다. 참고로 이 로컬 액티베이션 레이어는 트랜스포머의 어텐션 메커니즘과 유사합니다. 물론 트랜스포머보다 훨씬 간단하게 계산이 이루어지지만 이 타겟 아이템과","confidence":0.9748,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1322150,1322560,"계속해서"],[1322590,1322840,"같은"],[1322910,1323340,"차원으로"],[1323450,1324140,"유지시키도록"],[1324270,1324720,"만듭니다."],[1325390,1325780,"참고로"],[1326130,1326280,"이"],[1326370,1326660,"로컬"],[1326730,1327200,"액티베이션"],[1327230,1327640,"레이어는"],[1328210,1328900,"트랜스포머의"],[1328910,1329300,"어텐션"],[1329330,1329900,"메커니즘과"],[1329930,1330400,"유사합니다."],[1331410,1331600,"물론"],[1331610,1332500,"트랜스포머보다"],[1332550,1332840,"훨씬"],[1333170,1333640,"간단하게"],[1333640,1333927,"계산이"],[1333927,1334460,"이루어지지만"],[1335010,1335160,"이"],[1335230,1335500,"타겟"],[1335630,1336200,"아이템과"]],"textEdited":"계속해서 같은 차원으로 유지시키도록 만듭니다. 참고로 이 로컬 액티베이션 레이어는 트랜스포머의 어텐션 메커니즘과 유사합니다. 물론 트랜스포머보다 훨씬 간단하게 계산이 이루어지지만 이 타겟 아이템과"},{"start":1336600,"end":1351200,"text":"이 타겟 아이템을 기준으로 연관도가 높은 아이템 이 n개 중에 무엇이 높은지를 계산하여서 거기에 더 높은 웨이트를 주어서 신호를 더 많이 전달하려고 하는 원리가 바로 트랜스포머의 어텐션이 하는 역할과 유사하다고 볼 수 있습니다.","confidence":0.9234,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1337110,1337260,"이"],[1337290,1337520,"타겟"],[1337520,1337840,"아이템을"],[1337840,1338220,"기준으로"],[1338550,1339080,"연관도가"],[1339080,1339320,"높은"],[1339490,1339840,"아이템"],[1341190,1341340,"이"],[1341350,1341620,"n개"],[1341620,1341840,"중에"],[1341870,1342147,"무엇이"],[1342147,1342560,"높은지를"],[1342560,1343080,"계산하여서"],[1343470,1343780,"거기에"],[1343780,1343900,"더"],[1343910,1344140,"높은"],[1344230,1344700,"웨이트를"],[1344700,1344980,"주어서"],[1345070,1345380,"신호를"],[1345380,1345500,"더"],[1345500,1345680,"많이"],[1345790,1346320,"전달하려고"],[1346320,1346480,"하는"],[1346910,1347740,"원리가"],[1347750,1347960,"바로"],[1348010,1348560,"트랜스포머의"],[1348560,1349020,"어텐션이"],[1349020,1349200,"하는"],[1349210,1349600,"역할과"],[1350010,1350500,"유사하다고"],[1350500,1350620,"볼"],[1350620,1350714,"수"],[1350714,1351080,"있습니다."]],"textEdited":"이 타겟 아이템을 기준으로 연관도가 높은 아이템 이 n개 중에 무엇이 높은지를 계산하여서 거기에 더 높은 웨이트를 주어서 신호를 더 많이 전달하려고 하는 원리가 바로 트랜스포머의 어텐션이 하는 역할과 유사하다고 볼 수 있습니다."},{"start":1351200,"end":1364900,"text":"그래서 이 로컬 액티베이션 레이어를 통해 더해진 썸플링을 통해 더해진 모든 인베딩 값과 그 외에 다른 원핫 인코딩 피처들은 이 레이어를 통과하지 않고 곧바로 마지막 레이어로","confidence":0.9588,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1351750,1352020,"그래서"],[1352190,1352340,"이"],[1352370,1352660,"로컬"],[1352730,1353180,"액티베이션"],[1353230,1353600,"레이어를"],[1353610,1353840,"통해"],[1354110,1354480,"더해진"],[1354970,1355580,"썸플링을"],[1355730,1355980,"통해"],[1355980,1356280,"더해진"],[1356330,1356540,"모든"],[1356630,1357000,"인베딩"],[1357010,1357340,"값과"],[1357910,1358060,"그"],[1358060,1358260,"외에"],[1358290,1358480,"다른"],[1358550,1358820,"원핫"],[1358930,1359320,"인코딩"],[1359370,1359840,"피처들은"],[1360470,1360620,"이"],[1360630,1360940,"레이어를"],[1360990,1361380,"통과하지"],[1361380,1361660,"않고"],[1361930,1362400,"곧바로"],[1363290,1363620,"마지막"],[1364090,1364520,"레이어로"]],"textEdited":"그래서 이 로컬 액티베이션 레이어를 통해 더해진 썸플링을 통해 더해진 모든 인베딩 값과 그 외에 다른 원핫 인코딩 피처들은 이 레이어를 통과하지 않고 곧바로 마지막 레이어로"},{"start":1364900,"end":1379100,"text":"들어가게 됩니다. 그래서 이 모든 레이어를 cncat 네이트 한 다음에 마지막에는 MLP 레이어를 통과시켜서 최종 클릭 여부 01을 아웃풋 레이어에서 예측하게 됩니다. 이 로컬 액티베이션 레이어의 연산을","confidence":0.881,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1365290,1365627,"들어가게"],[1365627,1365920,"됩니다."],[1366430,1366620,"그래서"],[1366630,1366780,"이"],[1366810,1367060,"모든"],[1367210,1367600,"레이어를"],[1368250,1368727,"cncat"],[1368727,1369020,"네이트"],[1369020,1369140,"한"],[1369150,1369480,"다음에"],[1369630,1370120,"마지막에는"],[1370120,1370520,"MLP"],[1370970,1371280,"레이어를"],[1371330,1371860,"통과시켜서"],[1372370,1372660,"최종"],[1372710,1372960,"클릭"],[1372960,1373220,"여부"],[1373410,1373820,"01을"],[1374170,1374520,"아웃풋"],[1374570,1375000,"레이어에서"],[1375310,1375800,"예측하게"],[1375800,1376120,"됩니다."],[1377050,1377200,"이"],[1377230,1377520,"로컬"],[1377590,1378060,"액티베이션"],[1378110,1378460,"레이어의"],[1378460,1378920,"연산을"]],"textEdited":"들어가게 됩니다. 그래서 이 모든 레이어를 cncat 네이트 한 다음에 마지막에는 MLP 레이어를 통과시켜서 최종 클릭 여부 01을 아웃풋 레이어에서 예측하게 됩니다. 이 로컬 액티베이션 레이어의 연산을"},{"start":1379100,"end":1393400,"text":"좀 더 잘 이해할 수 있도록 그림으로 비주얼라이즈 한 결과입니다. 이 왼쪽에 있는 아이템들이 유저가 과거에 소비한 유저 비에뷰 피처이고 이 아이템은 이제 노출하려고 하는 즉 씨티알을 예측하려고 하는 아이템인데요.","confidence":0.8788,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1379370,1379520,"좀"],[1379520,1379660,"더"],[1379690,1379840,"잘"],[1379930,1380240,"이해할"],[1380250,1380340,"수"],[1380340,1380620,"있도록"],[1380710,1381120,"그림으로"],[1381290,1381847,"비주얼라이즈"],[1381847,1381980,"한"],[1382010,1382500,"결과입니다."],[1383410,1383560,"이"],[1383570,1384000,"왼쪽에"],[1384010,1384220,"있는"],[1385110,1385660,"아이템들이"],[1385730,1386040,"유저가"],[1386110,1386420,"과거에"],[1386470,1386840,"소비한"],[1387070,1387340,"유저"],[1387370,1387740,"비에뷰"],[1387830,1388340,"피처이고"],[1389390,1389540,"이"],[1389570,1390040,"아이템은"],[1390210,1390380,"이제"],[1390390,1390960,"노출하려고"],[1390960,1391120,"하는"],[1391190,1391340,"즉"],[1391410,1391840,"씨티알을"],[1391840,1392307,"예측하려고"],[1392307,1392480,"하는"],[1392480,1393100,"아이템인데요."]],"textEdited":"좀 더 잘 이해할 수 있도록 그림으로 비주얼라이즈 한 결과입니다. 이 왼쪽에 있는 아이템들이 유저가 과거에 소비한 유저 비에뷰 피처이고 이 아이템은 이제 노출하려고 하는 즉 씨티알을 예측하려고 하는 아이템인데요."},{"start":1393400,"end":1407200,"text":"이 아이템과 비슷한 아이템을 과거에 많이 소비했을수록 이 두 관계를 통해서 웨이트가 높게 반영되고 그렇지 않은 아이템들은 웨이트가 굉장히 낮게 반영됨을 볼 수 있습니다. 그래서 과거에","confidence":0.9874,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1394190,1394340,"이"],[1394350,1394920,"아이템과"],[1395370,1395780,"비슷한"],[1395850,1396280,"아이템을"],[1396330,1396627,"과거에"],[1396627,1396800,"많이"],[1396850,1397520,"소비했을수록"],[1397770,1397920,"이"],[1397930,1398080,"두"],[1398190,1399200,"관계를"],[1399230,1399580,"통해서"],[1399690,1400140,"웨이트가"],[1400210,1400500,"높게"],[1400570,1401100,"반영되고"],[1401610,1401940,"그렇지"],[1401940,1402100,"않은"],[1402110,1402660,"아이템들은"],[1403310,1403720,"웨이트가"],[1403730,1403980,"굉장히"],[1404010,1404340,"낮게"],[1404370,1404820,"반영됨을"],[1404850,1405000,"볼"],[1405000,1405107,"수"],[1405107,1405460,"있습니다."],[1405750,1405980,"그래서"],[1406050,1406420,"과거에"]],"textEdited":"이 아이템과 비슷한 아이템을 과거에 많이 소비했을수록 이 두 관계를 통해서 웨이트가 높게 반영되고 그렇지 않은 아이템들은 웨이트가 굉장히 낮게 반영됨을 볼 수 있습니다. 그래서 과거에"},{"start":1407200,"end":1417500,"text":"내가 소비한 아이템들과 지금 여기에 패딩이 있는데요. 이 패딩과 비슷한 아이템들을 많이 소비했을수록 웨이트가 높게 반영되고 즉 연관성이 높기 때문에","confidence":0.9934,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1407430,1407640,"내가"],[1407670,1408000,"소비한"],[1408030,1409140,"아이템들과"],[1409330,1409500,"지금"],[1409500,1409820,"여기에"],[1410470,1410787,"패딩이"],[1410787,1411100,"있는데요."],[1411130,1411280,"이"],[1411290,1411640,"패딩과"],[1411650,1412000,"비슷한"],[1412450,1412960,"아이템들을"],[1412960,1413160,"많이"],[1413290,1413980,"소비했을수록"],[1414370,1414800,"웨이트가"],[1414800,1415060,"높게"],[1415090,1415560,"반영되고"],[1415890,1416040,"즉"],[1416070,1416520,"연관성이"],[1416520,1416747,"높기"],[1416747,1417080,"때문에"]],"textEdited":"내가 소비한 아이템들과 지금 여기에 패딩이 있는데요. 이 패딩과 비슷한 아이템들을 많이 소비했을수록 웨이트가 높게 반영되고 즉 연관성이 높기 때문에"},{"start":1417500,"end":1431900,"text":"이 아이템에 대한 CTR, 즉 이러한 소비 패턴을 가지고 있는 유저에 대해서 이 패딩을 노출시켰을 때 CTR이 더 높게 학습될 수 있도록 모델의 구조를 설계한 것입니다. 이렇게 해서 유저가 과거에 소비했던","confidence":0.8936,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1417750,1417900,"이"],[1417900,1418294,"아이템에"],[1418294,1418480,"대한"],[1418590,1418920,"CTR,"],[1419130,1419280,"즉"],[1419430,1419720,"이러한"],[1419850,1420120,"소비"],[1420170,1420540,"패턴을"],[1420550,1420900,"가지고"],[1420910,1421140,"있는"],[1421830,1422167,"유저에"],[1422167,1422480,"대해서"],[1423130,1423280,"이"],[1423330,1423680,"패딩을"],[1423680,1424220,"노출시켰을"],[1424220,1424360,"때"],[1424390,1424800,"CTR이"],[1424800,1424920,"더"],[1424930,1425300,"높게"],[1425890,1426340,"학습될"],[1426390,1426494,"수"],[1426494,1426780,"있도록"],[1426850,1427180,"모델의"],[1427210,1427560,"구조를"],[1427930,1428267,"설계한"],[1428267,1428680,"것입니다."],[1429230,1429467,"이렇게"],[1429467,1429660,"해서"],[1429670,1429980,"유저가"],[1430170,1430560,"과거에"],[1431090,1431600,"소비했던"]],"textEdited":"이 아이템에 대한 CTR, 즉 이러한 소비 패턴을 가지고 있는 유저에 대해서 이 패딩을 노출시켰을 때 CTR이 더 높게 학습될 수 있도록 모델의 구조를 설계한 것입니다. 이렇게 해서 유저가 과거에 소비했던"},{"start":1431900,"end":1444900,"text":"피처를 사용하면 이 유저에게 아이템을 노출했을 때 이 예측 CTR이 더 높게 잘 되는지 혹은 낮게 잘 되는지를 정확하게 구할 수 있는 것입니다. 네 마지막으로 논문에서는 기존에 알려진","confidence":0.9846,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1432130,1432500,"피처를"],[1432500,1432880,"사용하면"],[1433210,1433360,"이"],[1433360,1433780,"유저에게"],[1433780,1434160,"아이템을"],[1434160,1434640,"노출했을"],[1434690,1434840,"때"],[1435710,1435860,"이"],[1435870,1436140,"예측"],[1436210,1436587,"CTR이"],[1436587,1436720,"더"],[1436730,1437020,"높게"],[1437050,1437200,"잘"],[1437210,1437580,"되는지"],[1437580,1437740,"혹은"],[1437750,1438040,"낮게"],[1438040,1438180,"잘"],[1438180,1438640,"되는지를"],[1438710,1439160,"정확하게"],[1439230,1439480,"구할"],[1439570,1439720,"수"],[1439910,1440220,"있는"],[1440230,1440660,"것입니다."],[1441630,1441780,"네"],[1441790,1442340,"마지막으로"],[1442870,1443460,"논문에서는"],[1443630,1443960,"기존에"],[1444090,1444400,"알려진"]],"textEdited":"피처를 사용하면 이 유저에게 아이템을 노출했을 때 이 예측 CTR이 더 높게 잘 되는지 혹은 낮게 잘 되는지를 정확하게 구할 수 있는 것입니다. 네 마지막으로 논문에서는 기존에 알려진"},{"start":1444900,"end":1454800,"text":"다른 CTR 예측 모델들과의 성능을 비교하였습니다. 여기 보시면 우리가 계속해서 보았던 기본적인 로지스틱 리그레션 모델과 와이드 앤 딥 딥fm","confidence":0.8023,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1445190,1445400,"다른"],[1445670,1446000,"CTR"],[1446000,1446220,"예측"],[1446220,1446740,"모델들과의"],[1446790,1447120,"성능을"],[1447130,1447780,"비교하였습니다."],[1448550,1448760,"여기"],[1448770,1449080,"보시면"],[1449090,1449320,"우리가"],[1449850,1450240,"계속해서"],[1450270,1450660,"보았던"],[1451550,1451940,"기본적인"],[1451940,1452280,"로지스틱"],[1452280,1452640,"리그레션"],[1452640,1453000,"모델과"],[1453030,1453327,"와이드"],[1453327,1453460,"앤"],[1453470,1453620,"딥"],[1454050,1454560,"딥fm"]],"textEdited":"다른 CTR 예측 모델들과의 성능을 비교하였습니다. 여기 보시면 우리가 계속해서 보았던 기본적인 로지스틱 리그레션 모델과 와이드 앤 딥 딥fm"},{"start":1454800,"end":1464300,"text":"등이 있는데요. 이제 이 모델들보다 이 디아이엔 모델이 더 높은 에유시 성능을 보임을 알 수 있습니다.","confidence":0.7198,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1455190,1455387,"등이"],[1455387,1455720,"있는데요."],[1456330,1456500,"이제"],[1456510,1456660,"이"],[1456660,1457220,"모델들보다"],[1457370,1457520,"이"],[1457530,1458020,"디아이엔"],[1458310,1458700,"모델이"],[1459190,1459340,"더"],[1460690,1461020,"높은"],[1462470,1462880,"에유시"],[1462930,1463260,"성능을"],[1463290,1463600,"보임을"],[1463630,1463780,"알"],[1463780,1463874,"수"],[1463874,1464280,"있습니다."]],"textEdited":"등이 있는데요. 이제 이 모델들보다 이 디아이엔 모델이 더 높은 에유시 성능을 보임을 알 수 있습니다."},{"start":1464300,"end":1478100,"text":"그리고 여기에 있는 이 다이스라는 것은 디아엔 논문에서 제시한 새로운 액티베이션 펑션인데요. 사실 강의 전체적으로 중요한 부분은 아니지만 이 액티베이션 펑션이 궁금하신 분들은 이 논문을 찾아서 읽어보시길 바랍니다.","confidence":0.9369,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1464930,1465087,"그리고"],[1465087,1465334,"여기에"],[1465334,1465500,"있는"],[1465500,1465640,"이"],[1465640,1466480,"다이스라는"],[1466970,1467260,"것은"],[1467730,1468160,"디아엔"],[1468170,1468560,"논문에서"],[1468590,1468940,"제시한"],[1469010,1469320,"새로운"],[1469390,1469880,"액티베이션"],[1469950,1470480,"펑션인데요."],[1471050,1471260,"사실"],[1471330,1471600,"강의"],[1471850,1472240,"전체적으로"],[1472270,1472580,"중요한"],[1472580,1472814,"부분은"],[1472814,1473180,"아니지만"],[1473730,1473880,"이"],[1473910,1474400,"액티베이션"],[1474450,1474780,"펑션이"],[1474810,1475240,"궁금하신"],[1475250,1475560,"분들은"],[1476050,1476200,"이"],[1476200,1476520,"논문을"],[1476520,1476800,"찾아서"],[1476830,1477380,"읽어보시길"],[1477430,1477920,"바랍니다."]],"textEdited":"그리고 여기에 있는 이 다이스라는 것은 디아엔 논문에서 제시한 새로운 액티베이션 펑션인데요. 사실 강의 전체적으로 중요한 부분은 아니지만 이 액티베이션 펑션이 궁금하신 분들은 이 논문을 찾아서 읽어보시길 바랍니다."},{"start":1478100,"end":1489200,"text":"그래서 이렇게 유저 비에비 피처를 사용했을 때 CTR 예측 정확도가 더 올라간다는 특징을 가진 디아엔 논문이었고요. 이상 디아엔 전체적인 내용을 모두 마쳤습니다. 네 마지막 파트","confidence":0.7696,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1478330,1478580,"그래서"],[1478580,1478780,"이렇게"],[1478830,1479080,"유저"],[1479110,1479480,"비에비"],[1479570,1479887,"피처를"],[1479887,1480300,"사용했을"],[1480330,1480480,"때"],[1480950,1481260,"CTR"],[1481260,1481440,"예측"],[1481440,1481840,"정확도가"],[1481840,1481960,"더"],[1481970,1482540,"올라간다는"],[1482910,1483200,"특징을"],[1483200,1483400,"가진"],[1483490,1483860,"디아엔"],[1483870,1484480,"논문이었고요."],[1485110,1485320,"이상"],[1485450,1485800,"디아엔"],[1485850,1486260,"전체적인"],[1486260,1486540,"내용을"],[1486690,1486940,"모두"],[1487010,1487580,"마쳤습니다."],[1487930,1488080,"네"],[1488150,1488480,"마지막"],[1488550,1488840,"파트"]],"textEdited":"그래서 이렇게 유저 비에비 피처를 사용했을 때 CTR 예측 정확도가 더 올라간다는 특징을 가진 디아엔 논문이었고요. 이상 디아엔 전체적인 내용을 모두 마쳤습니다. 네 마지막 파트"},{"start":1489200,"end":1500200,"text":"babr 시퀀스 트랜스포머라는 BST라는 CTR 예측 모델 논문입니다. DCTR의 마지막 파트인데요. CTR 예측에 여러분들도 잘 알고 있는 트랜스포머","confidence":0.9056,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1489430,1489900,"babr"],[1489970,1490340,"시퀀스"],[1490350,1491420,"트랜스포머라는"],[1491470,1492360,"BST라는"],[1492450,1492760,"CTR"],[1492760,1492960,"예측"],[1492960,1493180,"모델"],[1493450,1493960,"논문입니다."],[1494530,1495220,"DCTR의"],[1495230,1495540,"마지막"],[1495610,1496140,"파트인데요."],[1496790,1497120,"CTR"],[1497130,1497500,"예측에"],[1497870,1498340,"여러분들도"],[1498370,1498520,"잘"],[1498610,1498860,"알고"],[1498860,1499040,"있는"],[1499350,1500040,"트랜스포머"]],"textEdited":"babr 시퀀스 트랜스포머라는 BST라는 CTR 예측 모델 논문입니다. DCTR의 마지막 파트인데요. CTR 예측에 여러분들도 잘 알고 있는 트랜스포머"},{"start":1500200,"end":1508900,"text":"아키텍처를 사용한 모델입니다. 그래서 먼저 트랜스포머에 대해서 간단하게 리뷰를 한 이후 어떻게 트랜스포머를 씨티알 예측에 활용하고 있는지 살펴봅시다.","confidence":0.8982,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1500450,1501020,"아키텍처를"],[1501020,1501280,"사용한"],[1501280,1501700,"모델입니다."],[1501750,1501980,"그래서"],[1501990,1502260,"먼저"],[1502330,1502934,"트랜스포머에"],[1502934,1503200,"대해서"],[1503330,1503820,"간단하게"],[1503890,1504260,"리뷰를"],[1504290,1504440,"한"],[1504530,1504760,"이후"],[1505390,1505740,"어떻게"],[1505770,1506440,"트랜스포머를"],[1506490,1506800,"씨티알"],[1506800,1507100,"예측에"],[1507110,1507500,"활용하고"],[1507500,1507880,"있는지"],[1508170,1508820,"살펴봅시다."]],"textEdited":"아키텍처를 사용한 모델입니다. 그래서 먼저 트랜스포머에 대해서 간단하게 리뷰를 한 이후 어떻게 트랜스포머를 씨티알 예측에 활용하고 있는지 살펴봅시다."},{"start":1508900,"end":1519500,"text":"본 논문은 dim 모델을 발표한 알리바바에서 후속으로 발표한 예측 모델인데요. 트랜스포머를 CTR 예측에 사용했을 때 가장 뛰어난 성능을 보인다고 말하고 있습니다.","confidence":0.9604,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1509370,1509520,"본"],[1509590,1510000,"논문은"],[1510410,1510860,"dim"],[1510890,1511200,"모델을"],[1511250,1511640,"발표한"],[1511710,1512380,"알리바바에서"],[1512490,1512940,"후속으로"],[1512990,1513340,"발표한"],[1513450,1513700,"예측"],[1513700,1514200,"모델인데요."],[1514830,1515680,"트랜스포머를"],[1515790,1516080,"CTR"],[1516080,1516340,"예측에"],[1516340,1516780,"사용했을"],[1516790,1516940,"때"],[1517210,1517440,"가장"],[1517570,1517900,"뛰어난"],[1518010,1518360,"성능을"],[1518530,1518927,"보인다고"],[1518927,1519167,"말하고"],[1519167,1519500,"있습니다."]],"textEdited":"본 논문은 dim 모델을 발표한 알리바바에서 후속으로 발표한 예측 모델인데요. 트랜스포머를 CTR 예측에 사용했을 때 가장 뛰어난 성능을 보인다고 말하고 있습니다."},{"start":1519500,"end":1531000,"text":"그리고 방금 전 디아 논문에서는 유저 비에이비오 피처를 사용했지만 여기서는 비에이비오의 피처의 시퀀스까지 더 정확하게 모델링해서 어떤 순서로 이 유저가 행동을 했을 때 다음","confidence":0.8539,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1519710,1519940,"그리고"],[1519990,1520207,"방금"],[1520207,1520340,"전"],[1520370,1520620,"디아"],[1520730,1521360,"논문에서는"],[1521730,1522000,"유저"],[1522050,1522500,"비에이비오"],[1522530,1522867,"피처를"],[1522867,1523320,"사용했지만"],[1523510,1523880,"여기서는"],[1524070,1524620,"비에이비오의"],[1524670,1525400,"피처의"],[1525550,1526360,"시퀀스까지"],[1526690,1526840,"더"],[1526850,1527260,"정확하게"],[1527270,1527840,"모델링해서"],[1528330,1528580,"어떤"],[1528690,1529000,"순서로"],[1529010,1529160,"이"],[1529160,1529440,"유저가"],[1529490,1529760,"행동을"],[1529770,1530080,"했을"],[1530130,1530280,"때"],[1530350,1530580,"다음"]],"textEdited":"그리고 방금 전 디아 논문에서는 유저 비에이비오 피처를 사용했지만 여기서는 비에이비오의 피처의 시퀀스까지 더 정확하게 모델링해서 어떤 순서로 이 유저가 행동을 했을 때 다음"},{"start":1531000,"end":1544000,"text":"에 노출된 아이템의 CTR이 얼마일지를 더 정확하게 구하고 있습니다. 그렇다면 CTR 예측에도 어떻게 트랜스포머가 효과적으로 작용할 수 있을까요? 이 CTR 예측 데이터와 NLP 번역 데이터 간의 공통점이 있습니다.","confidence":0.8543,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1531230,1531380,"에"],[1531380,1531780,"노출된"],[1531790,1532200,"아이템의"],[1532210,1532640,"CTR이"],[1532670,1533300,"얼마일지를"],[1533300,1533440,"더"],[1533450,1533900,"정확하게"],[1534370,1534687,"구하고"],[1534687,1535060,"있습니다."],[1535790,1536200,"그렇다면"],[1536450,1536780,"CTR"],[1536790,1537240,"예측에도"],[1537310,1537640,"어떻게"],[1537710,1538340,"트랜스포머가"],[1538350,1538780,"효과적으로"],[1538790,1539120,"작용할"],[1539120,1539207,"수"],[1539207,1539600,"있을까요?"],[1540190,1540340,"이"],[1540340,1540660,"CTR"],[1540660,1540860,"예측"],[1540860,1541260,"데이터와"],[1541310,1541760,"NLP"],[1541850,1542100,"번역"],[1542150,1542520,"데이터"],[1542750,1543000,"간의"],[1543030,1543474,"공통점이"],[1543474,1543820,"있습니다."]],"textEdited":"에 노출된 아이템의 CTR이 얼마일지를 더 정확하게 구하고 있습니다. 그렇다면 CTR 예측에도 어떻게 트랜스포머가 효과적으로 작용할 수 있을까요? 이 CTR 예측 데이터와 NLP 번역 데이터 간의 공통점이 있습니다."},{"start":1544000,"end":1554500,"text":"이 엔엘피 데이터는 이미 트랜스포머에서 아주 좋은 성능을 보인다고 알려져 있는데요. 일단 씨티r 예측 데이터의 인풋 피처들이 대부분 스파스한 피처인데요.","confidence":0.849,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1544250,1544367,"이"],[1544367,1544720,"엔엘피"],[1544720,1545140,"데이터는"],[1545210,1545400,"이미"],[1545430,1546140,"트랜스포머에서"],[1546510,1546720,"아주"],[1546730,1546900,"좋은"],[1547030,1547360,"성능을"],[1547370,1547780,"보인다고"],[1547780,1548040,"알려져"],[1548040,1548380,"있는데요."],[1549170,1549440,"일단"],[1550250,1550560,"씨티r"],[1550560,1550780,"예측"],[1550780,1551500,"데이터의"],[1551610,1551920,"인풋"],[1552370,1552780,"피처들이"],[1552780,1553060,"대부분"],[1553070,1553560,"스파스한"],[1553570,1554240,"피처인데요."]],"textEdited":"이 엔엘피 데이터는 이미 트랜스포머에서 아주 좋은 성능을 보인다고 알려져 있는데요. 일단 씨티r 예측 데이터의 인풋 피처들이 대부분 스파스한 피처인데요."},{"start":1554500,"end":1568400,"text":"NLP 또한 대부분이 단어로 이루어져 있거나 혹은 서 월드로 이루어져 있기 때문에 둘 다 아주 스프레시티가 높은 피처로 이루어져 있습니다. 또한 이 로우와 하이오더 피처 인터랙션이 각각 모두 존재하기 때문에","confidence":0.8908,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1554790,1555240,"NLP"],[1555390,1555660,"또한"],[1555950,1556360,"대부분이"],[1556630,1557060,"단어로"],[1557060,1557400,"이루어져"],[1557400,1557660,"있거나"],[1557660,1557820,"혹은"],[1557910,1558060,"서"],[1558150,1558500,"월드로"],[1558710,1559040,"이루어져"],[1559040,1559154,"있기"],[1559154,1559480,"때문에"],[1559930,1560080,"둘"],[1560090,1560240,"다"],[1560770,1561000,"아주"],[1561990,1562580,"스프레시티가"],[1562580,1562820,"높은"],[1563070,1563400,"피처로"],[1563400,1563700,"이루어져"],[1563700,1564040,"있습니다."],[1564170,1564400,"또한"],[1564610,1564760,"이"],[1564770,1565120,"로우와"],[1565190,1565640,"하이오더"],[1565650,1565900,"피처"],[1565900,1566420,"인터랙션이"],[1566830,1567100,"각각"],[1567110,1567300,"모두"],[1567330,1567747,"존재하기"],[1567747,1568100,"때문에"]],"textEdited":"NLP 또한 대부분이 단어로 이루어져 있거나 혹은 서 월드로 이루어져 있기 때문에 둘 다 아주 스프레시티가 높은 피처로 이루어져 있습니다. 또한 이 로우와 하이오더 피처 인터랙션이 각각 모두 존재하기 때문에"},{"start":1568400,"end":1583200,"text":"비선형적인 관계를 모델이 캡처해야 하고요. 또한 문장의 순서가 중요하듯이 사용자의 행동 순서도 굉장히 중요합니다. 즉 핸드폰을 구매한 이후에 핸드폰 케이스 상품을 추천해 줘야 한다는 식의 순서라든지 바지를 구매하고 나서","confidence":0.9752,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1568610,1569180,"비선형적인"],[1569290,1569680,"관계를"],[1569810,1570180,"모델이"],[1570270,1570740,"캡처해야"],[1570740,1571040,"하고요."],[1571410,1571620,"또한"],[1571770,1572180,"문장의"],[1572270,1572640,"순서가"],[1572710,1573280,"중요하듯이"],[1573750,1574300,"사용자의"],[1574450,1574720,"행동"],[1574790,1575140,"순서도"],[1575150,1575420,"굉장히"],[1575470,1575920,"중요합니다."],[1575970,1576120,"즉"],[1576170,1576960,"핸드폰을"],[1577050,1577420,"구매한"],[1577450,1577720,"이후에"],[1577770,1578120,"핸드폰"],[1578210,1578500,"케이스"],[1578510,1578840,"상품을"],[1578850,1579134,"추천해"],[1579134,1579360,"줘야"],[1579360,1579700,"한다는"],[1580150,1580380,"식의"],[1580390,1580980,"순서라든지"],[1581590,1582020,"바지를"],[1582030,1582460,"구매하고"],[1582460,1582680,"나서"]],"textEdited":"비선형적인 관계를 모델이 캡처해야 하고요. 또한 문장의 순서가 중요하듯이 사용자의 행동 순서도 굉장히 중요합니다. 즉 핸드폰을 구매한 이후에 핸드폰 케이스 상품을 추천해 줘야 한다는 식의 순서라든지 바지를 구매하고 나서"},{"start":1583200,"end":1596600,"text":"이젠 바지에 맞는 신발을 찾아보려 한다는 그런 시퀀스가 있는데요. 이제 이런 시퀀스는 이미 자연어 처리에서는 굉장히 중요한 데이터죠. 따라서 CTR 예측 데이터와 NLP 번역 데이터 간에 공통점이 있다는 것을 언급하였고요.","confidence":0.8523,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1583470,1583680,"이젠"],[1583770,1584200,"바지에"],[1584230,1584480,"맞는"],[1584690,1585060,"신발을"],[1585060,1585460,"찾아보려"],[1585460,1585740,"한다는"],[1585750,1585920,"그런"],[1585990,1586460,"시퀀스가"],[1586460,1586800,"있는데요."],[1587530,1587667,"이제"],[1587667,1587840,"이런"],[1587910,1588380,"시퀀스는"],[1588590,1588760,"이미"],[1588810,1589180,"자연어"],[1589190,1589660,"처리에서는"],[1589730,1590000,"굉장히"],[1590110,1590440,"중요한"],[1590470,1590900,"데이터죠."],[1591510,1591840,"따라서"],[1591930,1592260,"CTR"],[1592260,1592440,"예측"],[1592440,1592820,"데이터와"],[1592850,1593260,"NLP"],[1593310,1593540,"번역"],[1593550,1593900,"데이터"],[1594110,1594400,"간에"],[1594790,1595360,"공통점이"],[1595360,1595647,"있다는"],[1595647,1595880,"것을"],[1595910,1596520,"언급하였고요."]],"textEdited":"이젠 바지에 맞는 신발을 찾아보려 한다는 그런 시퀀스가 있는데요. 이제 이런 시퀀스는 이미 자연어 처리에서는 굉장히 중요한 데이터죠. 따라서 CTR 예측 데이터와 NLP 번역 데이터 간에 공통점이 있다는 것을 언급하였고요."},{"start":1596600,"end":1610900,"text":"그래서 NLP 분야에 전반적으로 강력한 성능을 보이는 이 트랜스포머 구조를 CTR 예측에도 적용해 보면 좋지 않을까라는 점에서 이 논문이 등장하게 되었습니다. 그럼 제안 모델인 BST bar 시퀀스 트랜스포머","confidence":0.927,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1596890,1597140,"그래서"],[1597190,1597560,"NLP"],[1597560,1597900,"분야에"],[1598010,1598640,"전반적으로"],[1598790,1599180,"강력한"],[1599290,1599620,"성능을"],[1599630,1599920,"보이는"],[1599950,1600100,"이"],[1600100,1600620,"트랜스포머"],[1600630,1600960,"구조를"],[1601690,1602080,"CTR"],[1602090,1602560,"예측에도"],[1602890,1603147,"적용해"],[1603147,1603340,"보면"],[1603350,1603660,"좋지"],[1603660,1604820,"않을까라는"],[1604820,1605180,"점에서"],[1605450,1605600,"이"],[1605600,1605860,"논문이"],[1605890,1606247,"등장하게"],[1606247,1606740,"되었습니다."],[1607150,1607340,"그럼"],[1607990,1608260,"제안"],[1608290,1608620,"모델인"],[1608790,1609240,"BST"],[1609590,1610060,"bar"],[1610070,1610380,"시퀀스"],[1610380,1610840,"트랜스포머"]],"textEdited":"그래서 NLP 분야에 전반적으로 강력한 성능을 보이는 이 트랜스포머 구조를 CTR 예측에도 적용해 보면 좋지 않을까라는 점에서 이 논문이 등장하게 되었습니다. 그럼 제안 모델인 BST bar 시퀀스 트랜스포머"},{"start":1610900,"end":1625500,"text":"를 다루기 전에 간단하게 트랜스포머를 리뷰하고 넘어가겠습니다. 이미 너무나도 유명한 어텐션 어유니드라는 논문을 통해서 트랜스포머가 공개되었습니다. 트랜스포머에서 제일 중요한 개념은 바로 이 어텐션이라는 메커니즘입니다.","confidence":0.9075,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1611150,1611300,"를"],[1611310,1611594,"다루기"],[1611594,1611860,"전에"],[1612350,1612740,"간단하게"],[1612750,1613360,"트랜스포머를"],[1613360,1613667,"리뷰하고"],[1613667,1614320,"넘어가겠습니다."],[1615210,1615400,"이미"],[1615410,1615780,"너무나도"],[1615780,1616100,"유명한"],[1616330,1616740,"어텐션"],[1617190,1617940,"어유니드라는"],[1618370,1618740,"논문을"],[1618740,1619060,"통해서"],[1619530,1620120,"트랜스포머가"],[1620170,1620840,"공개되었습니다."],[1621370,1622100,"트랜스포머에서"],[1622310,1622620,"제일"],[1622830,1623160,"중요한"],[1623230,1623540,"개념은"],[1623550,1623760,"바로"],[1623760,1623880,"이"],[1623910,1624620,"어텐션이라는"],[1624630,1625300,"메커니즘입니다."]],"textEdited":"를 다루기 전에 간단하게 트랜스포머를 리뷰하고 넘어가겠습니다. 이미 너무나도 유명한 어텐션 어유니드라는 논문을 통해서 트랜스포머가 공개되었습니다. 트랜스포머에서 제일 중요한 개념은 바로 이 어텐션이라는 메커니즘입니다."},{"start":1625500,"end":1640500,"text":"입력값에 대해서 어떤 부분에 주의, 즉 어텐션을 기울일지를 찾는 원리입니다. 어떤 키 밸류 쌍들이 주어지고 우리가 알고자 하는 쿼리가 있을 때 이 쿼리와 키의 연관성을 가중치로 사용해 이 밸류에 곱하여서","confidence":0.9783,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1625850,1626547,"입력값에"],[1626547,1626860,"대해서"],[1627030,1627280,"어떤"],[1627370,1627700,"부분에"],[1627830,1628120,"주의,"],[1628210,1628360,"즉"],[1628410,1628960,"어텐션을"],[1629070,1629640,"기울일지를"],[1629710,1629940,"찾는"],[1630010,1630500,"원리입니다."],[1631110,1631320,"어떤"],[1631430,1631580,"키"],[1631630,1631860,"밸류"],[1631860,1632240,"쌍들이"],[1632240,1632720,"주어지고"],[1633110,1633360,"우리가"],[1633390,1633747,"알고자"],[1633747,1633940,"하는"],[1634050,1634440,"쿼리가"],[1634490,1634720,"있을"],[1634730,1634880,"때"],[1635370,1635520,"이"],[1635650,1636080,"쿼리와"],[1636350,1636680,"키의"],[1636810,1637440,"연관성을"],[1637510,1638700,"가중치로"],[1638710,1639060,"사용해"],[1639230,1639380,"이"],[1639390,1639740,"밸류에"],[1639790,1640280,"곱하여서"]],"textEdited":"입력값에 대해서 어떤 부분에 주의, 즉 어텐션을 기울일지를 찾는 원리입니다. 어떤 키 밸류 쌍들이 주어지고 우리가 알고자 하는 쿼리가 있을 때 이 쿼리와 키의 연관성을 가중치로 사용해 이 밸류에 곱하여서"},{"start":1640500,"end":1654800,"text":"최종적으로 이 값을 가중합으로 사용합니다. 간단하게 수식으로 정리하자면 이 QKV가 쿼리 키 밸류인데요. 쿼리와 키의 유사도를 가중치로 사용해서 이 밸류","confidence":0.9529,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1640790,1641300,"최종적으로"],[1641330,1641480,"이"],[1641530,1641880,"값을"],[1641930,1642440,"가중합으로"],[1642440,1642920,"사용합니다."],[1643910,1644340,"간단하게"],[1644390,1644840,"수식으로"],[1645430,1645940,"정리하자면"],[1646950,1647100,"이"],[1647150,1648020,"QKV가"],[1648130,1648420,"쿼리"],[1648670,1648820,"키"],[1648930,1649500,"밸류인데요."],[1650570,1650960,"쿼리와"],[1651130,1651420,"키의"],[1651420,1651880,"유사도를"],[1651930,1652420,"가중치로"],[1652420,1652940,"사용해서"],[1653850,1654000,"이"],[1654030,1654340,"밸류"]],"textEdited":"최종적으로 이 값을 가중합으로 사용합니다. 간단하게 수식으로 정리하자면 이 QKV가 쿼리 키 밸류인데요. 쿼리와 키의 유사도를 가중치로 사용해서 이 밸류"},{"start":1654800,"end":1668500,"text":"를 모두 더해준 값입니다. 또한 어텐션 메커니즘을 사용하면은 입력과 출력의 길이를 고려하지 않아도 단어 간의 의존성을 자연스럽게 파악할 수 있습니다. 그래서 이는 번역과 같은 자연어 처리에서 직관적으로 표현이 되는데요.","confidence":0.9773,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1655050,1655200,"를"],[1655210,1655420,"모두"],[1655430,1655760,"더해준"],[1655790,1656220,"값입니다."],[1656290,1656500,"또한"],[1656570,1656980,"어텐션"],[1657130,1657620,"메커니즘을"],[1657620,1658140,"사용하면은"],[1658750,1659180,"입력과"],[1659270,1659600,"출력의"],[1659610,1659920,"길이를"],[1659930,1660360,"고려하지"],[1660360,1660700,"않아도"],[1661250,1661527,"단어"],[1661527,1661780,"간의"],[1661790,1662280,"의존성을"],[1662350,1662860,"자연스럽게"],[1662950,1663320,"파악할"],[1663330,1663480,"수"],[1663480,1663800,"있습니다."],[1663970,1664127,"그래서"],[1664127,1664320,"이는"],[1664750,1665120,"번역과"],[1665120,1665320,"같은"],[1665350,1665660,"자연어"],[1665660,1666040,"처리에서"],[1666630,1667080,"직관적으로"],[1667080,1667294,"표현이"],[1667294,1667720,"되는데요."]],"textEdited":"를 모두 더해준 값입니다. 또한 어텐션 메커니즘을 사용하면은 입력과 출력의 길이를 고려하지 않아도 단어 간의 의존성을 자연스럽게 파악할 수 있습니다. 그래서 이는 번역과 같은 자연어 처리에서 직관적으로 표현이 되는데요."},{"start":1668500,"end":1682500,"text":"이 다음 예시가 자연어 처리에서 어떻게 어터션 메커니즘이 작동하는지를 간단하게 설명합니다. 그래서 이 주어진 단어가 이 이이라는 단어가 되겠고요. 이 전체 데이터는 키가 됩니다. 그래서 주어진 단어에 대해서 전체","confidence":0.9411,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1668790,1668940,"이"],[1668940,1669100,"다음"],[1669150,1669500,"예시가"],[1669790,1670140,"자연어"],[1670140,1670500,"처리에서"],[1670530,1670840,"어떻게"],[1670850,1671260,"어터션"],[1671290,1671820,"메커니즘이"],[1671890,1672600,"작동하는지를"],[1672710,1673100,"간단하게"],[1673110,1673600,"설명합니다."],[1673790,1673947,"그래서"],[1673947,1674080,"이"],[1674080,1674420,"주어진"],[1674510,1674900,"단어가"],[1674970,1675120,"이"],[1675210,1675700,"이이라는"],[1675730,1676120,"단어가"],[1676120,1676560,"되겠고요."],[1677310,1677460,"이"],[1677490,1677820,"전체"],[1678310,1678780,"데이터는"],[1679290,1679560,"키가"],[1679560,1679840,"됩니다."],[1680550,1680760,"그래서"],[1680770,1681160,"주어진"],[1681210,1681580,"단어에"],[1681580,1681860,"대해서"],[1681970,1682280,"전체"]],"textEdited":"이 다음 예시가 자연어 처리에서 어떻게 어터션 메커니즘이 작동하는지를 간단하게 설명합니다. 그래서 이 주어진 단어가 이 이이라는 단어가 되겠고요. 이 전체 데이터는 키가 됩니다. 그래서 주어진 단어에 대해서 전체"},{"start":1682500,"end":1691400,"text":"단어와 얼마나 관계가 있는지를 가중치로 사용하여서 가중치가 높을수록 색깔이 더 진한 것을 알 수 있는데요.","confidence":0.985,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1682710,1683300,"단어와"],[1683610,1683960,"얼마나"],[1684030,1684340,"관계가"],[1684340,1684760,"있는지를"],[1685170,1685740,"가중치로"],[1685740,1686220,"사용하여서"],[1686750,1687180,"가중치가"],[1687180,1687660,"높을수록"],[1687910,1688187,"색깔이"],[1688187,1688320,"더"],[1688330,1688580,"진한"],[1690070,1690320,"것을"],[1690320,1690427,"알"],[1690427,1690514,"수"],[1690514,1691320,"있는데요."]],"textEdited":"단어와 얼마나 관계가 있는지를 가중치로 사용하여서 가중치가 높을수록 색깔이 더 진한 것을 알 수 있는데요."},{"start":1691400,"end":1704200,"text":"그 가중치가 높을수록 이 키가 가지고 있는 밸류를 더 많이 참고해서 최종적으로 이 쿼리에 대한 표현을 어텐션을 통해서 계산할 수 있게 됩니다. 보시면 이 이이라는 명사가","confidence":0.9702,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1691670,1691820,"그"],[1691830,1692240,"가중치가"],[1692240,1692760,"높을수록"],[1693110,1693260,"이"],[1693270,1693540,"키가"],[1693550,1693880,"가지고"],[1693890,1694100,"있는"],[1694650,1695000,"밸류를"],[1695010,1695160,"더"],[1695190,1695400,"많이"],[1695690,1696160,"참고해서"],[1696490,1697040,"최종적으로"],[1697110,1697260,"이"],[1697470,1697827,"쿼리에"],[1697827,1698020,"대한"],[1698150,1698540,"표현을"],[1699590,1700080,"어텐션을"],[1700080,1700400,"통해서"],[1700970,1701340,"계산할"],[1701340,1701414,"수"],[1701414,1701547,"있게"],[1701547,1701860,"됩니다."],[1702390,1702680,"보시면"],[1702690,1702840,"이"],[1702870,1703320,"이이라는"],[1703350,1703780,"명사가"]],"textEdited":"그 가중치가 높을수록 이 키가 가지고 있는 밸류를 더 많이 참고해서 최종적으로 이 쿼리에 대한 표현을 어텐션을 통해서 계산할 수 있게 됩니다. 보시면 이 이이라는 명사가"},{"start":1704200,"end":1718600,"text":"어떤 단어와 가장 관련이 있을지를 봤을 때 이 스트릿이랑 애니멀이랑 스트릿이라는 단어의 유사도가 제일 높고 그래서 이 쿼리와 이 두 개의 키 간에 계산되는 가중치가 가장 높게","confidence":0.9144,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1704510,1704880,"어떤"],[1705270,1705640,"단어와"],[1705710,1705940,"가장"],[1706010,1706307,"관련이"],[1706307,1706720,"있을지를"],[1706720,1706980,"봤을"],[1706980,1707120,"때"],[1707120,1707240,"이"],[1707250,1707820,"스트릿이랑"],[1708590,1709120,"애니멀이랑"],[1709170,1709760,"스트릿이라는"],[1709760,1710120,"단어의"],[1711130,1711487,"유사도가"],[1711487,1711660,"제일"],[1711660,1711940,"높고"],[1712230,1712480,"그래서"],[1712480,1712620,"이"],[1712670,1713060,"쿼리와"],[1713710,1713860,"이"],[1713890,1714040,"두"],[1714040,1714300,"개의"],[1714450,1714600,"키"],[1715150,1715460,"간에"],[1716550,1716960,"계산되는"],[1716990,1717440,"가중치가"],[1717440,1717620,"가장"],[1717650,1717960,"높게"]],"textEdited":"어떤 단어와 가장 관련이 있을지를 봤을 때 이 스트릿이랑 애니멀이랑 스트릿이라는 단어의 유사도가 제일 높고 그래서 이 쿼리와 이 두 개의 키 간에 계산되는 가중치가 가장 높게"},{"start":1718600,"end":1731600,"text":"계산되고 그래서 이 애니멀과 스트릿에 해당하는 밸류를 가장 많이 참고하게 됩니다. 트랜스포머의 어텐션 연산은 스케일드 닷 프로덕트 어텐션을 사용하는데요. 이 수식은 다음과 같습니다.","confidence":0.9346,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1719210,1719700,"계산되고"],[1720070,1720247,"그래서"],[1720247,1720380,"이"],[1720380,1720920,"애니멀과"],[1721010,1721394,"스트릿에"],[1721394,1721700,"해당하는"],[1721750,1722160,"밸류를"],[1722290,1722520,"가장"],[1722550,1722760,"많이"],[1723710,1724140,"참고하게"],[1724140,1724440,"됩니다."],[1724930,1725580,"트랜스포머의"],[1725590,1726020,"어텐션"],[1726070,1726500,"연산은"],[1726930,1727420,"스케일드"],[1727420,1727540,"닷"],[1727710,1728100,"프로덕트"],[1728100,1728580,"어텐션을"],[1728580,1729080,"사용하는데요."],[1729710,1729860,"이"],[1729860,1730240,"수식은"],[1730530,1730940,"다음과"],[1731010,1731400,"같습니다."]],"textEdited":"계산되고 그래서 이 애니멀과 스트릿에 해당하는 밸류를 가장 많이 참고하게 됩니다. 트랜스포머의 어텐션 연산은 스케일드 닷 프로덕트 어텐션을 사용하는데요. 이 수식은 다음과 같습니다."},{"start":1731600,"end":1744400,"text":"아마 많이 보셨을 텐데요. 쿼리와 키 벡터를 내적한 값을 이제 이 키의 차원인 디케로 나눠서 스케일링 해주고 이를 소프트맥스 펑션에 통과시킵니다.","confidence":0.9068,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1732110,1732300,"아마"],[1732350,1732540,"많이"],[1732610,1733000,"보셨을"],[1733000,1733320,"텐데요."],[1734330,1734760,"쿼리와"],[1734910,1735060,"키"],[1735130,1735580,"벡터를"],[1735950,1736380,"내적한"],[1737030,1737360,"값을"],[1737850,1738020,"이제"],[1738020,1738160,"이"],[1738210,1738500,"키의"],[1738550,1738900,"차원인"],[1739330,1739960,"디케로"],[1740370,1740720,"나눠서"],[1740730,1741067,"스케일링"],[1741067,1741400,"해주고"],[1741830,1742040,"이를"],[1742110,1742800,"소프트맥스"],[1743210,1743620,"펑션에"],[1743710,1744400,"통과시킵니다."]],"textEdited":"아마 많이 보셨을 텐데요. 쿼리와 키 벡터를 내적한 값을 이제 이 키의 차원인 디케로 나눠서 스케일링 해주고 이를 소프트맥스 펑션에 통과시킵니다."},{"start":1744400,"end":1757000,"text":"이 소프트맥스 펑션은 각각의 쿼리 쿼리와 키의 유사도 즉 가중치가 되고요. 이 값을 밸류에 곱해서 전체 밸류의 가중합으로 표현해 주게 됩니다.","confidence":0.9274,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1745070,1745220,"이"],[1745220,1745700,"소프트맥스"],[1745710,1746120,"펑션은"],[1746690,1747080,"각각의"],[1747270,1747520,"쿼리"],[1748110,1748480,"쿼리와"],[1748530,1748800,"키의"],[1748800,1749180,"유사도"],[1749850,1750000,"즉"],[1750030,1750487,"가중치가"],[1750487,1750820,"되고요."],[1751870,1752020,"이"],[1752020,1752360,"값을"],[1752450,1752820,"밸류에"],[1752870,1753300,"곱해서"],[1754070,1754400,"전체"],[1754690,1755040,"밸류의"],[1755070,1755700,"가중합으로"],[1755930,1756214,"표현해"],[1756214,1756387,"주게"],[1756387,1756800,"됩니다."]],"textEdited":"이 소프트맥스 펑션은 각각의 쿼리 쿼리와 키의 유사도 즉 가중치가 되고요. 이 값을 밸류에 곱해서 전체 밸류의 가중합으로 표현해 주게 됩니다."},{"start":1757000,"end":1768700,"text":"또한 스케일드 닷 프로덕트 어텐션 같은 경우에는 셀프 어텐션에 해당되는데요. 이 셀프 어텐션이랑 퀄이나 키와 밸류 그 모든 것이 같은 도메인을 공유한다는 것입니다.","confidence":0.9457,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1757550,1757780,"또한"],[1757830,1758214,"스케일드"],[1758214,1758340,"닷"],[1758930,1759274,"프로덕트"],[1759274,1759620,"어텐션"],[1759630,1759847,"같은"],[1759847,1760180,"경우에는"],[1760210,1760540,"셀프"],[1760540,1760914,"어텐션에"],[1760914,1761480,"해당되는데요."],[1761910,1762060,"이"],[1762060,1762267,"셀프"],[1762267,1762780,"어텐션이랑"],[1763530,1763960,"퀄이나"],[1764170,1764520,"키와"],[1764750,1765060,"밸류"],[1765310,1765460,"그"],[1765460,1765660,"모든"],[1765660,1765880,"것이"],[1765890,1766160,"같은"],[1766210,1766740,"도메인을"],[1767330,1767774,"공유한다는"],[1767774,1768340,"것입니다."]],"textEdited":"또한 스케일드 닷 프로덕트 어텐션 같은 경우에는 셀프 어텐션에 해당되는데요. 이 셀프 어텐션이랑 퀄이나 키와 밸류 그 모든 것이 같은 도메인을 공유한다는 것입니다."},{"start":1768700,"end":1782200,"text":"똑같은 입력 값에 대해서 서로 다른 파라미터가 곱해져서 쿼리 키 밸류를 구성하고 있습니다. 다음은 방금 전에 배웠던 스케일 닷 프로덕트 어텐션을 병렬적으로 확장시킨 멀티헤드 어텐션입니다.","confidence":0.9429,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1768930,1769280,"똑같은"],[1769370,1769620,"입력"],[1769620,1769854,"값에"],[1769854,1770160,"대해서"],[1770650,1770867,"서로"],[1770867,1771060,"다른"],[1771090,1771620,"파라미터가"],[1771620,1772020,"곱해져서"],[1772390,1772700,"쿼리"],[1772810,1772960,"키"],[1773130,1773580,"밸류를"],[1773970,1774360,"구성하고"],[1774360,1774720,"있습니다."],[1776070,1776380,"다음은"],[1776950,1777147,"방금"],[1777147,1777360,"전에"],[1777360,1777600,"배웠던"],[1777670,1777980,"스케일"],[1777980,1778120,"닷"],[1778150,1778467,"프로덕트"],[1778467,1779340,"어텐션을"],[1779610,1780320,"병렬적으로"],[1780450,1780940,"확장시킨"],[1781050,1781467,"멀티헤드"],[1781467,1782080,"어텐션입니다."]],"textEdited":"똑같은 입력 값에 대해서 서로 다른 파라미터가 곱해져서 쿼리 키 밸류를 구성하고 있습니다. 다음은 방금 전에 배웠던 스케일 닷 프로덕트 어텐션을 병렬적으로 확장시킨 멀티헤드 어텐션입니다."},{"start":1782200,"end":1795300,"text":"차원이 큰 하나의 어텐션을 수행하는 것보다는 여러 개의 스케일 더 프로덕트 어텐션을 병렬적으로 처리하는 것이 더 효과적이라는 것인데요. 이 하나하나의 스케일 더 프로덕트 어텐션이 하나의 헤드가 됩니다.","confidence":0.899,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1782610,1782980,"차원이"],[1782990,1783140,"큰"],[1783190,1783500,"하나의"],[1783500,1783960,"어텐션을"],[1783970,1784347,"수행하는"],[1784347,1784840,"것보다는"],[1785410,1785660,"여러"],[1785660,1785940,"개의"],[1786270,1786607,"스케일"],[1786607,1786740,"더"],[1786790,1787160,"프로덕트"],[1787160,1787660,"어텐션을"],[1787790,1788440,"병렬적으로"],[1788470,1788847,"처리하는"],[1788847,1789014,"것이"],[1789014,1789140,"더"],[1789150,1789727,"효과적이라는"],[1789727,1790160,"것인데요."],[1791210,1791360,"이"],[1791670,1792200,"하나하나의"],[1792230,1792494,"스케일"],[1792494,1792620,"더"],[1792630,1793340,"프로덕트"],[1793490,1794060,"어텐션이"],[1794190,1794500,"하나의"],[1794570,1794900,"헤드가"],[1794900,1795200,"됩니다."]],"textEdited":"차원이 큰 하나의 어텐션을 수행하는 것보다는 여러 개의 스케일 더 프로덕트 어텐션을 병렬적으로 처리하는 것이 더 효과적이라는 것인데요. 이 하나하나의 스케일 더 프로덕트 어텐션이 하나의 헤드가 됩니다."},{"start":1795300,"end":1809000,"text":"그래서 이 하나의 헤드가 병렬로 처리되면서 각각의 헤드가 서로 다른 이 데이터의 특징을 잘 찾아내기를 잘 표현하기를 기대하는 것입니다. 네 그래서 트랜스포머 전체 구조를 살펴보면","confidence":0.969,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1795970,1796147,"그래서"],[1796147,1796280,"이"],[1796290,1796580,"하나의"],[1796610,1796960,"헤드가"],[1797830,1798200,"병렬로"],[1798200,1798720,"처리되면서"],[1798990,1799340,"각각의"],[1799340,1799620,"헤드가"],[1799650,1799960,"서로"],[1799970,1800240,"다른"],[1800830,1800980,"이"],[1800980,1801400,"데이터의"],[1801510,1801860,"특징을"],[1802090,1802240,"잘"],[1802690,1803240,"찾아내기를"],[1803450,1803600,"잘"],[1803690,1804200,"표현하기를"],[1804330,1804760,"기대하는"],[1804890,1805320,"것입니다."],[1806090,1806240,"네"],[1806240,1806480,"그래서"],[1806650,1807200,"트랜스포머"],[1807270,1807560,"전체"],[1807630,1807980,"구조를"],[1808010,1808420,"살펴보면"]],"textEdited":"그래서 이 하나의 헤드가 병렬로 처리되면서 각각의 헤드가 서로 다른 이 데이터의 특징을 잘 찾아내기를 잘 표현하기를 기대하는 것입니다. 네 그래서 트랜스포머 전체 구조를 살펴보면"},{"start":1809000,"end":1822000,"text":"인코더 구조와 디코더 구조로 이루어져 있고요. 각각의 인코더와 디코더는 6개의 동일한 레이어를 쌓아서 사용하고 있습니다. 이 어텐션 레이어 외에도 인코더와 디코더에 입력하기 전에","confidence":0.9793,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1809310,1809740,"인코더"],[1810010,1810340,"구조와"],[1810730,1811160,"디코더"],[1811730,1812060,"구조로"],[1812250,1812640,"이루어져"],[1812640,1812940,"있고요."],[1813330,1813660,"각각의"],[1813670,1814080,"인코더와"],[1814090,1814560,"디코더는"],[1815110,1815480,"6개의"],[1815510,1815800,"동일한"],[1815830,1816160,"레이어를"],[1816160,1816460,"쌓아서"],[1816710,1817100,"사용하고"],[1817100,1817460,"있습니다."],[1817670,1817820,"이"],[1817820,1818180,"어텐션"],[1818230,1818540,"레이어"],[1819030,1819420,"외에도"],[1819970,1820440,"인코더와"],[1820490,1820920,"디코더에"],[1820920,1821267,"입력하기"],[1821267,1821580,"전에"]],"textEdited":"인코더 구조와 디코더 구조로 이루어져 있고요. 각각의 인코더와 디코더는 6개의 동일한 레이어를 쌓아서 사용하고 있습니다. 이 어텐션 레이어 외에도 인코더와 디코더에 입력하기 전에"},{"start":1822000,"end":1837000,"text":"포지셔널 인코딩이라는 기법을 사용하는데요. 이 순서에 대한 정보를 주기 위해서 사인 함수와 코사인 함수를 사용하여서 기존의 RNN을 사용하지 않더라도 각각의 시퀀스에 대한 정보를","confidence":0.975,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1822290,1822840,"포지셔널"],[1823290,1824000,"인코딩이라는"],[1824330,1824660,"기법을"],[1824660,1825240,"사용하는데요."],[1825990,1826140,"이"],[1826230,1826640,"순서에"],[1826640,1826820,"대한"],[1827590,1827940,"정보를"],[1827940,1828180,"주기"],[1828180,1828480,"위해서"],[1828950,1829240,"사인"],[1829290,1829600,"함수와"],[1829630,1829980,"코사인"],[1830010,1830400,"함수를"],[1830400,1830920,"사용하여서"],[1831790,1832140,"기존의"],[1832310,1832920,"RNN을"],[1832930,1833340,"사용하지"],[1833350,1833820,"않더라도"],[1834370,1834720,"각각의"],[1834790,1835267,"시퀀스에"],[1835267,1835460,"대한"],[1836070,1836460,"정보를"]],"textEdited":"포지셔널 인코딩이라는 기법을 사용하는데요. 이 순서에 대한 정보를 주기 위해서 사인 함수와 코사인 함수를 사용하여서 기존의 RNN을 사용하지 않더라도 각각의 시퀀스에 대한 정보를"},{"start":1837000,"end":1850100,"text":"이 포지셔널 인코딩을 사용해서 표현하고 있습니다. 이제 그 외에도 이 모델을 살펴보면 여기 에드앤 롬이라는 부분이 있는데요. 이 멀티헤드 어텐션이 일어난 이후에 레지듀얼 커넥션과 레이어 노멀라이제이션을 사용하는","confidence":0.9084,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1837210,1837360,"이"],[1837370,1837780,"포지셔널"],[1837780,1838147,"인코딩을"],[1838147,1838600,"사용해서"],[1838790,1839127,"표현하고"],[1839127,1839460,"있습니다."],[1839510,1839680,"이제"],[1839680,1839800,"그"],[1839800,1840220,"외에도"],[1840810,1840960,"이"],[1840960,1841260,"모델을"],[1841260,1841680,"살펴보면"],[1841990,1842160,"여기"],[1842210,1842600,"에드앤"],[1842650,1843100,"롬이라는"],[1843100,1843327,"부분이"],[1843327,1843680,"있는데요."],[1844790,1844940,"이"],[1845090,1845500,"멀티헤드"],[1845510,1846400,"어텐션이"],[1846400,1846640,"일어난"],[1846690,1847020,"이후에"],[1847530,1847960,"레지듀얼"],[1847960,1848400,"커넥션과"],[1848410,1848660,"레이어"],[1848660,1849440,"노멀라이제이션을"],[1849440,1849820,"사용하는"]],"textEdited":"이 포지셔널 인코딩을 사용해서 표현하고 있습니다. 이제 그 외에도 이 모델을 살펴보면 여기 에드앤 롬이라는 부분이 있는데요. 이 멀티헤드 어텐션이 일어난 이후에 레지듀얼 커넥션과 레이어 노멀라이제이션을 사용하는"},{"start":1850100,"end":1862900,"text":"그런 테크닉도 추가되어 있습니다. 뭐 그 외에도 다양한 중요한 부분들이 트랜스포머에 포함되어 있지만 이제 우리는 트랜스포머를 배우려고 하는 것이 아니라 결국에는 이 트랜스포머를 CTR 예측에 어떻게 사용해야 했느냐","confidence":0.93,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1850410,1850580,"그런"],[1850990,1851460,"테크닉도"],[1851460,1851747,"추가되어"],[1851747,1852100,"있습니다."],[1852290,1852440,"뭐"],[1852440,1852580,"그"],[1852580,1852920,"외에도"],[1852990,1853340,"다양한"],[1853870,1854160,"중요한"],[1854160,1854520,"부분들이"],[1854520,1855100,"트랜스포머에"],[1855110,1855500,"포함되어"],[1855500,1855780,"있지만"],[1856430,1856600,"이제"],[1856610,1856920,"우리는"],[1857170,1857820,"트랜스포머를"],[1857820,1858160,"배우려고"],[1858160,1858287,"하는"],[1858287,1858487,"것이"],[1858487,1858740,"아니라"],[1858810,1859160,"결국에는"],[1859160,1859300,"이"],[1859310,1860000,"트랜스포머를"],[1860570,1860940,"CTR"],[1860950,1861260,"예측에"],[1861330,1861660,"어떻게"],[1861690,1862120,"사용해야"],[1862410,1862740,"했느냐"]],"textEdited":"그런 테크닉도 추가되어 있습니다. 뭐 그 외에도 다양한 중요한 부분들이 트랜스포머에 포함되어 있지만 이제 우리는 트랜스포머를 배우려고 하는 것이 아니라 결국에는 이 트랜스포머를 CTR 예측에 어떻게 사용해야 했느냐"},{"start":1862900,"end":1874400,"text":"때문에 간단히 이 부분을 리뷰했는데요. 이제 다시 돌아와서 이 비스티 모델에서 사용한 구조를 살펴봅시다. 네 다음은 비스티의 전체 구조입니다.","confidence":0.7571,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1863150,1863500,"때문에"],[1863630,1863940,"간단히"],[1863970,1864120,"이"],[1864450,1864740,"부분을"],[1864740,1865300,"리뷰했는데요."],[1866050,1866220,"이제"],[1866220,1866460,"다시"],[1866460,1866860,"돌아와서"],[1866950,1867100,"이"],[1867110,1867600,"비스티"],[1868250,1868720,"모델에서"],[1868830,1869180,"사용한"],[1869930,1870260,"구조를"],[1870270,1870800,"살펴봅시다."],[1871870,1872020,"네"],[1872110,1872460,"다음은"],[1872510,1873360,"비스티의"],[1873410,1873680,"전체"],[1873680,1874140,"구조입니다."]],"textEdited":"때문에 간단히 이 부분을 리뷰했는데요. 이제 다시 돌아와서 이 비스티 모델에서 사용한 구조를 살펴봅시다. 네 다음은 비스티의 전체 구조입니다."},{"start":1874400,"end":1884300,"text":"이 마지막 예측을 하는 것은 똑같이 클릭 여부가 될 텐데요. 이제 여기서 그동안의 CTR 모델과 가장 큰 차이점은 바로 이 입력값 부분입니다.","confidence":0.8375,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1875010,1875160,"이"],[1875160,1875540,"마지막"],[1875750,1876440,"예측을"],[1876440,1876660,"하는"],[1876660,1876900,"것은"],[1876990,1877320,"똑같이"],[1877350,1877560,"클릭"],[1877570,1877920,"여부가"],[1877920,1878060,"될"],[1878060,1878380,"텐데요."],[1879110,1879280,"이제"],[1879280,1879560,"여기서"],[1880430,1880840,"그동안의"],[1880890,1881260,"CTR"],[1881350,1881720,"모델과"],[1881750,1881980,"가장"],[1882050,1882200,"큰"],[1882230,1882680,"차이점은"],[1882750,1883000,"바로"],[1883010,1883160,"이"],[1883250,1883687,"입력값"],[1883687,1884140,"부분입니다."]],"textEdited":"이 마지막 예측을 하는 것은 똑같이 클릭 여부가 될 텐데요. 이제 여기서 그동안의 CTR 모델과 가장 큰 차이점은 바로 이 입력값 부분입니다."},{"start":1884300,"end":1897200,"text":"유저 BA뷰 시퀀스를 그대로 CTR 예측 모델의 입력 값으로 넣어주었다는 것인데요. 과거에 유저가 소비했던 아이템들의 집합이 아닌 그 시퀀스까지 이 모델이","confidence":0.9245,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1885190,1885480,"유저"],[1885530,1885940,"BA뷰"],[1886030,1886620,"시퀀스를"],[1886690,1887060,"그대로"],[1887530,1887900,"CTR"],[1887910,1888160,"예측"],[1888160,1888460,"모델의"],[1888510,1888760,"입력"],[1888790,1889220,"값으로"],[1889810,1890447,"넣어주었다는"],[1890447,1890860,"것인데요."],[1891970,1892360,"과거에"],[1892390,1892720,"유저가"],[1892770,1893140,"소비했던"],[1893170,1894100,"아이템들의"],[1894170,1894567,"집합이"],[1894567,1894740,"아닌"],[1895050,1895200,"그"],[1895270,1896060,"시퀀스까지"],[1896330,1896480,"이"],[1896480,1896880,"모델이"]],"textEdited":"유저 BA뷰 시퀀스를 그대로 CTR 예측 모델의 입력 값으로 넣어주었다는 것인데요. 과거에 유저가 소비했던 아이템들의 집합이 아닌 그 시퀀스까지 이 모델이"},{"start":1897200,"end":1909500,"text":"표현하여서 최종적으로 현재 타겟 아이템에 대한 CTR을 더 정확하게 구하겠다는 것이죠. 그래서 여기 트랜스포머 레이어가 있고요. 그 외에 아더 피처 같은 경우에는 시퀀스 피처가 아닌","confidence":0.9777,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1897430,1897980,"표현하여서"],[1898650,1899180,"최종적으로"],[1899270,1899540,"현재"],[1899850,1900100,"타겟"],[1900100,1900507,"아이템에"],[1900507,1900680,"대한"],[1900770,1901240,"CTR을"],[1901610,1901760,"더"],[1901790,1902200,"정확하게"],[1902200,1902667,"구하겠다는"],[1902667,1902980,"것이죠."],[1903710,1903980,"그래서"],[1904010,1904200,"여기"],[1904230,1904780,"트랜스포머"],[1904790,1905240,"레이어가"],[1905310,1905620,"있고요."],[1906010,1906160,"그"],[1906160,1906340,"외에"],[1906450,1906720,"아더"],[1906750,1907020,"피처"],[1907190,1907414,"같은"],[1907414,1907740,"경우에는"],[1907810,1908300,"시퀀스"],[1908410,1908714,"피처가"],[1908714,1908900,"아닌"]],"textEdited":"표현하여서 최종적으로 현재 타겟 아이템에 대한 CTR을 더 정확하게 구하겠다는 것이죠. 그래서 여기 트랜스포머 레이어가 있고요. 그 외에 아더 피처 같은 경우에는 시퀀스 피처가 아닌"},{"start":1909500,"end":1924400,"text":"그냥 기존에 기존에 CTR 예측 모델에서 많이 사용되던 유저 피처나 컨텍스트 피처가 있고요. 이것들은 그냥 인베딩 이후에 바로 반영이 되고요. 이제 이 시퀀스 피처 거기다가 우리가 예측해야 되는 아이템까지 한 번에 묶어서","confidence":0.9233,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1909690,1909880,"그냥"],[1909880,1910160,"기존에"],[1910450,1910800,"기존에"],[1911270,1911580,"CTR"],[1911580,1911767,"예측"],[1911767,1912034,"모델에서"],[1912034,1912200,"많이"],[1912230,1912620,"사용되던"],[1912730,1913020,"유저"],[1913250,1913620,"피처나"],[1913690,1914140,"컨텍스트"],[1914140,1914680,"피처가"],[1914690,1915000,"있고요."],[1915650,1916040,"이것들은"],[1916040,1916200,"그냥"],[1916250,1916600,"인베딩"],[1916610,1916920,"이후에"],[1916990,1917260,"바로"],[1917470,1917800,"반영이"],[1917800,1918120,"되고요."],[1918710,1918880,"이제"],[1918890,1919040,"이"],[1919070,1919460,"시퀀스"],[1919490,1919760,"피처"],[1920470,1920860,"거기다가"],[1921030,1921280,"우리가"],[1921550,1921974,"예측해야"],[1921974,1922160,"되는"],[1922160,1922840,"아이템까지"],[1923270,1923387,"한"],[1923387,1923620,"번에"],[1923690,1924060,"묶어서"]],"textEdited":"그냥 기존에 기존에 CTR 예측 모델에서 많이 사용되던 유저 피처나 컨텍스트 피처가 있고요. 이것들은 그냥 인베딩 이후에 바로 반영이 되고요. 이제 이 시퀀스 피처 거기다가 우리가 예측해야 되는 아이템까지 한 번에 묶어서"},{"start":1924400,"end":1937600,"text":"이 전체 리스트가 트랜스포머 레이어의 인풋으로 들어가게 됩니다. 그리고 이제 이 부분을 보시면은 트랜스포머 레이어라고 했지만 트랜스포머의 인코더 부분만을 사용했습니다. 그래서 디코더 부분을 제외하고","confidence":0.9492,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1925110,1925260,"이"],[1925430,1925860,"전체"],[1926090,1926560,"리스트가"],[1927090,1927640,"트랜스포머"],[1927670,1928040,"레이어의"],[1928110,1928580,"인풋으로"],[1928730,1929067,"들어가게"],[1929067,1929380,"됩니다."],[1929950,1930180,"그리고"],[1930180,1930340,"이제"],[1930350,1930467,"이"],[1930467,1930780,"부분을"],[1930780,1931240,"보시면은"],[1931710,1932240,"트랜스포머"],[1932240,1932607,"레이어라고"],[1932607,1932920,"했지만"],[1933070,1933620,"트랜스포머의"],[1933670,1934120,"인코더"],[1934120,1934660,"부분만을"],[1934670,1935300,"사용했습니다."],[1935690,1935880,"그래서"],[1935880,1936240,"디코더"],[1936240,1936520,"부분을"],[1936530,1937020,"제외하고"]],"textEdited":"이 전체 리스트가 트랜스포머 레이어의 인풋으로 들어가게 됩니다. 그리고 이제 이 부분을 보시면은 트랜스포머 레이어라고 했지만 트랜스포머의 인코더 부분만을 사용했습니다. 그래서 디코더 부분을 제외하고"},{"start":1937600,"end":1951100,"text":"이 인코더 부분만 이 안에 넣어서 전체 시퀀스를 적절하게 레프레젠테이션 하고 있습니다. 다음 부분은 비스티 모델에서 사용하는 트랜스포머 인코더 레이어를 수식으로 나타내는 것입니다.","confidence":0.8711,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1937930,1938080,"이"],[1938080,1938460,"인코더"],[1938460,1938840,"부분만"],[1939350,1939467,"이"],[1939467,1939680,"안에"],[1939680,1940000,"넣어서"],[1940670,1940960,"전체"],[1941050,1941620,"시퀀스를"],[1942310,1942780,"적절하게"],[1943230,1943900,"레프레젠테이션"],[1943900,1944100,"하고"],[1944100,1944460,"있습니다."],[1945210,1945440,"다음"],[1945470,1945800,"부분은"],[1946010,1946520,"비스티"],[1946650,1947040,"모델에서"],[1947040,1947420,"사용하는"],[1947590,1948200,"트랜스포머"],[1948550,1948920,"인코더"],[1948930,1949340,"레이어를"],[1949850,1950260,"수식으로"],[1950270,1950614,"나타내는"],[1950614,1951020,"것입니다."]],"textEdited":"이 인코더 부분만 이 안에 넣어서 전체 시퀀스를 적절하게 레프레젠테이션 하고 있습니다. 다음 부분은 비스티 모델에서 사용하는 트랜스포머 인코더 레이어를 수식으로 나타내는 것입니다."},{"start":1951100,"end":1961800,"text":"사실 기존의 트랜스포머와 완전 동일하고요. 먼저 트랜스포머의 입력 값 즉 아까 유저 BA뷰 시퀀스 전체 플러스 현재 예측하려고 하는 타겟 아이템 전체가","confidence":0.9501,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1951670,1951880,"사실"],[1951890,1952200,"기존의"],[1952270,1952747,"트랜스포머와"],[1952747,1952960,"완전"],[1953010,1953540,"동일하고요."],[1954270,1954500,"먼저"],[1954530,1955100,"트랜스포머의"],[1955150,1955440,"입력"],[1955490,1955640,"값"],[1955950,1956100,"즉"],[1956190,1956440,"아까"],[1957010,1957280,"유저"],[1957310,1957680,"BA뷰"],[1957750,1958100,"시퀀스"],[1958110,1958440,"전체"],[1958590,1958980,"플러스"],[1959110,1959380,"현재"],[1959510,1959954,"예측하려고"],[1959954,1960120,"하는"],[1960150,1960360,"타겟"],[1960360,1960680,"아이템"],[1961130,1961600,"전체가"]],"textEdited":"사실 기존의 트랜스포머와 완전 동일하고요. 먼저 트랜스포머의 입력 값 즉 아까 유저 BA뷰 시퀀스 전체 플러스 현재 예측하려고 하는 타겟 아이템 전체가"},{"start":1961800,"end":1975800,"text":"입력 값이 되고요. 그 입력 값은 이 인베딩 2 매트릭스로 표현이 됩니다. 이제 여기에 쿼리와 키와 밸류에 해당하는 각각의 웨이트 매트릭스를 곱해서 셀프 어텐션 즉 스케일 닷 프로덕트 어텐션을","confidence":0.9401,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1962050,1962280,"입력"],[1962280,1962494,"값이"],[1962494,1962800,"되고요."],[1963330,1963480,"그"],[1963480,1963700,"입력"],[1963700,1963980,"값은"],[1964090,1964240,"이"],[1964410,1964780,"인베딩"],[1964870,1965020,"2"],[1965310,1965780,"매트릭스로"],[1965780,1966014,"표현이"],[1966014,1966320,"됩니다."],[1967070,1967240,"이제"],[1967250,1967580,"여기에"],[1968150,1968560,"쿼리와"],[1968810,1969120,"키와"],[1969290,1969614,"밸류에"],[1969614,1970000,"해당하는"],[1970190,1970540,"각각의"],[1970610,1971020,"웨이트"],[1971150,1971740,"매트릭스를"],[1971850,1972260,"곱해서"],[1973250,1973527,"셀프"],[1973527,1973840,"어텐션"],[1973930,1974080,"즉"],[1974130,1974440,"스케일"],[1974440,1974560,"닷"],[1974610,1974894,"프로덕트"],[1974894,1975400,"어텐션을"]],"textEdited":"입력 값이 되고요. 그 입력 값은 이 인베딩 2 매트릭스로 표현이 됩니다. 이제 여기에 쿼리와 키와 밸류에 해당하는 각각의 웨이트 매트릭스를 곱해서 셀프 어텐션 즉 스케일 닷 프로덕트 어텐션을"},{"start":1975800,"end":1989600,"text":"거치고요. 이제 이 셀프 어텐션 부분이 각각의 하나하나 헤드가 되어서 최종적으로 멀티헤드 어텐션 연산을 수행합니다. 이 멀티헤드 어텐션 이후에는 피드 포워드 뉴널 네트워크","confidence":0.9393,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1976610,1977060,"거치고요."],[1977690,1977860,"이제"],[1977890,1978040,"이"],[1978110,1978400,"셀프"],[1978400,1978760,"어텐션"],[1978990,1979360,"부분이"],[1979690,1980020,"각각의"],[1980050,1980480,"하나하나"],[1980550,1980920,"헤드가"],[1980920,1981220,"되어서"],[1981370,1981820,"최종적으로"],[1981870,1982400,"멀티헤드"],[1982410,1982820,"어텐션"],[1982910,1983320,"연산을"],[1984310,1984820,"수행합니다."],[1985630,1985780,"이"],[1985790,1986320,"멀티헤드"],[1986320,1986700,"어텐션"],[1986830,1987340,"이후에는"],[1988110,1988380,"피드"],[1988380,1988627,"포워드"],[1988627,1988840,"뉴널"],[1988850,1989340,"네트워크"]],"textEdited":"거치고요. 이제 이 셀프 어텐션 부분이 각각의 하나하나 헤드가 되어서 최종적으로 멀티헤드 어텐션 연산을 수행합니다. 이 멀티헤드 어텐션 이후에는 피드 포워드 뉴널 네트워크"},{"start":1989600,"end":2002200,"text":"을 통과하는데요. 이 피드 포워드 뉴럴 네트워크 앞 뒤로 에드앤 놈 즉 레이어 노말라이제이션과 스킵 커넥션이 추가가 되어 있습니다. 이 s가 멀티 애드 어텐션","confidence":0.835,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1989830,1989980,"을"],[1990030,1990560,"통과하는데요."],[1990930,1991080,"이"],[1991410,1991640,"피드"],[1991640,1991867,"포워드"],[1991867,1992100,"뉴럴"],[1992100,1992540,"네트워크"],[1992610,1992760,"앞"],[1992770,1993060,"뒤로"],[1993910,1994260,"에드앤"],[1994260,1994380,"놈"],[1994470,1994620,"즉"],[1994710,1995020,"레이어"],[1995020,1997000,"노말라이제이션과"],[1997570,1997800,"스킵"],[1997810,1998240,"커넥션이"],[1998240,1998474,"추가가"],[1998474,1998660,"되어"],[1998660,1999040,"있습니다."],[1999870,2000020,"이"],[2000250,2000540,"s가"],[2000750,2001047,"멀티"],[2001047,2001240,"애드"],[2001240,2001620,"어텐션"]],"textEdited":"을 통과하는데요. 이 피드 포워드 뉴럴 네트워크 앞 뒤로 에드앤 놈 즉 레이어 노말라이제이션과 스킵 커넥션이 추가가 되어 있습니다. 이 s가 멀티 애드 어텐션"},{"start":2002200,"end":2015100,"text":"이 거친 결과고요. 이 FFN이라는 것은 여기에 있는 에드앤 롬, 피드 포워드 뉴럴 네트워크 다시 에드앤 롬을 설명하고 있습니다. 이제 여기서 기존 트랜스포머와 조금 다른 차이점은","confidence":0.8358,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2002630,2002780,"이"],[2002930,2003260,"거친"],[2003450,2003940,"결과고요."],[2004230,2004380,"이"],[2004430,2005200,"FFN이라는"],[2005200,2005460,"것은"],[2005950,2006180,"여기에"],[2006180,2006340,"있는"],[2006370,2006700,"에드앤"],[2006730,2006880,"롬,"],[2007270,2007520,"피드"],[2007530,2007860,"포워드"],[2007860,2008080,"뉴럴"],[2008080,2008480,"네트워크"],[2008530,2008780,"다시"],[2008780,2009040,"에드앤"],[2009040,2009320,"롬을"],[2009630,2010000,"설명하고"],[2010000,2010340,"있습니다."],[2011070,2011240,"이제"],[2011240,2011520,"여기서"],[2012570,2012780,"기존"],[2012810,2013360,"트랜스포머와"],[2013360,2013540,"조금"],[2013570,2013880,"다른"],[2014010,2014540,"차이점은"]],"textEdited":"이 거친 결과고요. 이 FFN이라는 것은 여기에 있는 에드앤 롬, 피드 포워드 뉴럴 네트워크 다시 에드앤 롬을 설명하고 있습니다. 이제 여기서 기존 트랜스포머와 조금 다른 차이점은"},{"start":2015100,"end":2028600,"text":"드롭 아웃을 사용하였고 그리고 액티베이션 펑션을 사용하여서 액티베이션 펑션 즉 피드 포워드 유얼 네트워크의 액티베이션 펑션으로 리키 엘류를 사용했다는 점이 기존 인코더 레이어와는 조금 다른 점입니다.","confidence":0.8717,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2015330,2015580,"드롭"],[2015630,2016040,"아웃을"],[2016040,2016620,"사용하였고"],[2017070,2017380,"그리고"],[2017810,2018340,"액티베이션"],[2018410,2018900,"펑션을"],[2018910,2019400,"사용하여서"],[2020710,2021140,"액티베이션"],[2021150,2021420,"펑션"],[2021490,2021640,"즉"],[2022130,2022380,"피드"],[2022390,2022594,"포워드"],[2022594,2022740,"유얼"],[2022750,2023740,"네트워크의"],[2023770,2024200,"액티베이션"],[2024230,2024660,"펑션으로"],[2024810,2025040,"리키"],[2025040,2025327,"엘류를"],[2025327,2025800,"사용했다는"],[2025800,2026060,"점이"],[2026570,2026800,"기존"],[2026850,2027180,"인코더"],[2027180,2027580,"레이어와는"],[2027610,2027800,"조금"],[2027830,2028080,"다른"],[2028080,2028460,"점입니다."]],"textEdited":"드롭 아웃을 사용하였고 그리고 액티베이션 펑션을 사용하여서 액티베이션 펑션 즉 피드 포워드 유얼 네트워크의 액티베이션 펑션으로 리키 엘류를 사용했다는 점이 기존 인코더 레이어와는 조금 다른 점입니다."},{"start":2028600,"end":2040800,"text":"하지만 사실 이 부분은 크게 중요한 포인트는 아니에요. 네 이렇게 해서 하나의 인코더 레이어가 완성된 이후에 이 하나의 인코딩 레이어 즉 아이 번째 인코딩 레이어를 계속 쌓아서 아이 플러스 1번째 인코더","confidence":0.9595,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2028990,2029240,"하지만"],[2029350,2029580,"사실"],[2029580,2029687,"이"],[2029687,2029940,"부분은"],[2030030,2030260,"크게"],[2030310,2030620,"중요한"],[2030650,2031027,"포인트는"],[2031027,2031340,"아니에요."],[2031970,2032120,"네"],[2032130,2032400,"이렇게"],[2032400,2032620,"해서"],[2032690,2032980,"하나의"],[2033030,2033380,"인코더"],[2033380,2033700,"레이어가"],[2033730,2034180,"완성된"],[2034230,2034580,"이후에"],[2035210,2035360,"이"],[2035490,2035760,"하나의"],[2035770,2036120,"인코딩"],[2036130,2036420,"레이어"],[2036490,2036640,"즉"],[2036670,2036900,"아이"],[2036900,2037160,"번째"],[2037160,2037460,"인코딩"],[2037460,2037840,"레이어를"],[2038510,2038740,"계속"],[2038770,2039160,"쌓아서"],[2039210,2039400,"아이"],[2039450,2039780,"플러스"],[2039810,2040200,"1번째"],[2040210,2040540,"인코더"]],"textEdited":"하지만 사실 이 부분은 크게 중요한 포인트는 아니에요. 네 이렇게 해서 하나의 인코더 레이어가 완성된 이후에 이 하나의 인코딩 레이어 즉 아이 번째 인코딩 레이어를 계속 쌓아서 아이 플러스 1번째 인코더"},{"start":2040800,"end":2054200,"text":"이렇게 블록으로 쌓을 수가 있습니다. 아까 트랜스포머는 총 6개의 블록을 사용했다고 했는데요. 이 비스티 모델도 트랜스포머 인코더 레이어를 n개 쌓아서 모델의 예측 정확도를 높이려고 시도하였습니다.","confidence":0.8567,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2041310,2041540,"이렇게"],[2041570,2042020,"블록으로"],[2042030,2042260,"쌓을"],[2042260,2042460,"수가"],[2042460,2042800,"있습니다."],[2043070,2043320,"아까"],[2043750,2044360,"트랜스포머는"],[2044360,2044500,"총"],[2044630,2044980,"6개의"],[2044990,2045300,"블록을"],[2045300,2045747,"사용했다고"],[2045747,2046100,"했는데요."],[2046270,2046420,"이"],[2046420,2046920,"비스티"],[2047430,2047840,"모델도"],[2048290,2048760,"트랜스포머"],[2048770,2049140,"인코더"],[2049190,2049600,"레이어를"],[2050070,2050360,"n개"],[2050450,2050820,"쌓아서"],[2051750,2052140,"모델의"],[2052150,2052400,"예측"],[2052400,2052820,"정확도를"],[2052820,2053300,"높이려고"],[2053350,2054080,"시도하였습니다."]],"textEdited":"이렇게 블록으로 쌓을 수가 있습니다. 아까 트랜스포머는 총 6개의 블록을 사용했다고 했는데요. 이 비스티 모델도 트랜스포머 인코더 레이어를 n개 쌓아서 모델의 예측 정확도를 높이려고 시도하였습니다."},{"start":2054200,"end":2067300,"text":"이 비스티 모델의 구조를 트랜스포머 인코더 레이어를 중심으로 살펴보았는데요. 이 비스티와 바로 이전 파트에서 배운 디아이엔 모델은 비교했을 때 어떠한 차이점이 있는지 간단하게 살펴보겠습니다.","confidence":0.7891,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2054490,2054640,"이"],[2054650,2055120,"비스티"],[2055170,2055520,"모델의"],[2055530,2055880,"구조를"],[2056150,2056720,"트랜스포머"],[2056790,2057160,"인코더"],[2057170,2057780,"레이어를"],[2057830,2058240,"중심으로"],[2058270,2058960,"살펴보았는데요."],[2059570,2059720,"이"],[2059720,2060260,"비스티와"],[2060570,2060840,"바로"],[2060840,2061040,"이전"],[2061130,2061580,"파트에서"],[2061630,2061840,"배운"],[2062230,2062680,"디아이엔"],[2063510,2063920,"모델은"],[2064290,2064727,"비교했을"],[2064727,2064860,"때"],[2064870,2065180,"어떠한"],[2065250,2065627,"차이점이"],[2065627,2065940,"있는지"],[2066110,2066520,"간단하게"],[2066530,2067240,"살펴보겠습니다."]],"textEdited":"이 비스티 모델의 구조를 트랜스포머 인코더 레이어를 중심으로 살펴보았는데요. 이 비스티와 바로 이전 파트에서 배운 디아이엔 모델은 비교했을 때 어떠한 차이점이 있는지 간단하게 살펴보겠습니다."},{"start":2067300,"end":2079700,"text":"먼저 din에서는 로컬 액티베이션 레이어를 통하여서 유저가 과거에 소비했던 아이템과 현재 아이템의 관련성을 구했는데요. 이 과정에서 단순하게 선플링으로","confidence":0.968,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2067550,2067820,"먼저"],[2067850,2068620,"din에서는"],[2069730,2070040,"로컬"],[2070130,2070700,"액티베이션"],[2070790,2071260,"레이어를"],[2071290,2071740,"통하여서"],[2072130,2072460,"유저가"],[2072510,2072820,"과거에"],[2072850,2073280,"소비했던"],[2073290,2073840,"아이템과"],[2073950,2074220,"현재"],[2074270,2074780,"아이템의"],[2075050,2075520,"관련성을"],[2075530,2076060,"구했는데요."],[2076650,2076800,"이"],[2076800,2077200,"과정에서"],[2077650,2078100,"단순하게"],[2078390,2079060,"선플링으로"]],"textEdited":"먼저 din에서는 로컬 액티베이션 레이어를 통하여서 유저가 과거에 소비했던 아이템과 현재 아이템의 관련성을 구했는데요. 이 과정에서 단순하게 선플링으로"},{"start":2079700,"end":2092800,"text":"과거에 소비했던 아이템들을 더하는 방식을 사용했습니다. 하지만 BST 모델에서는 과거에 어떤 아이템들을 소비했는지 뿐만이 아니라 어떤 순서로 소비했는지 그 시퀀스까지 모델의 피처로 사용하여서","confidence":0.9437,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2080110,2080480,"과거에"],[2080830,2081320,"소비했던"],[2081330,2081920,"아이템들을"],[2081930,2082300,"더하는"],[2082350,2082700,"방식을"],[2082700,2083260,"사용했습니다."],[2083450,2083760,"하지만"],[2083850,2084320,"BST"],[2084330,2084860,"모델에서는"],[2085390,2085760,"과거에"],[2085770,2085960,"어떤"],[2085990,2086460,"아이템들을"],[2086470,2087020,"소비했는지"],[2087020,2087327,"뿐만이"],[2087327,2087580,"아니라"],[2088390,2088700,"어떤"],[2088890,2089260,"순서로"],[2089290,2089880,"소비했는지"],[2089930,2090080,"그"],[2090130,2090860,"시퀀스까지"],[2091350,2091720,"모델의"],[2091770,2092080,"피처로"],[2092080,2092580,"사용하여서"]],"textEdited":"과거에 소비했던 아이템들을 더하는 방식을 사용했습니다. 하지만 BST 모델에서는 과거에 어떤 아이템들을 소비했는지 뿐만이 아니라 어떤 순서로 소비했는지 그 시퀀스까지 모델의 피처로 사용하여서"},{"start":2092800,"end":2107400,"text":"현재 아이템과의 관련성을 모델링하였습니다. 그래서 그 모델은 그 관련성은 트랜스포머 인코더 레이어를 사용하니 그리고 트랜스포머 레이어를 해당 모델에 응용한 것이기 때문에 본래 트랜스포머 모델과 어떻게 다르게 변형했는지 또 간단하게 살펴봅시다.","confidence":0.963,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2093170,2093460,"현재"],[2093470,2094140,"아이템과의"],[2094230,2094760,"관련성을"],[2094770,2095560,"모델링하였습니다."],[2095670,2095960,"그래서"],[2096230,2096380,"그"],[2096390,2096780,"모델은"],[2097530,2097680,"그"],[2097750,2098220,"관련성은"],[2098490,2098900,"트랜스포머"],[2098900,2099240,"인코더"],[2099240,2099560,"레이어를"],[2099560,2099840,"사용하니"],[2099870,2100080,"그리고"],[2100130,2100640,"트랜스포머"],[2100670,2101080,"레이어를"],[2101110,2101340,"해당"],[2101370,2101660,"모델에"],[2101660,2101960,"응용한"],[2101960,2102220,"것이기"],[2102220,2102560,"때문에"],[2102930,2103240,"본래"],[2103350,2103820,"트랜스포머"],[2103820,2104220,"모델과"],[2104530,2104860,"어떻게"],[2104860,2105120,"다르게"],[2105130,2105720,"변형했는지"],[2105870,2106020,"또"],[2106050,2106500,"간단하게"],[2106730,2107300,"살펴봅시다."]],"textEdited":"현재 아이템과의 관련성을 모델링하였습니다. 그래서 그 모델은 그 관련성은 트랜스포머 인코더 레이어를 사용하니 그리고 트랜스포머 레이어를 해당 모델에 응용한 것이기 때문에 본래 트랜스포머 모델과 어떻게 다르게 변형했는지 또 간단하게 살펴봅시다."},{"start":2107400,"end":2121000,"text":"먼저 앞서 인코더 레이어 부분을 언급했듯이 이 비스티의 트랜스포머 인코더는 주로 아웃과 리키 렐루를 사용했다는 점이 다르고요. 그리고 인코더 블록을 6개까지 쌓지 않고 하나에서 4개만 쌓았는데요.","confidence":0.8684,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2107810,2108040,"먼저"],[2108070,2108360,"앞서"],[2108450,2108840,"인코더"],[2108850,2109180,"레이어"],[2109630,2109960,"부분을"],[2109990,2110600,"언급했듯이"],[2111390,2111540,"이"],[2111540,2112120,"비스티의"],[2112130,2112600,"트랜스포머"],[2112600,2113000,"인코더는"],[2113230,2113460,"주로"],[2113460,2113840,"아웃과"],[2113950,2114180,"리키"],[2114180,2115260,"렐루를"],[2115260,2115687,"사용했다는"],[2115687,2115867,"점이"],[2115867,2116280,"다르고요."],[2117150,2117400,"그리고"],[2117530,2117920,"인코더"],[2117930,2118220,"블록을"],[2118290,2118660,"6개까지"],[2118660,2118940,"쌓지"],[2118940,2119200,"않고"],[2119350,2119820,"하나에서"],[2119870,2120280,"4개만"],[2120330,2120860,"쌓았는데요."]],"textEdited":"먼저 앞서 인코더 레이어 부분을 언급했듯이 이 비스티의 트랜스포머 인코더는 주로 아웃과 리키 렐루를 사용했다는 점이 다르고요. 그리고 인코더 블록을 6개까지 쌓지 않고 하나에서 4개만 쌓았는데요."},{"start":2121000,"end":2135500,"text":"뒤에 언급하겠지만 한계를 사용했을 때 가장 좋은 예측 성능을 보였습니다. 또한 트랜스포머는 사인 코사인을 사용한 포지셔널 인코딩을 사용했는데요. 이 모델에서는 사인 코사인이 아니라 직접","confidence":0.9456,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2121550,2121780,"뒤에"],[2121830,2122440,"언급하겠지만"],[2122710,2123100,"한계를"],[2123100,2123467,"사용했을"],[2123467,2123600,"때"],[2123630,2123880,"가장"],[2124390,2124560,"좋은"],[2124690,2124980,"예측"],[2125070,2125420,"성능을"],[2125430,2126000,"보였습니다."],[2126330,2126580,"또한"],[2127370,2128100,"트랜스포머는"],[2128210,2128500,"사인"],[2128530,2129160,"코사인을"],[2129170,2129500,"사용한"],[2129770,2130180,"포지셔널"],[2130230,2131560,"인코딩을"],[2131560,2132120,"사용했는데요."],[2132750,2132900,"이"],[2132910,2133460,"모델에서는"],[2133690,2133920,"사인"],[2133920,2134314,"코사인이"],[2134314,2134560,"아니라"],[2134890,2135240,"직접"]],"textEdited":"뒤에 언급하겠지만 한계를 사용했을 때 가장 좋은 예측 성능을 보였습니다. 또한 트랜스포머는 사인 코사인을 사용한 포지셔널 인코딩을 사용했는데요. 이 모델에서는 사인 코사인이 아니라 직접"},{"start":2135500,"end":2145500,"text":"다른 방식으로 포지셔널 인코딩을 사용하였습니다. 말 그대로 아이템을 소비한 시간 그 물리적인 시간을 사용했는데요.","confidence":0.9655,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2135790,2136060,"다른"],[2136190,2136600,"방식으로"],[2136670,2137080,"포지셔널"],[2137130,2137620,"인코딩을"],[2137630,2138320,"사용하였습니다."],[2138450,2138600,"말"],[2138610,2138920,"그대로"],[2139990,2140620,"아이템을"],[2140850,2141240,"소비한"],[2141770,2141980,"시간"],[2142850,2143000,"그"],[2143000,2143400,"물리적인"],[2143450,2144380,"시간을"],[2144380,2144940,"사용했는데요."]],"textEdited":"다른 방식으로 포지셔널 인코딩을 사용하였습니다. 말 그대로 아이템을 소비한 시간 그 물리적인 시간을 사용했는데요."},{"start":2145500,"end":2157600,"text":"현재 아이템을 예측하려고 하는 현재 시간과 과거에 그 아이템을 소비했던 시간의 차이를 포지셔널 인코딩을 사용했고 이렇게 사용했을 때 더 좋은 성능을 낸다고 말하고 있습니다.","confidence":0.9524,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2145890,2146180,"현재"],[2146210,2146620,"아이템을"],[2146620,2147180,"예측하려고"],[2147180,2147360,"하는"],[2147790,2148080,"현재"],[2148130,2148660,"시간과"],[2148870,2149280,"과거에"],[2149730,2149880,"그"],[2149880,2150240,"아이템을"],[2150250,2150680,"소비했던"],[2150750,2151140,"시간의"],[2151630,2152020,"차이를"],[2152570,2153000,"포지셔널"],[2153010,2153460,"인코딩을"],[2153460,2153960,"사용했고"],[2154370,2154640,"이렇게"],[2154640,2155040,"사용했을"],[2155040,2155160,"때"],[2155170,2155320,"더"],[2155790,2155960,"좋은"],[2156130,2156440,"성능을"],[2156440,2156800,"낸다고"],[2156830,2157160,"말하고"],[2157160,2157520,"있습니다."]],"textEdited":"현재 아이템을 예측하려고 하는 현재 시간과 과거에 그 아이템을 소비했던 시간의 차이를 포지셔널 인코딩을 사용했고 이렇게 사용했을 때 더 좋은 성능을 낸다고 말하고 있습니다."},{"start":2157600,"end":2169600,"text":"참고로 트랜스포머 모델은 요즘 많은 테스크에서 소타에 준하는 성능을 보이고 있는데요. 각각의 다양한 테스크별로 이 트랜스포머의 핵심적인 구조는 유지하되 이 BST 모델이 응용한 것처럼","confidence":0.9461,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2157890,2158220,"참고로"],[2158230,2158700,"트랜스포머"],[2158700,2159000,"모델은"],[2159010,2159240,"요즘"],[2159240,2159440,"많은"],[2159570,2160220,"테스크에서"],[2160450,2160880,"소타에"],[2160880,2161180,"준하는"],[2161230,2161540,"성능을"],[2161540,2161787,"보이고"],[2161787,2162140,"있는데요."],[2163010,2163360,"각각의"],[2163570,2163860,"다양한"],[2163910,2164500,"테스크별로"],[2164650,2164800,"이"],[2164810,2165360,"트랜스포머의"],[2165430,2165880,"핵심적인"],[2165950,2166380,"구조는"],[2166610,2167140,"유지하되"],[2167710,2167860,"이"],[2167860,2168280,"BST"],[2168290,2168620,"모델이"],[2168630,2168940,"응용한"],[2168940,2169260,"것처럼"]],"textEdited":"참고로 트랜스포머 모델은 요즘 많은 테스크에서 소타에 준하는 성능을 보이고 있는데요. 각각의 다양한 테스크별로 이 트랜스포머의 핵심적인 구조는 유지하되 이 BST 모델이 응용한 것처럼"},{"start":2169600,"end":2181400,"text":"각각의 데이터셋에 맞는 약간의 변형과 응용이 이루어진다는 점을 중심으로 기억하시면 좋겠습니다. 그래서 마지막으로 모델의 성능을 비교한 결과입니다. 트랜스포머 레이어는","confidence":0.9764,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2169950,2170300,"각각의"],[2170310,2170900,"데이터셋에"],[2170900,2171100,"맞는"],[2171230,2171580,"약간의"],[2171630,2172100,"변형과"],[2172290,2172680,"응용이"],[2172730,2173320,"이루어진다는"],[2173330,2173900,"점을"],[2173970,2174420,"중심으로"],[2174990,2175500,"기억하시면"],[2175500,2176020,"좋겠습니다."],[2177090,2177340,"그래서"],[2177340,2177820,"마지막으로"],[2177950,2178340,"모델의"],[2178450,2178760,"성능을"],[2178810,2179160,"비교한"],[2179210,2179740,"결과입니다."],[2180110,2180560,"트랜스포머"],[2180560,2180920,"레이어는"]],"textEdited":"각각의 데이터셋에 맞는 약간의 변형과 응용이 이루어진다는 점을 중심으로 기억하시면 좋겠습니다. 그래서 마지막으로 모델의 성능을 비교한 결과입니다. 트랜스포머 레이어는"},{"start":2181400,"end":2191500,"text":"씨티 프리딕션 테스크에서도 소타의 성능을 보인다고 이야기하고 있고요. 그리고 트랜스포머 블록을 기존 원래 논문에서 6개나 쌓았는데요.","confidence":0.8098,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2181850,2182140,"씨티"],[2182550,2182960,"프리딕션"],[2183070,2183840,"테스크에서도"],[2183890,2184260,"소타의"],[2184290,2184620,"성능을"],[2184790,2185240,"보인다고"],[2185250,2185700,"이야기하고"],[2185700,2185960,"있고요."],[2186510,2186760,"그리고"],[2186830,2187280,"트랜스포머"],[2187290,2187660,"블록을"],[2188450,2188680,"기존"],[2189190,2189440,"원래"],[2189450,2189800,"논문에서"],[2189930,2190380,"6개나"],[2190390,2190900,"쌓았는데요."]],"textEdited":"씨티 프리딕션 테스크에서도 소타의 성능을 보인다고 이야기하고 있고요. 그리고 트랜스포머 블록을 기존 원래 논문에서 6개나 쌓았는데요."},{"start":2191500,"end":2204700,"text":"여기서 보시면은 하나 쌓았을 때보다 2개 3개를 쌓았을 때 오히려 성능이 감소함을 알 수 있습니다. 그 이유는 CTR 예측 테스크에서의 시퀀스는 우리가 보통 알고 있는 NLP 머신 트랜슬레이션 같은","confidence":0.909,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2191730,2192020,"여기서"],[2192030,2192520,"보시면은"],[2193190,2193420,"하나"],[2193470,2193840,"쌓았을"],[2193840,2194240,"때보다"],[2194550,2194840,"2개"],[2194970,2195500,"3개를"],[2195510,2195860,"쌓았을"],[2195860,2195980,"때"],[2195990,2196300,"오히려"],[2196570,2196880,"성능이"],[2196950,2197540,"감소함을"],[2197630,2197780,"알"],[2197790,2197940,"수"],[2197940,2198300,"있습니다."],[2198450,2198600,"그"],[2198600,2198880,"이유는"],[2199110,2199460,"CTR"],[2199460,2199660,"예측"],[2199660,2200420,"테스크에서의"],[2200510,2201080,"시퀀스는"],[2201650,2201880,"우리가"],[2201910,2202180,"보통"],[2202180,2202367,"알고"],[2202367,2202540,"있는"],[2202550,2202960,"NLP"],[2203310,2203580,"머신"],[2203610,2204160,"트랜슬레이션"],[2204170,2204440,"같은"]],"textEdited":"여기서 보시면은 하나 쌓았을 때보다 2개 3개를 쌓았을 때 오히려 성능이 감소함을 알 수 있습니다. 그 이유는 CTR 예측 테스크에서의 시퀀스는 우리가 보통 알고 있는 NLP 머신 트랜슬레이션 같은"},{"start":2204700,"end":2219200,"text":"태스크보다는 훨씬 더 덜 복잡한 시퀀스를 가지고 있기 때문으로 보입니다. 그래서 기존에 우리가 언급했던 와이드 앤 딥러닝 그리고 와이드 앤 딥러닝의 시퀀스 정보를 추가한 것 그리고 우리가 바로 전 파트에서 배웠던 dim 모델","confidence":0.8972,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2204990,2205640,"태스크보다는"],[2205710,2206040,"훨씬"],[2206040,2206180,"더"],[2206590,2206740,"덜"],[2206870,2207260,"복잡한"],[2207370,2207860,"시퀀스를"],[2207870,2208180,"가지고"],[2208190,2208460,"있기"],[2208630,2209040,"때문으로"],[2209040,2209420,"보입니다."],[2209690,2209920,"그래서"],[2209950,2210260,"기존에"],[2210260,2210480,"우리가"],[2210550,2211040,"언급했던"],[2211090,2211387,"와이드"],[2211387,2211520,"앤"],[2211570,2212040,"딥러닝"],[2212110,2212300,"그리고"],[2212300,2212514,"와이드"],[2212514,2212640,"앤"],[2212640,2213080,"딥러닝의"],[2213090,2213440,"시퀀스"],[2213440,2213680,"정보를"],[2213730,2214140,"추가한"],[2214450,2214600,"것"],[2215410,2215700,"그리고"],[2215930,2216160,"우리가"],[2216270,2216487,"바로"],[2216487,2216620,"전"],[2216690,2217080,"파트에서"],[2217080,2217480,"배웠던"],[2218250,2218680,"dim"],[2218690,2218940,"모델"]],"textEdited":"태스크보다는 훨씬 더 덜 복잡한 시퀀스를 가지고 있기 때문으로 보입니다. 그래서 기존에 우리가 언급했던 와이드 앤 딥러닝 그리고 와이드 앤 딥러닝의 시퀀스 정보를 추가한 것 그리고 우리가 바로 전 파트에서 배웠던 dim 모델"},{"start":2219200,"end":2231900,"text":"들보다 이 BST 모델이 더 좋은 예측 성능 그리고 실제 온라인 테스트에서 CTR 개인이 이만큼 상승했다는 것을 볼 수 있습니다. 네 이상 bab어 시퀀스 트랜스포머 모델에 대한 내용이었고요.","confidence":0.9573,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2219410,2219800,"들보다"],[2220130,2220280,"이"],[2220280,2220720,"BST"],[2220730,2221040,"모델이"],[2221050,2221200,"더"],[2221290,2221460,"좋은"],[2221610,2221880,"예측"],[2221930,2222200,"성능"],[2222270,2222560,"그리고"],[2222990,2223260,"실제"],[2223290,2223600,"온라인"],[2223670,2224420,"테스트에서"],[2225010,2225380,"CTR"],[2225430,2226207,"개인이"],[2226207,2226540,"이만큼"],[2226570,2227034,"상승했다는"],[2227034,2227260,"것을"],[2227270,2227420,"볼"],[2227430,2227534,"수"],[2227534,2227900,"있습니다."],[2228410,2228560,"네"],[2228590,2228800,"이상"],[2229470,2229940,"bab어"],[2229970,2230340,"시퀀스"],[2230340,2230727,"트랜스포머"],[2230727,2230974,"모델에"],[2230974,2231140,"대한"],[2231170,2231720,"내용이었고요."]],"textEdited":"들보다 이 BST 모델이 더 좋은 예측 성능 그리고 실제 온라인 테스트에서 CTR 개인이 이만큼 상승했다는 것을 볼 수 있습니다. 네 이상 bab어 시퀀스 트랜스포머 모델에 대한 내용이었고요."},{"start":2231900,"end":2240700,"text":"이렇게 아홉 번째 강의가 모두 끝났습니다. 이번 강의를 통해서 다양한 딥씨티r 모델과 그 발전 과정을 학습하였습니다. 모두 수고하셨습니다.","confidence":0.8554,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[2232110,2232360,"이렇게"],[2232360,2232540,"아홉"],[2232540,2232780,"번째"],[2232810,2233100,"강의가"],[2233110,2233320,"모두"],[2233370,2233860,"끝났습니다."],[2234030,2234240,"이번"],[2234290,2234600,"강의를"],[2234600,2234900,"통해서"],[2235130,2235440,"다양한"],[2235610,2236220,"딥씨티r"],[2236230,2236600,"모델과"],[2236730,2236880,"그"],[2237070,2237380,"발전"],[2237430,2237780,"과정을"],[2237850,2238540,"학습하였습니다."],[2239410,2239640,"모두"],[2239670,2240320,"수고하셨습니다."]],"textEdited":"이렇게 아홉 번째 강의가 모두 끝났습니다. 이번 강의를 통해서 다양한 딥씨티r 모델과 그 발전 과정을 학습하였습니다. 모두 수고하셨습니다."}],"text":"안녕하세요. 추천 시스템 강의를 맡은 강사 이준원입니다. 이번 시간에는 지난 8강에 이어서 컨텍스트 어 레커멘데이션 모델에 대해서 학습합니다. 이 car 테스크의 대표적인 씨티알 예측 테스크 관련된 모델을 오늘 강의 동안 학습할 것인데요. 지난 강의에서 로지스틱 리게이션부터 시작하여서 프엠과 프프엠 등의 모델을 다루었고 바로 이 모델들이 씨티알을 예측하는 테스크에서 좋은 성능을 보였다고 말씀드렸습니다. 이번 시간에는 딥러닝의 모델로 넘어옵니다. 딥러닝 모델을 이용한 CTR 예측 모델들을 차례로 배워보고 어떻게 모델의 표현력과 성능이 발전했는지를 살펴봅시다. 네 오늘 다루게 될 모델은 총 4가지입니다. 먼저 CTR 예측 문제에서 왜 딥러닝 모델이 효과적인지를 간단하게 살펴본 뒤에 딥러닝 추천 모델 가운데 가장 많이 알려진 와이드 앤 딥과 딥 FM 모델을 학습합니다. 그다음에 유저의 행동 데이터를 인풋 피처로 사용한 딥 인터레스 네트워크와 트랜스포머를 이용한 비스티 모델까지 모델이 연구를 통해 공개된 시간 순서대로 그리고 모델의 표현력과 복잡도 성능이 향상된 순서대로 차례대로 다뤄보도록 하겠습니다. 네 먼저 CTR 예측 문제에서 딥러닝을 활용하는 이유를 간단하게 짚고 넘어가겠습니다. 지난 강의에서도 CTR 예측 문제의 정의와 중요성에 대해서 언급했는데요. 네 시티알 예측 문제란 주어진 아이템을 클릭할 확률을 구하는 문제라고 말씀드렸죠. 그래서 이 시티 예측에 딥러닝이 필요한 이유는 현실의 씨티알 데이터 주로 광고 추천 데이터가 되는데요. 이 데이터는 기존의 선형 모델로 예측하는 데에는 한계가 있습니다. 굉장히 스파스한 그리고 차원이 굉장히 높은 데이터 높은 피처로 이루어져 있고요. 그리고 그 피처 간의 어떠한 관계가 굉장히 높은 비선형성을 가지고 있기 때문에 기본적인 단순한 선형 모델로 이 문제를 풀기에는 다소 어려움이 있습니다. 그래서 이러한 데이터의 효과적인 딥러닝 모델들이 시티r 예측 문제에 적용되기 시작하였습니다. 첫 번째로 와이드 앤 딥 모델의 등장 배경과 장점을 살펴보고 논문의 제목에 나타나는 와이드 컴포넌트와 딥 컴포넌트를 각각 이해해 봅시다. 네 와이드앤딥 레코멘데이션은 선형 모델과 비선형 모델의 기존이 가지고 있는 장점들을 결합하여서 각각의 모델들이 가지고 있는 장점을 모두 취하고자 한 논문입니다. 본 논문은 구글에서 발표한 논문인데요. 플레이 스토어에서 사용자가 검색한 쿼리에 대해서 앱을 추천해 주는 테스크에도 실제로 사용하고 있다고 밝혔습니다. 이 와이드 앤 딥 논문에서는 추천 시스템에서 해결해야 하는 두 가지 과제가 있다고 말하는데요. 바로 메모라이제이션과 제너럴라이제이션입니다. 먼저 메모라이제이션을 해석하면 말 그대로 암기인데요. 우리가 이미 가지고 있는 학습 데이터에 자주 등장했던 패턴은 그대로 모델이 암기하여서 예측에 활용해야 한다는 것입니다. 먼저 어떤 남자라는 피처를 가진 사람이 컴퓨터에 해당하는 CTR 데이터가 많이 있을 때 이때 이 y 데이터 클릭 데이터가 1로 많이 존재한다고 합시다. 즉 남자 사용자가 컴퓨터에 대한 아이템에 대한 CTR이 높다는 것이죠. 이 피처 관계를 모델이 그대로 학습해줘야 한다는 것이 메모라이제이션입니다. 그래서 이 메모라이제이션은 로지스틱 리그레이션과 같은 선형 모델에 딱 맞는 설명인데요. 네 모델이 단순한 만큼 학습을 통한 파라미터의 수렴이 상대적으로 빠르고 모델에 사용되는 피처가 추가되더라도 모델의 확장성과 해석이 용이하다는 장점이 있습니다. 그러나 이러한 학습 모델이 가지는 바로 단점이 있는데요. 만약에 어떤 남자에 대해서 기존의 컴퓨터 아이템이 아니라 화장품이라는 아이템을 클릭할 확률을 예측해야 한다고 가정합시다. 이제 기존 학습 데이터를 살펴보니 남자가 화장품을 노출하고 클릭했던 데이터가 굉장히 부족하거나 혹은 아예 없을 경우에 이 메모라이제이션 모델 즉 로지스트 리액션과 같은 단순한 모델은 일반화 능력이 떨어지기 때문에 이런 남자가 화장품에 대해서 예측할 확률, 클릭할 확률을 제대로 구할 수 없습니다. 그래서 이러한 단점을 극복하기 위한 방법이 바로 제너럴라이제이션 일반화입니다. 방금 얘기한 남자와 화장품의 피처 조합은 실제 학습 데이터에 거의 발생하지 않았을지라도 각각 남자와 화장품에 대한 변수 특징을 다른 관계 데이터로부터 발견하여서 이를 적절하게 표현할 수 있습니다. 보통 인베딩을 사용하여서 각각의 피처를 적절하게 표현하는데요. 두 피처의 상호 작용은 인베딩이 곱해지면서 계산이 되겠죠. 그래서 이러한 일반화 능력은 FM 같은 지난 시간에 배운 모델도 해당이 되지만 이번 시간에 다룰 DNN 기반의 딥러닝 CTR 예측 모델은 제너럴라이제이션에 훨씬 더 강한 특징을 가지고 있습니다. 그래서 이 메모라이제이션과 제너럴라이제이션이 가지고 있는 각각의 특징을 결합한 모델이 바로 와이드앤디입니다. 그래서 이 두 가지 과제를 모두 커버한다면은 사용자의 어떤 검색 쿼리에 맞는 앱을 추천하는 추천 성능이 더 향상되게 됩니다. 먼저 와이드 컴포넌트를 살펴봅시다. 이 부분은 가장 기본적인 선형 모델 로지스틱 리그레션과 거의 비슷한 모델인데요. 입력 변수가 n개가 있고 이 엔 개에 해당하는 n개의 파라미터로 이루어져 있습니다. 각각의 변수는 모두 선형 결합으로 이루어져 있고 글로벌 바이오스 도 학습 파라미터로 존재합니다. 그러나 이렇게 모델링을 할 경우 이 두 개의 변수 즉 서로 다른 두 개의 변수의 인터랙션 그 관계를 전혀 이 모델을 학습할 수 없는데요. 그래서 와이드 파트에서도 변수 사이의 인터랙션을 표현하기 위해서 다음과 같은 크로스 프로덕트 트랜스포메이션을 사용합니다. 예를 들면 아래와 같이 성별이 여자라는 변수와 그 사용자의 주요 사용 랭귀즈가 영어라는 입력 변수가 동시에 1일 경우 그 두 변수의 프로덕트 텀, 크로스 프로덕트 텀 즉 성별이 여자면서 랭귀즈가 영어라는 변수를 모델에 하나 더 추가해 주게 됩니다. 이러한 크로스 프로덕트 텀을 일반화하여서 이렇게 파케스라고 표현을 할 수 있는데요. 이 파이케이에 해당하는 웨이트도 모델에 추가되어 학습에 반영됩니다. 다만 가능한 모든 변수들 사이에 크로스 프로덕트를 표현하게 된다면은 학습해야 되는 웨이트가 기하급수적으로 증가하게 됩니다. 그래서 본 논문에서는 주요 피처 2개에 대한 세컨 오더 크로스 프로덕트만을 사용하였습니다. 이 피처는 뒤에 등장하는 모델의 아키텍처에도 표현되어 있습니다. 방금까지 설명한 이 크로스 프로덕트 트랜스포메이션. 사실 이 부분의 모델링은 우리가 지난 시간에 배웠던 이 로지스틱 리그레션에다가 2차 폴리노미얼 항을 추가한 것과 거의 동일한 수식입니다. 이 수식에서는 두 개의 서로 다른 변수에 해당하는 학습 파라미터를 정의하고 이 학습 파라미터를 가지고 두 피처의 인터랙션을 표현하였는데요. 보시다시피 엔 곱하기 n만큼 즉 n의 제곱 배로 학습 웨이트가 늘어나게 됩니다. 즉 표현할 수 있는 인터랙션의 한계가 명확한 모델이 바로 이 와이드 컴포넌트 구분입니다. 네 딥 컴포넌트는 다음과 같습니다. 이 팁 컴포넌트는 피드 포워드 뉴럴 네트워크를 사용하는 것 외에는 큰 특징은 없습니다. 아주 단순한 구조인데요. 먼저 이 팁 컴포넌트는 오른쪽에 있는 이 부분입니다. 총 3 레이어로 구성하였으며 그 각각의 레이어에 대해서는 옐로 함수를 사용했는데요. 연속형 변수는 그대로 이 MLP 레이어에 넣어주고 카테고리형 변수는 피처 인베딩을 한 뒤 전체를 컨택하여서 최종 클릭 여부를 예측하고 있습니다. 그래서 이 두 개의 와이드 모델과 딥 모델을 합쳐서 가운데에 있는 와이드 앤 딥 모델이 완성됩니다. 그래서 두 모델을 합쳐서 다음과 같은 모델을 이 와이드 앤 딥을 사용하고 있다고 하는데요. 이 수식을 보시면 이쪽 부분이 와이드 파트고요. 이쪽 부분이 딥 파트입니다. 이 와이드 컴포넌트에는 x와 파스가 있는데요. 이 x는 주어진 n개의 변수를 의미하고요. 파스는 아까 설명했던 것처럼 n개의 변수 사이의 크로스 프로덕트 트랜스포메이션을 의미합니다. 이 모델에서는 앞서 언급했듯이 n개의 모든 변수 사이에 가능한 크로스 프로덕트를 정의하지 않았고요. 이 두 개의 피처 사용자가 과거에 설치한 앱과 지금 현재 CTR을 예측할 그 앱의 크로스 프로덕트만을 이 변수에 추가하였습니다. 그리고 이 뒷부분은 딥 컴포넌트에 있는 MLP 레이어를 일반화해서 표현한 것입니다. 그래서 와이드 프로덕 와이드 컴포넌트와 딥 컴포넌트의 최종 출력 값을 더한 뒤 글로벌 바이러스를 추가하여서 최종적으로 시그모이드를 취하게 되면은 우리가 예측할 클릭 여부가 완성이 됩니다. 네 마지막으로 본 논문에서 와이드 앤 딥 모델의 성능을 온라인 매트릭과 오프라인 매트릭으로 이용하여서 비교하였습니다. 이 비교 대상은 기본적인 제너럴 라이즈 리니어 모델 크로스 프로덕트를 추가한 이 와이드 모델이 되는데요. 그리고 추가적으로 딥 모델만 사용한 성능도 같이 비교하였습니다. 이 베이스 라인인 와이드 모델과 딥 모델 각각은 오프라인에서는 와이드 모델이 좀 더 좋은 예식 성능을 보이고 온라인 서빙을 했을 때는 실제로 얻는 개인는 딥 모델이 컸는데요. 이 두 개의 모델을 합친 와이드 앤 딥은 온라인 성능 과 오프라인 성능 모두 좋은 성능을 보이고 있다고 말합니다. 이상 와이드앤딥 모델에 대한 내용이었습니다. 다음은 딥fm 모델입니다. 딥fm도 와이드앤딥과 굉장히 비슷한 구조를 가지고 있습니다. 마찬가지로 두 개의 컴포넌트 프엠 컴포넌트와 딥 컴포넌트로 이루어져 있고 이 둘을 합쳐서 딥프엠이라는 이름을 명명하였습니다. 그래서 딥프엠에는 어떤 컴포넌트가 있는지 그리고 와이드앤 딥에 비해서 어떠한 장점이 있는지도 같이 살펴봅시다. 네 딥프엠 모델은 지난 시간에 배운 팩토라이제이션 모델의 딥러닝 구조를 추가한 모델입니다. CTR 예측에서 뛰어난 성능을 보인 팩토라이제이션 모델 프엠 모델에다가 딥 컴포넌트를 추가하여서 좀 더 복잡한 상호 작용을 캡처하기 위해서 이러한 모델 구조를 설계하였습니다. 그래서 와이드 앤 딥 모델과는 달리 이 와이드에 해당하는 프엠 컴포넌트와 딥 컴포넌트가 모두 하나의 입력 값으로 공유하고 있어서 따로 피처 엔지니어링이 필요 없는 앤드 투 엔드 방식의 모델을 설계하고 있습니다. 네 딥프엠의 등장 배경을 살펴보겠습니다. 추천 시스템에서 피처 인터랙션 계속 다른 모델에서도 언급했던 내용인데요. 그만큼 추천 시스템에서는 이 인플리시트한 피처의 인터랙션을 학습하는 것이 굉장히 중요합니다. 예를 들면 식사 시간 시간이라는 피처와 배달 앱이라는 피처가 두 개가 동시에 만났을 때 이 시간에 다운로드 수가 증가한다는 것을 모델링하기 위해서는 세컨 오더 인터랙션이 필요하고요. 10대 남성이 이러한 게임을 좋아한다라는 것을 모델링하기 위해서는 10대라는 피처와 남성이라는 피처 그리고 슈팅 RPG 카테고리라는 피처 3개의 피처가 동시에 인터랙션 했을 때 CTR이 높아진다는 것을 모델링해야 합니다. 근데 이제 기존의 CTR 예측 모델들은 로우 오더 혹은 하이 오더 한쪽에만 강한 특징을 보여왔는데요. 이 와이드앤틴 모델 바로 전 슬라이드에서 배운 모델은 이 둘을 통합하여서 문제를 해결했지만 와이드 컴포넌트의 크로스 프로덕트 트랜스포메이션 같은 어떤 피처 엔지니어링이 추가적으로 필요하다는 단점이 있습니다. 그래서 이 와이드 앤 디이 가지고 있던 피처 엔지니어링 파트, 즉 와이드 파트 대신에 지난 시간에 배운 프엠 모델을 그대로 와이드 컴포넌트로 대체하여서 사용하고 있는 것이 바로 딥프엠입니다. 프엠에는 두 변수 사이에 세컨오더 인터랙션이 그대로 표현되어 있기 때문에 강제로 피처 엔지니어링을 통해 크로스 프로덕트 트랜스포메이션을 해줄 필요 없이 그대로 사용하면 됩니다. 그래서 와이드 파트에는 FM을 사용하고 d 파트에는 기본적인 DNN 피드 포워드 n 네트워크를 사용하기 때문에 이 둘을 합쳐서 딥 FM이라고 명명하고 있습니다. 다음 그림과 수식을 통해 좀 더 자세히 살펴보겠습니다. 먼저 FM 컴포넌트인데요. FM은 지난 8강에서 배웠던 이 FM 포뮬라와 수식이 완전히 일치합니다. 그래서 이 FM 모델 수식의 변수들을 살펴보면은 x1부터 xn에 피처가 있고요. 그다음에 두 개의 피처 인터랙션을 표현하는 팩토라이제이션 파라미터 가 있죠. 그래서 이것을 기억하시면서 왼쪽에 있는 그림을 살펴봅시다. 먼저 각각의 필드가 하나하나의 피처를 의미하고 이 디프엠에서는 모두 스파스한 피처로 구성하고 있습니다. 먼저 바로 이 어디션이라고 되어 있는 이 부분으로 연결되어 있는 선은 이 1차 텀을 의미합니다. 각각의 피처가 갖는 그에 대응되는 웨이트가 학습되는 것이고요. 이제 그다음에 세컨오더 피처 인터랙션 FM에서 팩토라이제이션 파라미터 텀에 해당하는 부분인데요. 각각의 피처는 동일한 차원의 인베딩 여기서는 5개의 차원인데요. 5차원으로 인베딩 즉 팩토라이제이션 된 다음에 이들끼리 서로 내적으로 인해서 서로 피처 간의 인터랙션을 학습합니다. 그리고 이 모든 것이 더해지면 다음과 같은 에프엠 포뮬라와 동일한 모델을 이렇게 비주얼라이즈 할 수 있습니다. 다음은 딥 컴포넌트입니다. 딥 컴포넌트는 좀 더 고차원 즉 하이오더 피처 인터랙션을 모델링 해 줍니다. 여기서 모든 피처는 각각 다 동일한 차원, 여기서는 5차원인데요. k 차원으로 인베딩 되는데요. 여기서 이 인베딩 되는 이 인베딩 파라미터는 FM에서 사용하는 가중치와 동일하게 사용합니다. 그래서 이 딥 레이어의 인베딩과 FM 레이어의 인베딩이 따로따로 학습되지 않고 한꺼번에 엔드 투 엔드로 학습되는 것이죠. 각각의 인베딩은 모두 컨케이트네이트 돼서 쭉 가로로 붙게 됩니다. 그래서 이 컨케이트네이트 된 전체 임베딩 벡터가 map 레이어에 처음에 인플 레이어가 되고 이 이후에 엘게의 피디 4 디뉴 네트워크를 쌓게 되면 마지막 레이어에서 최종적으로 클릭 여부를 예측할 수 있게 됩니다. 그래서 전체 구조를 보시면 FM과 딥 컴포넌트 각각의 임베딩을 공유한다는 것을 더 쉽게 이해할 수 있습니다. 각각의 필드 각각의 피처가 인베딩 된 이후에 FM 컴포넌트 쪽으로 팩토라이제이션 즉 세컨오더 인터랙션이 이루어지는 부분이 있고요. 그리고 이 전체가 컨케이트네이트 돼서 딥 컴포넌트의 인풋으로 사용되는 것이죠. 그래서 정리하면은 딥fm은 와이드 앤 딥과 비슷하게 FM 컴포넌트와 딥 컴포넌트가 가지고 있는 각각의 장점, 아까 이야기했던 메모라이제이션과 제널라이제이션의 장점을 모두 활용하여서 좋은 예측 성능을 보입니다. 다음 부분은 딥fm을 비슷한 시기에 발표했던 다른 딥 CTR 모델 들과의 성능과 특징을 비교한 부분입니다. 이 DFM 외에 위에 있는 FNN, PNN, ydnt 같은 다른 CTR 예측 논문들도 이 DFM이 등장하기 전에 발표가 되었는데요. 먼저 FNN 같은 경우에는 에프엠 모델을 사용하지만 엔드 투 엔드로 학습하지 않고 에프엠 모델을 학습한 이후에 그 인베딩을 가지고 와서 다시 딥러닝 모델을 사용합니다. 그래서 프리트레이닝이 반드시 필요한 부분이 있죠. 또한 이 PNN 같은 경우에는 DFM과 굉장히 비슷한 부분이 있지만 로우 오더 인터랙션 즉 메모라이제이션 부분에 학습 파라미터가 빠져 있습니다. 그리고 바로 전 파트에서 설명했던 와이드 앤 딥은 와이드와 딥 컴포넌트로 나누어서 각각의 장점을 활용하였지만 이 와이드 컴포넌트 쪽에 크로스 프로덕트 트랜스포메이션 같은 피처 엔지니어링이 필요하기 때문에 모델을 구성할 때마다 상당히 불편한 프리 프로세싱 과정을 거쳐야 한다는 단점이 있습니다. 그래서 DFM은 이와 같은 모델들이 표현할 수 없는 일부 특징이나 혹은 모델이 가진 단점을 보완한 모델입니다. 네 그래서 최종적으로 모델의 예측 성능을 비교해 보았는데요. 이 표에는 우리가 지난 강의부터 다루었던 많은 CTR 예측 모델들을 확인할 수 있습니다. 가장 기본적인 로지스틱 리그레션과 프엠프엔엔 그리고 여기에서는 이렇게 표현하였지만 이 모델들은 모두 와이드 앤 딥 에 해당하는 모델입니다. 그래서 먼저 이 AUC라는 예측 성능은 보통 바이너리 클래시피케이션에서 쓰이는 성능 지표로 높을수록 좋고요. 이 로그로스 같은 경우에는 CTR 예측 모델의 바이너리 크로스 엔트로피 손실 함수를 의미합니다. 그래서 이 모델을 발표한 화웨이의 데이터셋과 그리고 오픈 시티알 데이터셋으로 유명한 크리테오에 대해서 모두 딥프엠이 가장 우수한 성능을 보이고 있는 것을 알 수 있습니다. 네 여기까지가 딥fm에 대한 내용이었습니다. 다음은 디아n 딥 인터레스 네트워크입니다. 이 디아에서 사용된 특징은 기존에 사용하지 않았던 유저의 행동 피처, 유저의 비에이비어 피처를 사용했다는 것인데요. 다음을 통해 모델의 전체 구조를 이해해 봅시다. 이 모델에서는 유저가 과거에 행동했던 기록인 유저 비에뷰어 피처를 인풋 피처로 사용하여서 좀 더 정확한 CTR을 예측 하고 있습니다. 이 피처를 어떻게 사용할 수 있도록 본 모델을 설계했는지 전체 구조를 이해해 봅시다. 먼저 dim 모델 이 딥 인터레스트 네트워크 모델은 더 많은 유저 정보 더 많은 유저의 과거 행동 정보와 같은 다양한 피처를 모델에 사용하고 싶다는 니즈에서 출발하였습니다. 기존의 딥러닝 기반 모델 우리가 방금 전에 배웠던 와이드 앤 딥과 딥fm 같은 경우에는 모두 인베딩 이후에 멀티 레이얼 퍼셉트론 즉 피드 포워드 뉴럴 네트워크를 통과시키는 이러한 패러다임을 계속해서 다뤘습니다. 그래서 스프러스한 피처들을 인베딩 변환한 이후에 컨택해서 이 풀 커넥티드 레이어인 MLP의 입력으로 사용하는 방식으로 계속 모델 구조를 설계하였는데요. 이제 이러한 기본 기존의 방식들은 사용자의 다양한 관심사를 반영할 수 없는 모델의 구조였습니다. 예를 들면 사용자는 여러 종류의 식재료와 생필품 같은 서로 다른 카테고리에 관심사가 동시에 존재할 수도 있고요. 그리고 원래 어떤 특정 카테고리의 상품을 검색하던 도중에 중간에 추천 목록에 상품이 떠 있을 때 그 클릭한 데이터를 살펴보면 사실 그 사용자는 원래 특정 카테고리의 상품에 관심이 있다는 컨텍스트가 있는 것이죠. 그래서 이러한 피처를 사용하기 위해서는 좀 더 사용자의 행동이나 관심을 더 잘 담을 수 있는 모델을 설계했어야 합니다. 그래서 사용자가 기존에 소비한 아이템을 리스트 즉 유저 비에이비어 피처로 만들어서 사용자의 관심사를 더 반영하고 그래서 예측 대상 아이템과 내가 과거에 소비한 아이템의 관련성을 더 학습할 수 있도록 그래서 씨티알 예측을 더 정확하게 할 수 있도록 디아의 모델을 설계하였습니다. 네 그래서 이 피처 목록은 din에서 사용한 피처들인데요. dim 모델에서는 지금까지 배웠던 CTR 예측 모델의 인풋에서는 없던 처음 등장하는 피처가 바로 이 유저 baby 피처입니다. 물론 dim 모델에서도 과거의 모델에서 사용하였던 워낫 인코딩 피처 유저 프로파일이나 우리가 지금 예측하려고 하는 그 아이템 여기서는 광고를 의미하는데요. 이런 피처들이 다 원핫 인코딩의 형태로 표현되어 있지만 가운데 있는 유저 BA비어는 특이하게 멀티아 인코딩으로 표현된다는 점이 있습니다. 그 이유는 유저가 과거에 소비했던 아이템이 하나가 아니라 2개 이상 즉 n 개까지 있을 수 있고 이를 사용하기 위해서는 이를 표현하기 위해서는 단순한 원핫 인코딩이 아니라 멀티샷 인코딩의 방식으로 모델 인풋에 넣어야 하기 때문입니다. 그럼 이러한 멀티엣 인코딩 피처를 어떻게 모델이 담고 있는지 다음 슬라이드를 통해 살펴봅시다. 네 모델의 구조는 다음과 같습니다. 크게 3개의 레이어로 구성되어 있는데요. 첫 번째 레이어는 따로 언급하진 않았지만 각각의 모든 스팟 피처를 임베딩하는 레이어이고요. 중요한 부분은 이 두 번째 로컬 액티베이션 레이어입니다. 그리고 마지막 부분은 여러분들이 이미 익숙한 피디 포드 뉴럴 네트워크로 이루어져 있는 엠엘피 레이어입니다. 다른 레이어에는 특별한 특징은 없고 이 두 번째 로컬 액티베이션 레이어에서 바로 유저 비에뷰 피처와 지금 노출하려고 하는 아이템의 관련성을 학습하게 되는데요. 여기서 사용하는 것이 바로 이 액티베이션 유닛입니다. 이 두 번째 레이어가 본 모델의 핵심적인 부분입니다. 그래서 이 로컬 액티베이션 레이어를 통해 좀 더 자세하게 유저 비에이비어 피처에 대해서 살펴봅시다. 먼저 이 우측에 보이는 아까 언급했던 로컬 액티베이션 유닛을 통해서 우리가 지금 노출하려고 하는 즉 CTR을 예측하려고 하는 후보 광고와 과거 유저 비에이뷰어, 과거 유저가 어떤 아이템을 소비했는지의 관련성을 계산하게 됩니다. 그래서 이 예측 아이템과 과거에 소비했던 아이템들을 각각 페어로 계산하여서 이 액티베이션 유닛에 넣게 되면 최종적으로 각각의 인베딩이 하나의 리니어한 즉 스칼라 값으로 출력되게 되고 이 값이 바로 액티베이션 웨이트 입니다. 이 액티베이션 웨이트는 내가 지금 예측하려고 하는 아이템과 과거에 내가 소비했던 아이템이 얼마나 연관이 있는지를 말하는 것이고요. 그래서 그 웨이트가 높다는 것은 연관성이 높아서 이 정보를 더 많이 활용하겠다는 것이고, 웨이트가 낮다는 것은 최근에 소비한 이 아이템의 경우에는 현재 노출하려고 하는 아이템과 연관성이 낮기 때문에 최대한 덜 반영하겠다는 것이죠. 그래서 여기에 있는 이 n개의 인베딩이 웨이트가 곱해지고 나서는 선플링 즉 이 전체 인베딩을 다 더해서 그 차원이 늘어나지 않고 계속해서 같은 차원으로 유지시키도록 만듭니다. 참고로 이 로컬 액티베이션 레이어는 트랜스포머의 어텐션 메커니즘과 유사합니다. 물론 트랜스포머보다 훨씬 간단하게 계산이 이루어지지만 이 타겟 아이템과 이 타겟 아이템을 기준으로 연관도가 높은 아이템 이 n개 중에 무엇이 높은지를 계산하여서 거기에 더 높은 웨이트를 주어서 신호를 더 많이 전달하려고 하는 원리가 바로 트랜스포머의 어텐션이 하는 역할과 유사하다고 볼 수 있습니다. 그래서 이 로컬 액티베이션 레이어를 통해 더해진 썸플링을 통해 더해진 모든 인베딩 값과 그 외에 다른 원핫 인코딩 피처들은 이 레이어를 통과하지 않고 곧바로 마지막 레이어로 들어가게 됩니다. 그래서 이 모든 레이어를 cncat 네이트 한 다음에 마지막에는 MLP 레이어를 통과시켜서 최종 클릭 여부 01을 아웃풋 레이어에서 예측하게 됩니다. 이 로컬 액티베이션 레이어의 연산을 좀 더 잘 이해할 수 있도록 그림으로 비주얼라이즈 한 결과입니다. 이 왼쪽에 있는 아이템들이 유저가 과거에 소비한 유저 비에뷰 피처이고 이 아이템은 이제 노출하려고 하는 즉 씨티알을 예측하려고 하는 아이템인데요. 이 아이템과 비슷한 아이템을 과거에 많이 소비했을수록 이 두 관계를 통해서 웨이트가 높게 반영되고 그렇지 않은 아이템들은 웨이트가 굉장히 낮게 반영됨을 볼 수 있습니다. 그래서 과거에 내가 소비한 아이템들과 지금 여기에 패딩이 있는데요. 이 패딩과 비슷한 아이템들을 많이 소비했을수록 웨이트가 높게 반영되고 즉 연관성이 높기 때문에 이 아이템에 대한 CTR, 즉 이러한 소비 패턴을 가지고 있는 유저에 대해서 이 패딩을 노출시켰을 때 CTR이 더 높게 학습될 수 있도록 모델의 구조를 설계한 것입니다. 이렇게 해서 유저가 과거에 소비했던 피처를 사용하면 이 유저에게 아이템을 노출했을 때 이 예측 CTR이 더 높게 잘 되는지 혹은 낮게 잘 되는지를 정확하게 구할 수 있는 것입니다. 네 마지막으로 논문에서는 기존에 알려진 다른 CTR 예측 모델들과의 성능을 비교하였습니다. 여기 보시면 우리가 계속해서 보았던 기본적인 로지스틱 리그레션 모델과 와이드 앤 딥 딥fm 등이 있는데요. 이제 이 모델들보다 이 디아이엔 모델이 더 높은 에유시 성능을 보임을 알 수 있습니다. 그리고 여기에 있는 이 다이스라는 것은 디아엔 논문에서 제시한 새로운 액티베이션 펑션인데요. 사실 강의 전체적으로 중요한 부분은 아니지만 이 액티베이션 펑션이 궁금하신 분들은 이 논문을 찾아서 읽어보시길 바랍니다. 그래서 이렇게 유저 비에비 피처를 사용했을 때 CTR 예측 정확도가 더 올라간다는 특징을 가진 디아엔 논문이었고요. 이상 디아엔 전체적인 내용을 모두 마쳤습니다. 네 마지막 파트 babr 시퀀스 트랜스포머라는 BST라는 CTR 예측 모델 논문입니다. DCTR의 마지막 파트인데요. CTR 예측에 여러분들도 잘 알고 있는 트랜스포머 아키텍처를 사용한 모델입니다. 그래서 먼저 트랜스포머에 대해서 간단하게 리뷰를 한 이후 어떻게 트랜스포머를 씨티알 예측에 활용하고 있는지 살펴봅시다. 본 논문은 dim 모델을 발표한 알리바바에서 후속으로 발표한 예측 모델인데요. 트랜스포머를 CTR 예측에 사용했을 때 가장 뛰어난 성능을 보인다고 말하고 있습니다. 그리고 방금 전 디아 논문에서는 유저 비에이비오 피처를 사용했지만 여기서는 비에이비오의 피처의 시퀀스까지 더 정확하게 모델링해서 어떤 순서로 이 유저가 행동을 했을 때 다음 에 노출된 아이템의 CTR이 얼마일지를 더 정확하게 구하고 있습니다. 그렇다면 CTR 예측에도 어떻게 트랜스포머가 효과적으로 작용할 수 있을까요? 이 CTR 예측 데이터와 NLP 번역 데이터 간의 공통점이 있습니다. 이 엔엘피 데이터는 이미 트랜스포머에서 아주 좋은 성능을 보인다고 알려져 있는데요. 일단 씨티r 예측 데이터의 인풋 피처들이 대부분 스파스한 피처인데요. NLP 또한 대부분이 단어로 이루어져 있거나 혹은 서 월드로 이루어져 있기 때문에 둘 다 아주 스프레시티가 높은 피처로 이루어져 있습니다. 또한 이 로우와 하이오더 피처 인터랙션이 각각 모두 존재하기 때문에 비선형적인 관계를 모델이 캡처해야 하고요. 또한 문장의 순서가 중요하듯이 사용자의 행동 순서도 굉장히 중요합니다. 즉 핸드폰을 구매한 이후에 핸드폰 케이스 상품을 추천해 줘야 한다는 식의 순서라든지 바지를 구매하고 나서 이젠 바지에 맞는 신발을 찾아보려 한다는 그런 시퀀스가 있는데요. 이제 이런 시퀀스는 이미 자연어 처리에서는 굉장히 중요한 데이터죠. 따라서 CTR 예측 데이터와 NLP 번역 데이터 간에 공통점이 있다는 것을 언급하였고요. 그래서 NLP 분야에 전반적으로 강력한 성능을 보이는 이 트랜스포머 구조를 CTR 예측에도 적용해 보면 좋지 않을까라는 점에서 이 논문이 등장하게 되었습니다. 그럼 제안 모델인 BST bar 시퀀스 트랜스포머 를 다루기 전에 간단하게 트랜스포머를 리뷰하고 넘어가겠습니다. 이미 너무나도 유명한 어텐션 어유니드라는 논문을 통해서 트랜스포머가 공개되었습니다. 트랜스포머에서 제일 중요한 개념은 바로 이 어텐션이라는 메커니즘입니다. 입력값에 대해서 어떤 부분에 주의, 즉 어텐션을 기울일지를 찾는 원리입니다. 어떤 키 밸류 쌍들이 주어지고 우리가 알고자 하는 쿼리가 있을 때 이 쿼리와 키의 연관성을 가중치로 사용해 이 밸류에 곱하여서 최종적으로 이 값을 가중합으로 사용합니다. 간단하게 수식으로 정리하자면 이 QKV가 쿼리 키 밸류인데요. 쿼리와 키의 유사도를 가중치로 사용해서 이 밸류 를 모두 더해준 값입니다. 또한 어텐션 메커니즘을 사용하면은 입력과 출력의 길이를 고려하지 않아도 단어 간의 의존성을 자연스럽게 파악할 수 있습니다. 그래서 이는 번역과 같은 자연어 처리에서 직관적으로 표현이 되는데요. 이 다음 예시가 자연어 처리에서 어떻게 어터션 메커니즘이 작동하는지를 간단하게 설명합니다. 그래서 이 주어진 단어가 이 이이라는 단어가 되겠고요. 이 전체 데이터는 키가 됩니다. 그래서 주어진 단어에 대해서 전체 단어와 얼마나 관계가 있는지를 가중치로 사용하여서 가중치가 높을수록 색깔이 더 진한 것을 알 수 있는데요. 그 가중치가 높을수록 이 키가 가지고 있는 밸류를 더 많이 참고해서 최종적으로 이 쿼리에 대한 표현을 어텐션을 통해서 계산할 수 있게 됩니다. 보시면 이 이이라는 명사가 어떤 단어와 가장 관련이 있을지를 봤을 때 이 스트릿이랑 애니멀이랑 스트릿이라는 단어의 유사도가 제일 높고 그래서 이 쿼리와 이 두 개의 키 간에 계산되는 가중치가 가장 높게 계산되고 그래서 이 애니멀과 스트릿에 해당하는 밸류를 가장 많이 참고하게 됩니다. 트랜스포머의 어텐션 연산은 스케일드 닷 프로덕트 어텐션을 사용하는데요. 이 수식은 다음과 같습니다. 아마 많이 보셨을 텐데요. 쿼리와 키 벡터를 내적한 값을 이제 이 키의 차원인 디케로 나눠서 스케일링 해주고 이를 소프트맥스 펑션에 통과시킵니다. 이 소프트맥스 펑션은 각각의 쿼리 쿼리와 키의 유사도 즉 가중치가 되고요. 이 값을 밸류에 곱해서 전체 밸류의 가중합으로 표현해 주게 됩니다. 또한 스케일드 닷 프로덕트 어텐션 같은 경우에는 셀프 어텐션에 해당되는데요. 이 셀프 어텐션이랑 퀄이나 키와 밸류 그 모든 것이 같은 도메인을 공유한다는 것입니다. 똑같은 입력 값에 대해서 서로 다른 파라미터가 곱해져서 쿼리 키 밸류를 구성하고 있습니다. 다음은 방금 전에 배웠던 스케일 닷 프로덕트 어텐션을 병렬적으로 확장시킨 멀티헤드 어텐션입니다. 차원이 큰 하나의 어텐션을 수행하는 것보다는 여러 개의 스케일 더 프로덕트 어텐션을 병렬적으로 처리하는 것이 더 효과적이라는 것인데요. 이 하나하나의 스케일 더 프로덕트 어텐션이 하나의 헤드가 됩니다. 그래서 이 하나의 헤드가 병렬로 처리되면서 각각의 헤드가 서로 다른 이 데이터의 특징을 잘 찾아내기를 잘 표현하기를 기대하는 것입니다. 네 그래서 트랜스포머 전체 구조를 살펴보면 인코더 구조와 디코더 구조로 이루어져 있고요. 각각의 인코더와 디코더는 6개의 동일한 레이어를 쌓아서 사용하고 있습니다. 이 어텐션 레이어 외에도 인코더와 디코더에 입력하기 전에 포지셔널 인코딩이라는 기법을 사용하는데요. 이 순서에 대한 정보를 주기 위해서 사인 함수와 코사인 함수를 사용하여서 기존의 RNN을 사용하지 않더라도 각각의 시퀀스에 대한 정보를 이 포지셔널 인코딩을 사용해서 표현하고 있습니다. 이제 그 외에도 이 모델을 살펴보면 여기 에드앤 롬이라는 부분이 있는데요. 이 멀티헤드 어텐션이 일어난 이후에 레지듀얼 커넥션과 레이어 노멀라이제이션을 사용하는 그런 테크닉도 추가되어 있습니다. 뭐 그 외에도 다양한 중요한 부분들이 트랜스포머에 포함되어 있지만 이제 우리는 트랜스포머를 배우려고 하는 것이 아니라 결국에는 이 트랜스포머를 CTR 예측에 어떻게 사용해야 했느냐 때문에 간단히 이 부분을 리뷰했는데요. 이제 다시 돌아와서 이 비스티 모델에서 사용한 구조를 살펴봅시다. 네 다음은 비스티의 전체 구조입니다. 이 마지막 예측을 하는 것은 똑같이 클릭 여부가 될 텐데요. 이제 여기서 그동안의 CTR 모델과 가장 큰 차이점은 바로 이 입력값 부분입니다. 유저 BA뷰 시퀀스를 그대로 CTR 예측 모델의 입력 값으로 넣어주었다는 것인데요. 과거에 유저가 소비했던 아이템들의 집합이 아닌 그 시퀀스까지 이 모델이 표현하여서 최종적으로 현재 타겟 아이템에 대한 CTR을 더 정확하게 구하겠다는 것이죠. 그래서 여기 트랜스포머 레이어가 있고요. 그 외에 아더 피처 같은 경우에는 시퀀스 피처가 아닌 그냥 기존에 기존에 CTR 예측 모델에서 많이 사용되던 유저 피처나 컨텍스트 피처가 있고요. 이것들은 그냥 인베딩 이후에 바로 반영이 되고요. 이제 이 시퀀스 피처 거기다가 우리가 예측해야 되는 아이템까지 한 번에 묶어서 이 전체 리스트가 트랜스포머 레이어의 인풋으로 들어가게 됩니다. 그리고 이제 이 부분을 보시면은 트랜스포머 레이어라고 했지만 트랜스포머의 인코더 부분만을 사용했습니다. 그래서 디코더 부분을 제외하고 이 인코더 부분만 이 안에 넣어서 전체 시퀀스를 적절하게 레프레젠테이션 하고 있습니다. 다음 부분은 비스티 모델에서 사용하는 트랜스포머 인코더 레이어를 수식으로 나타내는 것입니다. 사실 기존의 트랜스포머와 완전 동일하고요. 먼저 트랜스포머의 입력 값 즉 아까 유저 BA뷰 시퀀스 전체 플러스 현재 예측하려고 하는 타겟 아이템 전체가 입력 값이 되고요. 그 입력 값은 이 인베딩 2 매트릭스로 표현이 됩니다. 이제 여기에 쿼리와 키와 밸류에 해당하는 각각의 웨이트 매트릭스를 곱해서 셀프 어텐션 즉 스케일 닷 프로덕트 어텐션을 거치고요. 이제 이 셀프 어텐션 부분이 각각의 하나하나 헤드가 되어서 최종적으로 멀티헤드 어텐션 연산을 수행합니다. 이 멀티헤드 어텐션 이후에는 피드 포워드 뉴널 네트워크 을 통과하는데요. 이 피드 포워드 뉴럴 네트워크 앞 뒤로 에드앤 놈 즉 레이어 노말라이제이션과 스킵 커넥션이 추가가 되어 있습니다. 이 s가 멀티 애드 어텐션 이 거친 결과고요. 이 FFN이라는 것은 여기에 있는 에드앤 롬, 피드 포워드 뉴럴 네트워크 다시 에드앤 롬을 설명하고 있습니다. 이제 여기서 기존 트랜스포머와 조금 다른 차이점은 드롭 아웃을 사용하였고 그리고 액티베이션 펑션을 사용하여서 액티베이션 펑션 즉 피드 포워드 유얼 네트워크의 액티베이션 펑션으로 리키 엘류를 사용했다는 점이 기존 인코더 레이어와는 조금 다른 점입니다. 하지만 사실 이 부분은 크게 중요한 포인트는 아니에요. 네 이렇게 해서 하나의 인코더 레이어가 완성된 이후에 이 하나의 인코딩 레이어 즉 아이 번째 인코딩 레이어를 계속 쌓아서 아이 플러스 1번째 인코더 이렇게 블록으로 쌓을 수가 있습니다. 아까 트랜스포머는 총 6개의 블록을 사용했다고 했는데요. 이 비스티 모델도 트랜스포머 인코더 레이어를 n개 쌓아서 모델의 예측 정확도를 높이려고 시도하였습니다. 이 비스티 모델의 구조를 트랜스포머 인코더 레이어를 중심으로 살펴보았는데요. 이 비스티와 바로 이전 파트에서 배운 디아이엔 모델은 비교했을 때 어떠한 차이점이 있는지 간단하게 살펴보겠습니다. 먼저 din에서는 로컬 액티베이션 레이어를 통하여서 유저가 과거에 소비했던 아이템과 현재 아이템의 관련성을 구했는데요. 이 과정에서 단순하게 선플링으로 과거에 소비했던 아이템들을 더하는 방식을 사용했습니다. 하지만 BST 모델에서는 과거에 어떤 아이템들을 소비했는지 뿐만이 아니라 어떤 순서로 소비했는지 그 시퀀스까지 모델의 피처로 사용하여서 현재 아이템과의 관련성을 모델링하였습니다. 그래서 그 모델은 그 관련성은 트랜스포머 인코더 레이어를 사용하니 그리고 트랜스포머 레이어를 해당 모델에 응용한 것이기 때문에 본래 트랜스포머 모델과 어떻게 다르게 변형했는지 또 간단하게 살펴봅시다. 먼저 앞서 인코더 레이어 부분을 언급했듯이 이 비스티의 트랜스포머 인코더는 주로 아웃과 리키 렐루를 사용했다는 점이 다르고요. 그리고 인코더 블록을 6개까지 쌓지 않고 하나에서 4개만 쌓았는데요. 뒤에 언급하겠지만 한계를 사용했을 때 가장 좋은 예측 성능을 보였습니다. 또한 트랜스포머는 사인 코사인을 사용한 포지셔널 인코딩을 사용했는데요. 이 모델에서는 사인 코사인이 아니라 직접 다른 방식으로 포지셔널 인코딩을 사용하였습니다. 말 그대로 아이템을 소비한 시간 그 물리적인 시간을 사용했는데요. 현재 아이템을 예측하려고 하는 현재 시간과 과거에 그 아이템을 소비했던 시간의 차이를 포지셔널 인코딩을 사용했고 이렇게 사용했을 때 더 좋은 성능을 낸다고 말하고 있습니다. 참고로 트랜스포머 모델은 요즘 많은 테스크에서 소타에 준하는 성능을 보이고 있는데요. 각각의 다양한 테스크별로 이 트랜스포머의 핵심적인 구조는 유지하되 이 BST 모델이 응용한 것처럼 각각의 데이터셋에 맞는 약간의 변형과 응용이 이루어진다는 점을 중심으로 기억하시면 좋겠습니다. 그래서 마지막으로 모델의 성능을 비교한 결과입니다. 트랜스포머 레이어는 씨티 프리딕션 테스크에서도 소타의 성능을 보인다고 이야기하고 있고요. 그리고 트랜스포머 블록을 기존 원래 논문에서 6개나 쌓았는데요. 여기서 보시면은 하나 쌓았을 때보다 2개 3개를 쌓았을 때 오히려 성능이 감소함을 알 수 있습니다. 그 이유는 CTR 예측 테스크에서의 시퀀스는 우리가 보통 알고 있는 NLP 머신 트랜슬레이션 같은 태스크보다는 훨씬 더 덜 복잡한 시퀀스를 가지고 있기 때문으로 보입니다. 그래서 기존에 우리가 언급했던 와이드 앤 딥러닝 그리고 와이드 앤 딥러닝의 시퀀스 정보를 추가한 것 그리고 우리가 바로 전 파트에서 배웠던 dim 모델 들보다 이 BST 모델이 더 좋은 예측 성능 그리고 실제 온라인 테스트에서 CTR 개인이 이만큼 상승했다는 것을 볼 수 있습니다. 네 이상 bab어 시퀀스 트랜스포머 모델에 대한 내용이었고요. 이렇게 아홉 번째 강의가 모두 끝났습니다. 이번 강의를 통해서 다양한 딥씨티r 모델과 그 발전 과정을 학습하였습니다. 모두 수고하셨습니다.","confidence":0.9150358,"speakers":[{"label":"","name":"","edited":false}],"events":[],"eventTypes":[]}