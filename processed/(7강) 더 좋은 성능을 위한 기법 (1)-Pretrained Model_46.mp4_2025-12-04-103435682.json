{"result":"SUCCEEDED","message":"Succeeded","token":"e87380acbd2f4d6b882e858b52b87853","version":"ncp_v2_v2.4.6-c00dd1b-20250528__v4.2.20.1_ko_firedepartment_20250923_","params":{"service":"ncp","domain":"general","lang":"ko","completion":"sync","callback":"","diarization":{"enable":false,"speakerCountMin":-1,"speakerCountMax":-1},"sed":{"enable":false},"boostings":[],"forbiddens":"","wordAlignment":true,"fullText":true,"noiseFiltering":true,"priority":0,"userdata":{"_ncp_DomainCode":"tpc-boostcamp","_ncp_DomainId":13807,"_ncp_TaskId":42975698,"_ncp_TraceId":"859feb8fcdc54a15875a9dfd90a159bc"}},"progress":100,"keywords":{},"segments":[{"start":0,"end":13300,"text":"안녕하세요. 도메인 공통 프로젝트 7강 더 좋은 성능을 위한 기법 프리트레인드 모델 시작하겠습니다.","confidence":0.9759,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[5730,6160,"안녕하세요."],[6410,6760,"도메인"],[6810,7080,"공통"],[7110,7540,"프로젝트"],[8150,8460,"7강"],[8870,9020,"더"],[9110,9300,"좋은"],[9410,9720,"성능을"],[9720,9900,"위한"],[10030,10360,"기법"],[10890,11460,"프리트레인드"],[11470,11720,"모델"],[12210,12960,"시작하겠습니다."]],"textEdited":"안녕하세요. 도메인 공통 프로젝트 7강 더 좋은 성능을 위한 기법 프리트레인드 모델 시작하겠습니다."},{"start":13300,"end":25900,"text":"이번 강의에서는 사전 학습 모델의 중요성 이해하기 대표적인 사전 학습 모델 종류와 특징 살펴보기 마지막으로 허깅페이스 트랜스포머스 모듈 활용법 익히기에 대해서 배워보겠습니다.","confidence":0.959,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[13610,13820,"이번"],[13910,14400,"강의에서는"],[14810,15080,"사전"],[15090,15340,"학습"],[15340,15640,"모델의"],[15670,16100,"중요성"],[16210,16620,"이해하기"],[17250,17720,"대표적인"],[17790,18000,"사전"],[18000,18240,"학습"],[18240,18460,"모델"],[18510,18820,"종류와"],[19050,19320,"특징"],[19390,19840,"살펴보기"],[20310,20820,"마지막으로"],[21410,22020,"허깅페이스"],[22050,22680,"트랜스포머스"],[22690,22940,"모듈"],[23170,23540,"활용법"],[23590,24260,"익히기에"],[24270,24620,"대해서"],[24930,25740,"배워보겠습니다."]],"textEdited":"이번 강의에서는 사전 학습 모델의 중요성 이해하기 대표적인 사전 학습 모델 종류와 특징 살펴보기 마지막으로 허깅페이스 트랜스포머스 모듈 활용법 익히기에 대해서 배워보겠습니다."},{"start":25900,"end":37700,"text":"먼저 사전 학습 모델의 중요성 이해하기입니다. 사전 학습 모델의 배경인 전이 학습에 대해서 먼저 이야기해 보겠습니다. 일반적으로 새로운 지식을 배울 때 기존에 알고 있던 지식을 활용하면","confidence":0.9744,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[26230,26500,"먼저"],[26530,26780,"사전"],[26780,27020,"학습"],[27020,27380,"모델의"],[27530,27920,"중요성"],[27970,28620,"이해하기입니다."],[29330,29600,"사전"],[29610,29860,"학습"],[29870,30260,"모델의"],[30350,30720,"배경인"],[31070,31360,"전이"],[31370,31700,"학습에"],[31700,32000,"대해서"],[32090,32320,"먼저"],[32470,32807,"이야기해"],[32807,33280,"보겠습니다."],[33670,34260,"일반적으로"],[34390,34680,"새로운"],[34750,35040,"지식을"],[35040,35260,"배울"],[35260,35400,"때"],[35790,36180,"기존에"],[36250,36447,"알고"],[36447,36640,"있던"],[36730,37040,"지식을"],[37050,37440,"활용하면"]],"textEdited":"먼저 사전 학습 모델의 중요성 이해하기입니다. 사전 학습 모델의 배경인 전이 학습에 대해서 먼저 이야기해 보겠습니다. 일반적으로 새로운 지식을 배울 때 기존에 알고 있던 지식을 활용하면"},{"start":37700,"end":52300,"text":"효율적이라는 부분은 우리가 직관적으로 알고 있습니다. AI도 이미 학습된 지식을 가져와서 이용하게 하자라는 것이 이 전이 학습의 컨셉이라고 이해하시면 되겠는데요. 대규모 데이터셋으로 미리 학습된 모델","confidence":0.9381,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[37930,38640,"효율적이라는"],[38670,39020,"부분은"],[39230,39500,"우리가"],[39610,40200,"직관적으로"],[40530,40800,"알고"],[40800,41160,"있습니다."],[41810,42300,"AI도"],[42610,42800,"이미"],[42990,43320,"학습된"],[43430,43760,"지식을"],[43870,44340,"가져와서"],[44670,45100,"이용하게"],[45110,46040,"하자라는"],[46050,46320,"것이"],[46570,46720,"이"],[46720,47000,"전이"],[47010,47340,"학습의"],[47810,48540,"컨셉이라고"],[48550,49020,"이해하시면"],[49020,49500,"되겠는데요."],[49930,50280,"대규모"],[50290,50920,"데이터셋으로"],[51030,51260,"미리"],[51370,51720,"학습된"],[51790,52040,"모델"]],"textEdited":"효율적이라는 부분은 우리가 직관적으로 알고 있습니다. AI도 이미 학습된 지식을 가져와서 이용하게 하자라는 것이 이 전이 학습의 컨셉이라고 이해하시면 되겠는데요. 대규모 데이터셋으로 미리 학습된 모델"},{"start":52300,"end":63900,"text":"사전 학습 모델이라고 하죠. 프리트레인드 모델이라고 합니다. 이 모델들의 지식 가중치를 사용하는 방법으로 새로운 테스크에 맞게 미세 조정하는 학습 방식을 이야기합니다.","confidence":0.9677,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[52710,52980,"사전"],[52980,53147,"학습"],[53147,53514,"모델이라고"],[53514,53700,"하죠."],[53990,54487,"프리트레인드"],[54487,54874,"모델이라고"],[54874,55120,"합니다."],[55730,55880,"이"],[55890,56380,"모델들의"],[56650,56940,"지식"],[57230,57820,"가중치를"],[57830,58260,"사용하는"],[58430,58860,"방법으로"],[59250,59560,"새로운"],[59630,60060,"테스크에"],[60060,60360,"맞게"],[61010,61227,"미세"],[61227,61680,"조정하는"],[62010,62280,"학습"],[62310,62680,"방식을"],[63130,63880,"이야기합니다."]],"textEdited":"사전 학습 모델이라고 하죠. 프리트레인드 모델이라고 합니다. 이 모델들의 지식 가중치를 사용하는 방법으로 새로운 테스크에 맞게 미세 조정하는 학습 방식을 이야기합니다."},{"start":63900,"end":73200,"text":"밑에 그림에서 보시면 s스 데이터셋라고 하는 어떤 사전에 학습된 커다란 데이터셋을 기반으로 s스 모델을 학습을 시킵니다.","confidence":0.9611,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[64290,64580,"밑에"],[64590,64940,"그림에서"],[64970,65320,"보시면"],[66150,66434,"s스"],[66434,67400,"데이터셋라고"],[67400,67620,"하는"],[68090,68340,"어떤"],[68610,68940,"사전에"],[68950,69260,"학습된"],[69430,69800,"커다란"],[69850,70480,"데이터셋을"],[70510,70960,"기반으로"],[71410,71740,"s스"],[71740,72080,"모델을"],[72350,72680,"학습을"],[72710,73200,"시킵니다."]],"textEdited":"밑에 그림에서 보시면 s스 데이터셋라고 하는 어떤 사전에 학습된 커다란 데이터셋을 기반으로 s스 모델을 학습을 시킵니다."},{"start":73200,"end":86900,"text":"이 과정에서 프리트레인드 사전 학습이 이루어지게 되는데요. 이 모델을 가져와서 우리가 실제 풀고자 하는 타겟 데이터, 이 타겟 데이터에 맞게 미세 조정하는 파인 튜닝 하는 학습 방식입니다.","confidence":0.9592,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[73610,73760,"이"],[73810,74240,"과정에서"],[74670,75260,"프리트레인드"],[75550,75800,"사전"],[75850,76180,"학습이"],[76310,76820,"이루어지게"],[76850,77260,"되는데요."],[77830,77980,"이"],[78010,78420,"모델을"],[78610,79060,"가져와서"],[79530,79820,"우리가"],[79990,80320,"실제"],[80430,80820,"풀고자"],[80820,80980,"하는"],[81150,81400,"타겟"],[81450,81820,"데이터,"],[82210,82360,"이"],[82410,82640,"타겟"],[82670,83060,"데이터에"],[83070,83380,"맞게"],[83990,84280,"미세"],[84280,84700,"조정하는"],[85030,85300,"파인"],[85330,85620,"튜닝"],[85650,85840,"하는"],[85990,86240,"학습"],[86290,86860,"방식입니다."]],"textEdited":"이 과정에서 프리트레인드 사전 학습이 이루어지게 되는데요. 이 모델을 가져와서 우리가 실제 풀고자 하는 타겟 데이터, 이 타겟 데이터에 맞게 미세 조정하는 파인 튜닝 하는 학습 방식입니다."},{"start":86900,"end":97600,"text":"그림에서 보시면 아시겠지만 솔스 모델에서 가져온 모든 모델을 사용하지는 않을 수 있습니다. 특정 레이어를 freeze 시켜서 재학습되지 않도록 하거나","confidence":0.8813,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[87390,87780,"그림에서"],[87830,88160,"보시면"],[88210,88700,"아시겠지만"],[89190,89507,"솔스"],[89507,89860,"모델에서"],[89930,90260,"가져온"],[90590,90860,"모든"],[90970,91340,"모델을"],[91470,92000,"사용하지는"],[92410,92640,"않을"],[92650,92800,"수"],[92800,93140,"있습니다."],[93850,94180,"특정"],[94250,94660,"레이어를"],[94870,95187,"freeze"],[95187,95500,"시켜서"],[95950,96580,"재학습되지"],[96610,96960,"않도록"],[97070,97440,"하거나"]],"textEdited":"그림에서 보시면 아시겠지만 솔스 모델에서 가져온 모든 모델을 사용하지는 않을 수 있습니다. 특정 레이어를 freeze 시켜서 재학습되지 않도록 하거나"},{"start":97600,"end":110600,"text":"혹은 마지막에 새로운 레이어를 추가를 하는 방식으로 학습을 하게 됩니다. 전이 학습 과정은 다음 과정을 따르게 됩니다. 먼저 사전 훈련된 모델을 선택하게 됩니다. 내가 풀고자 하는","confidence":0.9784,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[98010,98260,"혹은"],[98690,99260,"마지막에"],[99370,99680,"새로운"],[99750,100160,"레이어를"],[100250,100620,"추가를"],[100710,100920,"하는"],[101050,101560,"방식으로"],[102150,102520,"학습을"],[102750,102960,"하게"],[102960,103220,"됩니다."],[103830,104087,"전이"],[104087,104300,"학습"],[104300,104600,"과정은"],[104990,105260,"다음"],[105550,105920,"과정을"],[105920,106220,"따르게"],[106220,106480,"됩니다."],[106870,107160,"먼저"],[107330,107620,"사전"],[107620,107940,"훈련된"],[107990,108340,"모델을"],[108350,108727,"선택하게"],[108727,109000,"됩니다."],[109570,109800,"내가"],[109870,110220,"풀고자"],[110220,110400,"하는"]],"textEdited":"혹은 마지막에 새로운 레이어를 추가를 하는 방식으로 학습을 하게 됩니다. 전이 학습 과정은 다음 과정을 따르게 됩니다. 먼저 사전 훈련된 모델을 선택하게 됩니다. 내가 풀고자 하는"},{"start":110600,"end":124400,"text":"혹은 우리 팀이 풀고자 하는 관련 작업에 대한 사전 지식이나 기술을 갖춘 모델을 선택합니다. 그다음 사전 훈련된 모델을 구성합니다. 사전 훈련 모델을 새로운 작업 요구 사항에 맞게 재구성을 합니다.","confidence":0.9783,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[110810,111020,"혹은"],[111370,111580,"우리"],[111580,111800,"팀이"],[111810,112160,"풀고자"],[112190,112400,"하는"],[112890,113120,"관련"],[113210,113487,"작업에"],[113487,113680,"대한"],[114270,114560,"사전"],[114570,114960,"지식이나"],[115310,115700,"기술을"],[115710,115980,"갖춘"],[116070,116400,"모델을"],[116400,116840,"선택합니다."],[117570,117860,"그다음"],[118230,118500,"사전"],[118500,118800,"훈련된"],[118870,119240,"모델을"],[119290,119760,"구성합니다."],[120350,120600,"사전"],[120600,120800,"훈련"],[120810,121180,"모델을"],[121650,121980,"새로운"],[122090,122360,"작업"],[122370,122580,"요구"],[122590,122900,"사항에"],[122900,123200,"맞게"],[123490,123940,"재구성을"],[123940,124180,"합니다."]],"textEdited":"혹은 우리 팀이 풀고자 하는 관련 작업에 대한 사전 지식이나 기술을 갖춘 모델을 선택합니다. 그다음 사전 훈련된 모델을 구성합니다. 사전 훈련 모델을 새로운 작업 요구 사항에 맞게 재구성을 합니다."},{"start":124400,"end":139100,"text":"사전 훈련 모델의 특정 계층을 동결 프리즈 하거나 최종 계층을 제거하거나 혹은 새로운 계층을 도입하는 등 여러 가지 방식을 통해서 재구성을 합니다. 마지막으로는 대상 도메인에 대한 모델을 훈련합니다.","confidence":0.9777,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[124810,125060,"사전"],[125070,125280,"훈련"],[125310,125660,"모델의"],[125890,126200,"특정"],[126250,126620,"계층을"],[126770,127000,"동결"],[127190,127500,"프리즈"],[127530,127860,"하거나"],[128590,128860,"최종"],[128860,129280,"계층을"],[129550,130120,"제거하거나"],[130630,130840,"혹은"],[130950,131220,"새로운"],[131250,131580,"계층을"],[131590,132100,"도입하는"],[132170,132320,"등"],[132530,132760,"여러"],[132760,132960,"가지"],[133030,133440,"방식을"],[133470,133780,"통해서"],[134030,134460,"재구성을"],[134460,134680,"합니다."],[135570,136240,"마지막으로는"],[136970,137280,"대상"],[137310,137714,"도메인에"],[137714,137920,"대한"],[138130,138480,"모델을"],[138490,138960,"훈련합니다."]],"textEdited":"사전 훈련 모델의 특정 계층을 동결 프리즈 하거나 최종 계층을 제거하거나 혹은 새로운 계층을 도입하는 등 여러 가지 방식을 통해서 재구성을 합니다. 마지막으로는 대상 도메인에 대한 모델을 훈련합니다."},{"start":139100,"end":150900,"text":"새 작업 데이터를 기반으로 모델을 학습 성능 모니터링, 필요하다면 하이퍼 파라미터 등을 조정하게 됩니다. 자연어 처리에서 사전 학습 모델 효과에 대해서 살펴보겠습니다.","confidence":0.96,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[139650,139800,"새"],[139910,140160,"작업"],[140160,140560,"데이터를"],[140560,140960,"기반으로"],[141210,141620,"모델을"],[142110,142400,"학습"],[142970,143220,"성능"],[143230,143640,"모니터링,"],[144270,144780,"필요하다면"],[144870,145160,"하이퍼"],[145170,145580,"파라미터"],[145590,145860,"등을"],[146250,146647,"조정하게"],[146647,146920,"됩니다."],[147470,147840,"자연어"],[147840,148240,"처리에서"],[148530,148760,"사전"],[148760,148980,"학습"],[148980,149180,"모델"],[149230,149587,"효과에"],[149587,149880,"대해서"],[150090,150860,"살펴보겠습니다."]],"textEdited":"새 작업 데이터를 기반으로 모델을 학습 성능 모니터링, 필요하다면 하이퍼 파라미터 등을 조정하게 됩니다. 자연어 처리에서 사전 학습 모델 효과에 대해서 살펴보겠습니다."},{"start":150900,"end":164300,"text":"먼저 성능 향상입니다. 사전 학습 모델은 대규모 텍스트를 학습하여 언어의 패턴, 문법, 의미론적 관계를 학습한 상태입니다. 특정 테스크에서 처음부터 학습하는 모델보다 뛰어난 성능을 보입니다.","confidence":0.9459,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[151270,151560,"먼저"],[151730,151980,"성능"],[152010,152480,"향상입니다."],[153090,153340,"사전"],[153340,153560,"학습"],[153560,153880,"모델은"],[154170,154520,"대규모"],[154610,155060,"텍스트를"],[155110,155560,"학습하여"],[155890,156200,"언어의"],[156250,156520,"패턴,"],[157010,157380,"문법,"],[157790,158300,"의미론적"],[158410,158740,"관계를"],[158750,159060,"학습한"],[159130,159600,"상태입니다."],[160230,160500,"특정"],[160530,161020,"테스크에서"],[161110,161560,"처음부터"],[161570,161980,"학습하는"],[162030,162540,"모델보다"],[162870,163200,"뛰어난"],[163290,163620,"성능을"],[163620,164000,"보입니다."]],"textEdited":"먼저 성능 향상입니다. 사전 학습 모델은 대규모 텍스트를 학습하여 언어의 패턴, 문법, 의미론적 관계를 학습한 상태입니다. 특정 테스크에서 처음부터 학습하는 모델보다 뛰어난 성능을 보입니다."},{"start":164300,"end":174700,"text":"학습 리소스를 단축시킬 수 있습니다. 초기 가중치가 이미 있는 상태이므로 랜덤 가중치로 시작하는 것보다 적은 데이터와 시간으로도 목표한 성능을 달성하기 용이하며,","confidence":0.9945,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[164670,164940,"학습"],[164940,165320,"리소스를"],[165330,165787,"단축시킬"],[165787,165874,"수"],[165874,166200,"있습니다."],[166650,166900,"초기"],[166900,167360,"가중치가"],[167360,167540,"이미"],[167610,167820,"있는"],[167910,168420,"상태이므로"],[168830,169120,"랜덤"],[169170,169580,"가중치로"],[169590,170000,"시작하는"],[170000,170340,"것보다"],[170630,170880,"적은"],[170930,171340,"데이터와"],[171570,172120,"시간으로도"],[172590,172980,"목표한"],[173070,173360,"성능을"],[173410,173920,"달성하기"],[174150,174620,"용이하며,"]],"textEdited":"학습 리소스를 단축시킬 수 있습니다. 초기 가중치가 이미 있는 상태이므로 랜덤 가중치로 시작하는 것보다 적은 데이터와 시간으로도 목표한 성능을 달성하기 용이하며,"},{"start":174700,"end":187400,"text":"대규모 데이터로 학습하는 데 필요한 컴퓨팅 자원을 절약할 수 있고, 적은 양의 데이터만으로도 사용 가능하기 때문에 데이터 수집 어려움이 많이 줄어들게 됩니다. 그렇다면 어떤 사전 학습 모델을 선택을 해야 될까요?","confidence":0.9454,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[174990,175340,"대규모"],[175370,175740,"데이터로"],[175790,176167,"학습하는"],[176167,176300,"데"],[176330,176600,"필요한"],[176750,177140,"컴퓨팅"],[177150,177520,"자원을"],[177570,178020,"절약할"],[178030,178147,"수"],[178147,178440,"있고,"],[178810,179080,"적은"],[179170,179440,"양의"],[179450,180140,"데이터만으로도"],[180310,180540,"사용"],[180550,180907,"가능하기"],[180907,181260,"때문에"],[181590,181980,"데이터"],[182050,182280,"수집"],[182310,182760,"어려움이"],[183210,183460,"많이"],[183610,184034,"줄어들게"],[184034,184320,"됩니다."],[184790,185180,"그렇다면"],[185270,185540,"어떤"],[185630,185840,"사전"],[185850,186080,"학습"],[186080,186380,"모델을"],[186380,186680,"선택을"],[186680,186807,"해야"],[186807,187180,"될까요?"]],"textEdited":"대규모 데이터로 학습하는 데 필요한 컴퓨팅 자원을 절약할 수 있고, 적은 양의 데이터만으로도 사용 가능하기 때문에 데이터 수집 어려움이 많이 줄어들게 됩니다. 그렇다면 어떤 사전 학습 모델을 선택을 해야 될까요?"},{"start":187400,"end":197300,"text":"사전 학습 모델의 핵심적인 역할 중 하나는 바로 효율적이고 의미론적인 임베딩 생성에 있습니다. NLP 모델에서 단어를 숫자로 표현하는 방식인 인베딩은","confidence":0.8896,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[187810,188100,"사전"],[188100,188320,"학습"],[188320,188660,"모델의"],[188730,189160,"핵심적인"],[189210,189480,"역할"],[189490,189640,"중"],[189650,190000,"하나는"],[190350,190640,"바로"],[190830,191480,"효율적이고"],[191670,192320,"의미론적인"],[192510,192880,"임베딩"],[192990,193320,"생성에"],[193320,193680,"있습니다."],[194130,194520,"NLP"],[194520,194900,"모델에서"],[195050,195440,"단어를"],[195450,195740,"숫자로"],[195740,196040,"표현하는"],[196110,196460,"방식인"],[196570,197080,"인베딩은"]],"textEdited":"사전 학습 모델의 핵심적인 역할 중 하나는 바로 효율적이고 의미론적인 임베딩 생성에 있습니다. NLP 모델에서 단어를 숫자로 표현하는 방식인 인베딩은"},{"start":197300,"end":209200,"text":"모델의 성능에 큰 영향을 미치게 됩니다. 첫 번째로 볼 인베딩은 정적 인베딩, 스테틱 인베딩입니다. 단어 하나의 임베딩 벡터가 항상 동일하게 유지되는 방식입니다.","confidence":0.9124,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[197790,198160,"모델의"],[198170,198480,"성능에"],[198650,198800,"큰"],[198890,199240,"영향을"],[199310,199647,"미치게"],[199647,199940,"됩니다."],[200470,200620,"첫"],[200630,200960,"번째로"],[200960,201100,"볼"],[201130,201580,"인베딩은"],[201970,202320,"정적"],[202350,202660,"인베딩,"],[203130,203500,"스테틱"],[203510,204080,"인베딩입니다."],[204630,204980,"단어"],[205470,205800,"하나의"],[205870,206200,"임베딩"],[206230,206600,"벡터가"],[207130,207440,"항상"],[207550,207960,"동일하게"],[207960,208320,"유지되는"],[208430,208940,"방식입니다."]],"textEdited":"모델의 성능에 큰 영향을 미치게 됩니다. 첫 번째로 볼 인베딩은 정적 인베딩, 스테틱 인베딩입니다. 단어 하나의 임베딩 벡터가 항상 동일하게 유지되는 방식입니다."},{"start":209200,"end":217100,"text":"가장 기본적이고 직관적인 임베딩 방식입니다. 어떤 문맥에서 사용되든지 모든 단어의 벡터는 불변합니다.","confidence":0.8729,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[209590,209820,"가장"],[209890,210540,"기본적이고"],[210990,211540,"직관적인"],[211610,211940,"임베딩"],[212050,212540,"방식입니다."],[213230,213500,"어떤"],[213570,213960,"문맥에서"],[213960,214460,"사용되든지"],[214910,215200,"모든"],[215250,215540,"단어의"],[215540,215900,"벡터는"],[216110,216700,"불변합니다."]],"textEdited":"가장 기본적이고 직관적인 임베딩 방식입니다. 어떤 문맥에서 사용되든지 모든 단어의 벡터는 불변합니다."},{"start":217100,"end":229300,"text":"정적 인베딩의 한계에 대해서 이야기해 보겠습니다. 워 투 백, 글로브 등 기존의 정적 인베딩은 단어 하나당 하나의 고정된 벡터를 가집니다. 동우미니어나 다이어","confidence":0.8942,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[217770,218100,"정적"],[218130,218540,"인베딩의"],[218590,218887,"한계에"],[218887,219160,"대해서"],[219670,219994,"이야기해"],[219994,220460,"보겠습니다."],[221190,221340,"워"],[221450,221600,"투"],[221600,221720,"백,"],[222030,222340,"글로브"],[222340,222480,"등"],[223090,223460,"기존의"],[223530,223840,"정적"],[223850,224300,"인베딩은"],[224610,224900,"단어"],[224950,225300,"하나당"],[225570,225900,"하나의"],[225930,226240,"고정된"],[226290,226660,"벡터를"],[226660,226980,"가집니다."],[227850,228540,"동우미니어나"],[228770,229160,"다이어"]],"textEdited":"정적 인베딩의 한계에 대해서 이야기해 보겠습니다. 워 투 백, 글로브 등 기존의 정적 인베딩은 단어 하나당 하나의 고정된 벡터를 가집니다. 동우미니어나 다이어"},{"start":229300,"end":242900,"text":"예를 들어서 사과가 과일 또는 행위의 경우 문맥적 의미를 반영하지 못한다는 문제가 있고요. 학습 이후 벡터 값이 고정되어 도메인 변화에 적응이 불가능하다는 부분 형태소나 서브워 수준의 표현 부족으로","confidence":0.9669,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[229610,229827,"예를"],[229827,230100,"들어서"],[230410,230820,"사과가"],[231050,231320,"과일"],[231530,231780,"또는"],[231890,232280,"행위의"],[232280,232500,"경우"],[233090,233580,"문맥적"],[233610,233900,"의미를"],[233900,234260,"반영하지"],[234260,234720,"못한다는"],[234730,235060,"문제가"],[235060,235300,"있고요."],[235950,236240,"학습"],[236240,236420,"이후"],[236610,236900,"벡터"],[236900,237140,"값이"],[237140,237540,"고정되어"],[237870,238240,"도메인"],[238310,238660,"변화에"],[238690,238980,"적응이"],[238980,239540,"불가능하다는"],[239540,239720,"부분"],[240450,241040,"형태소나"],[241330,241660,"서브워"],[241710,242040,"수준의"],[242070,242300,"표현"],[242310,242720,"부족으로"]],"textEdited":"예를 들어서 사과가 과일 또는 행위의 경우 문맥적 의미를 반영하지 못한다는 문제가 있고요. 학습 이후 벡터 값이 고정되어 도메인 변화에 적응이 불가능하다는 부분 형태소나 서브워 수준의 표현 부족으로"},{"start":242900,"end":254100,"text":"희귀어 또는 신조어 처리에 한계가 있습니다. 이에 따라 상위 수준 문장 문서를 구성하기 위한 추가적인 구조가 필요합니다. 오른쪽에 있는 예시를 보면 뱅크라고 하는 단어에 대해","confidence":0.9794,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[243150,243500,"희귀어"],[243730,244000,"또는"],[244170,244520,"신조어"],[244530,244840,"처리에"],[244840,245120,"한계가"],[245120,245460,"있습니다."],[245870,246080,"이에"],[246080,246320,"따라"],[246470,246667,"상위"],[246667,246880,"수준"],[246910,247140,"문장"],[247470,247880,"문서를"],[247880,248300,"구성하기"],[248300,248460,"위한"],[248810,249220,"추가적인"],[249230,249540,"구조가"],[249770,250220,"필요합니다."],[250710,251107,"오른쪽에"],[251107,251280,"있는"],[251310,251640,"예시를"],[251640,251840,"보면"],[252210,252800,"뱅크라고"],[252800,253020,"하는"],[253310,253700,"단어에"],[253700,253960,"대해"]],"textEdited":"희귀어 또는 신조어 처리에 한계가 있습니다. 이에 따라 상위 수준 문장 문서를 구성하기 위한 추가적인 구조가 필요합니다. 오른쪽에 있는 예시를 보면 뱅크라고 하는 단어에 대해"},{"start":254100,"end":267800,"text":"서로 다른 문장 혹은 서로 다른 컨텍스트에 있음에도 동일한 임베딩을 갖는 것을 예시로 보여줍니다. 이런 정적 인베딩의 한계점을 보완하기 위해 문맥에 따라 의미를 다르게 표현하는 동적 인베딩을 사용할 수 있습니다.","confidence":0.9655,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[254410,254660,"서로"],[254660,254840,"다른"],[254910,255180,"문장"],[255270,255480,"혹은"],[255530,255780,"서로"],[255780,256000,"다른"],[256170,256740,"컨텍스트에"],[256750,257220,"있음에도"],[257790,258180,"동일한"],[258650,259120,"임베딩을"],[259170,259440,"갖는"],[259450,259740,"것을"],[259830,260180,"예시로"],[260180,260620,"보여줍니다."],[261230,261420,"이런"],[261570,261880,"정적"],[261930,262380,"인베딩의"],[262410,262820,"한계점을"],[262850,263320,"보완하기"],[263320,263520,"위해"],[263810,264280,"문맥에"],[264290,264540,"따라"],[264650,265000,"의미를"],[265000,265260,"다르게"],[265260,265600,"표현하는"],[265970,266280,"동적"],[266310,266760,"인베딩을"],[266760,267060,"사용할"],[267090,267240,"수"],[267250,267660,"있습니다."]],"textEdited":"서로 다른 문장 혹은 서로 다른 컨텍스트에 있음에도 동일한 임베딩을 갖는 것을 예시로 보여줍니다. 이런 정적 인베딩의 한계점을 보완하기 위해 문맥에 따라 의미를 다르게 표현하는 동적 인베딩을 사용할 수 있습니다."},{"start":267800,"end":280800,"text":"동적 인베딩, 다이나믹 인베딩이라고 하는 이 방법은 단어의 인베딩 벡터가 문맥에 따라 동적으로 변경되는 방식입니다. RNN, LSTM 기반의 시퀀스 투 시퀀스 모델부터","confidence":0.9587,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[268170,268500,"동적"],[268530,268820,"인베딩,"],[269230,269660,"다이나믹"],[269690,270320,"인베딩이라고"],[270320,270520,"하는"],[271010,271160,"이"],[271230,271600,"방법은"],[272230,272600,"단어의"],[272630,272960,"인베딩"],[272990,273360,"벡터가"],[273650,274100,"문맥에"],[274100,274340,"따라"],[274590,275060,"동적으로"],[275070,275480,"변경되는"],[275570,276080,"방식입니다."],[276730,277060,"RNN,"],[277650,278140,"LSTM"],[278140,278500,"기반의"],[279150,279487,"시퀀스"],[279487,279620,"투"],[279630,279967,"시퀀스"],[279967,280400,"모델부터"]],"textEdited":"동적 인베딩, 다이나믹 인베딩이라고 하는 이 방법은 단어의 인베딩 벡터가 문맥에 따라 동적으로 변경되는 방식입니다. RNN, LSTM 기반의 시퀀스 투 시퀀스 모델부터"},{"start":280800,"end":293400,"text":"트랜스포머 기반 모델까지 발전하게 됩니다. 같은 단어라도 문맥에 따라 다른 임베딩 벡터를 가지게 됩니다. 오른쪽에 있는 예시를 보겠습니다. iw 투 더 뱅크, 리버뱅크, 워즈 머디,","confidence":0.7947,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[281050,281580,"트랜스포머"],[281650,281880,"기반"],[281970,282480,"모델까지"],[282690,283114,"발전하게"],[283114,283380,"됩니다."],[284090,284360,"같은"],[284360,284760,"단어라도"],[284950,285360,"문맥에"],[285360,285600,"따라"],[285950,286160,"다른"],[286230,286560,"임베딩"],[286570,286940,"벡터를"],[287110,287354,"가지게"],[287354,287620,"됩니다."],[288070,288560,"오른쪽에"],[288560,288720,"있는"],[288790,289080,"예시를"],[289080,289540,"보겠습니다."],[290230,290540,"iw"],[290590,290740,"투"],[290740,290880,"더"],[290880,291220,"뱅크,"],[292070,292600,"리버뱅크,"],[292610,292960,"워즈"],[292970,293240,"머디,"]],"textEdited":"트랜스포머 기반 모델까지 발전하게 됩니다. 같은 단어라도 문맥에 따라 다른 임베딩 벡터를 가지게 됩니다. 오른쪽에 있는 예시를 보겠습니다. iw 투 더 뱅크, 리버뱅크, 워즈 머디,"},{"start":293400,"end":307800,"text":"랭크 오브 아메리카 이런 식으로 같은 뱅크인 단어가 서로 다른 문장, 서로 다른 문맥이 있을 때 다른 벡터 값을 갖는 것을 보여줍니다. 다음은 대표적인 사전 학습 모델의 종류와 특징을 이해하기입니다.","confidence":0.9516,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[293690,293914,"랭크"],[293914,294034,"오브"],[294034,294400,"아메리카"],[294950,295140,"이런"],[295170,295500,"식으로"],[296010,296340,"같은"],[296550,296960,"뱅크인"],[297010,297360,"단어가"],[297810,298027,"서로"],[298027,298240,"다른"],[298310,298560,"문장,"],[298690,298887,"서로"],[298887,299060,"다른"],[299110,299460,"문맥이"],[299470,299700,"있을"],[299750,299900,"때"],[300650,300920,"다른"],[301110,301400,"벡터"],[301410,301720,"값을"],[301790,302020,"갖는"],[302020,302280,"것을"],[302430,302860,"보여줍니다."],[303570,303920,"다음은"],[304130,304600,"대표적인"],[304630,304860,"사전"],[304860,305100,"학습"],[305130,305480,"모델의"],[305570,305920,"종류와"],[306250,306620,"특징을"],[306790,307760,"이해하기입니다."]],"textEdited":"랭크 오브 아메리카 이런 식으로 같은 뱅크인 단어가 서로 다른 문장, 서로 다른 문맥이 있을 때 다른 벡터 값을 갖는 것을 보여줍니다. 다음은 대표적인 사전 학습 모델의 종류와 특징을 이해하기입니다."},{"start":307800,"end":317700,"text":"첫 번째로는 지피티 제너레이티브 프리 트렌드 트랜스포머라고 하는 모델의 종류입니다. 오토 리그레시브 자기회귀 생성 방식입니다.","confidence":0.7684,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[308650,308800,"첫"],[308810,309280,"번째로는"],[309510,309960,"지피티"],[310590,311140,"제너레이티브"],[311210,311380,"프리"],[311390,311740,"트렌드"],[311740,312980,"트랜스포머라고"],[312980,313180,"하는"],[313470,313840,"모델의"],[313950,314400,"종류입니다."],[314930,315220,"오토"],[315220,315740,"리그레시브"],[316350,316820,"자기회귀"],[316850,317080,"생성"],[317130,317580,"방식입니다."]],"textEdited":"첫 번째로는 지피티 제너레이티브 프리 트렌드 트랜스포머라고 하는 모델의 종류입니다. 오토 리그레시브 자기회귀 생성 방식입니다."},{"start":317700,"end":331900,"text":"자기 회귀 생성 방식이란 왼쪽에서 오른쪽으로 단어를 하나씩 예측해 가면서 텍스트를 생성하도록 학습하는 방식입니다. 퓨샷 학습이나 제로샷 학습과 같은 프롬프트 엔지니어링 방식이 각광받고 있습니다.","confidence":0.9646,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[318130,318420,"자기"],[318430,318660,"회귀"],[318710,318960,"생성"],[318990,319400,"방식이란"],[319890,320420,"왼쪽에서"],[320450,320980,"오른쪽으로"],[321270,321620,"단어를"],[321650,322020,"하나씩"],[322090,322520,"예측해"],[322520,322840,"가면서"],[323470,323940,"텍스트를"],[323970,324480,"생성하도록"],[324570,324960,"학습하는"],[325090,325600,"방식입니다."],[326310,326640,"퓨샷"],[326670,327040,"학습이나"],[327490,327860,"제로샷"],[327890,328220,"학습과"],[328220,328440,"같은"],[329090,329580,"프롬프트"],[329610,330000,"엔지니어링"],[330070,330440,"방식이"],[330710,331267,"각광받고"],[331267,331700,"있습니다."]],"textEdited":"자기 회귀 생성 방식이란 왼쪽에서 오른쪽으로 단어를 하나씩 예측해 가면서 텍스트를 생성하도록 학습하는 방식입니다. 퓨샷 학습이나 제로샷 학습과 같은 프롬프트 엔지니어링 방식이 각광받고 있습니다."},{"start":331900,"end":346100,"text":"별도의 파인튜닝 과정 없이 프롬프트에 몇 개의 예시만으로 다양한 테스크를 수행할 수 있게 되는 방식입니다. 다음은 벌츠입니다. 벌츠는 MLM 방식을 사용을 하게 됩니다. 이 마스크 랭귀지 모델 학습 방식은","confidence":0.8829,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[332210,332640,"별도의"],[332670,333160,"파인튜닝"],[333210,333460,"과정"],[333460,333660,"없이"],[334170,334660,"프롬프트에"],[334690,334840,"몇"],[334850,335100,"개의"],[335110,335680,"예시만으로"],[336050,336380,"다양한"],[336450,336900,"테스크를"],[336930,337260,"수행할"],[337270,337420,"수"],[337420,337620,"있게"],[337620,337840,"되는"],[338210,338720,"방식입니다."],[339350,339720,"다음은"],[339850,340400,"벌츠입니다."],[341030,341480,"벌츠는"],[341630,341980,"MLM"],[342130,342520,"방식을"],[342670,342960,"사용을"],[342970,343147,"하게"],[343147,343440,"됩니다."],[343810,343960,"이"],[343990,344320,"마스크"],[344350,344640,"랭귀지"],[344640,344900,"모델"],[345210,345480,"학습"],[345530,345900,"방식은"]],"textEdited":"별도의 파인튜닝 과정 없이 프롬프트에 몇 개의 예시만으로 다양한 테스크를 수행할 수 있게 되는 방식입니다. 다음은 벌츠입니다. 벌츠는 MLM 방식을 사용을 하게 됩니다. 이 마스크 랭귀지 모델 학습 방식은"},{"start":346100,"end":359000,"text":"문장 내 일부 단어를 마스크 토큰으로 가립니다. 그다음 모델에게 이 가려진 단어가 무엇인지 예측하도록 학습을 시킵니다. 예를 들어서 나는 땡땡을 타고 바다로 나갔다","confidence":0.9892,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[346590,346920,"문장"],[346950,347100,"내"],[347230,347460,"일부"],[347470,347840,"단어를"],[348150,348540,"마스크"],[348550,348940,"토큰으로"],[349070,349520,"가립니다."],[350010,350320,"그다음"],[350490,350960,"모델에게"],[351250,351400,"이"],[351450,351780,"가려진"],[351810,352160,"단어가"],[352310,352840,"무엇인지"],[353130,353760,"예측하도록"],[354010,354320,"학습을"],[354320,354680,"시킵니다."],[355270,355507,"예를"],[355507,355780,"들어서"],[356190,356500,"나는"],[357050,357520,"땡땡을"],[357570,357860,"타고"],[358070,358380,"바다로"],[358380,358780,"나갔다"]],"textEdited":"문장 내 일부 단어를 마스크 토큰으로 가립니다. 그다음 모델에게 이 가려진 단어가 무엇인지 예측하도록 학습을 시킵니다. 예를 들어서 나는 땡땡을 타고 바다로 나갔다"},{"start":359000,"end":369200,"text":"라고 했을 때 이 oo 부분에 배라고 하는 글자가 나올 수 있도록 혹은 베라는 글자가 나올 확률이 높아지도록 모델을","confidence":0.9513,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[359470,359720,"라고"],[359720,359940,"했을"],[359970,360120,"때"],[360530,360680,"이"],[360830,361240,"oo"],[361270,361600,"부분에"],[362150,362720,"배라고"],[362750,362940,"하는"],[363010,363400,"글자가"],[364030,364260,"나올"],[364390,364540,"수"],[364540,364900,"있도록"],[365390,365620,"혹은"],[365950,366380,"베라는"],[366410,366780,"글자가"],[366930,367160,"나올"],[367310,367580,"확률이"],[367580,368200,"높아지도록"],[368650,369040,"모델을"]],"textEdited":"라고 했을 때 이 oo 부분에 배라고 하는 글자가 나올 수 있도록 혹은 베라는 글자가 나올 확률이 높아지도록 모델을"},{"start":369200,"end":380200,"text":"학습시킵니다. 다음은 NSP라고 하는 기법을 또 사용합니다. next ST스 프레딕션이라고 하는데요. 두 개의 문장이 주어졌을 때 원문에서 이어지는 문장인지","confidence":0.8774,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[369490,370060,"학습시킵니다."],[370550,370880,"다음은"],[371010,371880,"NSP라고"],[371880,372060,"하는"],[372110,372460,"기법을"],[372460,372600,"또"],[372630,373020,"사용합니다."],[373330,373680,"next"],[373750,374200,"ST스"],[374270,375180,"프레딕션이라고"],[375190,375560,"하는데요."],[376150,376300,"두"],[376300,376560,"개의"],[376890,377280,"문장이"],[377290,377740,"주어졌을"],[377770,377920,"때"],[378410,379000,"원문에서"],[379070,379420,"이어지는"],[379470,380060,"문장인지"]],"textEdited":"학습시킵니다. 다음은 NSP라고 하는 기법을 또 사용합니다. next ST스 프레딕션이라고 하는데요. 두 개의 문장이 주어졌을 때 원문에서 이어지는 문장인지"},{"start":380200,"end":392700,"text":"아닌지를 예측하도록 학습합니다. 이를 통해 모든 단어가 앞뒤 문맥을 동시에 고려하여 학습할 수 있도록 구성합니다. 다음으로는 GPT와 벌트의 구조를 비교해 보겠습니다.","confidence":0.9121,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[380470,381060,"아닌지를"],[381190,381780,"예측하도록"],[381990,382480,"학습합니다."],[383270,383520,"이를"],[383530,383760,"통해"],[383950,384200,"모든"],[384270,384600,"단어가"],[385090,385460,"앞뒤"],[385490,385860,"문맥을"],[385970,386360,"동시에"],[386390,386720,"고려하여"],[386720,387020,"학습할"],[387020,387114,"수"],[387114,387440,"있도록"],[387750,388220,"구성합니다."],[389210,389720,"다음으로는"],[389770,390300,"GPT와"],[390450,390920,"벌트의"],[391190,391620,"구조를"],[391730,392027,"비교해"],[392027,392500,"보겠습니다."]],"textEdited":"아닌지를 예측하도록 학습합니다. 이를 통해 모든 단어가 앞뒤 문맥을 동시에 고려하여 학습할 수 있도록 구성합니다. 다음으로는 GPT와 벌트의 구조를 비교해 보겠습니다."},{"start":392700,"end":403600,"text":"그 전에 트랜스포머의 인코더와 디코더 구조에 대해서 먼저 살펴보도록 하겠습니다. 트랜스포머의 인코더는 입력된 텍스트를 분석하여 그 의미를 이해하는 역할을 합니다.","confidence":0.9221,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[393270,393387,"그"],[393387,393660,"전에"],[393890,394580,"트랜스포머의"],[394590,395060,"인코더와"],[395230,395680,"디코더"],[395730,396040,"구조에"],[396040,396300,"대해서"],[396350,396580,"먼저"],[396690,397200,"살펴보도록"],[397200,397620,"하겠습니다."],[398350,399040,"트랜스포머의"],[399070,399600,"인코더는"],[399950,400380,"입력된"],[400490,400900,"텍스트를"],[400930,401420,"분석하여"],[401730,401880,"그"],[401950,402320,"의미를"],[402370,402700,"이해하는"],[402730,403107,"역할을"],[403107,403360,"합니다."]],"textEdited":"그 전에 트랜스포머의 인코더와 디코더 구조에 대해서 먼저 살펴보도록 하겠습니다. 트랜스포머의 인코더는 입력된 텍스트를 분석하여 그 의미를 이해하는 역할을 합니다."},{"start":403600,"end":418600,"text":"주어진 문맥 속에서 단어들의 관계를 파악하고 이를 통해 전체 문장의 숨겨진 의미를 파악하는 데 특화되어 있는 구조입니다. 다음은 디코더입니다. 인코더의 정보를 바탕으로 새로운 텍스트를 생성해 내는 역할을 하는데요. 예측,","confidence":0.9737,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[403930,404300,"주어진"],[404350,404580,"문맥"],[404610,404900,"속에서"],[405150,405560,"단어들의"],[405590,405920,"관계를"],[405970,406420,"파악하고"],[406970,407220,"이를"],[407230,407460,"통해"],[408290,408640,"전체"],[408690,409080,"문장의"],[409290,409620,"숨겨진"],[409620,409920,"의미를"],[410030,410440,"파악하는"],[410440,410560,"데"],[410650,411080,"특화되어"],[411080,411260,"있는"],[411390,411820,"구조입니다."],[412430,412760,"다음은"],[412830,413460,"디코더입니다."],[414230,414740,"인코더의"],[414770,415100,"정보를"],[415100,415520,"바탕으로"],[415710,415980,"새로운"],[416050,416440,"텍스트를"],[416450,416714,"생성해"],[416714,416900,"내는"],[416910,417260,"역할을"],[417260,417580,"하는데요."],[418010,418340,"예측,"]],"textEdited":"주어진 문맥 속에서 단어들의 관계를 파악하고 이를 통해 전체 문장의 숨겨진 의미를 파악하는 데 특화되어 있는 구조입니다. 다음은 디코더입니다. 인코더의 정보를 바탕으로 새로운 텍스트를 생성해 내는 역할을 하는데요. 예측,"},{"start":418600,"end":431500,"text":"번역, 요약 등 새로운 결과물을 만들어내는 데 주로 사용합니다. GPT는 트랜스포머의 디코더를 사용하여 생성 능력에 특화되어 있는 구조입니다. gpt2는 오픈 소스인 반면","confidence":0.8979,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[418910,419180,"번역,"],[419850,420120,"요약"],[420210,420360,"등"],[420830,421120,"새로운"],[421210,421700,"결과물을"],[421710,422180,"만들어내는"],[422180,422300,"데"],[422490,422740,"주로"],[422750,423180,"사용합니다."],[424090,424660,"GPT는"],[424870,425460,"트랜스포머의"],[425470,426020,"디코더를"],[426030,426420,"사용하여"],[426590,426880,"생성"],[426930,427260,"능력에"],[427410,427840,"특화되어"],[427850,428040,"있는"],[428190,428640,"구조입니다."],[429410,430060,"gpt2는"],[430310,430560,"오픈"],[430570,430920,"소스인"],[430990,431260,"반면"]],"textEdited":"번역, 요약 등 새로운 결과물을 만들어내는 데 주로 사용합니다. GPT는 트랜스포머의 디코더를 사용하여 생성 능력에 특화되어 있는 구조입니다. gpt2는 오픈 소스인 반면"},{"start":431500,"end":444100,"text":"지피티3, 지피티4와 같은 최신 모델은 가중치가 비공개되어 있고 에피로만 사용이 가능합니다. 최근 지피티 5스스 같이 공개된 모델도 존재는 합니다.","confidence":0.5992,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[431710,432200,"지피티3,"],[432590,433360,"지피티4와"],[433430,433720,"같은"],[434190,434480,"최신"],[434480,434800,"모델은"],[435270,435840,"가중치가"],[436090,436640,"비공개되어"],[436640,436900,"있고"],[437250,437980,"에피로만"],[438130,438440,"사용이"],[438510,438940,"가능합니다."],[439630,439880,"최근"],[440150,440560,"지피티"],[440630,441220,"5스스"],[441250,441560,"같이"],[441950,442300,"공개된"],[442750,443200,"모델도"],[443330,443700,"존재는"],[443700,444080,"합니다."]],"textEdited":"지피티3, 지피티4와 같은 최신 모델은 가중치가 비공개되어 있고 에피로만 사용이 가능합니다. 최근 지피티 5스스 같이 공개된 모델도 존재는 합니다."},{"start":444100,"end":455700,"text":"유료 파인튜닝을 지원하지만 자원 소모 혹은 비용이 매우 높을 수 있습니다. 다음은 폴트입니다. 폴트는 트랜스포머 인코더 구조를 사용하여 문장 이해 능력의 특화","confidence":0.964,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[444550,444800,"유료"],[444850,445360,"파인튜닝을"],[445360,445780,"지원하지만"],[446190,446480,"자원"],[446530,446800,"소모"],[447150,447380,"혹은"],[447670,448080,"비용이"],[448110,448340,"매우"],[448350,448620,"높을"],[448670,448820,"수"],[448910,449280,"있습니다."],[449910,450180,"다음은"],[450210,450700,"폴트입니다."],[451350,451740,"폴트는"],[451810,452300,"트랜스포머"],[452370,452820,"인코더"],[452950,453360,"구조를"],[453470,453920,"사용하여"],[454350,454680,"문장"],[454750,454940,"이해"],[454950,455260,"능력의"],[455310,455580,"특화"]],"textEdited":"유료 파인튜닝을 지원하지만 자원 소모 혹은 비용이 매우 높을 수 있습니다. 다음은 폴트입니다. 폴트는 트랜스포머 인코더 구조를 사용하여 문장 이해 능력의 특화"},{"start":455700,"end":468300,"text":"라이브러리를 통한 쉬운 접근 및 활용이 가능합니다. 다음은 로v타입니다. 월트의 사전 학습 방식을 최적화하여 개선하고 더 많은 데이터로 더 긴 학습 시간을 사용한 모델입니다.","confidence":0.9638,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[456130,456680,"라이브러리를"],[456680,456880,"통한"],[457070,457280,"쉬운"],[457370,457660,"접근"],[457660,457800,"및"],[457990,458340,"활용이"],[458530,458980,"가능합니다."],[459630,459960,"다음은"],[460050,460740,"로v타입니다."],[461270,461700,"월트의"],[461730,462000,"사전"],[462050,462300,"학습"],[462350,462700,"방식을"],[462710,463300,"최적화하여"],[463550,464060,"개선하고"],[464350,464500,"더"],[464550,464740,"많은"],[464810,465220,"데이터로"],[465450,465600,"더"],[465750,465900,"긴"],[465990,466220,"학습"],[466220,466640,"시간을"],[467190,467520,"사용한"],[467750,468220,"모델입니다."]],"textEdited":"라이브러리를 통한 쉬운 접근 및 활용이 가능합니다. 다음은 로v타입니다. 월트의 사전 학습 방식을 최적화하여 개선하고 더 많은 데이터로 더 긴 학습 시간을 사용한 모델입니다."},{"start":468300,"end":480800,"text":"NSP 테스크가 성능 향상에 큰 기여를 하지 않는다고 판단하여 제거하는 대신 엠엘엠 테스크에 집중하게 된 모델입니다. 폴트는 사전 학습 단계에서 마스킹 패턴을 한 번으로 고정하지만","confidence":0.8098,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[468610,469040,"NSP"],[469070,469480,"테스크가"],[469750,470040,"성능"],[470110,470420,"향상에"],[470470,470620,"큰"],[470710,471040,"기여를"],[471040,471240,"하지"],[471240,471700,"않는다고"],[471750,472200,"판단하여"],[472590,473020,"제거하는"],[473020,473240,"대신"],[473550,473920,"엠엘엠"],[473970,474400,"테스크에"],[474490,474907,"집중하게"],[474907,475040,"된"],[475270,475760,"모델입니다."],[476390,476860,"폴트는"],[477450,477687,"사전"],[477687,477900,"학습"],[477930,478400,"단계에서"],[478690,479060,"마스킹"],[479090,479480,"패턴을"],[479710,479860,"한"],[479860,480180,"번으로"],[480190,480680,"고정하지만"]],"textEdited":"NSP 테스크가 성능 향상에 큰 기여를 하지 않는다고 판단하여 제거하는 대신 엠엘엠 테스크에 집중하게 된 모델입니다. 폴트는 사전 학습 단계에서 마스킹 패턴을 한 번으로 고정하지만"},{"start":480800,"end":492200,"text":"로버타는 학습 데이터가 모델에 주입될 때마다 다른 마스킹 패턴을 적용하는 다이나믹 마스킹을 사용합니다. 이를 통해 더 다양한 더 복잡한 패턴을 잘 학습할 수 있게 됩니다.","confidence":0.9521,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[481130,481720,"로버타는"],[482130,482400,"학습"],[482430,482880,"데이터가"],[483070,483460,"모델에"],[483490,483860,"주입될"],[483970,484360,"때마다"],[484650,484880,"다른"],[484990,485340,"마스킹"],[485390,485760,"패턴을"],[485760,486140,"적용하는"],[486390,486780,"다이나믹"],[486810,487247,"마스킹을"],[487247,487640,"사용합니다."],[488150,488380,"이를"],[488380,488580,"통해"],[488580,488700,"더"],[488770,489140,"다양한"],[489410,489560,"더"],[489690,490100,"복잡한"],[490130,490540,"패턴을"],[490890,491040,"잘"],[491150,491500,"학습할"],[491500,491574,"수"],[491574,491740,"있게"],[491740,492120,"됩니다."]],"textEdited":"로버타는 학습 데이터가 모델에 주입될 때마다 다른 마스킹 패턴을 적용하는 다이나믹 마스킹을 사용합니다. 이를 통해 더 다양한 더 복잡한 패턴을 잘 학습할 수 있게 됩니다."},{"start":492200,"end":502600,"text":"다음은 효율성을 위한 경량화에 집중한 알버트 모델에 대해서 이야기해 보겠습니다. 파라미터를 줄여 학습 속도를 올리기 위해 다음과 같은 테크닉을 사용합니다.","confidence":0.9821,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[492510,492880,"다음은"],[493070,493507,"효율성을"],[493507,493680,"위한"],[493890,494420,"경량화에"],[494590,494960,"집중한"],[495390,495840,"알버트"],[495840,496180,"모델에"],[496180,496480,"대해서"],[496830,497134,"이야기해"],[497134,497600,"보겠습니다."],[498210,498760,"파라미터를"],[498760,498940,"줄여"],[499050,499300,"학습"],[499330,499680,"속도를"],[499730,500060,"올리기"],[500060,500260,"위해"],[500630,500960,"다음과"],[500960,501180,"같은"],[501270,501780,"테크닉을"],[501950,502480,"사용합니다."]],"textEdited":"다음은 효율성을 위한 경량화에 집중한 알버트 모델에 대해서 이야기해 보겠습니다. 파라미터를 줄여 학습 속도를 올리기 위해 다음과 같은 테크닉을 사용합니다."},{"start":502600,"end":510800,"text":"거대한 인베딩 행렬을 2개의 행렬로 분해하고 히든 레이어와 단어 임베딩을 분리하여 학습해야 되는 파라미터 수를 감소시켰습니다.","confidence":0.9304,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[503050,503380,"거대한"],[503430,503740,"인베딩"],[503750,504100,"행렬을"],[504250,504620,"2개의"],[504620,504860,"행렬로"],[504860,505320,"분해하고"],[505650,505940,"히든"],[505950,506300,"레이어와"],[506510,506780,"단어"],[506810,507260,"임베딩을"],[507330,507760,"분리하여"],[508110,508620,"학습해야"],[508620,508840,"되는"],[509070,509540,"파라미터"],[509570,509900,"수를"],[510070,510800,"감소시켰습니다."]],"textEdited":"거대한 인베딩 행렬을 2개의 행렬로 분해하고 히든 레이어와 단어 임베딩을 분리하여 학습해야 되는 파라미터 수를 감소시켰습니다."},{"start":510800,"end":522800,"text":"기존 트랜스포머에서 각 레이어 간 같은 파라미터를 공유하도록 변경하였고, 이를 통해 모델 크기 및 메모리 사용량 그리고 학습 시간을 효율화하는 개선점이 있었습니다.","confidence":0.9908,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[511110,511360,"기존"],[511470,512180,"트랜스포머에서"],[512570,512720,"각"],[512770,513080,"레이어"],[513080,513200,"간"],[513430,513700,"같은"],[513770,514260,"파라미터를"],[514350,514860,"공유하도록"],[514990,515640,"변경하였고,"],[516230,516500,"이를"],[516510,516740,"통해"],[516910,517160,"모델"],[517250,517460,"크기"],[517460,517580,"및"],[517910,518240,"메모리"],[518240,518540,"사용량"],[518970,519280,"그리고"],[519630,519900,"학습"],[519900,520280,"시간을"],[520370,521020,"효율화하는"],[521430,521960,"개선점이"],[521990,522580,"있었습니다."]],"textEdited":"기존 트랜스포머에서 각 레이어 간 같은 파라미터를 공유하도록 변경하였고, 이를 통해 모델 크기 및 메모리 사용량 그리고 학습 시간을 효율화하는 개선점이 있었습니다."},{"start":522800,"end":534800,"text":"또한 문장 사이의 순서를 학습하여 문장 간의 일관성을 효율적으로 학습할 수 있도록 합니다. 에오피를 통해 실제 담화 흐름에서 앞뒤 문장 순서가 자연스러운지를 학습합니다.","confidence":0.9133,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[523170,523400,"또한"],[523690,524040,"문장"],[524110,524420,"사이의"],[524470,524800,"순서를"],[524810,525200,"학습하여"],[525570,525900,"문장"],[525910,526160,"간의"],[526210,526740,"일관성을"],[527010,527460,"효율적으로"],[527460,527760,"학습할"],[527760,527834,"수"],[527834,528120,"있도록"],[528120,528360,"합니다."],[528830,529400,"에오피를"],[529400,529620,"통해"],[530070,530440,"실제"],[530450,530700,"담화"],[530710,531060,"흐름에서"],[531530,531860,"앞뒤"],[531910,532160,"문장"],[532210,532560,"순서가"],[532790,533700,"자연스러운지를"],[533910,534640,"학습합니다."]],"textEdited":"또한 문장 사이의 순서를 학습하여 문장 간의 일관성을 효율적으로 학습할 수 있도록 합니다. 에오피를 통해 실제 담화 흐름에서 앞뒤 문장 순서가 자연스러운지를 학습합니다."},{"start":534800,"end":546000,"text":"다음은 디스티 v트입니다. 지식을 증여하는 널리지 디스틸레이션 기법을 적용한 경량화 모델입니다. 벌트 모델을 티처 모델로 볼트 모델의 출력을","confidence":0.8544,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[535110,535440,"다음은"],[535650,536000,"디스티"],[536010,536500,"v트입니다."],[536930,537380,"지식을"],[537650,538060,"증여하는"],[538650,539020,"널리지"],[539020,539580,"디스틸레이션"],[539690,540120,"기법을"],[540610,540960,"적용한"],[541330,541700,"경량화"],[541730,542220,"모델입니다."],[542870,543220,"벌트"],[543220,543520,"모델을"],[543570,543840,"티처"],[543840,544240,"모델로"],[544870,545180,"볼트"],[545180,545460,"모델의"],[545510,545860,"출력을"]],"textEdited":"다음은 디스티 v트입니다. 지식을 증여하는 널리지 디스틸레이션 기법을 적용한 경량화 모델입니다. 벌트 모델을 티처 모델로 볼트 모델의 출력을"},{"start":546000,"end":559400,"text":"학생 모델인 디스트리 버트가 모방하도록 학습을 합니다. 이를 통해 벌트 베이스 모델에 비해 파라미터 수가 40% 적어 추론 속도가 약 60% 더 빨라지게 되는 결과를 얻었습니다.","confidence":0.933,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[546350,546680,"학생"],[546710,547060,"모델인"],[547510,547920,"디스트리"],[547920,548280,"버트가"],[548370,548980,"모방하도록"],[549250,549580,"학습을"],[549590,549860,"합니다."],[550510,550760,"이를"],[550770,551000,"통해"],[551450,551780,"벌트"],[551810,552140,"베이스"],[552140,552447,"모델에"],[552447,552680,"비해"],[553370,553780,"파라미터"],[553780,554020,"수가"],[554130,554560,"40%"],[554690,554960,"적어"],[555430,555680,"추론"],[555710,556040,"속도가"],[556190,556340,"약"],[556570,557020,"60%"],[557070,557220,"더"],[557290,557740,"빨라지게"],[557740,557960,"되는"],[558430,558840,"결과를"],[558840,559360,"얻었습니다."]],"textEdited":"학생 모델인 디스트리 버트가 모방하도록 학습을 합니다. 이를 통해 벌트 베이스 모델에 비해 파라미터 수가 40% 적어 추론 속도가 약 60% 더 빨라지게 되는 결과를 얻었습니다."},{"start":559400,"end":569900,"text":"다음으로는 라마입니다. 라즐 랭귀지 모델 메타 AI라고 하는 모델이고요. 사전 학습 방식을 따릅니다. 지피티와 유사하게 오토 리그레시브 생성 방식입니다.","confidence":0.7981,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[559770,560360,"다음으로는"],[560650,561240,"라마입니다."],[561770,562080,"라즐"],[562080,562340,"랭귀지"],[562340,562560,"모델"],[562850,563120,"메타"],[563150,563960,"AI라고"],[563960,564160,"하는"],[564170,564680,"모델이고요."],[564990,565260,"사전"],[565270,565500,"학습"],[565530,565900,"방식을"],[565910,566300,"따릅니다."],[566890,567420,"지피티와"],[567450,567880,"유사하게"],[568170,568440,"오토"],[568450,568940,"리그레시브"],[568990,569220,"생성"],[569250,569800,"방식입니다."]],"textEdited":"다음으로는 라마입니다. 라즐 랭귀지 모델 메타 AI라고 하는 모델이고요. 사전 학습 방식을 따릅니다. 지피티와 유사하게 오토 리그레시브 생성 방식입니다."},{"start":569900,"end":584300,"text":"세븐빌리언, 퍼틴 빌리언 혹은 다른 여러 가지 버전에 따라 자원 요구량이 다양합니다. 당연히 파라미터 수가 많으면 많을수록 높은 성능을 가지고 있습니다. 오픈 소스 및 접근 방식에서는 gpt3,","confidence":0.8512,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[570370,570920,"세븐빌리언,"],[570970,571260,"퍼틴"],[571260,571540,"빌리언"],[572170,572400,"혹은"],[572550,572780,"다른"],[573370,573620,"여러"],[573620,573820,"가지"],[573830,574180,"버전에"],[574180,574440,"따라"],[574830,575100,"자원"],[575150,575620,"요구량이"],[575830,576320,"다양합니다."],[576830,577220,"당연히"],[577790,578220,"파라미터"],[578250,578500,"수가"],[578530,578820,"많으면"],[578820,579260,"많을수록"],[579610,579860,"높은"],[579950,580280,"성능을"],[580510,580787,"가지고"],[580787,581120,"있습니다."],[581710,581980,"오픈"],[581980,582187,"소스"],[582187,582320,"및"],[582430,582720,"접근"],[582750,583260,"방식에서는"],[583590,584120,"gpt3,"]],"textEdited":"세븐빌리언, 퍼틴 빌리언 혹은 다른 여러 가지 버전에 따라 자원 요구량이 다양합니다. 당연히 파라미터 수가 많으면 많을수록 높은 성능을 가지고 있습니다. 오픈 소스 및 접근 방식에서는 gpt3,"},{"start":584300,"end":598300,"text":"지피티4와 달리 가중치 자체가 연구 목적으로 공개되었다는 점이 핵심적인 특징입니다. 연구 라이선스로 공개되어 있어 다양한 실험 가능 연구 목적 외 상업적 사용은 제한됩니다. 다음은 허깅페이스 트랜스포머스 모듈 사용법 익히기입니다.","confidence":0.8814,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[584570,585140,"지피티4와"],[585140,585340,"달리"],[585850,586240,"가중치"],[586250,586600,"자체가"],[586610,586860,"연구"],[586890,587200,"목적으로"],[587200,587727,"공개되었다는"],[587727,587980,"점이"],[588030,588400,"핵심적인"],[588430,588840,"특징입니다."],[589310,589620,"연구"],[589620,590080,"라이선스로"],[590090,590440,"공개되어"],[590440,590660,"있어"],[590870,591200,"다양한"],[591290,591520,"실험"],[591570,591820,"가능"],[592170,592480,"연구"],[592530,592800,"목적"],[592850,593000,"외"],[593190,593580,"상업적"],[593650,593960,"사용은"],[594070,594600,"제한됩니다."],[595130,595440,"다음은"],[595610,596200,"허깅페이스"],[596200,596800,"트랜스포머스"],[596800,597060,"모듈"],[597090,597420,"사용법"],[597450,598000,"익히기입니다."]],"textEdited":"지피티4와 달리 가중치 자체가 연구 목적으로 공개되었다는 점이 핵심적인 특징입니다. 연구 라이선스로 공개되어 있어 다양한 실험 가능 연구 목적 외 상업적 사용은 제한됩니다. 다음은 허깅페이스 트랜스포머스 모듈 사용법 익히기입니다."},{"start":598300,"end":610800,"text":"허깅페이스 트랜스포머스 라이브러리는 많은 트랜스포머 계열의 모델들을 쉽게 사용할 수 있도록 다양한 기능을 제공하는 라이브러리입니다. 관련된 코드와 사전 학습 모델을 쉽게 다운로드, 파인 튜닝 할 수 있도록 지원합니다.","confidence":0.919,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[598630,599220,"허깅페이스"],[599220,599800,"트랜스포머스"],[599800,600300,"라이브러리는"],[600670,600920,"많은"],[601110,601540,"트랜스포머"],[601570,601880,"계열의"],[601910,602380,"모델들을"],[602410,602680,"쉽게"],[602680,602940,"사용할"],[602940,603020,"수"],[603020,603320,"있도록"],[603430,603760,"다양한"],[603850,604160,"기능을"],[604210,604620,"제공하는"],[604870,605560,"라이브러리입니다."],[606050,606420,"관련된"],[606490,606820,"코드와"],[607350,607600,"사전"],[607600,607820,"학습"],[607820,608100,"모델을"],[608100,608320,"쉽게"],[608320,608720,"다운로드,"],[609050,609300,"파인"],[609300,609520,"튜닝"],[609520,609660,"할"],[609660,609720,"수"],[609720,610000,"있도록"],[610030,610520,"지원합니다."]],"textEdited":"허깅페이스 트랜스포머스 라이브러리는 많은 트랜스포머 계열의 모델들을 쉽게 사용할 수 있도록 다양한 기능을 제공하는 라이브러리입니다. 관련된 코드와 사전 학습 모델을 쉽게 다운로드, 파인 튜닝 할 수 있도록 지원합니다."},{"start":610800,"end":620100,"text":"LLM의 배포 및 접근성 측면에서 주요 LLM들은 허깅페이스 같은 플랫폼을 통해 사전 학습 모델의 형태로 배포하게 됩니다. 허깅페이스는","confidence":0.9346,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[611230,611700,"LLM의"],[611710,611960,"배포"],[611960,612080,"및"],[612110,612620,"접근성"],[612750,613160,"측면에서"],[613690,613940,"주요"],[613990,614480,"LLM들은"],[614850,615440,"허깅페이스"],[615440,615660,"같은"],[615670,616040,"플랫폼을"],[616040,616280,"통해"],[616710,616960,"사전"],[616960,617200,"학습"],[617200,617500,"모델의"],[617500,617840,"형태로"],[618090,618487,"배포하게"],[618487,618760,"됩니다."],[619250,619920,"허깅페이스는"]],"textEdited":"LLM의 배포 및 접근성 측면에서 주요 LLM들은 허깅페이스 같은 플랫폼을 통해 사전 학습 모델의 형태로 배포하게 됩니다. 허깅페이스는"},{"start":620100,"end":631100,"text":"라마 연구 라이선스나 블룸 오픈 라이선스처럼 가중치를 공개한 오픈 소스 대형 모델들을 제공하여 연구 및 개발 커뮤니티가 직접 모델을 분석하고 커스터마이징 할 수 있도록 돕습니다.","confidence":0.9799,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[620430,620740,"라마"],[620810,621060,"연구"],[621060,621500,"라이선스나"],[622050,622280,"블룸"],[622370,622580,"오픈"],[622580,623080,"라이선스처럼"],[623490,624020,"가중치를"],[624050,624340,"공개한"],[624410,624640,"오픈"],[624640,624880,"소스"],[624880,625120,"대형"],[625150,625580,"모델들을"],[625590,626000,"제공하여"],[626290,626620,"연구"],[626620,626740,"및"],[627010,627280,"개발"],[627310,627780,"커뮤니티가"],[627910,628180,"직접"],[628190,628480,"모델을"],[628510,629020,"분석하고"],[629430,630040,"커스터마이징"],[630040,630160,"할"],[630160,630220,"수"],[630220,630460,"있도록"],[630460,630980,"돕습니다."]],"textEdited":"라마 연구 라이선스나 블룸 오픈 라이선스처럼 가중치를 공개한 오픈 소스 대형 모델들을 제공하여 연구 및 개발 커뮤니티가 직접 모델을 분석하고 커스터마이징 할 수 있도록 돕습니다."},{"start":631100,"end":644100,"text":"파인 튜닝 용이성 측면에서는 적은 수의 데이터를 이용한 로컬 파인튜닝 학습으로도 높은 성능을 기대할 수 있습니다. 문장 분류, 질의응답, 개체명 인식 등에 맞춰 마지막 레이어를 추가하여 학습하면 됩니다.","confidence":0.9118,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[631530,631780,"파인"],[631780,632020,"튜닝"],[632030,632380,"용이성"],[632470,632940,"측면에서는"],[633470,633740,"적은"],[633750,634020,"수의"],[634020,634360,"데이터를"],[634360,634580,"이용한"],[634630,634920,"로컬"],[634950,635360,"파인튜닝"],[635410,635920,"학습으로도"],[636210,636460,"높은"],[636550,636860,"성능을"],[636860,637160,"기대할"],[637160,637254,"수"],[637254,637580,"있습니다."],[638270,638560,"문장"],[638610,638860,"분류,"],[639330,639780,"질의응답,"],[640350,640740,"개체명"],[640830,641080,"인식"],[641080,641260,"등에"],[641260,641500,"맞춰"],[641790,642100,"마지막"],[642170,642540,"레이어를"],[642610,643040,"추가하여"],[643210,643600,"학습하면"],[643610,643900,"됩니다."]],"textEdited":"파인 튜닝 용이성 측면에서는 적은 수의 데이터를 이용한 로컬 파인튜닝 학습으로도 높은 성능을 기대할 수 있습니다. 문장 분류, 질의응답, 개체명 인식 등에 맞춰 마지막 레이어를 추가하여 학습하면 됩니다."},{"start":644100,"end":652500,"text":"다음은 허깅페이스 트랜스포머스 라이브러리에 있는 파이프라인에 대해서 설명드리겠습니다. 사전 훈련된 모델을 추론을 수행할 때 사용하는","confidence":0.9464,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[644510,644860,"다음은"],[645050,645640,"허깅페이스"],[645640,646200,"트랜스포머스"],[646210,646667,"라이브러리에"],[646667,646860,"있는"],[647310,648240,"파이프라인에"],[648240,648560,"대해서"],[648630,649300,"설명드리겠습니다."],[649570,649840,"사전"],[649850,650180,"훈련된"],[650230,650600,"모델을"],[650830,651220,"추론을"],[651450,651767,"수행할"],[651767,651900,"때"],[651910,652320,"사용하는"]],"textEdited":"다음은 허깅페이스 트랜스포머스 라이브러리에 있는 파이프라인에 대해서 설명드리겠습니다. 사전 훈련된 모델을 추론을 수행할 때 사용하는"},{"start":652500,"end":663600,"text":"인터페이스입니다. 사용자가 모델 코드를 직접 작성하거나 복잡한 설정을 할 필요 없이 특정 테스크를 위한 사전 훈련된 모델과 토크나이저를 자동으로 다운로드하고 캐시하여","confidence":0.9775,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[652770,653460,"인터페이스입니다."],[653890,654360,"사용자가"],[654530,654800,"모델"],[654870,655180,"코드를"],[655210,655440,"직접"],[655470,656020,"작성하거나"],[656410,656840,"복잡한"],[656890,657167,"설정을"],[657167,657300,"할"],[657300,657480,"필요"],[657480,657700,"없이"],[658350,658620,"특정"],[658650,659007,"테스크를"],[659007,659200,"위한"],[659430,659660,"사전"],[659660,659940,"훈련된"],[659950,660320,"모델과"],[660870,661540,"토크나이저를"],[661540,661860,"자동으로"],[661860,662440,"다운로드하고"],[662910,663420,"캐시하여"]],"textEdited":"인터페이스입니다. 사용자가 모델 코드를 직접 작성하거나 복잡한 설정을 할 필요 없이 특정 테스크를 위한 사전 훈련된 모델과 토크나이저를 자동으로 다운로드하고 캐시하여"},{"start":663600,"end":678100,"text":"사용합니다. 여러 모달리티 자연어 처리, 컴퓨터, 비전, 오디오 혹은 멀티 모델에서 다양한 과업을 기본적으로 지원합니다. 사용 방법은 다음과 같습니다. 파이프라인 함수에 수행하고자 하는 태스크를 지정하여 인스턴스를 생성합니다.","confidence":0.9162,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[663850,664320,"사용합니다."],[664530,664820,"여러"],[664820,665240,"모달리티"],[665590,665920,"자연어"],[665920,666140,"처리,"],[666450,666760,"컴퓨터,"],[666770,667020,"비전,"],[667370,667720,"오디오"],[668070,668300,"혹은"],[668750,669080,"멀티"],[669080,669480,"모델에서"],[669790,670140,"다양한"],[670250,670620,"과업을"],[670750,671380,"기본적으로"],[671670,672120,"지원합니다."],[672770,673020,"사용"],[673070,673420,"방법은"],[673430,673740,"다음과"],[673740,674040,"같습니다."],[674510,674960,"파이프라인"],[674970,675280,"함수에"],[675310,675800,"수행하고자"],[675800,675960,"하는"],[676030,676480,"태스크를"],[676490,676880,"지정하여"],[676970,677540,"인스턴스를"],[677540,677920,"생성합니다."]],"textEdited":"사용합니다. 여러 모달리티 자연어 처리, 컴퓨터, 비전, 오디오 혹은 멀티 모델에서 다양한 과업을 기본적으로 지원합니다. 사용 방법은 다음과 같습니다. 파이프라인 함수에 수행하고자 하는 태스크를 지정하여 인스턴스를 생성합니다."},{"start":678100,"end":688300,"text":"예를 들어 감정 분석을 위해 파이프라인 센티멘트 아네시스와 같이 사용하면 됩니다. 생성된 클래스 파이어 객체에 대상 텍스트를 전달하여 추론을 수행합니다.","confidence":0.8987,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[678310,678620,"예를"],[678620,678800,"들어"],[679070,679400,"감정"],[679470,679800,"분석을"],[679800,679980,"위해"],[680290,680720,"파이프라인"],[680990,681460,"센티멘트"],[681460,681980,"아네시스와"],[681980,682240,"같이"],[682590,683000,"사용하면"],[683150,683460,"됩니다."],[684050,684380,"생성된"],[684430,684680,"클래스"],[684730,685000,"파이어"],[685030,685380,"객체에"],[685630,685900,"대상"],[685910,686280,"텍스트를"],[686290,686760,"전달하여"],[687130,687460,"추론을"],[687690,688140,"수행합니다."]],"textEdited":"예를 들어 감정 분석을 위해 파이프라인 센티멘트 아네시스와 같이 사용하면 됩니다. 생성된 클래스 파이어 객체에 대상 텍스트를 전달하여 추론을 수행합니다."},{"start":688300,"end":702200,"text":"파이프라인 예제를 보겠습니다. 텍스트 생성용 파이프라인이고 결과는 리스트 형태로 변환되며 첫 번째 결과에 제너레이티드 텍스트를 사용하시면 됩니다. 이 예제에 사용한 옵션을 설명드리면 모델은 gpd2 GPU는 0번 디바이스","confidence":0.8801,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[688590,688960,"파이프라인"],[688960,689240,"예제를"],[689240,689640,"보겠습니다."],[690210,690540,"텍스트"],[690550,690880,"생성용"],[690910,691560,"파이프라인이고"],[691970,692420,"결과는"],[692650,692980,"리스트"],[692980,693227,"형태로"],[693227,693640,"변환되며"],[694090,694240,"첫"],[694250,694520,"번째"],[694530,694860,"결과에"],[694930,695440,"제너레이티드"],[695490,695900,"텍스트를"],[695900,696307,"사용하시면"],[696307,696580,"됩니다."],[696930,697080,"이"],[697130,697440,"예제에"],[697440,697700,"사용한"],[697700,698000,"옵션을"],[698000,698460,"설명드리면"],[698930,699280,"모델은"],[699280,699700,"gpd2"],[700130,700720,"GPU는"],[701110,701400,"0번"],[701450,701920,"디바이스"]],"textEdited":"파이프라인 예제를 보겠습니다. 텍스트 생성용 파이프라인이고 결과는 리스트 형태로 변환되며 첫 번째 결과에 제너레이티드 텍스트를 사용하시면 됩니다. 이 예제에 사용한 옵션을 설명드리면 모델은 gpd2 GPU는 0번 디바이스"},{"start":702200,"end":714200,"text":"마이너스 1을 넣는 경우는 CPU가 됩니다. 입력 길이 제한을 넣었고요. 생성 결과 수 제한을 넣었습니다. 오른쪽 예시를 보시면 모델 이름에 오픈 AI, 커뮤니티 SLA시 gpt2라는 이름으로","confidence":0.935,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[702430,702760,"마이너스"],[702760,702980,"1을"],[702980,703180,"넣는"],[703180,703480,"경우는"],[703550,703914,"CPU가"],[703914,704180,"됩니다."],[704790,705060,"입력"],[705090,705320,"길이"],[705320,705567,"제한을"],[705567,706000,"넣었고요."],[706610,706880,"생성"],[706910,707127,"결과"],[707127,707260,"수"],[707260,707540,"제한을"],[707540,708020,"넣었습니다."],[708430,708780,"오른쪽"],[708810,709180,"예시를"],[709180,709520,"보시면"],[709810,710080,"모델"],[710110,710420,"이름에"],[710650,710880,"오픈"],[710890,711140,"AI,"],[711170,711560,"커뮤니티"],[711970,712280,"SLA시"],[712280,713280,"gpt2라는"],[713390,713860,"이름으로"]],"textEdited":"마이너스 1을 넣는 경우는 CPU가 됩니다. 입력 길이 제한을 넣었고요. 생성 결과 수 제한을 넣었습니다. 오른쪽 예시를 보시면 모델 이름에 오픈 AI, 커뮤니티 SLA시 gpt2라는 이름으로"},{"start":714200,"end":727000,"text":"파이프라인 객체를 생성을 했고요. 디바이스에 제로를 넣었기 때문에 0번 GPU에서 실행이 됩니다. 그다음 파이프라고 하는 이 인스턴스에 아이캔 NDS al 데이라고 하는 텍스트를 집어넣고","confidence":0.8353,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[714490,715000,"파이프라인"],[715130,715460,"객체를"],[715460,715634,"생성을"],[715634,715940,"했고요."],[716270,716800,"디바이스에"],[716810,717180,"제로를"],[717250,717580,"넣었기"],[717580,717940,"때문에"],[718350,718700,"0번"],[718790,719280,"GPU에서"],[719450,719707,"실행이"],[719707,720020,"됩니다."],[720990,721260,"그다음"],[721650,722800,"파이프라고"],[722810,723020,"하는"],[723330,723480,"이"],[723530,724160,"인스턴스에"],[724510,724780,"아이캔"],[724780,725027,"NDS"],[725027,725154,"al"],[725154,725940,"데이라고"],[725940,726100,"하는"],[726110,726460,"텍스트를"],[726460,726880,"집어넣고"]],"textEdited":"파이프라인 객체를 생성을 했고요. 디바이스에 제로를 넣었기 때문에 0번 GPU에서 실행이 됩니다. 그다음 파이프라고 하는 이 인스턴스에 아이캔 NDS al 데이라고 하는 텍스트를 집어넣고"},{"start":727000,"end":735800,"text":"트렁케이션 2 넘 리턴 시퀀시스 1이라고 하는 값을 전달하여 결과를 반환합니다. 생성된 결과는 다음과 같습니다.","confidence":0.8637,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[727250,727820,"트렁케이션"],[727830,727980,"2"],[728330,728480,"넘"],[728550,728780,"리턴"],[728790,729280,"시퀀시스"],[729290,729960,"1이라고"],[729970,730200,"하는"],[730610,730920,"값을"],[730920,731380,"전달하여"],[731810,732300,"결과를"],[732510,732980,"반환합니다."],[733810,734180,"생성된"],[734210,734600,"결과는"],[734930,735280,"다음과"],[735280,735660,"같습니다."]],"textEdited":"트렁케이션 2 넘 리턴 시퀀시스 1이라고 하는 값을 전달하여 결과를 반환합니다. 생성된 결과는 다음과 같습니다."},{"start":735800,"end":746700,"text":"다음 예제로는 제로샷 분류를 보겠습니다. 미리 정의된 레이블에 대해 추가 학습 없이 즉시 다중 클래스로 분류될 수 있습니다. 오른쪽 예시에서 보면 캔디데이트 라벨스","confidence":0.9516,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[736370,736620,"다음"],[736650,737080,"예제로는"],[737290,737660,"제로샷"],[737690,737927,"분류를"],[737927,738360,"보겠습니다."],[739030,739260,"미리"],[739310,739620,"정의된"],[739650,739954,"레이블에"],[739954,740160,"대해"],[740590,740840,"추가"],[740910,741160,"학습"],[741160,741400,"없이"],[741850,742160,"즉시"],[742160,742380,"다중"],[742380,742760,"클래스로"],[742770,743080,"분류될"],[743090,743207,"수"],[743207,743540,"있습니다."],[744210,744640,"오른쪽"],[744670,745080,"예시에서"],[745080,745260,"보면"],[745610,746060,"캔디데이트"],[746070,746500,"라벨스"]],"textEdited":"다음 예제로는 제로샷 분류를 보겠습니다. 미리 정의된 레이블에 대해 추가 학습 없이 즉시 다중 클래스로 분류될 수 있습니다. 오른쪽 예시에서 보면 캔디데이트 라벨스"},{"start":746700,"end":760900,"text":"라고 하는 부분에서 파서브 인파서브이라고 하는 라벨을 전달해 보겠습니다. 입력은 기존과 동일한 아이캔 두디스 올 데이를 넣겠습니다. 여기서 결과를 받는 리졸트를 확인을 해보면 모델이 선택할 수 있는 라벨 후보를 입력을 받아서","confidence":0.89,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[747050,747320,"라고"],[747320,747480,"하는"],[747480,747860,"부분에서"],[748170,748600,"파서브"],[748830,749880,"인파서브이라고"],[749880,750060,"하는"],[750110,750480,"라벨을"],[751150,751467,"전달해"],[751467,751940,"보겠습니다."],[752710,753120,"입력은"],[753230,753620,"기존과"],[753620,753920,"동일한"],[754150,754400,"아이캔"],[754400,754680,"두디스"],[754680,754767,"올"],[754767,755300,"데이를"],[755300,755800,"넣겠습니다."],[756110,756420,"여기서"],[756430,756727,"결과를"],[756727,756900,"받는"],[756910,757460,"리졸트를"],[757630,757887,"확인을"],[757887,758220,"해보면"],[758590,758960,"모델이"],[758960,759340,"선택할"],[759340,759420,"수"],[759420,759560,"있는"],[759590,759820,"라벨"],[759870,760200,"후보를"],[760200,760480,"입력을"],[760480,760740,"받아서"]],"textEdited":"라고 하는 부분에서 파서브 인파서브이라고 하는 라벨을 전달해 보겠습니다. 입력은 기존과 동일한 아이캔 두디스 올 데이를 넣겠습니다. 여기서 결과를 받는 리졸트를 확인을 해보면 모델이 선택할 수 있는 라벨 후보를 입력을 받아서"},{"start":760900,"end":771300,"text":"라벨별 스코어를 반환합니다. 결과를 확인해 봤을 땐 파서블이라는 라벨이 선택된 것을 보실 수가 있습니다. 다음은 오토 클래스입니다. 오토 클래스에는 오토 토크나이저,","confidence":0.9172,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[761210,761600,"라벨별"],[761670,762080,"스코어를"],[762350,762780,"반환합니다."],[763330,763740,"결과를"],[763740,763967,"확인해"],[763967,764220,"봤을"],[764230,764380,"땐"],[764730,765380,"파서블이라는"],[765490,765820,"라벨이"],[765820,766140,"선택된"],[766140,766420,"것을"],[766710,766980,"보실"],[766980,767180,"수가"],[767190,767540,"있습니다."],[768030,768360,"다음은"],[768410,768680,"오토"],[768690,769160,"클래스입니다."],[769630,769900,"오토"],[769900,770320,"클래스에는"],[770350,770620,"오토"],[770620,771080,"토크나이저,"]],"textEdited":"라벨별 스코어를 반환합니다. 결과를 확인해 봤을 땐 파서블이라는 라벨이 선택된 것을 보실 수가 있습니다. 다음은 오토 클래스입니다. 오토 클래스에는 오토 토크나이저,"},{"start":771300,"end":786200,"text":"오토 모델이 있습니다. 엘엘엠을 포함한 다양한 트랜스포머 계열 모델의 접근성과 활용성을 크게 향상시키는 핵심 기능이 됩니다. 사전 학습 모델의 아키텍처와 관련된 토크나이저를 자동으로 가져와 로드하는 바로 가기 역할이다 정도로 이해하시면 됩니다.","confidence":0.8866,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[771610,771900,"오토"],[771900,772207,"모델이"],[772207,772560,"있습니다."],[772970,773400,"엘엘엠을"],[773430,773720,"포함한"],[773810,774100,"다양한"],[774130,774600,"트랜스포머"],[774650,774880,"계열"],[775090,775480,"모델의"],[775770,776400,"접근성과"],[776810,777340,"활용성을"],[777390,777600,"크게"],[777610,778120,"향상시키는"],[778230,778520,"핵심"],[778520,778800,"기능이"],[778800,779060,"됩니다."],[779650,779900,"사전"],[779900,780120,"학습"],[780120,780420,"모델의"],[780420,781040,"아키텍처와"],[781050,781340,"관련된"],[781390,782020,"토크나이저를"],[782190,782600,"자동으로"],[782630,782940,"가져와"],[783150,783600,"로드하는"],[783950,784200,"바로"],[784200,784380,"가기"],[784380,784780,"역할이다"],[785150,785500,"정도로"],[785500,785854,"이해하시면"],[785854,786100,"됩니다."]],"textEdited":"오토 모델이 있습니다. 엘엘엠을 포함한 다양한 트랜스포머 계열 모델의 접근성과 활용성을 크게 향상시키는 핵심 기능이 됩니다. 사전 학습 모델의 아키텍처와 관련된 토크나이저를 자동으로 가져와 로드하는 바로 가기 역할이다 정도로 이해하시면 됩니다."},{"start":786200,"end":797900,"text":"대형 모델들을 직접 개발할 필요 없이 손쉽게 로드하고 활용할 수 있게 하여 대규모 데이터 컴퓨팅, 파워 시간 등의 장벽을 극복하고 조직이 맞춤형 엠엘을 채택할 수 있도록 돕습니다.","confidence":0.9267,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[786610,786860,"대형"],[786870,787340,"모델들을"],[787450,787680,"직접"],[787680,788020,"개발할"],[788020,788200,"필요"],[788200,788440,"없이"],[788750,789200,"손쉽게"],[789200,789640,"로드하고"],[789950,790300,"활용할"],[790300,790407,"수"],[790407,790620,"있게"],[790630,790840,"하여"],[791110,791440,"대규모"],[791450,791740,"데이터"],[792130,792460,"컴퓨팅,"],[792490,792740,"파워"],[793270,793500,"시간"],[793530,793740,"등의"],[793770,794080,"장벽을"],[794110,794620,"극복하고"],[794970,795340,"조직이"],[795350,795720,"맞춤형"],[795730,796120,"엠엘을"],[796270,796587,"채택할"],[796587,796660,"수"],[796660,796920,"있도록"],[796970,797780,"돕습니다."]],"textEdited":"대형 모델들을 직접 개발할 필요 없이 손쉽게 로드하고 활용할 수 있게 하여 대규모 데이터 컴퓨팅, 파워 시간 등의 장벽을 극복하고 조직이 맞춤형 엠엘을 채택할 수 있도록 돕습니다."},{"start":797900,"end":811700,"text":"먼저 오토 토크나이저입니다. 텍스트를 모델 입력에 필요한 숫자 배열 형태로 전처리하는 역할을 합니다. 단어를 어디서 끊을지, 어느 수준까지 나눌지와 같은 토크나 규칙들이 포함되어 있습니다. 퍼트 모델에는 월드 피스 토크나이저가,","confidence":0.9307,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[798210,798500,"먼저"],[798510,798780,"오토"],[798780,799420,"토크나이저입니다."],[800110,800580,"텍스트를"],[800610,800860,"모델"],[800870,801180,"입력에"],[801270,801540,"필요한"],[801550,801747,"숫자"],[801747,801940,"배열"],[801940,802260,"형태로"],[802430,802960,"전처리하는"],[802960,803274,"역할을"],[803274,803500,"합니다."],[803950,804340,"단어를"],[804340,804620,"어디서"],[804620,804980,"끊을지,"],[805630,805840,"어느"],[805870,806247,"수준까지"],[806247,806680,"나눌지와"],[806680,806920,"같은"],[807170,807560,"토크나"],[807560,808000,"규칙들이"],[808030,808420,"포함되어"],[808430,808760,"있습니다."],[809310,809660,"퍼트"],[809660,810100,"모델에는"],[810530,810840,"월드"],[810840,811060,"피스"],[811060,811520,"토크나이저가,"]],"textEdited":"먼저 오토 토크나이저입니다. 텍스트를 모델 입력에 필요한 숫자 배열 형태로 전처리하는 역할을 합니다. 단어를 어디서 끊을지, 어느 수준까지 나눌지와 같은 토크나 규칙들이 포함되어 있습니다. 퍼트 모델에는 월드 피스 토크나이저가,"},{"start":811700,"end":823300,"text":"지피티 모델에는 바이트 페어 인코딩 토크나이저가 사용되는 등 모델마다 다른 토크나이저가 활용됩니다. 모델이 사전 훈련된 모델과 동일한 모델 이름으로 토크나이저를 인스턴스화해야 합니다.","confidence":0.918,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[811990,812380,"지피티"],[812380,812760,"모델에는"],[813170,813540,"바이트"],[813540,813740,"페어"],[813750,814160,"인코딩"],[814250,814780,"토크나이저가"],[814780,815120,"사용되는"],[815120,815260,"등"],[815690,816200,"모델마다"],[816450,816640,"다른"],[816710,817240,"토크나이저가"],[817240,817680,"활용됩니다."],[818570,818960,"모델이"],[819010,819240,"사전"],[819240,819540,"훈련된"],[819570,819940,"모델과"],[819970,820280,"동일한"],[820330,820560,"모델"],[820570,820960,"이름으로"],[821430,822100,"토크나이저를"],[822110,822940,"인스턴스화해야"],[822940,823280,"합니다."]],"textEdited":"지피티 모델에는 바이트 페어 인코딩 토크나이저가 사용되는 등 모델마다 다른 토크나이저가 활용됩니다. 모델이 사전 훈련된 모델과 동일한 모델 이름으로 토크나이저를 인스턴스화해야 합니다."},{"start":823300,"end":831500,"text":"다음은 오토 모델입니다. 사전 훈련된 인스턴스를 간단하고 통합된 방법으로 로드합니다. 특정 과업에 맞는 알맞은 모델을 선택합니다.","confidence":0.9274,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[823650,823980,"다음은"],[824010,824260,"오토"],[824260,824700,"모델입니다."],[825150,825420,"사전"],[825420,825680,"훈련된"],[825710,826260,"인스턴스를"],[826370,826860,"간단하고"],[827150,827540,"통합된"],[827650,828000,"방법으로"],[828010,828460,"로드합니다."],[829190,829520,"특정"],[829530,829800,"과업에"],[829800,830020,"맞는"],[830170,830520,"알맞은"],[830530,830840,"모델을"],[830840,831320,"선택합니다."]],"textEdited":"다음은 오토 모델입니다. 사전 훈련된 인스턴스를 간단하고 통합된 방법으로 로드합니다. 특정 과업에 맞는 알맞은 모델을 선택합니다."},{"start":831500,"end":842900,"text":"예를 들어 텍스트 또는 시퀀스 분류를 위해서는 오토 모델 폴 시퀀스 커시피케이션을 로드해야 합니다. 오토 토크나이저 예제를 보겠습니다. 오른쪽 예시에서","confidence":0.9531,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[831750,832040,"예를"],[832040,832220,"들어"],[832730,833120,"텍스트"],[833230,833460,"또는"],[833550,834000,"시퀀스"],[834010,834340,"분류를"],[834390,834860,"위해서는"],[835630,835940,"오토"],[835940,836140,"모델"],[836190,836340,"폴"],[836390,836820,"시퀀스"],[836950,837780,"커시피케이션을"],[838030,838440,"로드해야"],[838440,838680,"합니다."],[839370,839620,"오토"],[839620,840060,"토크나이저"],[840070,840360,"예제를"],[840360,840860,"보겠습니다."],[841390,841840,"오른쪽"],[842210,842680,"예시에서"]],"textEdited":"예를 들어 텍스트 또는 시퀀스 분류를 위해서는 오토 모델 폴 시퀀스 커시피케이션을 로드해야 합니다. 오토 토크나이저 예제를 보겠습니다. 오른쪽 예시에서"},{"start":842900,"end":855900,"text":"오토 토크나이저, 프럼 프리트레인드에서 구글 버트 버트 베이스 언 케이스라고 하는 사전 학습 버츠 모델의 토크나이저를 로드를 합니다. 문장을 월드 피스 기반으로 토크화를 하고","confidence":0.846,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[843350,843620,"오토"],[843620,844060,"토크나이저,"],[844750,844940,"프럼"],[845210,846200,"프리트레인드에서"],[846690,846920,"구글"],[846970,847360,"버트"],[847870,848200,"버트"],[848200,848500,"베이스"],[848550,848700,"언"],[848790,849680,"케이스라고"],[849680,849880,"하는"],[850310,850580,"사전"],[850580,850820,"학습"],[850850,851140,"버츠"],[851140,851460,"모델의"],[851460,851987,"토크나이저를"],[851987,852227,"로드를"],[852227,852460,"합니다."],[853230,853680,"문장을"],[853830,854140,"월드"],[854140,854460,"피스"],[854490,854920,"기반으로"],[854990,855420,"토크화를"],[855420,855620,"하고"]],"textEdited":"오토 토크나이저, 프럼 프리트레인드에서 구글 버트 버트 베이스 언 케이스라고 하는 사전 학습 버츠 모델의 토크나이저를 로드를 합니다. 문장을 월드 피스 기반으로 토크화를 하고"},{"start":855900,"end":868100,"text":"각 토큰을 정수 인덱스로 매핑합니다. 매핑된 결과는 다음과 같습니다. 인풋 아이디스 각 토큰에 대한 정수 아이디 리스트입니다. 토큰 타입 아이디스, 문장 구분 아이디입니다.","confidence":0.9651,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[856410,856560,"각"],[856670,857060,"토큰을"],[857230,857560,"정수"],[857560,857960,"인덱스로"],[857960,858420,"매핑합니다."],[859270,859760,"매핑된"],[859830,860260,"결과는"],[860410,860760,"다음과"],[860760,861100,"같습니다."],[861490,861760,"인풋"],[861810,862240,"아이디스"],[862630,862780,"각"],[862890,863194,"토큰에"],[863194,863380,"대한"],[863530,863820,"정수"],[863820,864080,"아이디"],[864110,864640,"리스트입니다."],[865310,865600,"토큰"],[865650,865860,"타입"],[865870,866300,"아이디스,"],[866750,867120,"문장"],[867190,867400,"구분"],[867470,868040,"아이디입니다."]],"textEdited":"각 토큰을 정수 인덱스로 매핑합니다. 매핑된 결과는 다음과 같습니다. 인풋 아이디스 각 토큰에 대한 정수 아이디 리스트입니다. 토큰 타입 아이디스, 문장 구분 아이디입니다."},{"start":868100,"end":879100,"text":"단일 문장에서는 모두 0으로 들어갑니다. 어텐션 마스크 각 토큰이 실제 입력인지 혹은 패딩인지를 표시를 합니다. 그 결과가 오른쪽에서 보여지고 있습니다.","confidence":0.964,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[868630,868880,"단일"],[868970,869480,"문장에서는"],[869490,869720,"모두"],[869720,870040,"0으로"],[870040,870420,"들어갑니다."],[871150,871580,"어텐션"],[871610,871960,"마스크"],[872770,872920,"각"],[873070,873460,"토큰이"],[873510,873800,"실제"],[873810,874320,"입력인지"],[874630,874860,"혹은"],[874990,875520,"패딩인지를"],[875520,875720,"표시를"],[875720,875960,"합니다."],[876730,876880,"그"],[876910,877280,"결과가"],[877510,878020,"오른쪽에서"],[878170,878527,"보여지고"],[878527,878960,"있습니다."]],"textEdited":"단일 문장에서는 모두 0으로 들어갑니다. 어텐션 마스크 각 토큰이 실제 입력인지 혹은 패딩인지를 표시를 합니다. 그 결과가 오른쪽에서 보여지고 있습니다."},{"start":879100,"end":892100,"text":"다음은 오토 모델 예제입니다. 오른쪽 예시에서 보시면 모델 이름을 디스티벌트 디스티버트 베이스 언 케이스트라는 모델을 사용합니다. 이전에 설명드렸던 것과 동일하게 오토 토크나이저와","confidence":0.904,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[879710,880040,"다음은"],[880090,880320,"오토"],[880320,880520,"모델"],[880530,880960,"예제입니다."],[881550,881940,"오른쪽"],[881970,882380,"예시에서"],[882390,882720,"보시면"],[883050,883340,"모델"],[883370,883660,"이름을"],[883950,884660,"디스티벌트"],[885090,885740,"디스티버트"],[885750,886060,"베이스"],[886110,886260,"언"],[886260,887580,"케이스트라는"],[887610,887960,"모델을"],[887960,888340,"사용합니다."],[888950,889320,"이전에"],[889350,889840,"설명드렸던"],[889840,890040,"것과"],[890090,890540,"동일하게"],[891090,891380,"오토"],[891380,891920,"토크나이저와"]],"textEdited":"다음은 오토 모델 예제입니다. 오른쪽 예시에서 보시면 모델 이름을 디스티벌트 디스티버트 베이스 언 케이스트라는 모델을 사용합니다. 이전에 설명드렸던 것과 동일하게 오토 토크나이저와"},{"start":892100,"end":904400,"text":"오토 모델에는 동일한 모델 이름을 전달해야 됩니다. 그래서 예시에서도 프럼 프리 트레인트라는 메서드를 호출할 때 동일한 모델 이름을 전달하는 것을 보실 수가 있습니다.","confidence":0.9286,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[892630,892960,"오토"],[892960,893420,"모델에는"],[893890,894260,"동일한"],[894350,894620,"모델"],[894770,895100,"이름을"],[895230,895680,"전달해야"],[895680,895940,"됩니다."],[896590,896840,"그래서"],[896870,897440,"예시에서도"],[897950,898180,"프럼"],[898230,898387,"프리"],[898387,899580,"트레인트라는"],[899810,900260,"메서드를"],[900260,900620,"호출할"],[900630,900780,"때"],[901190,901540,"동일한"],[901610,901860,"모델"],[901870,902160,"이름을"],[902430,902880,"전달하는"],[902880,903120,"것을"],[903130,903400,"보실"],[903400,903600,"수가"],[903600,904060,"있습니다."]],"textEdited":"오토 모델에는 동일한 모델 이름을 전달해야 됩니다. 그래서 예시에서도 프럼 프리 트레인트라는 메서드를 호출할 때 동일한 모델 이름을 전달하는 것을 보실 수가 있습니다."},{"start":904400,"end":912600,"text":"다음으로는 이제 입력 텍스트를 토크나이저 방금 생성한 토크나이저에 전달을 해서 인풋 벡터를 만들어 냅니다.","confidence":0.9597,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[904690,905260,"다음으로는"],[905770,905940,"이제"],[906070,906380,"입력"],[906570,907080,"텍스트를"],[907690,908220,"토크나이저"],[908610,908840,"방금"],[908890,909200,"생성한"],[909290,909900,"토크나이저에"],[909910,910214,"전달을"],[910214,910420,"해서"],[910990,911300,"인풋"],[911450,911800,"벡터를"],[911810,912047,"만들어"],[912047,912440,"냅니다."]],"textEdited":"다음으로는 이제 입력 텍스트를 토크나이저 방금 생성한 토크나이저에 전달을 해서 인풋 벡터를 만들어 냅니다."},{"start":912600,"end":922000,"text":"인풋 벡터를 모델에 전달하는 것으로 그 모델의 로지 값을 만들어 낼 수 있고, 소프트맥스를 통해서 결과 확률을 얻어낼 수 있습니다.","confidence":0.9621,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[912930,913200,"인풋"],[913250,913620,"벡터를"],[913910,914280,"모델에"],[914290,914720,"전달하는"],[914750,915120,"것으로"],[915570,915720,"그"],[915730,916100,"모델의"],[916190,916480,"로지"],[916510,916840,"값을"],[917170,917394,"만들어"],[917394,917520,"낼"],[917520,917607,"수"],[917607,917900,"있고,"],[918330,919020,"소프트맥스를"],[919020,919320,"통해서"],[919770,920140,"결과"],[920210,920560,"확률을"],[920990,921280,"얻어낼"],[921310,921460,"수"],[921460,921900,"있습니다."]],"textEdited":"인풋 벡터를 모델에 전달하는 것으로 그 모델의 로지 값을 만들어 낼 수 있고, 소프트맥스를 통해서 결과 확률을 얻어낼 수 있습니다."},{"start":922000,"end":934200,"text":"여기에서 사용하는 디스트리버트 디스트리버트 베이스 언 케이스드는 특정 목적의 태스크로 파인튜닝 되어 있지 않은 모델이기 때문에 사실 여기에서 출력되는 로지이나 예측 확률은 의미 없는 값입니다.","confidence":0.9032,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[922490,922900,"여기에서"],[922910,923360,"사용하는"],[923630,924280,"디스트리버트"],[924450,925100,"디스트리버트"],[925150,925540,"베이스"],[925670,925820,"언"],[925870,926380,"케이스드는"],[926910,927220,"특정"],[927270,927580,"목적의"],[927590,928020,"태스크로"],[928190,928627,"파인튜닝"],[928627,928800,"되어"],[928800,929000,"있지"],[929050,929240,"않은"],[929350,929760,"모델이기"],[929760,930140,"때문에"],[930470,930720,"사실"],[930750,931080,"여기에서"],[931110,931460,"출력되는"],[931510,931960,"로지이나"],[932170,932440,"예측"],[932490,932800,"확률은"],[932930,933200,"의미"],[933210,933420,"없는"],[933490,934040,"값입니다."]],"textEdited":"여기에서 사용하는 디스트리버트 디스트리버트 베이스 언 케이스드는 특정 목적의 태스크로 파인튜닝 되어 있지 않은 모델이기 때문에 사실 여기에서 출력되는 로지이나 예측 확률은 의미 없는 값입니다."},{"start":934200,"end":945800,"text":"그래서 여러분들이 실제로 사용하실 때는 추가적인 파인 튜닝이 반드시 필요하다는 점 염두에 두시면 좋겠습니다. 다음은 트레이너입니다. 트레이너는 모델 훈련을 최적화하는 클래스입니다.","confidence":0.9398,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[934450,934720,"그래서"],[934830,935320,"여러분들이"],[935370,935720,"실제로"],[935720,936100,"사용하실"],[936100,936320,"때는"],[936730,937140,"추가적인"],[937190,937440,"파인"],[937440,937840,"튜닝이"],[938090,938480,"반드시"],[938950,939480,"필요하다는"],[939490,939640,"점"],[939970,940254,"염두에"],[940254,940520,"두시면"],[940520,940980,"좋겠습니다."],[941790,942120,"다음은"],[942170,942800,"트레이너입니다."],[943190,943680,"트레이너는"],[943830,944100,"모델"],[944130,944500,"훈련을"],[944570,945060,"최적화하는"],[945130,945660,"클래스입니다."]],"textEdited":"그래서 여러분들이 실제로 사용하실 때는 추가적인 파인 튜닝이 반드시 필요하다는 점 염두에 두시면 좋겠습니다. 다음은 트레이너입니다. 트레이너는 모델 훈련을 최적화하는 클래스입니다."},{"start":945800,"end":958900,"text":"훈련 루프를 효율적으로 관리하고 대규모 언어 모델을 파인튜닝 하는 데 필수적인 기능을 제공합니다. 분산 학습과 같은 고급 기능을 추가로 제공하여 대규모 모델 훈련 시 자원 효율성을 높여줍니다. 손실 함수 옵티마이저,","confidence":0.9503,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[946130,946380,"훈련"],[946380,946740,"루프를"],[946770,947200,"효율적으로"],[947210,947640,"관리하고"],[947910,948280,"대규모"],[948330,948520,"언어"],[948520,948820,"모델을"],[948870,949287,"파인튜닝"],[949287,949447,"하는"],[949447,949580,"데"],[949670,950080,"필수적인"],[950110,950400,"기능을"],[950430,950880,"제공합니다."],[951390,951620,"분산"],[951630,951980,"학습과"],[951990,952240,"같은"],[952550,952800,"고급"],[952800,953060,"기능을"],[953110,953380,"추가로"],[953410,953820,"제공하여"],[954370,954760,"대규모"],[954790,955020,"모델"],[955020,955220,"훈련"],[955220,955360,"시"],[955590,955840,"자원"],[955870,956227,"효율성을"],[956227,956680,"높여줍니다."],[957210,957540,"손실"],[957540,957800,"함수"],[958210,958780,"옵티마이저,"]],"textEdited":"훈련 루프를 효율적으로 관리하고 대규모 언어 모델을 파인튜닝 하는 데 필수적인 기능을 제공합니다. 분산 학습과 같은 고급 기능을 추가로 제공하여 대규모 모델 훈련 시 자원 효율성을 높여줍니다. 손실 함수 옵티마이저,"},{"start":958900,"end":971400,"text":"스케줄러와 같은 훈련 루프의 기능을 변경 가능합니다. 콜백을 사용하여 훈련 루프를 바꾸지 않으면서도 다른 라이브러리와 통합하거나 훈련 진행 상황을 보고받거나 조기에 훈련을 중단하는 것이 가능합니다.","confidence":0.9483,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[959270,959780,"스케줄러와"],[959790,960020,"같은"],[960110,960300,"훈련"],[960300,960600,"루프의"],[960600,960900,"기능을"],[961010,961300,"변경"],[961350,961780,"가능합니다."],[962430,962880,"콜백을"],[962880,963220,"사용하여"],[963410,963640,"훈련"],[963640,963980,"루프를"],[964030,964400,"바꾸지"],[964450,965000,"않으면서도"],[965250,965480,"다른"],[965530,966020,"라이브러리와"],[966170,966760,"통합하거나"],[967290,967540,"훈련"],[967650,967840,"진행"],[967840,968080,"상황을"],[968080,968580,"보고받거나"],[969170,969540,"조기에"],[969570,969860,"훈련을"],[969890,970300,"중단하는"],[970300,970540,"것이"],[970810,971280,"가능합니다."]],"textEdited":"스케줄러와 같은 훈련 루프의 기능을 변경 가능합니다. 콜백을 사용하여 훈련 루프를 바꾸지 않으면서도 다른 라이브러리와 통합하거나 훈련 진행 상황을 보고받거나 조기에 훈련을 중단하는 것이 가능합니다."},{"start":971400,"end":984500,"text":"다음으로는 트레이너 예제에 대해서 보겠습니다. 이전에 배웠던 것과 같이 오토 토크나이저, 오토 모델을 통해서 우리가 사용할 모델을 불러옵니다. 멀티 베이스 케이스 모델의 텍스트 처리 규칙을 따르는 토크나이저를 로드합니다.","confidence":0.9345,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[972050,972460,"다음으로는"],[972460,972760,"트레이너"],[972760,973027,"예제에"],[973027,973260,"대해서"],[973260,973760,"보겠습니다."],[974410,974740,"이전에"],[974740,975060,"배웠던"],[975060,975260,"것과"],[975260,975500,"같이"],[975930,976180,"오토"],[976180,976580,"토크나이저,"],[976990,977280,"오토"],[977280,977620,"모델을"],[977620,977960,"통해서"],[978390,978660,"우리가"],[978710,979000,"사용할"],[979230,979580,"모델을"],[979630,980080,"불러옵니다."],[980590,980887,"멀티"],[980887,981140,"베이스"],[981150,981440,"케이스"],[981490,981840,"모델의"],[981990,982320,"텍스트"],[982320,982467,"처리"],[982467,982727,"규칙을"],[982727,983020,"따르는"],[983330,983940,"토크나이저를"],[983940,984400,"로드합니다."]],"textEdited":"다음으로는 트레이너 예제에 대해서 보겠습니다. 이전에 배웠던 것과 같이 오토 토크나이저, 오토 모델을 통해서 우리가 사용할 모델을 불러옵니다. 멀티 베이스 케이스 모델의 텍스트 처리 규칙을 따르는 토크나이저를 로드합니다."},{"start":984500,"end":997800,"text":"이 토크나이저는 자연어를 모델이 이해하는 숫자 형태로 변환하는 역할을 합니다. 다음과 같이 벌트 베이스 케이스 모델의 아키텍처와 대규모 데이터셋으로 학습된 가중치를 가진 모델 인스턴스를 가져올 수 있습니다.","confidence":0.9656,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[984850,985000,"이"],[985070,985680,"토크나이저는"],[986270,986760,"자연어를"],[986910,987280,"모델이"],[987280,987640,"이해하는"],[987730,987927,"숫자"],[987927,988167,"형태로"],[988167,988480,"변환하는"],[988480,988860,"역할을"],[988870,989140,"합니다."],[989730,990100,"다음과"],[990100,990360,"같이"],[990630,990960,"벌트"],[990960,991240,"베이스"],[991250,991560,"케이스"],[991570,991960,"모델의"],[992490,993120,"아키텍처와"],[993390,993740,"대규모"],[993770,994380,"데이터셋으로"],[994390,994720,"학습된"],[994830,995320,"가중치를"],[995320,995520,"가진"],[995810,996060,"모델"],[996090,996660,"인스턴스를"],[996950,997280,"가져올"],[997280,997387,"수"],[997387,997720,"있습니다."]],"textEdited":"이 토크나이저는 자연어를 모델이 이해하는 숫자 형태로 변환하는 역할을 합니다. 다음과 같이 벌트 베이스 케이스 모델의 아키텍처와 대규모 데이터셋으로 학습된 가중치를 가진 모델 인스턴스를 가져올 수 있습니다."},{"start":997800,"end":1011100,"text":"이는 오른쪽 예제의 맨 밑에 해당합니다. 데이터 셋은 1점부터 5점까지의 리뷰 평점을 나타내는 데이터이고요. 오토 토크나이저를 사용해서 텍스트를 토큰화합니다. 옵션을 살펴보면","confidence":0.9504,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[998110,998400,"이는"],[999190,999560,"오른쪽"],[999590,999920,"예제의"],[1000210,1000360,"맨"],[1000410,1000700,"밑에"],[1000810,1001280,"해당합니다."],[1001750,1002080,"데이터"],[1002080,1002340,"셋은"],[1002670,1003300,"1점부터"],[1003350,1004000,"5점까지의"],[1004000,1004220,"리뷰"],[1004250,1004620,"평점을"],[1004770,1005220,"나타내는"],[1005230,1005820,"데이터이고요."],[1006510,1006800,"오토"],[1006810,1007440,"토크나이저를"],[1007440,1007780,"사용해서"],[1008310,1008760,"텍스트를"],[1008790,1009360,"토큰화합니다."],[1010070,1010460,"옵션을"],[1010470,1010900,"살펴보면"]],"textEdited":"이는 오른쪽 예제의 맨 밑에 해당합니다. 데이터 셋은 1점부터 5점까지의 리뷰 평점을 나타내는 데이터이고요. 오토 토크나이저를 사용해서 텍스트를 토큰화합니다. 옵션을 살펴보면"},{"start":1011100,"end":1025200,"text":"트렁케이션 트루 텍스트가 모델의 최대 입력 길이를 초과하는 경우 자동으로 자르게 되어 있고요. 배치 트루 여러 샘플을 한 번에 처리해서 속도를 향상시킬 수 있습니다. 분리된 트레인 테스트 셋을 다음과 같이 구성합니다.","confidence":0.9447,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1011310,1011920,"트렁케이션"],[1012130,1012320,"트루"],[1012910,1013340,"텍스트가"],[1013410,1013760,"모델의"],[1013790,1014040,"최대"],[1014070,1014300,"입력"],[1014330,1014620,"길이를"],[1014650,1015060,"초과하는"],[1015060,1015280,"경우"],[1015670,1016060,"자동으로"],[1016070,1016347,"자르게"],[1016347,1016520,"되어"],[1016520,1016780,"있고요."],[1017090,1017400,"배치"],[1017470,1017660,"트루"],[1018130,1018360,"여러"],[1018390,1018740,"샘플을"],[1018740,1018814,"한"],[1018814,1019040,"번에"],[1019040,1019420,"처리해서"],[1019950,1020380,"속도를"],[1020510,1021000,"향상시킬"],[1021000,1021094,"수"],[1021094,1021420,"있습니다."],[1022110,1022440,"분리된"],[1022650,1022940,"트레인"],[1023410,1023707,"테스트"],[1023707,1024000,"셋을"],[1024130,1024460,"다음과"],[1024460,1024660,"같이"],[1024690,1025140,"구성합니다."]],"textEdited":"트렁케이션 트루 텍스트가 모델의 최대 입력 길이를 초과하는 경우 자동으로 자르게 되어 있고요. 배치 트루 여러 샘플을 한 번에 처리해서 속도를 향상시킬 수 있습니다. 분리된 트레인 테스트 셋을 다음과 같이 구성합니다."},{"start":1025200,"end":1037100,"text":"정확도를 평가 매트릭으로 설정을 하고요. 로지 값이 높을수록 모델이 해당 클래스라고 강력하게 예측하고 있음을 의미합니다. 로짓은 음수이거나 양수일 수 있고 합이 1이 아닐 수 있습니다.","confidence":0.9582,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1025790,1026340,"정확도를"],[1026370,1026620,"평가"],[1026650,1027060,"매트릭으로"],[1027060,1027340,"설정을"],[1027350,1027660,"하고요."],[1028330,1028640,"로지"],[1028650,1028874,"값이"],[1028874,1029320,"높을수록"],[1029550,1029940,"모델이"],[1030090,1030340,"해당"],[1030430,1030920,"클래스라고"],[1030970,1031420,"강력하게"],[1031420,1031800,"예측하고"],[1031800,1032040,"있음을"],[1032050,1032460,"의미합니다."],[1032890,1033280,"로짓은"],[1033470,1034080,"음수이거나"],[1034170,1034507,"양수일"],[1034507,1034607,"수"],[1034607,1034860,"있고"],[1035190,1035520,"합이"],[1035730,1036040,"1이"],[1036040,1036220,"아닐"],[1036270,1036387,"수"],[1036387,1036860,"있습니다."]],"textEdited":"정확도를 평가 매트릭으로 설정을 하고요. 로지 값이 높을수록 모델이 해당 클래스라고 강력하게 예측하고 있음을 의미합니다. 로짓은 음수이거나 양수일 수 있고 합이 1이 아닐 수 있습니다."},{"start":1037100,"end":1050600,"text":"매트릭은 로짓을 토대로 추정 라벨과 실제 라벨의 일치 정확도로 설정합니다. 이는 오른쪽에 있는 컴퓨터 매트릭스라고 하는 함수로 정의할 수 있습니다. 그다음 트레이닝 아규먼츠의","confidence":0.9367,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1037390,1037840,"매트릭은"],[1038190,1038560,"로짓을"],[1038560,1038840,"토대로"],[1039050,1039320,"추정"],[1039330,1039720,"라벨과"],[1040090,1040380,"실제"],[1040390,1040720,"라벨의"],[1040810,1041080,"일치"],[1041080,1041540,"정확도로"],[1041870,1042400,"설정합니다."],[1042910,1043180,"이는"],[1043550,1044080,"오른쪽에"],[1044080,1044240,"있는"],[1044590,1044920,"컴퓨터"],[1044920,1045840,"매트릭스라고"],[1045840,1046060,"하는"],[1046470,1046860,"함수로"],[1047230,1047580,"정의할"],[1047610,1047760,"수"],[1047870,1048220,"있습니다."],[1048770,1049080,"그다음"],[1049430,1049760,"트레이닝"],[1049790,1050360,"아규먼츠의"]],"textEdited":"매트릭은 로짓을 토대로 추정 라벨과 실제 라벨의 일치 정확도로 설정합니다. 이는 오른쪽에 있는 컴퓨터 매트릭스라고 하는 함수로 정의할 수 있습니다. 그다음 트레이닝 아규먼츠의"},{"start":1050600,"end":1061500,"text":"모델을 저장할 출력 경로 그리고 평가 주기 등을 설정합니다. 트레이너 객체는 모델 데이터셋 매트릭 등을 받아 학습과 평가 루프를 자동화합니다.","confidence":0.9108,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1050930,1051380,"모델을"],[1051410,1051800,"저장할"],[1052010,1052280,"출력"],[1052370,1052660,"경로"],[1052890,1053200,"그리고"],[1053490,1053800,"평가"],[1053890,1054140,"주기"],[1054150,1054380,"등을"],[1054670,1055200,"설정합니다."],[1055770,1056120,"트레이너"],[1056130,1056500,"객체는"],[1056710,1056980,"모델"],[1057530,1058000,"데이터셋"],[1058350,1058660,"매트릭"],[1058670,1058900,"등을"],[1058900,1059100,"받아"],[1059490,1059900,"학습과"],[1060070,1060360,"평가"],[1060360,1060720,"루프를"],[1060790,1061320,"자동화합니다."]],"textEdited":"모델을 저장할 출력 경로 그리고 평가 주기 등을 설정합니다. 트레이너 객체는 모델 데이터셋 매트릭 등을 받아 학습과 평가 루프를 자동화합니다."},{"start":1061500,"end":1076500,"text":"트레인 메소드 호출로 지정한 설정에 따라 파인튜닝을 수행하며 평가 및 체크 포인트를 저장하게 됩니다. 의미 문장 아캔두디스 올데이라는 텍스트를 추정하도록 합니다. 입력 값을 토크나이징하고 버드 모드에 전달합니다.","confidence":0.8911,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1061750,1062060,"트레인"],[1062070,1062360,"메소드"],[1062360,1062720,"호출로"],[1062990,1063300,"지정한"],[1063310,1063574,"설정에"],[1063574,1063800,"따라"],[1064130,1064700,"파인튜닝을"],[1064710,1065100,"수행하며"],[1065570,1065860,"평가"],[1065860,1065980,"및"],[1066290,1066560,"체크"],[1066560,1067000,"포인트를"],[1067230,1067647,"저장하게"],[1067647,1067900,"됩니다."],[1068450,1068700,"의미"],[1068730,1068980,"문장"],[1069250,1069740,"아캔두디스"],[1069740,1070660,"올데이라는"],[1071130,1071580,"텍스트를"],[1071950,1072460,"추정하도록"],[1072710,1073000,"합니다."],[1073810,1074080,"입력"],[1074080,1074340,"값을"],[1074430,1075140,"토크나이징하고"],[1075390,1075634,"버드"],[1075634,1075920,"모드에"],[1075920,1076420,"전달합니다."]],"textEdited":"트레인 메소드 호출로 지정한 설정에 따라 파인튜닝을 수행하며 평가 및 체크 포인트를 저장하게 됩니다. 의미 문장 아캔두디스 올데이라는 텍스트를 추정하도록 합니다. 입력 값을 토크나이징하고 버드 모드에 전달합니다."},{"start":1076500,"end":1091000,"text":"예측된 리뷰 점수를 출력합니다. 이번 강의를 요약해 보겠습니다. 먼저 우리는 사전 학습 모델의 중요성에 대해서 배웠습니다. 미리 학습된 모델의 지식을 새로운 과제에 맞게 재활용하는 방식인 전이 학습에 대해서 이야기했습니다.","confidence":0.9583,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1076990,1077460,"예측된"],[1077530,1077740,"리뷰"],[1077750,1078120,"점수를"],[1078390,1078900,"출력합니다."],[1079850,1080080,"이번"],[1080170,1080387,"강의를"],[1080387,1080627,"요약해"],[1080627,1081080,"보겠습니다."],[1081630,1081900,"먼저"],[1081910,1082200,"우리는"],[1082670,1082920,"사전"],[1082920,1083160,"학습"],[1083160,1083480,"모델의"],[1083490,1083934,"중요성에"],[1083934,1084220,"대해서"],[1084220,1084660,"배웠습니다."],[1085230,1085480,"미리"],[1085530,1085840,"학습된"],[1085870,1086220,"모델의"],[1086270,1086600,"지식을"],[1086830,1087100,"새로운"],[1087150,1087440,"과제에"],[1087440,1087640,"맞게"],[1087690,1088200,"재활용하는"],[1088270,1088620,"방식인"],[1089030,1089320,"전이"],[1089330,1089627,"학습에"],[1089627,1089920,"대해서"],[1090110,1090700,"이야기했습니다."]],"textEdited":"예측된 리뷰 점수를 출력합니다. 이번 강의를 요약해 보겠습니다. 먼저 우리는 사전 학습 모델의 중요성에 대해서 배웠습니다. 미리 학습된 모델의 지식을 새로운 과제에 맞게 재활용하는 방식인 전이 학습에 대해서 이야기했습니다."},{"start":1091000,"end":1103000,"text":"전이 학습은 높은 성능, 적은 데이터 자원으로도 빠른 학습 데이터 부족 문제를 완화시킵니다. 또한 정적 인베딩의 한계에 대해서도 이야기를 했습니다. 다이어와 희귀어 문제","confidence":0.9654,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1091330,1091620,"전이"],[1091630,1091940,"학습은"],[1092230,1092480,"높은"],[1092570,1092820,"성능,"],[1093290,1093520,"적은"],[1093570,1093840,"데이터"],[1093840,1094300,"자원으로도"],[1094610,1094820,"빠른"],[1094950,1095240,"학습"],[1095670,1096020,"데이터"],[1096070,1096320,"부족"],[1096350,1096680,"문제를"],[1096790,1097400,"완화시킵니다."],[1098030,1098240,"또한"],[1098630,1099000,"정적"],[1099030,1099440,"인베딩의"],[1099490,1099747,"한계에"],[1099747,1100160,"대해서도"],[1100390,1100654,"이야기를"],[1100654,1100960,"했습니다."],[1101470,1102000,"다이어와"],[1102310,1102620,"희귀어"],[1102620,1102840,"문제"]],"textEdited":"전이 학습은 높은 성능, 적은 데이터 자원으로도 빠른 학습 데이터 부족 문제를 완화시킵니다. 또한 정적 인베딩의 한계에 대해서도 이야기를 했습니다. 다이어와 희귀어 문제"},{"start":1103000,"end":1115800,"text":"문맥을 잘 반영하지 못하는 부분에 대해서도 이야기를 했었고요. 이를 극복하기 위해 동적 인베딩이 필요하다라고 이야기했습니다. 다음으로는 대표적인 사전 학습 모델인 지피티 종류와 벌 모델 종류에 대해서 이야기해 봤습니다.","confidence":0.9019,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1103450,1103820,"문맥을"],[1103820,1103960,"잘"],[1104070,1104460,"반영하지"],[1104460,1104760,"못하는"],[1104760,1105027,"부분에"],[1105027,1105420,"대해서도"],[1105450,1105694,"이야기를"],[1105694,1106120,"했었고요."],[1106610,1106840,"이를"],[1106950,1107400,"극복하기"],[1107400,1107580,"위해"],[1107870,1108200,"동적"],[1108210,1108640,"인베딩이"],[1108650,1109680,"필요하다라고"],[1109710,1110220,"이야기했습니다."],[1110750,1111280,"다음으로는"],[1111330,1111700,"대표적인"],[1111710,1111907,"사전"],[1111907,1112087,"학습"],[1112087,1112380,"모델인"],[1112690,1113100,"지피티"],[1113110,1113420,"종류와"],[1113710,1113860,"벌"],[1113970,1114220,"모델"],[1114390,1114687,"종류에"],[1114687,1114980,"대해서"],[1115050,1115320,"이야기해"],[1115320,1115660,"봤습니다."]],"textEdited":"문맥을 잘 반영하지 못하는 부분에 대해서도 이야기를 했었고요. 이를 극복하기 위해 동적 인베딩이 필요하다라고 이야기했습니다. 다음으로는 대표적인 사전 학습 모델인 지피티 종류와 벌 모델 종류에 대해서 이야기해 봤습니다."},{"start":1115800,"end":1129400,"text":"GPT는 트랜스포머의 디코더를 기반으로 구성되어 있으며 생성 능력에 특화되어 있습니다. 단순히 프롬프트를 제공하는 것으로 퓨샷, 제로샷 학습이 가능합니다. FT는 인코더 기반 모델로","confidence":0.8938,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1116330,1116820,"GPT는"],[1117250,1117840,"트랜스포머의"],[1117870,1118400,"디코더를"],[1118400,1118800,"기반으로"],[1118910,1119280,"구성되어"],[1119290,1119580,"있으며"],[1120030,1120300,"생성"],[1120310,1120600,"능력에"],[1120630,1120967,"특화되어"],[1120967,1121260,"있습니다."],[1122070,1122360,"단순히"],[1122360,1122880,"프롬프트를"],[1122950,1123360,"제공하는"],[1123360,1123640,"것으로"],[1123790,1124100,"퓨샷,"],[1124670,1125020,"제로샷"],[1125130,1125460,"학습이"],[1125570,1126020,"가능합니다."],[1126690,1127120,"FT는"],[1127930,1128300,"인코더"],[1128310,1128540,"기반"],[1128650,1129040,"모델로"]],"textEdited":"GPT는 트랜스포머의 디코더를 기반으로 구성되어 있으며 생성 능력에 특화되어 있습니다. 단순히 프롬프트를 제공하는 것으로 퓨샷, 제로샷 학습이 가능합니다. FT는 인코더 기반 모델로"},{"start":1129400,"end":1143300,"text":"문맥 이해에 강점이 있으며 MLM, NSP 기반 방식으로 모델을 학습합니다. 마지막으로는 허깅페이스 트랜스포머스 라이브러리에 대해서 이야기를 했는데요. 다양한 트랜스포머 기반 모델을 표준화된 방식으로 쉽게 불러오고","confidence":0.9639,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1129670,1130020,"문맥"],[1130110,1130420,"이해에"],[1130510,1130847,"강점이"],[1130847,1131140,"있으며"],[1131710,1132020,"MLM,"],[1132310,1132760,"NSP"],[1132910,1133140,"기반"],[1133290,1133740,"방식으로"],[1133950,1134300,"모델을"],[1134300,1134680,"학습합니다."],[1135330,1135960,"마지막으로는"],[1136210,1136760,"허깅페이스"],[1136770,1137360,"트랜스포머스"],[1137360,1137734,"라이브러리에"],[1137734,1137980,"대해서"],[1137990,1138214,"이야기를"],[1138214,1138560,"했는데요."],[1139150,1139520,"다양한"],[1139730,1140200,"트랜스포머"],[1140210,1140420,"기반"],[1140490,1140820,"모델을"],[1141170,1141580,"표준화된"],[1141650,1142100,"방식으로"],[1142350,1142620,"쉽게"],[1142690,1143140,"불러오고"]],"textEdited":"문맥 이해에 강점이 있으며 MLM, NSP 기반 방식으로 모델을 학습합니다. 마지막으로는 허깅페이스 트랜스포머스 라이브러리에 대해서 이야기를 했는데요. 다양한 트랜스포머 기반 모델을 표준화된 방식으로 쉽게 불러오고"},{"start":1143300,"end":1156900,"text":"파인 튜닝이 가능하도록 도와주는 라이브러리였습니다. 그중 파이프라인은 감정 분석, 텍스트 생성, 제로샷 분류 등 손쉬운 추론을 지원하는 인터페이스였었고요. 오토, 토크나이저, 오토 모델 등은 모델별 토크나이저 및","confidence":0.9002,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1143570,1143800,"파인"],[1143800,1144100,"튜닝이"],[1144170,1144740,"가능하도록"],[1144950,1145400,"도와주는"],[1145430,1146160,"라이브러리였습니다."],[1146830,1147060,"그중"],[1147130,1147680,"파이프라인은"],[1148010,1148300,"감정"],[1148370,1148640,"분석,"],[1149090,1149420,"텍스트"],[1149420,1149640,"생성,"],[1149930,1150240,"제로샷"],[1150250,1150427,"분류"],[1150427,1150560,"등"],[1150850,1151300,"손쉬운"],[1151350,1151680,"추론을"],[1151680,1152020,"지원하는"],[1152070,1152940,"인터페이스였었고요."],[1153350,1153587,"오토,"],[1153587,1154040,"토크나이저,"],[1154370,1154574,"오토"],[1154574,1154800,"모델"],[1155110,1155360,"등은"],[1155730,1156080,"모델별"],[1156110,1156580,"토크나이저"],[1156580,1156720,"및"]],"textEdited":"파인 튜닝이 가능하도록 도와주는 라이브러리였습니다. 그중 파이프라인은 감정 분석, 텍스트 생성, 제로샷 분류 등 손쉬운 추론을 지원하는 인터페이스였었고요. 오토, 토크나이저, 오토 모델 등은 모델별 토크나이저 및"},{"start":1156900,"end":1170800,"text":"모델 구조를 자동으로 로드해 주는 인터페이스였습니다. 마지막으로 트레이너는 학습, 루프, 평가, 분산 학습 등 효율적인 파인트윈을 지원하기 위한 인터페이스입니다. 이번 강의는 여기까지입니다. 고생 많으셨습니다.","confidence":0.9273,"diarization":{"label":""},"speaker":{"label":"","name":"","edited":false},"words":[[1157170,1157460,"모델"],[1157550,1157900,"구조를"],[1158070,1158460,"자동으로"],[1158460,1158714,"로드해"],[1158714,1158920,"주는"],[1159130,1159920,"인터페이스였습니다."],[1160510,1160960,"마지막으로"],[1160960,1161380,"트레이너는"],[1161790,1162020,"학습,"],[1162020,1162260,"루프,"],[1162910,1163180,"평가,"],[1163790,1164040,"분산"],[1164040,1164240,"학습"],[1164250,1164400,"등"],[1164930,1165360,"효율적인"],[1165410,1165840,"파인트윈을"],[1165840,1166220,"지원하기"],[1166220,1166420,"위한"],[1166670,1167400,"인터페이스입니다."],[1168050,1168260,"이번"],[1168310,1168580,"강의는"],[1168580,1169100,"여기까지입니다."],[1169650,1169847,"고생"],[1169847,1170300,"많으셨습니다."]],"textEdited":"모델 구조를 자동으로 로드해 주는 인터페이스였습니다. 마지막으로 트레이너는 학습, 루프, 평가, 분산 학습 등 효율적인 파인트윈을 지원하기 위한 인터페이스입니다. 이번 강의는 여기까지입니다. 고생 많으셨습니다."}],"text":"안녕하세요. 도메인 공통 프로젝트 7강 더 좋은 성능을 위한 기법 프리트레인드 모델 시작하겠습니다. 이번 강의에서는 사전 학습 모델의 중요성 이해하기 대표적인 사전 학습 모델 종류와 특징 살펴보기 마지막으로 허깅페이스 트랜스포머스 모듈 활용법 익히기에 대해서 배워보겠습니다. 먼저 사전 학습 모델의 중요성 이해하기입니다. 사전 학습 모델의 배경인 전이 학습에 대해서 먼저 이야기해 보겠습니다. 일반적으로 새로운 지식을 배울 때 기존에 알고 있던 지식을 활용하면 효율적이라는 부분은 우리가 직관적으로 알고 있습니다. AI도 이미 학습된 지식을 가져와서 이용하게 하자라는 것이 이 전이 학습의 컨셉이라고 이해하시면 되겠는데요. 대규모 데이터셋으로 미리 학습된 모델 사전 학습 모델이라고 하죠. 프리트레인드 모델이라고 합니다. 이 모델들의 지식 가중치를 사용하는 방법으로 새로운 테스크에 맞게 미세 조정하는 학습 방식을 이야기합니다. 밑에 그림에서 보시면 s스 데이터셋라고 하는 어떤 사전에 학습된 커다란 데이터셋을 기반으로 s스 모델을 학습을 시킵니다. 이 과정에서 프리트레인드 사전 학습이 이루어지게 되는데요. 이 모델을 가져와서 우리가 실제 풀고자 하는 타겟 데이터, 이 타겟 데이터에 맞게 미세 조정하는 파인 튜닝 하는 학습 방식입니다. 그림에서 보시면 아시겠지만 솔스 모델에서 가져온 모든 모델을 사용하지는 않을 수 있습니다. 특정 레이어를 freeze 시켜서 재학습되지 않도록 하거나 혹은 마지막에 새로운 레이어를 추가를 하는 방식으로 학습을 하게 됩니다. 전이 학습 과정은 다음 과정을 따르게 됩니다. 먼저 사전 훈련된 모델을 선택하게 됩니다. 내가 풀고자 하는 혹은 우리 팀이 풀고자 하는 관련 작업에 대한 사전 지식이나 기술을 갖춘 모델을 선택합니다. 그다음 사전 훈련된 모델을 구성합니다. 사전 훈련 모델을 새로운 작업 요구 사항에 맞게 재구성을 합니다. 사전 훈련 모델의 특정 계층을 동결 프리즈 하거나 최종 계층을 제거하거나 혹은 새로운 계층을 도입하는 등 여러 가지 방식을 통해서 재구성을 합니다. 마지막으로는 대상 도메인에 대한 모델을 훈련합니다. 새 작업 데이터를 기반으로 모델을 학습 성능 모니터링, 필요하다면 하이퍼 파라미터 등을 조정하게 됩니다. 자연어 처리에서 사전 학습 모델 효과에 대해서 살펴보겠습니다. 먼저 성능 향상입니다. 사전 학습 모델은 대규모 텍스트를 학습하여 언어의 패턴, 문법, 의미론적 관계를 학습한 상태입니다. 특정 테스크에서 처음부터 학습하는 모델보다 뛰어난 성능을 보입니다. 학습 리소스를 단축시킬 수 있습니다. 초기 가중치가 이미 있는 상태이므로 랜덤 가중치로 시작하는 것보다 적은 데이터와 시간으로도 목표한 성능을 달성하기 용이하며, 대규모 데이터로 학습하는 데 필요한 컴퓨팅 자원을 절약할 수 있고, 적은 양의 데이터만으로도 사용 가능하기 때문에 데이터 수집 어려움이 많이 줄어들게 됩니다. 그렇다면 어떤 사전 학습 모델을 선택을 해야 될까요? 사전 학습 모델의 핵심적인 역할 중 하나는 바로 효율적이고 의미론적인 임베딩 생성에 있습니다. NLP 모델에서 단어를 숫자로 표현하는 방식인 인베딩은 모델의 성능에 큰 영향을 미치게 됩니다. 첫 번째로 볼 인베딩은 정적 인베딩, 스테틱 인베딩입니다. 단어 하나의 임베딩 벡터가 항상 동일하게 유지되는 방식입니다. 가장 기본적이고 직관적인 임베딩 방식입니다. 어떤 문맥에서 사용되든지 모든 단어의 벡터는 불변합니다. 정적 인베딩의 한계에 대해서 이야기해 보겠습니다. 워 투 백, 글로브 등 기존의 정적 인베딩은 단어 하나당 하나의 고정된 벡터를 가집니다. 동우미니어나 다이어 예를 들어서 사과가 과일 또는 행위의 경우 문맥적 의미를 반영하지 못한다는 문제가 있고요. 학습 이후 벡터 값이 고정되어 도메인 변화에 적응이 불가능하다는 부분 형태소나 서브워 수준의 표현 부족으로 희귀어 또는 신조어 처리에 한계가 있습니다. 이에 따라 상위 수준 문장 문서를 구성하기 위한 추가적인 구조가 필요합니다. 오른쪽에 있는 예시를 보면 뱅크라고 하는 단어에 대해 서로 다른 문장 혹은 서로 다른 컨텍스트에 있음에도 동일한 임베딩을 갖는 것을 예시로 보여줍니다. 이런 정적 인베딩의 한계점을 보완하기 위해 문맥에 따라 의미를 다르게 표현하는 동적 인베딩을 사용할 수 있습니다. 동적 인베딩, 다이나믹 인베딩이라고 하는 이 방법은 단어의 인베딩 벡터가 문맥에 따라 동적으로 변경되는 방식입니다. RNN, LSTM 기반의 시퀀스 투 시퀀스 모델부터 트랜스포머 기반 모델까지 발전하게 됩니다. 같은 단어라도 문맥에 따라 다른 임베딩 벡터를 가지게 됩니다. 오른쪽에 있는 예시를 보겠습니다. iw 투 더 뱅크, 리버뱅크, 워즈 머디, 랭크 오브 아메리카 이런 식으로 같은 뱅크인 단어가 서로 다른 문장, 서로 다른 문맥이 있을 때 다른 벡터 값을 갖는 것을 보여줍니다. 다음은 대표적인 사전 학습 모델의 종류와 특징을 이해하기입니다. 첫 번째로는 지피티 제너레이티브 프리 트렌드 트랜스포머라고 하는 모델의 종류입니다. 오토 리그레시브 자기회귀 생성 방식입니다. 자기 회귀 생성 방식이란 왼쪽에서 오른쪽으로 단어를 하나씩 예측해 가면서 텍스트를 생성하도록 학습하는 방식입니다. 퓨샷 학습이나 제로샷 학습과 같은 프롬프트 엔지니어링 방식이 각광받고 있습니다. 별도의 파인튜닝 과정 없이 프롬프트에 몇 개의 예시만으로 다양한 테스크를 수행할 수 있게 되는 방식입니다. 다음은 벌츠입니다. 벌츠는 MLM 방식을 사용을 하게 됩니다. 이 마스크 랭귀지 모델 학습 방식은 문장 내 일부 단어를 마스크 토큰으로 가립니다. 그다음 모델에게 이 가려진 단어가 무엇인지 예측하도록 학습을 시킵니다. 예를 들어서 나는 땡땡을 타고 바다로 나갔다 라고 했을 때 이 oo 부분에 배라고 하는 글자가 나올 수 있도록 혹은 베라는 글자가 나올 확률이 높아지도록 모델을 학습시킵니다. 다음은 NSP라고 하는 기법을 또 사용합니다. next ST스 프레딕션이라고 하는데요. 두 개의 문장이 주어졌을 때 원문에서 이어지는 문장인지 아닌지를 예측하도록 학습합니다. 이를 통해 모든 단어가 앞뒤 문맥을 동시에 고려하여 학습할 수 있도록 구성합니다. 다음으로는 GPT와 벌트의 구조를 비교해 보겠습니다. 그 전에 트랜스포머의 인코더와 디코더 구조에 대해서 먼저 살펴보도록 하겠습니다. 트랜스포머의 인코더는 입력된 텍스트를 분석하여 그 의미를 이해하는 역할을 합니다. 주어진 문맥 속에서 단어들의 관계를 파악하고 이를 통해 전체 문장의 숨겨진 의미를 파악하는 데 특화되어 있는 구조입니다. 다음은 디코더입니다. 인코더의 정보를 바탕으로 새로운 텍스트를 생성해 내는 역할을 하는데요. 예측, 번역, 요약 등 새로운 결과물을 만들어내는 데 주로 사용합니다. GPT는 트랜스포머의 디코더를 사용하여 생성 능력에 특화되어 있는 구조입니다. gpt2는 오픈 소스인 반면 지피티3, 지피티4와 같은 최신 모델은 가중치가 비공개되어 있고 에피로만 사용이 가능합니다. 최근 지피티 5스스 같이 공개된 모델도 존재는 합니다. 유료 파인튜닝을 지원하지만 자원 소모 혹은 비용이 매우 높을 수 있습니다. 다음은 폴트입니다. 폴트는 트랜스포머 인코더 구조를 사용하여 문장 이해 능력의 특화 라이브러리를 통한 쉬운 접근 및 활용이 가능합니다. 다음은 로v타입니다. 월트의 사전 학습 방식을 최적화하여 개선하고 더 많은 데이터로 더 긴 학습 시간을 사용한 모델입니다. NSP 테스크가 성능 향상에 큰 기여를 하지 않는다고 판단하여 제거하는 대신 엠엘엠 테스크에 집중하게 된 모델입니다. 폴트는 사전 학습 단계에서 마스킹 패턴을 한 번으로 고정하지만 로버타는 학습 데이터가 모델에 주입될 때마다 다른 마스킹 패턴을 적용하는 다이나믹 마스킹을 사용합니다. 이를 통해 더 다양한 더 복잡한 패턴을 잘 학습할 수 있게 됩니다. 다음은 효율성을 위한 경량화에 집중한 알버트 모델에 대해서 이야기해 보겠습니다. 파라미터를 줄여 학습 속도를 올리기 위해 다음과 같은 테크닉을 사용합니다. 거대한 인베딩 행렬을 2개의 행렬로 분해하고 히든 레이어와 단어 임베딩을 분리하여 학습해야 되는 파라미터 수를 감소시켰습니다. 기존 트랜스포머에서 각 레이어 간 같은 파라미터를 공유하도록 변경하였고, 이를 통해 모델 크기 및 메모리 사용량 그리고 학습 시간을 효율화하는 개선점이 있었습니다. 또한 문장 사이의 순서를 학습하여 문장 간의 일관성을 효율적으로 학습할 수 있도록 합니다. 에오피를 통해 실제 담화 흐름에서 앞뒤 문장 순서가 자연스러운지를 학습합니다. 다음은 디스티 v트입니다. 지식을 증여하는 널리지 디스틸레이션 기법을 적용한 경량화 모델입니다. 벌트 모델을 티처 모델로 볼트 모델의 출력을 학생 모델인 디스트리 버트가 모방하도록 학습을 합니다. 이를 통해 벌트 베이스 모델에 비해 파라미터 수가 40% 적어 추론 속도가 약 60% 더 빨라지게 되는 결과를 얻었습니다. 다음으로는 라마입니다. 라즐 랭귀지 모델 메타 AI라고 하는 모델이고요. 사전 학습 방식을 따릅니다. 지피티와 유사하게 오토 리그레시브 생성 방식입니다. 세븐빌리언, 퍼틴 빌리언 혹은 다른 여러 가지 버전에 따라 자원 요구량이 다양합니다. 당연히 파라미터 수가 많으면 많을수록 높은 성능을 가지고 있습니다. 오픈 소스 및 접근 방식에서는 gpt3, 지피티4와 달리 가중치 자체가 연구 목적으로 공개되었다는 점이 핵심적인 특징입니다. 연구 라이선스로 공개되어 있어 다양한 실험 가능 연구 목적 외 상업적 사용은 제한됩니다. 다음은 허깅페이스 트랜스포머스 모듈 사용법 익히기입니다. 허깅페이스 트랜스포머스 라이브러리는 많은 트랜스포머 계열의 모델들을 쉽게 사용할 수 있도록 다양한 기능을 제공하는 라이브러리입니다. 관련된 코드와 사전 학습 모델을 쉽게 다운로드, 파인 튜닝 할 수 있도록 지원합니다. LLM의 배포 및 접근성 측면에서 주요 LLM들은 허깅페이스 같은 플랫폼을 통해 사전 학습 모델의 형태로 배포하게 됩니다. 허깅페이스는 라마 연구 라이선스나 블룸 오픈 라이선스처럼 가중치를 공개한 오픈 소스 대형 모델들을 제공하여 연구 및 개발 커뮤니티가 직접 모델을 분석하고 커스터마이징 할 수 있도록 돕습니다. 파인 튜닝 용이성 측면에서는 적은 수의 데이터를 이용한 로컬 파인튜닝 학습으로도 높은 성능을 기대할 수 있습니다. 문장 분류, 질의응답, 개체명 인식 등에 맞춰 마지막 레이어를 추가하여 학습하면 됩니다. 다음은 허깅페이스 트랜스포머스 라이브러리에 있는 파이프라인에 대해서 설명드리겠습니다. 사전 훈련된 모델을 추론을 수행할 때 사용하는 인터페이스입니다. 사용자가 모델 코드를 직접 작성하거나 복잡한 설정을 할 필요 없이 특정 테스크를 위한 사전 훈련된 모델과 토크나이저를 자동으로 다운로드하고 캐시하여 사용합니다. 여러 모달리티 자연어 처리, 컴퓨터, 비전, 오디오 혹은 멀티 모델에서 다양한 과업을 기본적으로 지원합니다. 사용 방법은 다음과 같습니다. 파이프라인 함수에 수행하고자 하는 태스크를 지정하여 인스턴스를 생성합니다. 예를 들어 감정 분석을 위해 파이프라인 센티멘트 아네시스와 같이 사용하면 됩니다. 생성된 클래스 파이어 객체에 대상 텍스트를 전달하여 추론을 수행합니다. 파이프라인 예제를 보겠습니다. 텍스트 생성용 파이프라인이고 결과는 리스트 형태로 변환되며 첫 번째 결과에 제너레이티드 텍스트를 사용하시면 됩니다. 이 예제에 사용한 옵션을 설명드리면 모델은 gpd2 GPU는 0번 디바이스 마이너스 1을 넣는 경우는 CPU가 됩니다. 입력 길이 제한을 넣었고요. 생성 결과 수 제한을 넣었습니다. 오른쪽 예시를 보시면 모델 이름에 오픈 AI, 커뮤니티 SLA시 gpt2라는 이름으로 파이프라인 객체를 생성을 했고요. 디바이스에 제로를 넣었기 때문에 0번 GPU에서 실행이 됩니다. 그다음 파이프라고 하는 이 인스턴스에 아이캔 NDS al 데이라고 하는 텍스트를 집어넣고 트렁케이션 2 넘 리턴 시퀀시스 1이라고 하는 값을 전달하여 결과를 반환합니다. 생성된 결과는 다음과 같습니다. 다음 예제로는 제로샷 분류를 보겠습니다. 미리 정의된 레이블에 대해 추가 학습 없이 즉시 다중 클래스로 분류될 수 있습니다. 오른쪽 예시에서 보면 캔디데이트 라벨스 라고 하는 부분에서 파서브 인파서브이라고 하는 라벨을 전달해 보겠습니다. 입력은 기존과 동일한 아이캔 두디스 올 데이를 넣겠습니다. 여기서 결과를 받는 리졸트를 확인을 해보면 모델이 선택할 수 있는 라벨 후보를 입력을 받아서 라벨별 스코어를 반환합니다. 결과를 확인해 봤을 땐 파서블이라는 라벨이 선택된 것을 보실 수가 있습니다. 다음은 오토 클래스입니다. 오토 클래스에는 오토 토크나이저, 오토 모델이 있습니다. 엘엘엠을 포함한 다양한 트랜스포머 계열 모델의 접근성과 활용성을 크게 향상시키는 핵심 기능이 됩니다. 사전 학습 모델의 아키텍처와 관련된 토크나이저를 자동으로 가져와 로드하는 바로 가기 역할이다 정도로 이해하시면 됩니다. 대형 모델들을 직접 개발할 필요 없이 손쉽게 로드하고 활용할 수 있게 하여 대규모 데이터 컴퓨팅, 파워 시간 등의 장벽을 극복하고 조직이 맞춤형 엠엘을 채택할 수 있도록 돕습니다. 먼저 오토 토크나이저입니다. 텍스트를 모델 입력에 필요한 숫자 배열 형태로 전처리하는 역할을 합니다. 단어를 어디서 끊을지, 어느 수준까지 나눌지와 같은 토크나 규칙들이 포함되어 있습니다. 퍼트 모델에는 월드 피스 토크나이저가, 지피티 모델에는 바이트 페어 인코딩 토크나이저가 사용되는 등 모델마다 다른 토크나이저가 활용됩니다. 모델이 사전 훈련된 모델과 동일한 모델 이름으로 토크나이저를 인스턴스화해야 합니다. 다음은 오토 모델입니다. 사전 훈련된 인스턴스를 간단하고 통합된 방법으로 로드합니다. 특정 과업에 맞는 알맞은 모델을 선택합니다. 예를 들어 텍스트 또는 시퀀스 분류를 위해서는 오토 모델 폴 시퀀스 커시피케이션을 로드해야 합니다. 오토 토크나이저 예제를 보겠습니다. 오른쪽 예시에서 오토 토크나이저, 프럼 프리트레인드에서 구글 버트 버트 베이스 언 케이스라고 하는 사전 학습 버츠 모델의 토크나이저를 로드를 합니다. 문장을 월드 피스 기반으로 토크화를 하고 각 토큰을 정수 인덱스로 매핑합니다. 매핑된 결과는 다음과 같습니다. 인풋 아이디스 각 토큰에 대한 정수 아이디 리스트입니다. 토큰 타입 아이디스, 문장 구분 아이디입니다. 단일 문장에서는 모두 0으로 들어갑니다. 어텐션 마스크 각 토큰이 실제 입력인지 혹은 패딩인지를 표시를 합니다. 그 결과가 오른쪽에서 보여지고 있습니다. 다음은 오토 모델 예제입니다. 오른쪽 예시에서 보시면 모델 이름을 디스티벌트 디스티버트 베이스 언 케이스트라는 모델을 사용합니다. 이전에 설명드렸던 것과 동일하게 오토 토크나이저와 오토 모델에는 동일한 모델 이름을 전달해야 됩니다. 그래서 예시에서도 프럼 프리 트레인트라는 메서드를 호출할 때 동일한 모델 이름을 전달하는 것을 보실 수가 있습니다. 다음으로는 이제 입력 텍스트를 토크나이저 방금 생성한 토크나이저에 전달을 해서 인풋 벡터를 만들어 냅니다. 인풋 벡터를 모델에 전달하는 것으로 그 모델의 로지 값을 만들어 낼 수 있고, 소프트맥스를 통해서 결과 확률을 얻어낼 수 있습니다. 여기에서 사용하는 디스트리버트 디스트리버트 베이스 언 케이스드는 특정 목적의 태스크로 파인튜닝 되어 있지 않은 모델이기 때문에 사실 여기에서 출력되는 로지이나 예측 확률은 의미 없는 값입니다. 그래서 여러분들이 실제로 사용하실 때는 추가적인 파인 튜닝이 반드시 필요하다는 점 염두에 두시면 좋겠습니다. 다음은 트레이너입니다. 트레이너는 모델 훈련을 최적화하는 클래스입니다. 훈련 루프를 효율적으로 관리하고 대규모 언어 모델을 파인튜닝 하는 데 필수적인 기능을 제공합니다. 분산 학습과 같은 고급 기능을 추가로 제공하여 대규모 모델 훈련 시 자원 효율성을 높여줍니다. 손실 함수 옵티마이저, 스케줄러와 같은 훈련 루프의 기능을 변경 가능합니다. 콜백을 사용하여 훈련 루프를 바꾸지 않으면서도 다른 라이브러리와 통합하거나 훈련 진행 상황을 보고받거나 조기에 훈련을 중단하는 것이 가능합니다. 다음으로는 트레이너 예제에 대해서 보겠습니다. 이전에 배웠던 것과 같이 오토 토크나이저, 오토 모델을 통해서 우리가 사용할 모델을 불러옵니다. 멀티 베이스 케이스 모델의 텍스트 처리 규칙을 따르는 토크나이저를 로드합니다. 이 토크나이저는 자연어를 모델이 이해하는 숫자 형태로 변환하는 역할을 합니다. 다음과 같이 벌트 베이스 케이스 모델의 아키텍처와 대규모 데이터셋으로 학습된 가중치를 가진 모델 인스턴스를 가져올 수 있습니다. 이는 오른쪽 예제의 맨 밑에 해당합니다. 데이터 셋은 1점부터 5점까지의 리뷰 평점을 나타내는 데이터이고요. 오토 토크나이저를 사용해서 텍스트를 토큰화합니다. 옵션을 살펴보면 트렁케이션 트루 텍스트가 모델의 최대 입력 길이를 초과하는 경우 자동으로 자르게 되어 있고요. 배치 트루 여러 샘플을 한 번에 처리해서 속도를 향상시킬 수 있습니다. 분리된 트레인 테스트 셋을 다음과 같이 구성합니다. 정확도를 평가 매트릭으로 설정을 하고요. 로지 값이 높을수록 모델이 해당 클래스라고 강력하게 예측하고 있음을 의미합니다. 로짓은 음수이거나 양수일 수 있고 합이 1이 아닐 수 있습니다. 매트릭은 로짓을 토대로 추정 라벨과 실제 라벨의 일치 정확도로 설정합니다. 이는 오른쪽에 있는 컴퓨터 매트릭스라고 하는 함수로 정의할 수 있습니다. 그다음 트레이닝 아규먼츠의 모델을 저장할 출력 경로 그리고 평가 주기 등을 설정합니다. 트레이너 객체는 모델 데이터셋 매트릭 등을 받아 학습과 평가 루프를 자동화합니다. 트레인 메소드 호출로 지정한 설정에 따라 파인튜닝을 수행하며 평가 및 체크 포인트를 저장하게 됩니다. 의미 문장 아캔두디스 올데이라는 텍스트를 추정하도록 합니다. 입력 값을 토크나이징하고 버드 모드에 전달합니다. 예측된 리뷰 점수를 출력합니다. 이번 강의를 요약해 보겠습니다. 먼저 우리는 사전 학습 모델의 중요성에 대해서 배웠습니다. 미리 학습된 모델의 지식을 새로운 과제에 맞게 재활용하는 방식인 전이 학습에 대해서 이야기했습니다. 전이 학습은 높은 성능, 적은 데이터 자원으로도 빠른 학습 데이터 부족 문제를 완화시킵니다. 또한 정적 인베딩의 한계에 대해서도 이야기를 했습니다. 다이어와 희귀어 문제 문맥을 잘 반영하지 못하는 부분에 대해서도 이야기를 했었고요. 이를 극복하기 위해 동적 인베딩이 필요하다라고 이야기했습니다. 다음으로는 대표적인 사전 학습 모델인 지피티 종류와 벌 모델 종류에 대해서 이야기해 봤습니다. GPT는 트랜스포머의 디코더를 기반으로 구성되어 있으며 생성 능력에 특화되어 있습니다. 단순히 프롬프트를 제공하는 것으로 퓨샷, 제로샷 학습이 가능합니다. FT는 인코더 기반 모델로 문맥 이해에 강점이 있으며 MLM, NSP 기반 방식으로 모델을 학습합니다. 마지막으로는 허깅페이스 트랜스포머스 라이브러리에 대해서 이야기를 했는데요. 다양한 트랜스포머 기반 모델을 표준화된 방식으로 쉽게 불러오고 파인 튜닝이 가능하도록 도와주는 라이브러리였습니다. 그중 파이프라인은 감정 분석, 텍스트 생성, 제로샷 분류 등 손쉬운 추론을 지원하는 인터페이스였었고요. 오토, 토크나이저, 오토 모델 등은 모델별 토크나이저 및 모델 구조를 자동으로 로드해 주는 인터페이스였습니다. 마지막으로 트레이너는 학습, 루프, 평가, 분산 학습 등 효율적인 파인트윈을 지원하기 위한 인터페이스입니다. 이번 강의는 여기까지입니다. 고생 많으셨습니다.","confidence":0.925329,"speakers":[{"label":"","name":"","edited":false}],"events":[],"eventTypes":[]}