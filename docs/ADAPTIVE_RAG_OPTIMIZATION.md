# Adaptive RAG ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Adaptive RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ì‹¤ìš©ì ì¸ ì „ëµê³¼ êµ¬í˜„ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## 1. Send APIë¥¼ í™œìš©í•œ ë³‘ë ¬í™”

LangGraphì˜ **Send API**ëŠ” ë™ì¼í•œ ë…¸ë“œë¥¼ ì—¬ëŸ¬ ë²ˆ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ fan-out/fan-in íŒ¨í„´ì„ êµ¬í˜„í•˜ì—¬ ì²˜ë¦¬ëŸ‰ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1.1 ë¬¸ì„œ í‰ê°€ ë³‘ë ¬í™”

ê²€ìƒ‰ëœ ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë™ì‹œì— í‰ê°€í•˜ì—¬ ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤.

```python
from langgraph.graph import Send, StateGraph
from typing import List

# ë‹¨ì¼ ë¬¸ì„œ í‰ê°€ ë…¸ë“œ
async def evaluate_single_document(state: dict, llm: Runnable) -> dict:
    """ë‹¨ì¼ ë¬¸ì„œë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    doc = state["doc"]
    question = state["question"]
    
    # í‰ê°€ ë¡œì§
    evaluator = create_document_evaluator(llm)
    result = await evaluator.ainvoke({
        "messages": [{
            "role": "user",
            "content": f"question: {question}\n\ndocument: {doc.page_content}"
        }]
    })
    
    return {
        "doc_id": state["doc_id"],
        "evaluation": result,
    }

# ë³‘ë ¬ í‰ê°€ë¥¼ ìœ„í•œ ë¼ìš°íŒ… í•¨ìˆ˜
def route_to_parallel_evaluation(state: AdaptiveRAGState):
    """ê° ë¬¸ì„œë¥¼ ë³„ë„ì˜ í‰ê°€ ë…¸ë“œë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    documents = state["documents"]
    question = state["question"]
    
    # Send APIë¡œ ê° ë¬¸ì„œë¥¼ ë³‘ë ¬ ì²˜ë¦¬
    return [
        Send(
            "evaluate_single_document",
            {
                "doc": doc,
                "doc_id": i,
                "question": question,
            }
        )
        for i, doc in enumerate(documents)
    ]

# í‰ê°€ ê²°ê³¼ ì§‘ê³„ ë…¸ë“œ
def aggregate_evaluations(state: AdaptiveRAGState) -> dict:
    """ë³‘ë ¬ í‰ê°€ ê²°ê³¼ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤."""
    # stateì—ëŠ” ëª¨ë“  í‰ê°€ ê²°ê³¼ê°€ ìˆ˜ì§‘ë˜ì–´ ìˆìŒ
    evaluations = state.get("evaluations", [])
    
    relevant_count = sum(1 for e in evaluations if e.get("evaluation", {}).get("relevant", False))
    sufficient = relevant_count >= 2  # ìµœì†Œ 2ê°œ ì´ìƒ ê´€ë ¨ ë¬¸ì„œ í•„ìš”
    
    return {
        "relevant_doc_count": relevant_count,
        "sufficient_context": sufficient,
        "document_evaluation": {
            "relevant_count": relevant_count,
            "total_count": len(evaluations),
        }
    }

# ì›Œí¬í”Œë¡œìš°ì— ì¶”ê°€
workflow = StateGraph(AdaptiveRAGState)

workflow.add_node("evaluate_single_document", evaluate_single_document)
workflow.add_node("aggregate_evaluations", aggregate_evaluations)

# ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ë³‘ë ¬ ë¼ìš°íŒ…
workflow.add_conditional_edges(
    "retrieve",
    route_to_parallel_evaluation,
)

# ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ë©´ ì§‘ê³„ë¡œ
workflow.add_edge("evaluate_single_document", "aggregate_evaluations")
```

**ì„±ëŠ¥ í–¥ìƒ**: 5ê°œ ë¬¸ì„œ í‰ê°€ ì‹œ ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ **ìµœëŒ€ 5ë°°** ë¹ ë¦„

### 1.2 Multi-Query ë³‘ë ¬ ê²€ìƒ‰

ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë™ì‹œì— ê²€ìƒ‰í•˜ì—¬ ì‹œê°„ì„ ì ˆì•½í•©ë‹ˆë‹¤.

```python
# ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ ë…¸ë“œ
async def retrieve_single_query(state: dict, retriever: BaseRetriever) -> dict:
    """ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    query = state["query"]
    query_id = state["query_id"]
    
    documents = await retrieve_documents_async(retriever, query)
    
    return {
        "query_id": query_id,
        "documents": documents,
    }

# ë³‘ë ¬ ê²€ìƒ‰ ë¼ìš°íŒ…
def route_to_parallel_retrieval(state: AdaptiveRAGState):
    """ê° ì¿¼ë¦¬ë¥¼ ë³„ë„ì˜ ê²€ìƒ‰ ë…¸ë“œë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    queries = state.get("refined_queries", [state["question"]])
    
    return [
        Send(
            "retrieve_single_query",
            {
                "query": query,
                "query_id": i,
            }
        )
        for i, query in enumerate(queries)
    ]

# ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë…¸ë“œ
def merge_retrieval_results(state: AdaptiveRAGState) -> dict:
    """ë³‘ë ¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¤‘ë³µ ì œê±°í•˜ë©° í†µí•©í•©ë‹ˆë‹¤."""
    all_documents = []
    seen_contents = set()
    
    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
    for result in state.get("retrieval_results", []):
        for doc in result.get("documents", []):
            content_hash = hash(doc.page_content)
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                all_documents.append(doc)
    
    return {
        "documents": all_documents,
        "context": all_documents,
    }

# ì›Œí¬í”Œë¡œìš°ì— ì¶”ê°€
workflow.add_node("retrieve_single_query", retrieve_single_query)
workflow.add_node("merge_retrieval_results", merge_retrieval_results)

workflow.add_conditional_edges(
    "analyze_query",
    route_to_parallel_retrieval,
)
workflow.add_edge("retrieve_single_query", "merge_retrieval_results")
```

**ì„±ëŠ¥ í–¥ìƒ**: 3ê°œ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹œ ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ **ìµœëŒ€ 3ë°°** ë¹ ë¦„

### 1.3 Send API ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­

1. **State ë¶„í• **: ê° ë³‘ë ¬ ë…¸ë“œëŠ” ë…ë¦½ì ì¸ stateë¥¼ ë°›ìŠµë‹ˆë‹¤. í•„ìš”í•œ ë°ì´í„°ë§Œ ì „ë‹¬í•˜ì„¸ìš”.
2. **ê²°ê³¼ ì§‘ê³„**: ë³‘ë ¬ ì‹¤í–‰ í›„ ë°˜ë“œì‹œ aggregate ë…¸ë“œë¥¼ í†µí•´ ê²°ê³¼ë¥¼ í†µí•©í•˜ì„¸ìš”.
3. **ì˜¤ë¥˜ ì²˜ë¦¬**: ì¼ë¶€ ë…¸ë“œê°€ ì‹¤íŒ¨í•´ë„ ì „ì²´ê°€ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ try-exceptë¥¼ ì ìš©í•˜ì„¸ìš”.

```python
async def evaluate_single_document_safe(state: dict, llm: Runnable) -> dict:
    """ì•ˆì „í•œ ë‹¨ì¼ ë¬¸ì„œ í‰ê°€ (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)"""
    try:
        return await evaluate_single_document(state, llm)
    except Exception as e:
        logger.error(f"Document evaluation failed: {e}")
        return {
            "doc_id": state["doc_id"],
            "evaluation": {"relevant": False, "error": str(e)},
        }
```

## 2. Timeout ì„¤ì • ë° ì˜¤ë¥˜ ì²˜ë¦¬

### 2.1 ChatOpenAI Timeout íŒŒë¼ë¯¸í„°

LangChainì˜ `ChatOpenAI`ëŠ” ë‚´ì¥ timeout ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë³„ë„ì˜ `asyncio.wait_for`ëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤.

```python
from langchain_openai import ChatOpenAI

# Timeoutì´ ì„¤ì •ëœ LLM ì¸ìŠ¤í„´ìŠ¤
fast_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.0,
    timeout=20,  # 20ì´ˆ timeout
    max_retries=2,  # ì‹¤íŒ¨ ì‹œ 2íšŒ ì¬ì‹œë„
    request_timeout=20,  # ìš”ì²­ë‹¹ timeout (ì´ˆ)
)

powerful_llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.3,
    timeout=45,  # 45ì´ˆ timeout (ë³µì¡í•œ ì‘ì—…ìš©)
    max_retries=2,
)

# ì›Œí¬í”Œë¡œìš°ì— ì ìš©
graph = build_adaptive_rag_graph(
    retriever=retriever,
    llm=powerful_llm,
    fast_llm=fast_llm,
)
```

### 2.2 ë…¸ë“œ ë ˆë²¨ ì˜¤ë¥˜ ì²˜ë¦¬

ê° ë…¸ë“œì—ì„œ LLM timeoutì„ ì ì ˆíˆ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
from openai import APITimeoutError, APIError

async def classify_intent_node_safe(
    state: AdaptiveRAGState,
    llm: Runnable
) -> dict:
    """ì•ˆì „í•œ intent classification (timeout ì²˜ë¦¬)"""
    try:
        return await classify_intent_node(state, llm)
    
    except APITimeoutError:
        logger.warning("Intent classification timed out, using default")
        return {
            "intent": "SIMPLE_QA",
            "intent_confidence": 0.5,
            "intent_reasoning": "Timeout - default classification applied"
        }
    
    except APIError as e:
        logger.error(f"API error during intent classification: {e}")
        return {
            "intent": "SIMPLE_QA",
            "intent_confidence": 0.3,
            "intent_reasoning": f"API error - default classification applied"
        }
    
    except Exception as e:
        logger.error(f"Unexpected error during intent classification: {e}")
        return {
            "intent": "SIMPLE_QA",
            "intent_confidence": 0.3,
            "intent_reasoning": f"Error - default classification applied"
        }
```

### 2.3 ì „ì²´ ì›Œí¬í”Œë¡œìš° Timeout

ì „ì²´ ì›Œí¬í”Œë¡œìš°ì— ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ì„ ì„¤ì •í•˜ë ¤ë©´ `asyncio.wait_for`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
import asyncio

async def run_workflow_with_timeout(
    graph,
    input_state: dict,
    timeout: int = 120  # 2ë¶„
) -> dict:
    """Timeoutì´ ì ìš©ëœ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    try:
        result = await asyncio.wait_for(
            graph.ainvoke(input_state),
            timeout=timeout
        )
        return result
    
    except asyncio.TimeoutError:
        logger.error(f"Workflow execution timed out after {timeout}s")
        return {
            **input_state,
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "error": "workflow_timeout",
            "workflow_stage": "timeout",
        }
```

### 2.4 ê¶Œì¥ Timeout ì„¤ì •

| ì‘ì—… | ëª¨ë¸ | Timeout | ì¬ì‹œë„ |
|------|------|---------|--------|
| Intent Classification | gpt-4o-mini | 20ì´ˆ | 2íšŒ |
| Query Analysis | gpt-4o | 30ì´ˆ | 2íšŒ |
| Document Evaluation | gpt-4o-mini | 20ì´ˆ | 1íšŒ |
| Answer Generation | gpt-4o | 45ì´ˆ | 2íšŒ |
| Answer Validation | gpt-4o | 30ì´ˆ | 1íšŒ |
| Correction | gpt-4o | 30ì´ˆ | 1íšŒ |

**ì „ì²´ ì›Œí¬í”Œë¡œìš°**: 120ì´ˆ (2ë¶„) ê¶Œì¥

## 3. ëª¨ë¸ ì„ íƒ ìµœì í™”

### 3.1 ì‘ì—…ë³„ ëª¨ë¸ ë¶„ë¦¬

ë¹„ìš©ê³¼ ì„±ëŠ¥ì„ ê³ ë ¤í•˜ì—¬ ì‘ì—…ë³„ë¡œ ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.

```python
from langchain_openai import ChatOpenAI

# ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸ (ë¶„ë¥˜, í‰ê°€)
fast_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.0,  # ê²°ì •ì  ì¶œë ¥
    timeout=20,
    max_retries=2,
)

# ê°•ë ¥í•œ ëª¨ë¸ (ë³µì¡í•œ ì¶”ë¡ , ìƒì„±)
powerful_llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.3,  # ì•½ê°„ì˜ ì°½ì˜ì„±
    timeout=45,
    max_retries=2,
)

# ë¹„ìš© ìµœì í™” ì „ëµ
cost_optimized_graph = build_adaptive_rag_graph(
    retriever=retriever,
    llm=powerful_llm,      # ë‹µë³€ ìƒì„±ìš©
    fast_llm=fast_llm,     # ë¶„ë¥˜, í‰ê°€ìš©
)
```

### 3.2 ëª¨ë¸ë³„ ë¹„ìš© ë¹„êµ

| ì‘ì—… | ëª¨ë¸ | 1M tokens ë¹„ìš© | ì‘ì—…ë‹¹ ë¹„ìš© (ì˜ˆìƒ) |
|------|------|---------------|-----------------|
| Intent Classification | gpt-4o-mini | $0.15 | $0.0001 |
| Document Evaluation | gpt-4o-mini | $0.15 | $0.0003 |
| Answer Generation | gpt-4o | $2.50 | $0.005 |
| Answer Validation | gpt-4o | $2.50 | $0.002 |

**ë¹„ìš© ì ˆê°**: fast_llm í™œìš© ì‹œ ì „ì²´ ë¹„ìš© **ì•½ 40% ì ˆê°**

## 4. Early Stopping

### 4.1 ë‹¨ìˆœ ì§ˆë¬¸ ê³ ì† ì²˜ë¦¬

ë‹¨ìˆœí•œ ì§ˆë¬¸ì€ ê²€ì¦ì„ ê±´ë„ˆë›°ê³  ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
def should_skip_validation(state: AdaptiveRAGState) -> bool:
    """ê²€ì¦ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸"""
    intent = state.get("intent")
    relevant_count = state.get("relevant_doc_count", 0)
    
    # ë‹¨ìˆœ QA + ì¶©ë¶„í•œ ê´€ë ¨ ë¬¸ì„œ
    if intent == "SIMPLE_QA" and relevant_count >= 3:
        return True
    
    return False

# ë¼ìš°íŒ…ì— ì ìš©
def route_after_generation(state: AdaptiveRAGState):
    if should_skip_validation(state):
        return "finalize"  # ê²€ì¦ ìŠ¤í‚µ
    else:
        return "validate_answer"  # ê²€ì¦ ìˆ˜í–‰
```

### 4.2 ì¬ì‹œë„ ì œí•œ ì—„ê²©í™”

ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¬ì‹œë„ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.

```python
# AdaptiveRAGSettingsì—ì„œ ì„¤ì •
update_adaptive_rag_settings(
    max_retrieval_retries=1,  # í”„ë¡œë•ì…˜: 1íšŒë§Œ
    max_correction_retries=1,  # í”„ë¡œë•ì…˜: 1íšŒë§Œ
)
```

**ì„±ëŠ¥ í–¥ìƒ**: Early stopping ì ìš© ì‹œ ë‹¨ìˆœ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œê°„ **ì•½ 50% ë‹¨ì¶•**

## 5. ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

### 5.1 Connection Pooling

HTTP connection poolì„ ì¬ì‚¬ìš©í•˜ì—¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì…ë‹ˆë‹¤.

```python
import httpx
from langchain_openai import ChatOpenAI

# ê³µìœ  HTTP client (connection pooling)
http_client = httpx.AsyncClient(
    timeout=httpx.Timeout(60.0),
    limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
)

llm = ChatOpenAI(
    model="gpt-4o",
    http_async_client=http_client,  # ì¬ì‚¬ìš©
)
```

### 5.2 ë©”ëª¨ë¦¬ ê´€ë¦¬

ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.

```python
def truncate_documents(
    documents: List[Document],
    max_length: int = 8000  # í† í° ì œí•œ ê³ ë ¤
) -> List[Document]:
    """ë¬¸ì„œ ë‚´ìš©ì„ truncateí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½"""
    return [
        Document(
            page_content=doc.page_content[:max_length],
            metadata=doc.metadata
        )
        for doc in documents
    ]

# ë…¸ë“œì—ì„œ ì ìš©
async def generate_answer_node(state, llm):
    documents = state.get("documents", [])
    documents = truncate_documents(documents, max_length=8000)
    # ... ë‚˜ë¨¸ì§€ ë¡œì§
```

## 6. ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### 6.1 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

ê° ë…¸ë“œì˜ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

```python
import time
from functools import wraps

def measure_performance(node_name: str):
    """ë…¸ë“œ ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start
                logger.info(
                    f"[PERF] {node_name}: {duration:.2f}s",
                    extra={"node": node_name, "duration": duration}
                )
                return result
            except Exception as e:
                duration = time.time() - start
                logger.error(
                    f"[PERF] {node_name}: FAILED after {duration:.2f}s",
                    extra={"node": node_name, "duration": duration, "error": str(e)}
                )
                raise
        return wrapper
    return decorator

# ë…¸ë“œì— ì ìš©
@measure_performance("classify_intent")
async def classify_intent_node(state, llm):
    # ... êµ¬í˜„
    pass
```

### 6.2 LangSmith í†µí•©

LangSmithë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¶”ì í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

```python
import os

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "adaptive-rag-production"

# ì´ì œ ëª¨ë“  ì‹¤í–‰ì´ ìë™ìœ¼ë¡œ LangSmithì— ê¸°ë¡ë¨
result = await graph.ainvoke(input_state)
```

### 6.3 ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ

ì£¼ìš” ë©”íŠ¸ë¦­ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤:

```python
import structlog

# êµ¬ì¡°í™”ëœ ë¡œê¹…
perf_logger = structlog.get_logger("performance")

async def log_workflow_metrics(state: AdaptiveRAGState, start_time: float):
    """ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ë¡œê¹…"""
    duration = time.time() - start_time
    
    perf_logger.info(
        "workflow_completed",
        duration=duration,
        intent=state.get("intent"),
        retry_count=state.get("retry_count", 0),
        correction_count=state.get("correction_count", 0),
        quality_score=state.get("quality_score", 0),
        has_hallucination=state.get("has_hallucination", False),
    )
```

## 7. êµ¬í˜„ ìš°ì„ ìˆœìœ„

ì‹¤ì œ ì ìš© ì‹œ ë‹¤ìŒ ìˆœì„œë¡œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:

### í•„ìˆ˜ (ì¦‰ì‹œ ì ìš©)
1. âœ… **Timeout ì„¤ì •**: ChatOpenAIì˜ timeout íŒŒë¼ë¯¸í„°
2. âœ… **ì¬ì‹œë„ ì œí•œ**: max_retrieval_retries, max_correction_retries
3. âœ… **ì˜¤ë¥˜ ì²˜ë¦¬**: try-exceptë¡œ ì•ˆì „í•œ fallback

### ê¶Œì¥ (ë‹¨ê¸°)
4. âœ… **ëª¨ë¸ ì„ íƒ**: ì‘ì—…ë³„ fast_llm/powerful_llm ë¶„ë¦¬
5. âœ… **Early stopping**: ë‹¨ìˆœ ì§ˆë¬¸ ê³ ì† ì²˜ë¦¬
6. âœ… **ë©”ëª¨ë¦¬ ìµœì í™”**: ë¬¸ì„œ truncation

### ì„ íƒ (ì¤‘ê¸°)
7. âš¡ **Send API ë³‘ë ¬í™”**: ë¬¸ì„œ í‰ê°€, multi-query ê²€ìƒ‰
8. âš¡ **Connection pooling**: HTTP client ì¬ì‚¬ìš©
9. âš¡ **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: LangSmith í†µí•©

### ê³ ê¸‰ (ì¥ê¸°)
10. ğŸ“Š **Auto-tuning**: ì„±ëŠ¥ ë°ì´í„° ê¸°ë°˜ ìë™ íŒŒë¼ë¯¸í„° ì¡°ì •
11. ğŸ“Š **Load balancing**: ì—¬ëŸ¬ LLM API ë¶„ì‚°
12. ğŸ“Š **Advanced profiling**: ë³‘ëª© ì§€ì  ìë™ íƒì§€

## 8. ì„±ëŠ¥ ëª©í‘œ

ë‹¤ìŒ ì„±ëŠ¥ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë„ë¡ ìµœì í™”í•©ë‹ˆë‹¤:

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ìµœì í™” ì „ | ìµœì í™” í›„ |
|--------|------|----------|----------|
| í‰ê·  ì‘ë‹µ ì‹œê°„ (Simple QA) | < 10ì´ˆ | ~15ì´ˆ | ~8ì´ˆ |
| í‰ê·  ì‘ë‹µ ì‹œê°„ (Complex) | < 30ì´ˆ | ~45ì´ˆ | ~25ì´ˆ |
| P95 ì‘ë‹µ ì‹œê°„ | < 60ì´ˆ | ~80ì´ˆ | ~50ì´ˆ |
| Timeout ë°œìƒë¥  | < 1% | ~5% | < 1% |
| ë¹„ìš© (1000 queries) | < $5 | ~$8 | ~$4.50 |

## 9. ì°¸ê³  ìë£Œ

- [LangGraph Send API Documentation](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/)
- [LangChain Timeouts](https://python.langchain.com/docs/how_to/chat_model_rate_limiting)
- [OpenAI API Best Practices](https://platform.openai.com/docs/guides/production-best-practices)
- [Async Programming in Python](https://docs.python.org/3/library/asyncio.html)

