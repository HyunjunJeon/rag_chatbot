{
  "source_file": "4강_행렬이랑_더_친해져보자.json",
  "lecture_name": "4강_행렬이랑_더_친해져보자",
  "course": "AI Math",
  "total_chunks": 7,
  "chunks": [
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c000_e2b4d4",
      "content": "[강의 녹취록] 과목: AI Math | 강의: 4강 | 제목: 행렬이랑_더_친해져보자\n\n네 안녕하세요 메스메렉스 포 아티피셜 인텔리전스 네 번째 강의 행렬이랑 더 친해져 보자 강의를 맡게 된 고려대학교 통계학과 임성빈 교수입니다. 이번 시간에는 선형대수 시간 때 배웠던 행렬의 내용들을 조금 더 심화적으로 배우는 시간이 되겠습니다. 네 지난 시간까지 배웠던 거는 행렬의 기본적인 정의 그리고 행렬의 덧셈 뺄셈 곱셈 그리고 행렬의 역행렬 그리고 유사역행렬을 이용해서 우리가 연립 방정식이나 또는 선형 회귀 분석 문제를 푸는 걸 소개했었는데요. 이번에는 행렬의 분해 즉 매트릭스 디컴포지션에 두 가지 기법들을 좀 소개를 해 보려고 합니다. 행렬 분해라는 것은 어떤 행렬이 있을 때 그 행렬을 여러 행렬들의 곱으로 분해해서 표현하는 기법을 말합니다. 그림과 같이 행렬 a가 있을 때 이렇게 m개의 행렬로 분해를 해주게 되는 기법을 말하는 건데요. 어 그냥 분해를 해주게 되면은 사실 용도가 어떻게 되는지 잘 모르겠습니다만 우리가 특별한 용도를 가지고 행렬 분해를 했을 때 어 그렇게 분해한 행렬들의 성질을 잘 이용해 주게 되면은 우리가 여러 가지 이점들을 누리는 행렬 분해 기법들이 있습니다. 그래서 행렬 분해는 한 종류만 있는 것이 아니고 여러 종류로 우리가 행렬 분해를 쓸 수가 있는데요. 목적에 따라 쓰이는 방법이 달라지기 때문에 우리가 여러 종류의 행렬 분해를 실제로 가져다 쓸 수가 있습니다. 참고로 여기 있는 행렬 분해들은 우리가 넘파에서 실제로 구현이 되어 있는 행렬 분해들이기 때문에 여러분들께서 한번 탐색해 보시면 좋은데요. 이번 강의에서는 저희가 가장 많이 쓰이는 행렬 분해 중 하나인 아이젠 밸류 디컴포지션 고유값 분해와 그리고 싱귤러 밸류 디컴포지션 특이값 분해 이 두 개를 한번 배워보도록 하겠습니다. 먼저 고유값 분해를 한번 살펴보도록 할게요. 고윳값이라는 거를 먼저 알아야 될 텐데요. 어떤 행렬에 벡터 v를 곱했을 때 그 벡터의 상수배가 되는 경우 즉 다시 말해서 어떤 행렬 a를 이 벡터 부위에다가 곱했을 때요. 그 결과가 이 행렬 부위를 방향은 똑같고 그 길이만 달라지게 조정해 주는 네 그런 벡터를 찾을 수가 있습니다. 다시 말해서 어떤 행렬에다가 어떤 벡터를 곱했을 때 그 방향이 변하지 않고 그 행렬 부위에 이 길이만 변해 주는 요런 벡터 부가 있을 때 이 벡터를 우리는 아이젠 벡터라고 부르고요. 그리고 이 아이젠 벡터 부위를 어 길이를 변해주게 해주는 이 스칼라 곱에 해당하는 이 숫자 이 숫자를 우리는 아이젠 밸류 즉 고유 값이라고 부르게 됩니다. 우리가 수식어로 표현해 주면 이렇게 쓸 수가 있습니다. 행렬 a에다가 벡터 브를 곱했을 때 그 벡터 부의 람다베 즉 아이젠 밸류만큼 곱해준 걸로 표현이 되게 되면은 아 이 벡터 v는 행렬 a의 아이젠 팩터 이 람다는 행렬 a의 아이젠 밸류라고 부르게 됩니다. 네 여러분들께서 눈치를 채셨는지 모르겠지만 행렬 이 벡터 부위가 방향은 똑같고 길이만 변한다라는 개념에서 보셨다시피 사실은 이 이 벡터 부가 속하는 공간의 차원이 같습니다. 다시 말해 행렬 a가 엔 차원 공간에서 n 차원 공간으로 보내주는 연산자이기 때문에 당연히 엠바이엔 행렬이 되겠죠. 즉 행과 열이 같은 경우에 우리는 고유 값과 고유 벡터를 구할 수가 있게 됩니다. 즉 역행렬을 구하는 거랑 좀 비슷한 상황인 것이죠. 네 행과 열의 개수가 같을 때만 우리가 고유 값 그리고 고유 벡터를 구할 수 있다라는 거를 여러분들께서 기억해 주시면 좋겠습니다. 참고로 고유값 분해 즉 고유값과 고유 벡터는 하나만 있는 것은 아니고요. 네 여러 개가 있을 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 0,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 973,
      "char_count": 1794
    },
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c001_8a3ffa",
      "content": "[AI Math] 4강_행렬이랑_더_친해져보자\n\n다. 다시 말해 행렬 a가 엔 차원 공간에서 n 차원 공간으로 보내주는 연산자이기 때문에 당연히 엠바이엔 행렬이 되겠죠. 즉 행과 열이 같은 경우에 우리는 고유 값과 고유 벡터를 구할 수가 있게 됩니다. 즉 역행렬을 구하는 거랑 좀 비슷한 상황인 것이죠. 네 행과 열의 개수가 같을 때만 우리가 고유 값 그리고 고유 벡터를 구할 수 있다라는 거를 여러분들께서 기억해 주시면 좋겠습니다. 참고로 고유값 분해 즉 고유값과 고유 벡터는 하나만 있는 것은 아니고요. 네 여러 개가 있을 수가 있습니다. 참고로 이 고유값과 고유 벡터의 개수가 행과 열의 개수인 즉 행렬의 행열의 개수인 n과 같게 우리가 찾아볼 수가 있는데 우리가 이거를 조금 수식적으로 표현해 주게 되면은 다음과 같이 좀 살펴볼 수가 있어요. 즉 브원서부터 브엔까지 즉 케가 1서부터 n까지 엔개의 고유 벡터랑 그리고 엔개의 고유 값에 대해서 우리가 행렬 a와 그리고 고유 벡터와 이 고유 값의 고유값과 고유 벡터의 곱을 다음과 같이 표현해 볼 수가 있는데요. 저희가 부를 어떤 행렬로 명시를 하자면요. 고유 벡터가 열 벡터인 행렬로 우리가 표시를 해 보겠습니다. 즉 위에서 얘기한 고유 벡터 v1 그리고 vn들이 열 벡터로 구성되어 있는 행렬을 이 대문자 v라는 행렬이라고 표시를 하게 될 거고 그리고 이 각각의 열벡터 v1서부터 vn까지 네 이 고유 벡터들한테 고유값 람다 원 람다 엔을 곱해주는 네 요 행렬은 사실 어떻게 이해해 볼 수 있냐면요. 원래 브라는 벡터에다가 대각선 성분이 고유 값인 즉 나머지는 0이고 이 대각 성분이 각각의 고유 값들로 이루어져 있는 이 대각 성분인 행렬 네 이런 행렬을 우리는 대각 행렬이라고 부르는데요. 대각 행렬의 대각 성분이 고유 값인 행렬을 우리가 이렇게 람다로 표현해 보도록 하겠습니다. 그랬을 때 이 avk는 람다 k 곱하기 vk 즉 고유 벡터와 고유 값 그리고 원래 행렬 a 사이의 관계를 우리가 복수계의 고유 벡터와 고유 값에 대해서 다음과 같이 표현할 수가 있게 됩니다. 즉 행렬 a와 그리고 고유 벡터로 이루어져 있는 행렬 브의 곱은 브라는 행렬과 이 고유 값으로 이루어져 있는 대각 행렬 람다의 곱으로 표현할 수가 있게 됩니다. 그래서 고유값과 고유 벡터가 모두 엔 개가 있을 때 이와 같은 관계식이 성립한다는 거를 우리가 살펴볼 수가 있습니다. 사실 이 관계식은요. 위에서 살펴본 에유랑 고유 벡터의 곱이 고유 벡터 곱하기 고유 값에 일치한다라는 이 수식을 우리가 행렬 곱으로 표현한 것에 불과합니다. 그랬을 때 만약 모든 고유 벡터들끼리 서로 선형 독립인 경우에는요. 즉 고유 벡터들이 서로 선형 결합이 되지 않고 이렇게 선형 독립인 그런 벡터들로 이어져 있는 행렬 브라면은 우리가 이 행렬 부위의 역행렬을 계산해 볼 수가 있게 됩니다. 모든 벡터들끼리 선형 독립이게 되면은 행렬식이 0이 아니다라는 결과 혹시 기억하시나요? 네 그걸 이용해서 고유 벡터로 이루어져 있는 행렬 부위에 역행렬이 존재하기 때문에 이걸 이용해서 우리가 행렬 a를 표현할 수가 있게 됩니다. 정리하면은 고유 벡터로 이루어져 있는 행렬 부위와 그리고 고유 값으로 대각 성분이 이루어져 있는 대각행렬 람다와 그리고 이 고유 벡터로 이루어져 있는 행렬 부위의 역행렬을 가지고 우리가 원래 행렬 a를 표현할 수가 있게 되고 이거를 우리가 아이젠 밸류 디컴포지션이라고 부르게 되는 것입니다. 즉 행렬 a를 3개의 행렬의 곱으로 우리가 표현할 수가 있게 되는 것이죠. 이 아이젠 밸류 디컴포지션은 굉장히 많이 쓰이는 기법이기 때문에 여러분들께서 기억을 하시면 좋겠습니다. 대표적인 예시 중 하나로 우리가 프린스플 컴퍼넌트 어널리시스 즉 주성분 분석에서 어떻게 쓰이는지를 한번 살펴보도록 하겠습니다. 주성분 분석의 목적은 즉 피시의 목적은 어떤 데이터가 주어져 있을 때 이 데이터가 고차원 데이터라고 가정을 해 보겠습니다. 지금 하이디맨저널 데이터일 때 저차원 공간으로 이 데이터를 효율적으로 압축을 시키고 싶다고 우리가 목적을 가지고 있다고 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 1,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 1085,
      "char_count": 2005
    },
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c002_4dabc5",
      "content": "[AI Math] 4강_행렬이랑_더_친해져보자\n\n다. 대표적인 예시 중 하나로 우리가 프린스플 컴퍼넌트 어널리시스 즉 주성분 분석에서 어떻게 쓰이는지를 한번 살펴보도록 하겠습니다. 주성분 분석의 목적은 즉 피시의 목적은 어떤 데이터가 주어져 있을 때 이 데이터가 고차원 데이터라고 가정을 해 보겠습니다. 지금 하이디맨저널 데이터일 때 저차원 공간으로 이 데이터를 효율적으로 압축을 시키고 싶다고 우리가 목적을 가지고 있다고 하겠습니다. 이때 저차원 공간으로 압축시키기 위해서 쓰는 기법이 바로 고유값 분해가 되겠습니다. 네 먼저 이거를 개념적으로 좀 살펴볼게요. 데이터들이 다음과 같이 빨간 점의 형태로 스원서부터 그다음에 이 캐피탈 엔 즉 엔개의 데이터들이 주어져 있다고 가정하겠습니다. 네 이 빨간 점들의 개수가 이 n이라고 보시면 될 거고 이 빨간 점들 하나하나가 전부 다 데이터가 되는 것입니다. 이때 이 각각의 데이터는 우리가 벡터로 x아라고 표현할 수가 있게 되고 이때 이 벡터는 디디맨전 즉 디차원 벡터가 되겠습니다. 이때 저차원 공간에 해당하는 어떤 그 이 CV로 이루어져 있는 이 집합으로 저희가 한번 압축을 하고 싶은 건데요. 이거를 여러분들께서 어떻게 이해하실 수 있냐면은 이렇게 데이터가 주어져 있을 때 이 벡터 브로 구성되어 있는 어떤 스페이스 공간에다가 이 데이터들을 압축시키고 싶은 겁니다. 이때 이 저차원 공간이라 함은 이 벡터 v로 구성되어 있는 벡터들의 모임으로 보시면 되는데요. 이 벡터들의 모임은 어떤 모임이냐면은 이 대표 벡터 v에다가 스칼라 c 즉 어떤 실수값 c를 곱해서 이루어져 있는 벡터들의 모임이 되겠습니다. 즉 벡터 부위에 평행한 벡터들의 모임이 되는 것이죠. 즉 부위에다가 어떤 스칼러 급을 때린 즉 이 벡터 부위에 스칼러 급을 취한 벡터들의 모임이 이 저차한 공간을 상정하는 것입니다. 참고로 이 벡터 부는 어 그냥 임의의 벡터는 아니고요. 어 노름 값이 1인 즉 이 벡터 부의 길이가 1인 벡터 브이를 하나 가져오겠는 것입니다. 그랬을 때 어떤 벡터 v에 대해서 이 저차원 공간에 데이터를 압축했을 때 우리가 가장 정보 손실이 덜 할지를 문제를 풀고 싶은 것입니다. 즉 데이터를 저차원으로 축소하려면 어떤 기준으로 압축을 해야 즉 부위를 선택을 해야 가장 전보 손실이 덜할지를 문제를 우리가 풀어볼 수가 있는데요. 이때 문제를 풀 때 우리가 앞서 살펴본 벡터 시간 때 살펴본 정사형이라는 개념을 우리가 사용할 것입니다. 즉 어떤 데이터 x가 아이가 있을 때요. 이 XI를 브로 구성되어 있는 저차원 공간에 정사형 시킬 때는 우리가 앞서 살펴봤던 것처럼 내적을 이용한다고 배웠습니다. 즉 XI를 브라는 벡터로 정상용 즉 프로젝션 시키게 되면은 우리가 XY를 브랑 내적을 시킨 다음에 이 내적한 값을 부위에다가 곱한 것으로 우리가 이해를 해 볼 수가 있게 되는데 이때 이 정상형 시켰을 때 원래 데이터 XI랑 정보 손실이 가장 덜한 벡터 v를 찾는 게 목적이 되겠습니다. 이거를 수식적으로 쓰면 다음과 같이 표현해 볼 수가 있겠습니다. 원래 데이터 XI가 있고 그다음에 이 XI를 저차원 공간 v로 정상을 시켰을 때 프로젝션 v로 표현이 될 텐데 이 XI와 그다음에 XI의 프로젝션 v에 노름 값이 가장 미니마이즈 해 줄 수 있는 부위를 찾고 싶은 것이고요. 참고로 이때 이 정보 손실의 값은 노름 값을 취한 다음에 전부 다 더해주는 모든 데이터에 대해서 다 더해주는 방식으로 계산을 해 주게 되고 이때 이 부위의 노름 값이 1인 제약식을 만족해야 됩니다. 이때 위와 같이 x에서부터 정상형 시킨 그 정보가 손실 값을 우리가 두 벡터의 뺄셈으로 정리를 해 주게 되고, 이 벡터 값을 누름의 합이 가장 작은 벡터 부위를 찾는 게 주성분 분석 즉 피시에 목적이라고 정리해 볼 수가 있겠습니다. 네 그렇다면은 앞서 계산했던 수식을 이용해서 한번 주성분 분석 문제를 풀어보도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 2,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 1035,
      "char_count": 1919
    },
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c003_40e19e",
      "content": "[AI Math] 4강_행렬이랑_더_친해져보자\n\n다. 이때 위와 같이 x에서부터 정상형 시킨 그 정보가 손실 값을 우리가 두 벡터의 뺄셈으로 정리를 해 주게 되고, 이 벡터 값을 누름의 합이 가장 작은 벡터 부위를 찾는 게 주성분 분석 즉 피시에 목적이라고 정리해 볼 수가 있겠습니다. 네 그렇다면은 앞서 계산했던 수식을 이용해서 한번 주성분 분석 문제를 풀어보도록 하겠습니다. 네 앞서 형식화했던 수식을 다음과 같이 정리해 볼 수가 있었고요. 이때 이 노름에 해당하는 거는 엘투 노름이기 때문에 우리가 엘투 노름에 해당하는 수식으로 전개를 해 보게 되면은 다음과 같이 스아이에 대한 엘투놈 아 여기 지금 제곱이 빠져 있네요. 그리고 두 번째 항에 대한 엘투놈 그리고 첫 번째 항과 두 번째 항을 이너 프로덕트 즉 내적을 계산한 다음에 곱하기 2를 한 이 두 번째 항이 튀어나오게 됩니다. 네 이거를 좀 더 다시 한번 살펴보시게 되면은 앞서 정상형으로 표시된 수식 대신에 스아와 부의 내적 곱하기 브로 우리가 수식을 대체를 한 것이고요. 엘투 노름의 계산 공식 특성상 먼저 첫 번째 항의 l2 노름의 제곱 더하기 두 번째 항에 해당하는 거에 엘투 노름의 제곱 따라서 v의 l2 노름의 제곱과 그리고 앞서 곱해져 있는 XIV 내적 값의 제곱이 등장하게 되는 것입니다. 그리고 마지막으로 XI와 XI 내적 v의 곱하기 v의 서로 간의 내적이 계산이 되는 것인데요. 이 XIV 내적은 스칼라이기 때문에 잠깐 무시를 하고 XI랑 v끼리 서로 내적을 해 주게 되면은 앞서 곱해져 있던 이 값이랑 같은 값이라는 걸 보실 수가 있게 됩니다. 즉 다시 말해 스아와 이 두 번째 항의 내적은 사실 XIV 내적의 제곱으로 튀어나오게 되겠죠. 그리고 마이너스가 있으니까 마이너스 2가 곱해지는 것이고요. 그래서 이 l2 노름으로 우리가 형식화된 목적식이 다음과 같이 전개를 할 수가 있게 됩니다. 그랬을 때 v라는 벡터의 l2 노름은 우리가 1이다라고 제약식을 걸고 찾고 있기 때문에 이 맨 마지막 항에 있는 v의 l2 누르면 1이 되고요. 그러면 2항과 이 두 번째 항이 마이너스 2 더하기 1 그래서 마이너스 스와 곱하기 내적의 제곱으로 바꿔줄 수가 있게 됩니다. 그러면 우리가 가만히 살펴보시면요. 우리가 풀고자 하는 최적화 식은 부에 대한 최적화 식입니다. 그때 목적식에서 잘 살펴보시면 이 두 번째 항은 부에 대해서 움직이는 항이 되니까 부위를 바꿨을 때 이 최적화 목적식이 바뀌어지게 되는데 첫 번째 행위에 해당하는 스와 엘투노롬의 제곱 값은 부위를 바꿔도 바뀌지 않게 되므로 최적화를 수행할 때 고려하지 않아도 됩니다. 따라서 정리해 주면은 XIV 내적의 제곱 값의 합을 우리가 고려하면 되는데 앞서 마이너스가 있었고 미니마이제이션이었잖아요 근데 마이너스를 바깥으로 빼주게 되면은 미니마이제이션을 우리가 맥스마이저로 수식을 바꿀 수가 있게 됩니다. 네 이때 이 수식을 우리가 가만히 잘 살펴보시면 사실 x라는 데이터 행렬의 전치 행렬과 그리고 브라는 벡터의 행렬 곱백 그럼 결과값이 또한 벡터가 되는데요. 이 벡터의 엘트놈의 제곱으로 나타낼 수가 있고 이 수식을 다시 쓰게 되면은 브에 부는 원래 열벡터입니다만 브 열벡터의 전치면 행벡터죠 그래서 행벡터 곱하기 엑스와 엑스의 전치 행렬 곱하기 그리고 브로 표현할 수가 있게 됩니다. 이때 스는 데이터를 표현하는 행렬이라고 말씀드렸는데요. 어 참고로 디게의 변수를 차원을 가지는 각각의 벡터들이 엔 개가 있는 상황입니다. 앞에서는 행벡터를 기준으로 우리가 사용했었는데 지금과 같은 경우에는 열 벡터를 기준으로 표현했다는 사실을 잘 기억하시기 바랍니다. 그러면은 위 목적식을 우리가 통계학적으로 한번 해석해 보면요. 우리가 데이터를 브라는 저차원 공간으로 정사형시켰을 때 그 정사용된 데이터에 엘투놈의 제곱 우리는 이거를 이 데이터 정사용된 데이터의 분산으로 해석해 볼 수가 있습니다. 그때 그 분산을 가장 최대화하는 단위 벡터 브를 찾는 문제로 우리가 이 문제를 해석할 수가 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 3,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 1064,
      "char_count": 1974
    },
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c004_2d3aa6",
      "content": "[AI Math] 4강_행렬이랑_더_친해져보자\n\n다. 앞에서는 행벡터를 기준으로 우리가 사용했었는데 지금과 같은 경우에는 열 벡터를 기준으로 표현했다는 사실을 잘 기억하시기 바랍니다. 그러면은 위 목적식을 우리가 통계학적으로 한번 해석해 보면요. 우리가 데이터를 브라는 저차원 공간으로 정사형시켰을 때 그 정사용된 데이터에 엘투놈의 제곱 우리는 이거를 이 데이터 정사용된 데이터의 분산으로 해석해 볼 수가 있습니다. 그때 그 분산을 가장 최대화하는 단위 벡터 브를 찾는 문제로 우리가 이 문제를 해석할 수가 있게 됩니다. 그렇다면 왜 분산을 가장 크게 만드는 것이 유효할까요? 저희는 저차원 공간으로 데이터를 압축을 시키고 싶은 것인데 어 분산이 가장 큰 경우에 단위 벡터 부위를 찾는 것이 원래 데이터가 가지고 있는 이런 정보들을 최대한 담고 있게 됩니다. 그 정보들이란 사실 분산 안에 담겨 있기 때문에 그렇습니다. 그러므로 가장 분산이 큰 벡터를 찾는 것이 가장 정보 손실이 덜한 네 가장 효율적으로 압축할 수 있는 벡터 부위를 찾는 문제로 해석할 수가 있는 것이죠. 네 그렇다면 이 최적화 문제를 풀려면 제약식이 있는 상황에서의 최적화 문제를 풀어야 되는데요. 보통 이런 문제를 풀 때는 나그랑지 승수법이라는 거를 우리가 적용하게 됩니다. 네 라그랑지 승수법은 이 강의에서는 다루지 않겠지만 우리가 이제 그 미분이라는 걸 통해서 라그랑지 승수법으로 최적화된 제약식을 구할 수는 있는데 이거의 결과값만 한번 살펴보시게 되면요. x랑 x의 전치 행렬로 이루어져 있는 두 행렬의 곱으로 이루어져 있는 이 행렬을 가지고 그다음에 v라는 벡터를 곱했을 때 그 결과가 어떤 상수 람다에다가 벡터 부위를 곱한 녀석의 결과가 됩니다. 참고로 이 람다는 라그랑지 승수법에서 사용되는 람다를 가져온 것인데 이 결과를 잘 보시면요. x랑 x의 전치 행렬 그리고 곱하기 v 벡터를 곱했을 때 이 v 벡터의 스칼라 곱의 형태로 표현되는 거니까 아 이 x랑 x 전치 행렬에 고유 벡터와 그리고 고유 값을 찾는 문제가 된다는 것을 앞서 살펴보실 수 있게 됩니다. 그러면은 이 결과를 가지고 위에 있는 목적지에 한번 넣어보겠습니다. 그러면 스 스 전체 행렬에 브를 곱한 거는 람다 부로 바꿀 수 있기 때문에 바꿔서 넣어주면 부의 행벡터 곱하기 람다 부가 되니까 람다 곱하기 브의 엘투 노름의 제곱으로 표현이 되는데요. 브의 제약 식이 엘투노름 값이 1위 돼야 된다는 조건이 있었기 때문에 이걸 대입해 주게 되면은 람다가 됩니다. 즉 다시 말해 저희가 찾고자 하는 부위가 만족하는 목적식은 이 람다가 돼야 된다는 목적식으로 표현할 수가 있고요. 이 말은 어떤 데이터를 브라는 벡터로 정사용시켰을 때 가장 효율적인 정보 압축이 가능한 후보들은 모두 고유 벡터가 된다는 사실입니다. 그렇다면 이 값을 맥시마이즈 해주는 네 가장 큰 값을 고르라고 한다면은 이 고유값 중에서 가장 큰 값을 고르면 되겠죠 네 그 고유 값을 우리가 프린시플 컴퍼넌트 즉 주성분이라고 부르게 되는 것이고 이와 같이 데이터를 압축할 수 있는 고유 벡터와 고유 값을 구해서 네 우리가 분석하는 거를 주성분 분석이라고 부르게 되는 것입니다. 정리하자면 데이터 공분산 행렬인 x와 x의 전치 행렬의 행렬 곱에 고유값 중에서 가장 큰 성분에 해당하는 고유 벡터의 데이터를 정사양하면은 정보 손실이 가장 최소화되는 것이고요. 이 데이터의 공분산 행렬을 우리가 고유값 분해로 다음과 같이 분해를 해주게 된 후에 이 고유값 중에서 가장 큰 값에 해당하는 고유 값과 거기에 해당하는 고유 벡터를 가지고 우리가 정보를 압축시켰을 때 가장 효율적인 그러니까 정보 손실이 가장 최소화된 압축이 가능한 것이고 이거를 우리가 이 고유값의 주성분이라고 부른다는 거를 기억하시면 좋겠습니다. 네 이번에는 고유값 분해와는 조금 다른 것이지만 고유값 분해와 좀 비슷한 형태로 사용해 볼 수 있는 특이값 분해를 한번 배워보도록 하겠습니다. 특이값이라는 거는 우리가 싱귤러 벨로라고 부릅니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 4,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 1072,
      "char_count": 1960
    },
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c005_eb907e",
      "content": "[AI Math] 4강_행렬이랑_더_친해져보자\n\n다. 네 이번에는 고유값 분해와는 조금 다른 것이지만 고유값 분해와 좀 비슷한 형태로 사용해 볼 수 있는 특이값 분해를 한번 배워보도록 하겠습니다. 특이값이라는 거는 우리가 싱귤러 벨로라고 부릅니다. 고유 값은 아이젠 밸류, 특기 값은 싱귤러 밸류인데요. 고유값 분해는 앞서 설명드렸듯이 이렇게 고유 벡터로 이루어져 있는 행렬의 역행렬을 계산해야 되기 때문에 우리가 어 좀 예외적인 즉 다시 말해서 어떤 행렬이 정사각인 경우에만 우리가 사용할 수가 있는데 이 행렬 a가 정사각일 때만 어 이 람다도 같은 엔바이엔 행렬이 되고요. 그리고 이 고유 벡터로 이루어져 있는 이 행렬도 실제로 역행렬이 계산 가능하게 되는 것이죠. 고유값 분해는 이렇게 좀 예외적인 케이스에서 사실 쓸 수가 있습니다. 즉 행렬이 정사각격일 때만 쓸 수 있는데 특이값 분해 즉 싱귤러 밸류 디컴포지션 스브디는 정사각 행렬뿐만 아니라 일반적인 행렬 즉 행과 열의 개수가 다를 때에도 쓸 수 있는 기법입니다. 네 조금 다른 성질을 가지고 있는데요. 어 고유값 분해랑 좀 비슷해 보이지만 네 다른 행렬 분해가 사용됩니다. 먼저 세 개의 행렬이 사용되는 걸 보실 수가 있는데 먼저 첫 번째는 유라는 행렬이 쓰이게 되고 그다음에 맨 끝에 v라는 행렬이 쓰이게 됩니다. 참고로 이 유랑 v는 이 고유값 분에서 썼던 고유 벡터랑은 전혀 다른 행렬이고요. 이 행렬들은 오쏘그날 행렬 즉 서로 독립인 벡터들로 이루어져 있는 행렬들이 되겠습니다. 그리고 이 가운데에 있는 시그마라는 행렬은 고유값 분해 이 가운데에 있는 것처럼 대각 행렬의 역할을 하지만 주의하실 점은 이 스퀘어 행렬 즉 정사각 행렬이 아니라 그 원래 행렬 에랑 똑같이 엔바이 엠 행렬의 대각 행렬이고요. 이때 그 대각 성분을 이루는 것을 우리가 싱귤러 밸류라고 부르게 됩니다. 그랬을 때 어 어떤 행렬 a가 정사각 행렬이 아니더라도 우리는 특이값 분해는 항상 할 수가 있다라는 게 알려져 있습니다. 참고로 에브디를 통해서 우리는 행렬을 다음과 같은 수직으로 분해를 하는 것이 가능합니다. 행렬 a를요 k는 14부터 이 행렬 a의 랭크 값까지 서메이션을 표현했을 때 이 싱글러 밸류에 해당하는 거를 시그마 k라고 쓰겠습니다. 이 시그마 케이가 가운데 시그마 행렬의 대각 성분이 들어가는 값이 되는 것이고 케 번째 대각 성분을 우리가 시그마 케이라고 부르게 될 것이고요. 이때 유케랑 그다음에 브케는 각각 유라는 행렬과 브라는 행렬의 구성 벡터가 되겠습니다. 이때 중요한 점은 이 케는 이 유에 들어가 있는 모든 벡터를 쓰거나 부에 있는 모든 벡터를 쓰는 것이 아니고 이 케는 14부터 에의 랭크 값에 해당하는 인덱스까지 이거를 쓰게 됩니다. 만약에 랭크 값이 전체 행렬에 이 행과 열의 개수보다 작게 되면은 모든 벡터를 쓰는 것이 아니라 이 유랑 브케의 이 랭크 인덱스에 해당하는 어 오쏘거나 벡터들만 쓸 수 있고 특기 값도 자연스럽게 이 랭크의 개수만큼만 우리가 정의를 할 수가 있게 됩니다. 기억하실 점은 어 고유값 분해랑 달리 에브디는 항상 일반적으로 모든 행렬에 적용할 수 있다라는 사실을 기억하시면 되겠습니다. 네 그래서 앞서 살펴본 예제가 바로 역행렬을 계산할 수 없을 때 유사 역행렬을 이용해서 무어 펜로즈 역행렬을 계산하는 거라고 말씀을 드렸는데요. 이때 행렬 a의 랭크값이 이 행과 열의 최솟값 중 하나랑 같을 때 이 무얼 펠로즈 역행렬을 사용할 수 있다고 설명을 드렸었는데요. 그렇다면 만약에 랭크 값이 이 두 개의 행과 열의 최솟값보다 만약에 같지 않으면 즉 2개보다 만약 작으면 어떻게 될까요? 이 경우를 프랭크가 아니다라고 표현하게 되는데 플랭크가 아니어도 우리는 앞서 배웠던 SVD를 이용해서 유사 역행렬 대신에 우리가 사용할 수가 있게 됩니다. 즉 어떤 행렬 a가 있을 때 그 행렬 a를 특이값 분해로 이렇게 분해를 한 후에요. 이 행렬 a의 유사 역행렬을 다음과 같이 정리를 해주게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 5,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 1056,
      "char_count": 1950
    },
    {
      "id": "transcript_ai_math_4강_행렬이랑_더_친해져보자_c006_88a7b2",
      "content": "[AI Math] 4강_행렬이랑_더_친해져보자\n\n다. 즉 어떤 행렬 a가 있을 때 그 행렬 a를 특이값 분해로 이렇게 분해를 한 후에요. 이 행렬 a의 유사 역행렬을 다음과 같이 정리를 해주게 됩니다. 즉 부의 전체 행렬을 다시 부로 바꿔주고요. 그다음에 특위 값의 대각 성분인 이 행렬에 이 대각 행렬을 가지고 이 역수를 취해준 행렬을 만들어 줍니다. 이 행렬은 어 앞서 살펴본 시그마라는 행렬의 대각 성분들의 역수를 취해주는 것이고 만약에 특기 값이 0인 경우에는 0으로 둡니다. 마지막으로 유라는 행렬의 전체 행렬을 마지막에 곱해주게 되면은 이 행렬에 플러스는 원래 행렬 a의 유사역 행렬로 사용하실 수가 있게 됩니다. 만약에 행렬 a가 풀 랭크인 경우에는 앞서 보여드렸던 무어 펠러즈 역행렬의 이 공식과 그리고 에브이로 구한 이 공식과 결과적으로 같다는 것을 여러분이 확인해 볼 수가 있고요. 네 결론적으로 어떤 행렬 a가 굳이 풀 랭크가 아니더라도 에브드를 통해서 우리가 유사역 행렬처럼 사용할 수가 있게 됩니다. 네 그래서 선형 회귀 분석의 경우에서 우리가 유사 역행렬을 이용해 가지고 선형 회귀 분석을 할 수가 있었죠. 이 경우 무어 펠로즈 역행렬을 이용해서 l2 손실 함수를 최소화하는 와 h 즉 베타를 찾는 게 목적이라고 말씀을 드렸는데요. 네 우리는 사실 이 유사역 행렬을 쓰지 않고 데이터 행렬에 SVD를 적용해서 유사 역행렬을 계산해 가지고 회귀 분석에 적용할 수가 있습니다. 네 이거의 장점은 실제 역행렬을 계산하는 것보다 계산 복잡도가 낮다는 것입니다. 네 에브d를 사용하는 게 실제로는 역행렬을 계산하는 것보다 훨씬 더 나은 측면이 있는 것입니다. 그래서 여러분들께서 실제로 선형 회기를 수행하실 때는 역행렬을 계산하는 거는 이론적으로는 가능하지만 실제로 역행률을 계산하는 것보단 에브디 연산을 이용해서 유사 역행률로 계산하면은 비슷한 정확도로 실제로 선형 회귀 분석이나 연립 방정식 풀이가 가능하다는 점을 기억하시면 좋겠습니다. 사실 앞서 지난 시간에 예제에서 보여드렸던 리니어 아이즈 브라의 피 인버스 함수는요 원래는 무어 펠로즈 역행렬을 계산하는 공식 그 자체보다는 에브디를 이용해서 유사 역행렬을 계산하기 때문에 비슷한 그 결과를 낼 수 있었다라는 거를 기억하시면 좋겠습니다. 네 에브디는 굉장히 많이 쓰이고요. 선형 행위 분석뿐만 아니라 실제로 행렬 분해를 이용한 여러 애플리케이션에서 많이 사용되기 때문에 여러분들께서 꼭 기억하시고 에브디 연산이 역행렬을 계산하는 것보다 더 효율적이다라는 점도 기억하시면 좋겠습니다. 네 이것으로 행렬 강의에 대해서 마치도록 하겠습니다. 수고하셨습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "4강_행렬이랑_더_친해져보자.json",
        "lecture_name": "4강_행렬이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "4강",
        "lecture_title": "행렬이랑_더_친해져보자",
        "chunk_idx": 6,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aa17282ea7186efd6f67417a6de543ae9e58d8178dd270d4f435c80f93ff4244"
      },
      "token_estimate": 711,
      "char_count": 1301
    }
  ]
}