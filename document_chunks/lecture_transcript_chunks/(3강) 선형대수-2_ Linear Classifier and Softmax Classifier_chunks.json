{
  "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
  "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
  "course": "AI Math",
  "total_chunks": 17,
  "chunks": [
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c000_f253ed",
      "content": "[강의 녹취록] 과목: AI Math | 강의: 3강 | 제목: 선형대수-2_ Linear Classifier and Softmax Classifier\n\n안녕하세요. 여러분 어 저번 시간에 이어서 머신 러닝 라이프 사이클을 강의하고 있는 김수경입니다. 저번 시간에 저희가 리얼리스트 네이버 클래시 파이어에 대해서 배워봤는데요. 이번 시간에서는 리얼리스트 네이버 클래시 파이어를 보완한 새로운 뉴럴넷의 개념을 도입한 리니어 클래식 파이어랑 소프트맥스 클래스 파이어를 배워보도록 하겠습니다. 그래서 사실 오늘 다룰 내용이 되게 많고 테크니컬한 내용들이 많아요. 실제적으로 이 리얼스 네이버 클래스 파이어의 단점을 보완한 이 리니어 클래스 파이어에서부터 소프트맥스 클래스 파이어 그리고 이 클래스 파이어를 트레이닝 하기 위한 로스 펑션 손실 함수라고 써 있죠 그리고 이 로스 펑션을 이용해 가지고 클래식 바이어를 트레이닝하는 최적화 방법까지 배울 것이기 때문에 어 여러분들 잘 따라오시길 바랍니다. 자 먼저 기본적인 이 리니어 클래식 파이어를 배워보도록 하겠습니다. 저번 시간에 약간 리니어 클래시 파이어에 대해서 배워봤는데요. 다시 리뷰하는 차원에서 내용을 리뷰하면요. 기본적으로 저희가 저번 시간에 배웠던 그 리얼리스트 네이버 클래시파이어는 기본적으로 아이디어가 어떤 쿼리가 들어오면은 그 쿼리랑 가장 가까운 k개의 이그 샘플 데이터를 트레이닝 데이터 셋을 찾아 가지고 그곳에 그 유사도를 계산해서 가장 유사한 케게 데이터만 보고 그 케게 데이터의 레이블을 다수결 해 가지고 그 타겟 그 저희 그 쿼리 데이터의 레이블을 예측하는 식으로 클래시 파이어를 했어요. 그래 가지고 기본적으로 아이디어가 모든 트레이닝 이그 샘플을 메모라이즈 하고 있어야 된다는 가정을 가지고 있었죠 그런데 그렇게 하면은 너무 공간적으로도 비효율적이고 테스트 타임에 시간이 많이 걸리는 단점이 있었어요. 그래서 이 다른 접근을 한번 해보도록 해요. 이런 리얼리스트 네이버 클래스 파이어 대신에 어떤 슈퍼바이즈 러닝을 이용해 가지고 데이터를 다 학습해서 그 데이터를 학습한 결과가 되는 요 펑션 f를 배워가지고 요 프만 있으면은 우리가 트레이닝 데이터 데이터셋을 다 외우지 않아도 어떤 쿼리 이미지 셀에 대해서 효율적으로 클래시파이 할 수 있는 이 함수 프를 갖다가 슈퍼바이즈 러닝으로 배워보고자 합니다. 그래서 기본적으로 이 프라는 함수의 파라미터를 데이터로부터 우리가 학습시켜서 배우는 이런 어프로치를 파라매트릭 어프로치라고 그래요. 여기서 한국말로는 매개 변수적인 접근이라고 그랬죠. 그래서 이런 파라매트릭 어프로치를 이용해서 리니어 크레스 파이어를 배워보도록 할 거예요. 자 그러면은 우리가 어떤 펑션 프를 갖다가 트레이닝 데이터셋으로부터 배운다고 그랬는데 어 그러면 이 펑션 프는 어떤 형태여야 될까요라는 질문이 어 나올 수가 있어요. 그래서 이 펑션 에프를 갖다가 모델링 하기 위해서 이 펑션 프를 모델링하는 여러 가지 방법이 있을 테지만 가장 간단하고 쉬운 요 리니어 클래시 바이어 선형 함수부터 시작해 보도록 하겠습니다. 자 이 이미지는 기본적으로 무수한 그 숫자로 이루어진 픽셀 값들로 이루어져 있다고 그랬어요. 그래서 이미지가 2차원이니까 알지비 채널이 있어서 이미지는 기본적으로 3차원 행렬이라고 그랬습니다. 그러면은 이 어 파라매트릭 어프로치를 이용해서 어떻게 만드냐면은 각 픽셀에 해당하는 매개 변수 더블의 가중치들을 우리가 학습시켜 보도록 할 거예요. 그래서 기본적으로 이 인풋 이미지랑 똑같은 크기의 매개 변수 w를 만들 텐데요. 그러니까 만약에 이미지가 2D면은 2차원이면은 매개 변수 더블도 투지가 되겠죠. 그리고 같은 크기여야 될 거예요. 그래서 각각의 픽셀에 매핑되는 매개 변수 더블의 파라미터 값들이 있을 거예요. 그래서 기본적으로 굉장히 쉽게 각각의 픽셀에 매핑되는 이 더블유라는 매트릭스의 엘레멘트랑 이미지 픽셀 값이랑 곱해가지고 얘랑 얘랑 곱하고 그 다음 값 얘랑 얘랑 곱하고 이걸 다 해가지고 그거를 갖다가 다 합계를 낼 거예요. 그리고 그러면은 어떤 값이 나오겠죠 그리고 그 값이 어 우리가 원하는 어떤 클래스를 갖다가 분류할 수 있는 어떤 레이블이 될 수 있도록 트레이닝을 할 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 0,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1134,
      "char_count": 2070
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c001_ab95df",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 이게 기본적으로 리니어 클래스 바이어에요. 기본적으로 다시 한 번 말하자면 이미지가 스가 있으면은 이 이미지랑 똑같은 크기의 행렬인 파라미터 더블 매트릭스를 우리가 트레이닝을 하는데 이 이 더블유를 트레이닝 한 다음에 어떻게 레이블을 그러니까 어떻게 아웃풋을 예측을 하냐면은 이 이미지의 픽셀 값이랑 요 가중치 혹은 파라미터 스페이스인 블라는 행렬에 매핑되는 웨이 값을 이렇게 곱해서 서로 다 더한 다음에 각각의 클래스에 해당하는 점수를 갖다가 우리가 예측을 하는 겁니다. 예를 들어서 우리가 멀티 레벨 클래스 파이어를 한번 생각을 해볼게요. 이미지 클래시피케이션 테스크에서 이미지를 갖다가 여러 개의 클래스 중에 하나로 클래시파이 하는 거예요. 이 이미지는 에어플레인일 수도 있고 뭐 볼드일 수도 있고 패스일 수도 있고 여러 가지 클래스가 있겠죠. 가령 클래스가 10개가 있다고 생각을 해봐요. 그러면은 우리가 배워야 되는 이 w 파라미터는 클래스 개수만큼이 돼요. 즉 10개의 w 파라미터를 우리가 배우게 되는 거죠. 그리고 다시 한 번 이제 또 리뷰를 하면은 각 클래스가 올바른 이미지에 대해서 가장 높은 점수를 받도록 더블의 값을 결정해야 됩니다. 그 말은 무슨 말이냐 하면은 어 우리가 10개의 클래스가 있는 클래스 파이어를 트레이닝 한다고 그랬죠 그러면은 우리가 만드는 이 함수 프라는 거는요 블스라는 꼴로 나타내어지는데 이 더블엑스의 값이 기본적으로 어떤 그 캣이라는 분류가 되는 확률 값이 되는 겁니다. 그래서 이 더블가 10개가 있으니까 우리가 각 클래스에 대해서 그 각 클래스의 레이블에 해당하는 어떤 점수를 다 예측을 할 수 있고 그 점수 중에 최대가 되는 값을 분류를 하게 되는 거죠. 자 그래서 이렇게 이제 리니어 모델로 리니어 클래스 파이어를 우리가 모델링 할 수가 있는데 이제 차원을 한번 확인해 볼게요. 실제로 모델링할 때 각 텐서 즉 행렬의 차원을 갖다가 계산하는 게 가장 중요하거든요. 그래서 한번 볼게요. 가령 우리가 한 이그 샘플로 이 이미지를 32바이 32바이 3이라고 그럴게요. 즉 이미지의 크기는 가로축은 32픽셀, 세로축은 32픽셀 그리고 채널이 3개 있으니까 RGB 채널 이렇게 32 32b 3개의 이미지 사이즈를 가지고 있는 인풋이 들어온다고 가정해 봅시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 1,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 627,
      "char_count": 1188
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c002_342f1d",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그러면은 이 32b 32b 3을 다 곱하면은 전체 픽셀의 값은 그러니까 전체 데이터 셋의 사이즈는 3072개가 되겠죠. 그래서 이 3072개가 되는 파라미터들이 인풋으로 들어오면은 이 인풋을 통해서 우리가 예측하고자 하는 레이블 y는 기본적으로 10개여야 돼요. 왜냐면은 우리가 지금 이 모델을 가정하기를 10개의 다른 클라스가 있다고 가정을 했기 때문이죠. 그러면은 이 인풋을 가지고 10개의 레이블을 예측하기 위해서 우리의 펑션인 프는 어떻게 변수의 차원을 가지고 있어야 될까요? 먼저 요 프의 아웃풋은 요 레이블 아웃풋과 똑같으니까 요 프를 통과한 어떤 아웃풋의 값은 그 차원이 뭐겠어요? 10바이1이죠. 왜냐하면 우리가 10개의 클라스가 있으니까 10개의 클라스 바이 1 하나의 그 벡터가 이렇게 나오기를 바래요. 그런데 우리는 얘를 갖다가 더블 x로 모델링을 할 거예요. 그러면은 여기에서 x는 우리의 이미지죠. 우리가 아까 계산했듯이 3072개의 픽셀 값으로 이루어진 어떤 벡터로 우리의 스를 나타낼 수가 있어요. 그래서 우리의 x의 값은 3072바 1개의 차원을 가지게 될 거예요. 그러면은 여기서 우리가 볼 수 있는 게 3072바 1을 더블랑 곱했더니 아웃풋이 10바이 1이 돼야 돼요. 그러면 이 더블의 차원은 뭐가 되겠죠? 네 여러분 행렬 곱을 배우셨으면 금방 알 수 있을 거예요. 어떤 벡터 곱하기 x를 했더니 10바이 1의 사이즈 디멘전이 같게 나오려면은 이 더블도 10바이 3072가 돼야 돼요. 왜 그렇죠? 이 스가 3072바 1이니까 이 10바이 어떤 디멘전 스의 디멘전 바이 1은 여기 이렇게 지워져 가지고 10바이 1이 되니까 이 더블의 사이즈는 10바이 3072가 되야겠죠. 그쵸 그래서 이렇게 우선 우리가 변수의 사이즈를 10바이 3072로 정해 준 다음에 여기다가 한 가지 더 더해줘야 될 게 있어요. 비라고 되어 있는데 이 비가 바이어스 텀입니다. 자 이 바이어스 텀이 되게 중요합니다. 이 바이어스라는 게 뭘 의미하냐 우리가 이 바이어스라는 건요. 우리가 어떤 인풋 데이터를 보지 않고 이 인풋 데이터에 영향을 주지 않고 여전히 아웃풋에 영향을 주는 어떤 파라미터예요. 그래서 예를 들어서 어 가령 우리가 이 데이터셋을 통해서 애플을 갖다가 트레이닝을 하는데 이 데이터셋 스는 여러 가지 종류로 이루어진 어떤 이미지 데이터일 거예요. 그래서 이 이미지 데이터를 통해서 이 10개의 클라스를 갖다가 클래스 파이 하는데 가령 우리가 생각해 볼 때 이 이미지 데이터 트레이닝 데이터 중에서 유독 고양이 데이터가 많다고 생각을 해봐요. 그러면은 이 인풋이 뭐가 들어가든 상관없이 고양이일 확률이 많잖아요. 왜냐하면 트레이닝 데이터셋이 고양이로 많이 이루어져 있으니까 그래서 인풋 보지 않고도 우선 고양이라고 예측을 하는 거예요. 이렇게 데이터 디스트리뷰션이 어떻게 스큐 되어 있을 때 그러니까 데이터 디스트리뷰션이 이렇게 이분하게 디스트리뷰 되어 있지 않고 하나의 클래스만 집중적으로 분포되어 있을 때 어떤 특정 클래스만 많이 가지고 있을 때 이렇게 많이 있는 데이터로 바이어스를 잡아줌으로써 실제 웨잇은 어떤 데이터 자체의 본성에 더 집중하게 이렇게 디자인을 해주는 겁니다. 그래서 이 바이어스는 실제로 어떤 그 픽셀 밸류와 상관없는 어떤 데이터에 내재된 디스트리뷰션이나 본성을 모델링하는 데 쓰이게 됩니다. 그래서 이 아웃풋의 사이즈가 아까 말했듯이 10바이1이죠. 10개의 클래스니까 그래가지고 이 바이러스의 사이즈도 10b1이 됩니다. 한국말로 하면 편향이라고 그래요. 여기 이렇게 써 있듯이 데이터 x랑 상호작용하지 않고 출력에 직접적으로 영향을 미치는 어떤 파라미터를 바이러스 한국말로는 편향이라고 부릅니다. 네 그래서 이렇게 어 우리가 리니어 클래식 파이어를 FXW는 wx 플러스 비 이렇게 리니어한 그러니까 선형 함수로 이렇게 표현을 할 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 2,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1034,
      "char_count": 1969
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c003_600596",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 이 아웃풋의 사이즈가 아까 말했듯이 10바이1이죠. 10개의 클래스니까 그래가지고 이 바이러스의 사이즈도 10b1이 됩니다. 한국말로 하면 편향이라고 그래요. 여기 이렇게 써 있듯이 데이터 x랑 상호작용하지 않고 출력에 직접적으로 영향을 미치는 어떤 파라미터를 바이러스 한국말로는 편향이라고 부릅니다. 네 그래서 이렇게 어 우리가 리니어 클래식 파이어를 FXW는 wx 플러스 비 이렇게 리니어한 그러니까 선형 함수로 이렇게 표현을 할 수가 있습니다. 그러면은 우리가 딱 봤을 때 이 더블엑스 더하기 비 이퀄 이렇게 행렬의 곱으로 표현을 할 수 있겠죠. 더블 콤마 비랑 스 콤마 원이랑 이렇게 곱한 행렬 곱으로 표현을 할 수가 있을 거예요. 그래서 이 앞에 행렬은 더블 프라임이라고 부르고 어 로켓이 되어 있는 행렬은 엑스프라임이라고 해가지고 이 프라는 펑션 이퀄 더블 프라임 곱하기 엑스프라임 이렇게 쓸 수가 있어요. 잘 따라오고 계시죠? 그냥 행렬 곱으로 바꾼 거예요. 그러면은 우리가 이제 이 FX를 블스 이렇게 심플한 텀으로 다시 이렇게 치환해 가지고 표현을 할 수가 있지만 이 안에는 항상 바이오스 텀이 있다는 것을 기억해 두세요. 그래서 어 이렇게 리니어 클래스 파이어를 우리가 정의를 했습니다. 그래서 여기까지가 이제 리니어 클래스 파이어고 이 리니어 클래식 파이어는 우리가 여기서 이제 파라미트릭 모델이라고 했죠. 매개 변수적인 모델이라고 한 어 그 모델링 방법에 가장 심플한 폼이라고 생각을 할 수가 있어요. 우리가 어떤 인풋 스랑 아웃풋 와의 관계가 선형적이라는 가정을 통해서 이 파라미터링 모델로 만든 가장 간단한 어떤 클래스 파이어 모델입니다. 그래서 우리가 이 리얼리스트 네이버 클래스 바이어 저번 시간에 배운 거랑 이 리니어 클래스 바이어 혹은 파라매트릭 모델과 비교를 했을 때 파라매트릭 모델이 굉장히 장점이 많아요. 첫 번째로 학습이 완료되면은 요 가중치 블만을 필요로 해요. 따라서 우리가 모든 트레이닝 셋을 기억하지 않아도 어떤 특정 테스트 데이터에 대해서 우리가 이 트레이닝 한 더블만 가지고 있으면은 어떤 엑스 데이터든 어 이 펑션을 갖다가 어 러닝을 해 가지고 아웃풋을 예측을 할 수가 있겠죠. 그래서 그냥 오비어스하게 그러니까 뭐겠어요? 리얼리스트 네이버에 비해서 훨씬 공간 효율성이 높죠. 왜냐하면 우리가 트레이닝 데이터 셋을 모든 메모리에 저장하지 않아도 되니까 그렇죠 그리고 두 번째는 테스트를 할 때 단일 행렬과 벡터의 곱인 블스로 예제를 평가를 할 수가 있죠. 그렇죠 우리가 어떤 임의의 테스트 데이터 스가 들어왔을 때 예측을 하기 위해서는 이 행렬 곱만 하면 돼요. 따라서 모든 훈련 데이터와 비교하는 것보다 훨씬 빠르죠 리얼리스트 네이버 클래스 파이어는 어떻게 예측을 했어요? 모든 훈련 데이터를 보면서 디스턴스를 계산을 해서 그중에 k겔을 다수결해가지고 레이블을 예측했잖아요. 이렇게 모든 트레이닝 데이터를 우리가 포루프를 돌면서 디스턴스를 계산하지 않아도 이 더블만 있으면 더블 곱하기 엑스를 해 가지고 굉장히 빠르게 예측을 할 수 있다는 굉장히 큰 장점을 가지고 있습니다. 그래서 이제 저희가 이제 보고 있는 이미지 클래시피케이션 경우에 이 리니어 클래스 파이어의 그 예측 과정을 한번 보도록 합시다. 저희가 가령 이렇게 더블의 값을 트레이닝을 했다고 봅시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 3,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 915,
      "char_count": 1702
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c004_19f335",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 이제 저희가 이제 보고 있는 이미지 클래시피케이션 경우에 이 리니어 클래스 파이어의 그 예측 과정을 한번 보도록 합시다. 저희가 가령 이렇게 더블의 값을 트레이닝을 했다고 봅시다. 그리고 우리가 이 이그샘플에서 보는 경우에는 멀티 레이블 클래스 파이어예요. 우리가 이 이미지를 보고 이 이미지가 고양이인지 개인지 아니면 배인지 이 3개의 클래스 중에 어떤 거에 속하는지를 클래식 파이 하는 테스크라고 생각을 해볼게요. 그러면 저희가 모델링 한 것처럼 이 wx 플러스 b라는 꼴에다가 이 x에 해당하는 부분에 테스트 데이터를 갖다가 이렇게 길게 하나의 백도로 늘어뜨려 가지고 넣은 다음에 계산을 하게 돼요. 그러면은 레이블이 3개니까 그러니까 저희가 분류하려고 하는 클라스가 3개니까 아웃풋도 3 곱하기 1에 텐서로 매트릭스로 이렇게 y가 나오겠죠. 그리고 각각의 로에 해당하는 게 그 각각의 그 분류가 되는 대상의 어떤 점수처럼 이렇게 생각이 되면은 됩니다. 그래서 이 w를 보시면은 이렇게 곱해가는 과정이 첫 번째로 더블의 첫 번째로랑 인풋이랑 곱해가지고 이 캣의 스코어가 나오고 그 두 번째 로우랑 그 x를 갖다가 곱해가지고 도그의 스코어가 나오고 세 번째 로우랑 x를 곱해서 10의 스코어가 나오는 것을 볼 수가 있어요. 즉 w라는 파라미터에 각각의 로우에 해당하는 게 그 어떤 인풋의 각각 클래스에 해당하는 어떤 웨이 값이라고 생각을 할 수가 있겠습니다. 네 그러면은 이제 리니어 클래식 파이어를 기하학적인 관점에서 한번 해석해 볼게요. 저희가 저번 시간에 요 어 2차원의 리니어 클래스 파이어에 대해서 기하학적인 관점에서 한번 해석을 해봤었는데 다시 한번 보면은 이 여기 그 이미지 클래스 파이어 같은 경우에는 다차원이에요. 하지만 그 리니어 모델의 골은 똑같아요. y 콜 더블 x 플러스 b 똑같죠. 그리고 여전히 w는 기울기이고 b는 y 절편이라고 그랬어요. 그래서 여기서 한번 보시면은 이 우리가 이렇게 배우는 클래식 파이어가 이 기하학적으로 표현을 해보면은 어떤 오브젝트나 이미지를 클래식 파이어 하는 디시전 바운더리의 식을 나타내게 되고 이 슬로프 즉 기울기가 어 이 더블유가 되겠죠 그리고 비는 y 절편이 돼요. 그래 가지고 기본적으로 우리가 이 더블 값을 갖다가 변경을 하게 되면 어떤 효과가 나오냐면 이 기울기가 바뀌니까 더블 값이 변경되면 이 해당 디시전 바운더리가 회전을 하게 됩니다. 그리고 이 비라는 그 바이어스 텀이 변경이 되면은 해당 디시전 바운더리가 위아래로 이렇게 이동을 하게 되겠죠. 그러면 이거는 이제 이미지 클래시피케이션에서 특화된 어떤 해석 방법인데요. 우리가 이렇게 이미지를 통해서 그 이미지의 클라스를 갖다가 어 클래시파이 하는 어떤 웨이 파라미터를 배웠어요 그러면은 이 웨이 파라미터가 도대체 무슨 의미가 있느냐 각각의 클래스별로 다른 웨이 파라미터를 이렇게 배웠죠. 그래서 우리가 배운 이 웨이 파라미터들을 이렇게 시각화해 보면은 되게 재밌는 결과가 나와요. 이 여기 예를 들어서 이거는 차에 해당하는 웨이 파라미터를 시각화한 거고 얘는 이 말이라는 클라스에 해당하는 웨이 파라미터를 시각화했어요. 그랬더니 여러분들 보시는 바와 같이 각각의 클래스에 대해서 어렴풋이 그 클래스에 해당하는 어떤 어 뭐 그 오브젝트의 이미지가 그 이미지의 패턴이 이렇게 그림처럼 보이는 거를 볼 수 있어요. 특히 이 말 같은 경우에는 말이 이렇게 초원에 이렇게 서 있는 그런 어떤 패턴이 보이잖아요. 그래서 이렇게 이 웨이 파라미터를 시각화하면 대충 얘가 데이터를 통해서 뭘 배웠는지를 우리가 눈치챌 수가 있습니다. 그래서 이 웨이 파라미터는 이 비주얼 클래시피케이션 테스크에 한해서 어떤 템플릿처럼 작용을 하는 겁니다. 어떤 오브젝트를 각각의 클래스별로 클래스 파일 할 수 있는 그 오브젝트의 패턴을 갖다가 추상화하는 템플릿 같은 역할을 하는 것을 알 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 4,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1066,
      "char_count": 1974
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c005_2b9a02",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 이 웨이 파라미터는 이 비주얼 클래시피케이션 테스크에 한해서 어떤 템플릿처럼 작용을 하는 겁니다. 어떤 오브젝트를 각각의 클래스별로 클래스 파일 할 수 있는 그 오브젝트의 패턴을 갖다가 추상화하는 템플릿 같은 역할을 하는 것을 알 수가 있습니다. 그래서 어 이미지가 딱 들어왔을 때 이렇게 웨이 파라미터를 갖다가 이미지랑 이렇게 딱 곱해가지고 그러니까 각각의 그 픽셀에 해당하는 웨이 파라미터를 이렇게 곱해가지고 어 이렇게 점수를 내잖아요. 이 오퍼레이션이 뭘 하는 거냐면요. 그 우리가 배운 그 웨이 파라미터의 템플릿과 나의 테스트 이미지랑 매칭을 해 가지고 얼마만큼 나의 테스트 이미지가 그 우리 그 모델에서 배운 템플릿에 해당하는 이 블랑 매칭이 되는지를 점수화할 수 있다고 우리가 또 이렇게 이해를 할 수가 있습니다. 그렇죠 그러면은 다시금 이제 리얼리스트 네이버랑 비교를 해볼게요. 리얼리스트 네이버의 경우는 다시 또 리마인드를 해보면 어떤 테스트 이미지가 들어왔을 때 이미 저장된 우리의 모든 트레이닝 데이터랑 나의 테스트 데이터랑 이렇게 매칭을 해 가지고 클래스 파일을 했어요. 요 리니어 클래시피케이션 테스크도 리니어 클래스 파이어도 결국은 같은 논리예요. 하지만 이 리얼리스트 네이버가 트레이닝 데이터 n 개랑 나의 테스트 데이터랑 다 비교를 하는 것과는 달리 이 리니어 클래스 파이어는 우리가 그 클래스의 개수에 해당하는 어떤 템플렛을 갖다가 학습을 시키고 그 템플릿의 개수만큼만 나의 이미지랑 비교한다는 게 다른 거예요. 그래서 예를 들어서 우리가 어 10개의 클래스로 이루어진 그 이미지 클래시피케이션 테스크를 푼다고 생각해 보세요. 그러면은 가령 이 클래스의 개수를 케이라고 하면은 케이는 10개가 되겠죠. 그러면은 테스트 이미지가 들어왔을 때 우리는 이 테스트 이미지랑 케이의 템플렛이랑만 비교를 하면 돼요. 만약에 클라스가 10개면은 10번만 비교를 하면 돼요. 그래서 훨씬 빠르죠. 리얼리스트 네이버의 경우에는 모든 트레이닝 데이터 셋 예를 들어 트레이닝 데이터셋이 어 캐피털 엔 개가 있으면은 그 캐피털 엔 개를 갖다가 다 비교를 해야 되니까 훨씬 오래 걸리지만 이 리니어 클래스 파이어는 이 클래스 개수인 케이개만큼만 비교하면 되니까 훨씬 빠르다 라는 이런 큰 장점을 이렇게 보여드렸습니다. 그래서 가장 기본적인 리니어 클래스 파이어를 배웠으니까 이제 조금 더 어드밴스 된 소프트 맥스 클래스 파이어를 배우도록 할게요. 이것도 그렇게 다르지 않아요 어 저희가 리니어 클래스 파이어를 배웠는데 리니어 클래스 파이어에 한계가 있어요. 우리가 결국은 리니어 클래스 파이어를 통해서 어떤 클래스의 스코어를 갖다가 예측을 했어요. 개면 개 고양이면 고양이 아니면은 쉽이면 쉽 이렇게 각각의 오브젝트에 대한 어떤 스코어를 갖다가 점수를 갖다가 모델이 예측을 하게 했는데 이런 질문을 할 수가 있어요. 그래서 점수를 갖다가 어떤 숫자로 예측을 했는데 결국 이 점수란 어떤 의미인가요? 이런 점수라고 하는 어떤 숫자의 느낌이 팍 오지 않습니다. 이게 큰 건가요? 작은 건가요? 이런 거를 알 수가 없는 거예요. 그래서 문제는 우리가 예측하는 점수라는 게 사실은 어떤 바운드가 없어요 제한이 없어요. 그래서 임의로 굉장히 커질 수도 있고 되게 작아질 수도 있어요. 스코어라는 게 천일 수도 있고 마이너스 5천일 수도 있고 이렇게 제한이 없습니다. 그래서 이 제한이 없기 때문에 해석하기가 되게 어려워요. 가령 점수가 437.9라고 그러면은 그 의미가 뭔지를 알 수가 없는 거예요. 그래서 우리가 원하는 거는 우리가 예측하는 스코어가 어떤 임의의 바운드가 없는 숫자가 되는 게 아니고 확률처럼 0과 1 사이로 딱 떨어지게 만들었으면 좋겠습니다. 그래서 어 0과 1 사이의 경계 점수를 얻어서 우리의 스코어를 어떤 확률로 해석할 수 있다면 더 좋을 것입니다라고 썼습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 5,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1062,
      "char_count": 1964
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c006_08efeb",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 이 제한이 없기 때문에 해석하기가 되게 어려워요. 가령 점수가 437.9라고 그러면은 그 의미가 뭔지를 알 수가 없는 거예요. 그래서 우리가 원하는 거는 우리가 예측하는 스코어가 어떤 임의의 바운드가 없는 숫자가 되는 게 아니고 확률처럼 0과 1 사이로 딱 떨어지게 만들었으면 좋겠습니다. 그래서 어 0과 1 사이의 경계 점수를 얻어서 우리의 스코어를 어떤 확률로 해석할 수 있다면 더 좋을 것입니다라고 썼습니다. 그렇죠 이렇게 이미지가 들어왔을 때 나오는 스코어가 어 그 스코어의 그 밸류의 값이 들쭉날쭉이니까 해석하는 데 어려움이 있다는 거죠. 그래서 이렇게 확률로 아웃풋이 나오게 하기 위해서 이미지가 각 클래스에 속할 확률이라는 것을 정의를 해볼게요. 가령 우리가 바이너리 클래시피케이션 테스크를 생각을 해볼게요. 우리가 딱 두 개의 클래스밖에 없어요. 그러면은 스코어드 두 개죠 s1이랑 s2 그러면 비교가 쉬울 거예요. 만약에 에스원이 스2보다 크면은 그 클래시피케이션의 결과는 에스원이죠. 그리고 반대로 에스원보다 스투가 크면은 그 클래시피케이션의 결과는 스투가 될 거예요. 그쵸 이렇게 두 개만 비교를 하면 돼요. 그래서 여기 이렇게 써 있듯이 만약에 그 두 개의 스코어 중에 1개가 크면 해당 이미지가 나머지 이미지의 클래스보다 클래스 원에 있을 확률이 높다고 생각을 할 수가 있습니다. 그래서 기본적으로 이 클래스의 스코어 간의 갭 이 스1 빼기 스2가 더 클수록 이 결과는 첫 번째 클래스일 확률이 높고 반대로 에스2 빼기 에스원이 크면 스2가 더 크다는 거잖아요. 그러면은 두 번째 클래스일 가능성이 높죠. 이렇게 두 개의 클라스만 있을 때 그 점수를 갖다가 우리가 상대적으로 비교를 하면서 쉽게 클래시피케이션 테스크를 할 수가 있어요. 이거를 한번 펑션으로 만들어 볼 거예요. 자 우리가 아까 말했듯이 이 점수의 차이 s2콜 스 빼기 2 s1이랑 s2의 두 가지 스코어가 있을 때 그 스코어의 차이가 이렇게 주어질 때 우리는 다음과 같은 함수를 필요로 해요. 기본적으로 우리는 아웃풋이 확률 분포를 따르도록 이 모델을 만들고 싶어요. 그래서 만약에 이 점수의 차이가 1에 가깝거나 아니면 0에 가깝거나 이 두 개 중에 하나만 있었으면 좋겠어요. 그리고 만약에 동률일 때 바이너리 클래시피케이션에서 두 개의 점수가 똑같을 때는 그러면은 이 어 우리의 그 아웃풋이 0.5가 됐으면 좋겠습니다. 이렇게 우리가 확률 분포를 만들고 싶다고 이렇게 먼저 디자인을 해 놓은 거예요. 그러면은 우리가 원하는 확률 분포는 이런 모양이 될 거예요. 즉 이 어떤 점수의 차이를 갖다가 가로축이라고 할 때 이 점수의 차이가 크면은 첫 번째 클래스로 나오고 점수의 차이가 작으면 다른 클라스가 나오고 이렇게 1과 0만 나오게 만들고 싶은 거예요. 그래서 이런 함수를 만들기 위해서 이런 시그모이드 펑션을 갖다가 갖다 쓸 수가 있어요. 요 시그모이드 펑션은 기본적으로 이렇게 익스포넨셜이 분모에 있는 이런 식 모양을 가지고 있는데 이거를 갖다가 그래프를 그리면은 이렇게 와가 1 아니면은 0으로 이렇게 레귤러라이즈 되게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 6,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 847,
      "char_count": 1596
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c007_69e3f2",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 이 펑션을 갖다가 여기 그 익스포넨셜에 어 위에다가 s 우리가 원하는 이 점수의 차이를 갖다가 넣으면은 만약에 첫 번째 클래스일 때는 이 점수의 차이는 s1 빼기 s2죠. 그리고 두 번째 클래스일 때는 이 점수의 차이는 s2b이 s1이에요. 그래서 이렇게 두 개의 다른 이 익스포넨셜 펑션으로 표현이 되게 될 거예요. 그러면은 이 펑션 요 시그모이드 펑션을 갖고 왔을 때 이 값이 정말 확률에 해당하는지를 확인해 볼게요. 우리가 원하는 어떤 성질을 갖고 있는지 확인을 해볼게요. 먼저 요 시그모이드 펑션을 보시면 알다시피 항상 y 값은 0과 1 사이에요. 그러니까 확률 분포로 사용을 할 수가 있겠죠. 왜냐하면 확률 분포는 항상 0과 1 사이니까 그리고 만약에 y가 c라는 클라스로 이렇게 확률이 정해졌을 때랑 y가 c2라는 클라스로 확률이 정해졌을 때 이 두 개의 확률을 더하면은 항상 1이 돼야 돼요. 이게 성립하는지 한번 볼게요. 먼저 첫 번째 클래스로 클래시피케이션 되는 경우에 이 그 클래시피케이션 스코어의 차이는 첫 번째 클라스가 더 크니까 s1 빼기 s2일 거예요. 그리고 만약에 두 번째 클래스로 클래시피케이션 됐을 때 이 확률의 그 차이는 s2가 더 크니까 s2 빼기 s1이 될 거예요. 그래서 이 두 개의 텀을 더하면 1이 나오는 거를 확인할 수가 있습니다. 여러분들 한번 스스로 한 번 계산을 해보세요. 그러면 1이 나오실 거예요. 그리고 이 소프트맥스 크래시 파이어를요. 또 그 바이너리 클래시피케이션 테스크가 아니고 멀티 레벨 클래시피케이션 테스크에도 적용을 할 수가 있어요. 무슨 말이냐면은 우리가 요 위에 이 10은 우리의 결과가 두 개의 클래스일 경우에만 해당되는 거잖아요. 그런데 만약에 클라스가 2개 이상일 경우 예를 들어서 10개의 클라스가 있는 경우에도 어 정확히 적용을 할 수가 있습니다. 어떻게 하면 되냐면요. 분모에다가 다 익스포넨셜 각각의 클래스에 해당하는 스코어를 익스포넨셜 스1 더하기 익스포넨셜 스2 해서 익스포넨셜 스n까지 이렇게 다 더해준 다음에 우리가 원하는 클라스의 그 확률을 2si로 이렇게 놓고 계산을 해주면 됩니다. 그러면은 똑같이 이 소프트 맥스 클래스 파이어를 갖다가 멀티 클라스에도 적용을 할 수가 있게 돼요. 자 이 예시로 다시 돌아와 봅시다. 그래서 우리가 이 측정된 확률 값에 따라서 우리가 여기 이 스코어를 계산을 했는데 그다음에 소프트맥스 클래시 파이어를 여기다가 더 씌워볼게요. 이 스코어에다가 소프트맥스 클래시 파이어를 갖다가 적용을 한 거예요. 즉 기본적으로 아까 앞 슬라이드에서 나온 이 식을 그대로 적용해서 이 스코어에다가 매기면은 어떻게 나오냐면은요 각각의 클래스가 될 확률이 나와요. 여기서 보시면은 여기 고양이가 될 확률이 거의 1에 가까운 즉 원 HDD % 100%의 확률로 고양이가 될 거라고 이렇게 예측을 하는 거를 볼 수가 있어요. 그냥 요 그 임의적인 스코어 밸류보다 이렇게 확률 분포로 나오니까 훨씬 더 이해하기가 쉬운 것을 볼 수가 있습니다. 자 그리고 우리가 아까 앞에서 그 바이어스라는 거를 설명을 했는데 비라는 텀이죠 이 바이어스는 어떻게 설정을 해야 될까요라는 질문이 생길 수가 있습니다. 이 가장 중요한 문제에 대해서 이야기하지 않았기 때문에 요 가중치라든지 아니면은 매개 변수 이 더블유의 값을 어떻게 정할 것인지가 약간은 엠비리오스 잘 이해가 안 될 거라고 생각을 합니다. 그래서 앞으로의 이제 렉처는요 요 가중치를 갖다가 어떻게 설정을 하는지에 대해서 배우도록 할게요. 자 먼저 이 어 어떤 그 리니어 클래스 파이어든 소프트 맥스 클래스 파이어든 뉴럴넷 기반 혹은 파라미터릭 모델에 기반한 어떤 그 모델을 트레이닝 하는 거는 이런 방법을 통해서 트레이닝이 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 7,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1009,
      "char_count": 1906
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c008_f595d7",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 이 가장 중요한 문제에 대해서 이야기하지 않았기 때문에 요 가중치라든지 아니면은 매개 변수 이 더블유의 값을 어떻게 정할 것인지가 약간은 엠비리오스 잘 이해가 안 될 거라고 생각을 합니다. 그래서 앞으로의 이제 렉처는요 요 가중치를 갖다가 어떻게 설정을 하는지에 대해서 배우도록 할게요. 자 먼저 이 어 어떤 그 리니어 클래스 파이어든 소프트 맥스 클래스 파이어든 뉴럴넷 기반 혹은 파라미터릭 모델에 기반한 어떤 그 모델을 트레이닝 하는 거는 이런 방법을 통해서 트레이닝이 됩니다. 기본적으로 우리가 먼저 어 모델의 형식을 갖다가 디자인해야 돼요. 우리가 쉬운 요 리니어 클래시피케이션 테스크를 먼저 보도록 할게요. 우리가 그 리니어 클래스 파이어에서는 FX가 더블스였어요. 이렇게 먼저 모델의 형식을 갖다가 디자인을 해요. 모델을 만드는 거죠. 그다음에 우리가 원하는 거는 이 블라는 매개 변수의 값을 데이터를 통해서 트레이닝 하는 거잖아요 이거를 어떻게 할까 처음에 가장 처음에는 이 더블의 값을 랜덤하게 설정을 해요. 랜덤 이니셜라이제이션을 하는 거죠. 어떤 랜덤한 값을 더블로 설정을 합니다. 자 그다음에 우리가 스랑 와는 가지고 있죠 어 인풋에 대해서 레이블 와는 이렇게 레이블 돼서 다 가지고 있을 거예요. 이렇게 랜덤하게 정해진 매개 변수에 대해서 x를 곱해가지고 y를 예측을 해요. 그래서 이 레이블 예측 값은 y 프라임이라고 써 있는데 이 y 프라임은 기본적으로 매개 변수랑 우리의 학습 데이터의 곱이 되겠죠. 그래서 이렇게 예측을 해요. 그럼 예측을 하면은 우리가 뭘 할 수가 있냐면은 이 예측 값이랑 실제로 그라운드 트루스 사실인 값 y를 갖다가 비교를 할 수가 있습니다. 이 예측 값은 우리가 와 프라임이라고 썼고 실제 관측값 그라운드 트루스 정답이 되겠죠. 요 정답 값은 y라고 그래요. 그래서 와이 프라임이랑 y를 빼면은 요 와이 프라임과 y의 차이는 이 모델이 얼마나 잘 하는지를 나타내는 어떤 로스 펑션이 되겠죠. 가령 와이 프라임 백기 와가 굉장히 크면 예측 값이 그라운드 트루스랑 너무 다른 거죠. 그러니까 모델이 되게 안 좋은 거예요. 만약에 이 와이 프라임이랑 y가 거의 비슷하거나 같아요. 그러면은 모델의 예측이 실제 관측값 그라운드 트루스랑 거의 똑같으니까 모델이 되게 잘 동작을 하는 거예요. 그래서 이렇게 추정값이랑 기준값의 차이를 이용을 해 가지고 우리의 모델이 얼마나 좋은지 나쁜지를 평가를 합니다. 그러면은 이 로스 그러니까 이 와이 프라임과 와의 차이를 통해서 이 매개 변수를 갖다가 최적화를 시킬 수가 있어요. 어떻게 최적화를 시키냐 와 프라임 빼기 y가 최소화가 될 때까지 즉 거의 0이 될 때까지 거의 0이 된다는 거는 뭐예요? 그 예측 값이랑 관측값이 거의 같다는 거죠. 즉 예측값이 거의 관측값에 일치할 때까지 이 w를 갖다가 최적화를 하는 겁니다. 그래서 이 앞에 프로세스를 갖다가 반복을 하면서 와 프라임 이퀄 와가 될 때까지 즉 우리의 모델의 예측값이 실제 관측 값과 같아질 때까지 이 최적화 과정을 반복을 하는 겁니다. 그러면은 여기서 이제 복잡한 수학적인 얘기가 나올 거예요. 먼저 제가 여기서 이렇게 여러 번 강조했듯이 우리가 그 모델을 통해서 예측한 값 와 프라임이랑 실제 그라운드 트루스 관측 값에 해당하는 y의 값을 비교를 해야 된다고 그랬어요. 어떻게 비교를 하냐 그거의 차이를 계산을 해요. 그거의 차이를 계산을 하는 거를 손실 함수라고 그래요. 여러 가지 방법들이 있어요. 영어로 하면은 로스 펑션이라고 하는데 여러분 로스펑션이라는 말 굉장히 많이 들으실 거예요. 이 손실 함수를 갖다가 여러 가지 그 손실 함수 계산하는 법 중에 하나를 갖다 택해서 우리 추정값이랑 우리의 그라운드 트로스 기준값이랑 얼마나 다른지를 우리가 계산을 할 수 있어야 돼요. 그리고 얼마나 좋은지 나쁜지를 이 로스 펑션을 통해서 계산을 한 다음에 이 로스 펑션을 최소화할 수 있게 이 매개 변수 파라미터 블를 갖다가 옵티마이즈 해야 돼요. 이게 최적화 부분입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 8,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1092,
      "char_count": 2037
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c009_b95a62",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 이 손실 함수량 최적화 즉 로스펑션이랑 옵티마이제이션 이 두 개의 프로세스가 이 뉴럴넷을 트레이닝하는 데 있어서 가장 중요한 개념입니다. 그래서 이번 시간에 그 남은 부분에 대해서는 이 두 개의 컨셉트에 대해서 좀 더 자세하게 배워보도록 하겠습니다. 먼저 손실 함수 손실 함수 여기 l이라고 많이 쓰는데요. 로스 펑션이라고 그래요. 즉 예측값 와이 프라임이랑 실제 관측값 y 간의 차이를 갖다가 계산한 함수입니다. 그래서 여기 이렇게 써 있죠. 손실 함수 즉 로스 펑션은요. 해당 머신 러닝 모델이 얼마나 좋은지 아니면 나쁜지를 숫자화해서 표현하는 어떤 매트릭입니다. 그래서 우리가 그 모델이 있을 때 그 모델을 통해서 예측한 예측값 y 프라임이랑 실제 값 그라운드 트루스라고 부릅니다. 실제 값 y랑 그거의 차이를 갖다가 계산하는 어떤 함수를 l이라고 표현을 하고 얘를 갖다가 손실 함수라고 불러줍니다. 그래서 기본적으로 우리는 손실 함수가 이런 성질을 가지고 있었으면 좋겠어요. 뭐냐면요. 만약에 예측 값이랑 실제 관측 값이 같은 경우에 모델이 진짜 좋은 거예요. 그러면은 페널티를 주지 말아야 돼요. 로스 펑션이 0이 돼야 돼요. 그리고 만약에 극단적으로 와 프라임이랑 와가 되게 크면 즉 예측 값이랑 그라운드 트루스가 너무 다르면은 페널티를 세게 줘야 돼요. 이럴 경우에는 로스가 되게 크겠죠. 그리고 와이 프라임이 거의 와랑 같다. 즉 조금은 다르지만 거의 같다면은 로스 펑션이 아주 작아야 돼요. 즉 많이 틀리면은 로스를 많이 줘야 되고 조금 틀리면은 로스를 조금 주기를 원해요. 그래서 이 손실 함수를 정의하는 방법이 두 가지가 있어요. 첫 번째는 디스크리미티브 세팅이 있고 두 번째는 프로버블리스틱 세팅이 있어요. 실제로는 이 프로블리스틱 세팅을 많이 쓰는데 여기에서는 이 디스크리미티브 세팅도 설명을 드리도록 할게요. 먼저 디스크리미티브 설정은 차별적 설정이라고 하는데요. 예를 들어서 이진 분류를 생각해 볼게요. 바이너리 클래시피케이션을 생각해 볼게요. 그러면은 예측 값이 2개예요. 어 0 아니면 1 아니면 플러스 원 아니면 마이너스 원 이 디스크리미네티브 설정에서는요. 그 클래스를 플러스 원 혹은 마이너스 원으로 정해 줍니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 9,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 617,
      "char_count": 1161
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c010_c725ab",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 즉 양수 혹은 음수 이렇게 두 개의 클래스가 있다고 설정을 해줘요. 그러면은 모델은 어떤 예측 값을 예측하는데 이 예측 값은 양수 혹은 음수로 이렇게 구분이 될 거예요. 그러면은 여기서 로스 펑션은 어떻게 설정이 되냐면은 만약에 모델이 좋은 맞는 예측을 했어요. 예를 들어서 실제 값이 양수예요. 그러면은 모델이 맞는 예측을 했으면 양수로 예측을 해야 될 거잖아요. 이럴 경우에 y 프라임 곱하기 y는 양수예요. 그렇죠 그리고 만약에 그라운드 트루스가 음수예요. 그럼 모델 값이 마이너스라고 음수라고 예측을 하면은 이 경우에도 와이 프라임 곱하기 와 이퀄 양수죠. 왜냐면은 마이너스 곱하기 마이너스는 양수니까 따라서 어떻게 됐건 간에 와 곱하기 와 프라임이 양수가 되면 맞는 예측이에요. 하지만 와이 프라임 곱하기 와이를 곱했을 때 음수가 되면은 그거는 틀린 예측이 되겠죠. 이해되세요? 네 다시 한 번 설명드릴게요. 기본적으로 이 이진 분류 모델의 경우에 우리가 기준값을 양수 혹은 음수로만 설정을 했어요. 따라서 모델이 제대로 예측을 했으면은 예측값 곱하기 그라운드 트루스 값은 무조건 양수여야 돼요. 하지만 모델이 틀리게 예측을 했으면은 그라운드 트루스랑 예측 값을 곱했을 때 음수가 나와야 된다는 거죠. 그래서 보시면은 예를 들어서 와랑 와이 프라임이랑 곱했을 때 양수가 나오면은 그거는 맞는 예측이죠. 하지만 음수가 나오면 그거는 틀린 예측이에요. 그리고 이 y랑 y 프라임의 곱에 그리고 절대값이 커지면 커질수록 이 모델이 자신 있게 예측을 한 거예요. 그러니까는 굉장히 큰 양수 값을 냈으면 오 이거는 플러스 원에 가깝다라고 자신 있게 예측을 했는데 만약에 작은 그 와 곱하기 와 프라임 값을 내보냈으면은 자신 없는 예측을 한 거죠. 그래서 요 와랑 와이 프라임의 절대값이 클수록 이 부분 이 부분 이 부분은 자신 있는 예측이고 0에서 가까울수록 자신 없는 예측이라고 생각을 하시면 됩니다. 그렇죠 그래서 이거를 갖다가 모델 하는 방법이 여러 가지가 있는데 먼저 가장 쉬운 방법은 이렇게 유닛 펑션으로 모델링을 하는 거겠죠. 만약에 이게 그 와랑 와이 프라임을 곱했을 때 양수면 맞는 예측이니까 로스를 0을 주고 아니면은 이거 와랑 와이 프라임을 곱했을 때 음수가 나오면은 틀린 예측이니까 로스를 갖다가 1이라고 주는 거죠. 네 이거는 가장 단순한 마진 기반 손실 0일 손실의 로스 펑션이라고 생각을 하시면 돼요. 이거는 단점이 있어요. 단점이 뭐냐면은요 와랑 와이 프라임 곱한 게 0일 경우에 여기 미분 불가한 영역이 있죠. 이렇게 이렇게 갑자기 탁 떨어지는 스텝 펑션은 미분 불가하죠. 그래서 여기는 미분이 불가하기 때문에 사실 미분 불가한 로스 펑션은 실제로 쓰기가 힘듭니다. 그래서 이런 단점이 있어요. 그래서 이 단점을 보완한 로스 펑션이 로그 로스 펑션입니다. 로그 손실이라고 써 있죠 로그 로스 펑션은 이렇게 빨간 선처럼 생겼어요. 식은 이렇게 표현이 되고 그런데 이것도 와랑 와이 프라임을 갖다가 곱한 게 양수면은 로스를 작게 하고 만약에 음수면은 로스를 크게 하는 우리가 원하는 성질을 가지고 있습니다. 그리고 이거에 좀 더 극단적인 형태가 익스포넨셜 로스예요. 이 익스포넨셜 로스는 로그로스보다 이렇게 좀 더 가파른 성질을 가지고 있습니다. 그래서 어 우리가 이 절대값이 크면 클수록 더 자신 있는 예측이라고 그랬어요. 그래 가지고 만약에 이렇게 와 와 프라임이 양수인데 절대 값도 크다 그러면은 자신 있게 맞게 판단한 거죠. 그래서 로스를 더 적게 주고 만약에 와 와 프라임이 음수인데 되게 자신 있게 틀렸으면은 훨씬 더 큰 로스를 주는 거예요. 이해되시죠? 그래서 이게 익스포넨셜 로스고 그다음에 또 한 가지가 이 힌지 로스입니다. 요 힌지 로스는요 이런 식으로 생겼는데 여기 이렇게 양수인 부분에 마진을 두어서 어느 정도 자신 있게 우리가 판단했을 때 맞았으면은 로스를 안 주고 자신 없게 판단했는데 맞았으면은 여전히 이렇게 로스를 주는 식으로 그 로스 펑션을 디자인했습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 10,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1093,
      "char_count": 2032
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c011_a1a7e5",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 요 힌지 로스는요 이런 식으로 생겼는데 여기 이렇게 양수인 부분에 마진을 두어서 어느 정도 자신 있게 우리가 판단했을 때 맞았으면은 로스를 안 주고 자신 없게 판단했는데 맞았으면은 여전히 이렇게 로스를 주는 식으로 그 로스 펑션을 디자인했습니다. 그래서 여러 가지 로스 펑션들을 이렇게 리뷰를 드렸는데요. 그 로스 펑션들이 다 장단점이 있어요. 그래서 예를 들어서 지수 손실은 익스포넨셜 노스는요 아웃라이어에 매우 큰 손실을 할당하므로 아웃라이어의 영향을 강하게 받습니다. 무슨 말이냐면은 어 이 지수 손실 같은 경우에는 굉장히 가파른 슬로프를 가지고 있어요. 그래서 만약에 어 어떤 케이스에 이 모델이 잘못 판단을 했는데 나머지 모든 것은 잘 판단했고 하나만 틀렸어요. 그래도 그 틀린 거에 대해서 강하게 영향을 받아 가지고 로스를 틀린 거에 대해서 되게 강하게 주는 그런 특성을 가지고 있습니다. 그래서 노이즈가 많은 데이터에 적합하지 않아요. 노이즈가 많으면은 대부분은 맞고 틀린 게 몇 개가 있는데 그 틀린 거에 대해서 너무 큰 로스를 주기 때문에 노이즈가 많은 데이터는 익스포넨셜 로스를 쓰기에 적합하지가 않습니다. 그래 가지고 익스포넨셜 로스보다는 실은 힌지 로스랑 로그로스가 훨씬 더 널리 사용이 되는데요. 이 힌지 로스도 장점이 있어요. 요 힌지 로스가 이렇게 이렇게 생겼잖아요 그러니까 어 미분 값이 항상 상수로 나와요. 만약에 큰 와 와 프라임이 있으면 미분 값이 0이죠 아닐 경우에는 미분 값이 마이너스 1이죠. 그래서 굳이 미분을 안 해도 돼서 계산적으로 더 효율적인 면이 있습니다. 그리고 로그 로스는 확률 값으로 아웃풋을 내기 때문에 훨씬 더 인터프리터블하다는 이런 장점이 있습니다. 그래서 여러 가지 로스가 있다 이거를 이해해 두시면 되고 실제로 이제 우리가 현업에서는 각각의 문제에 따라서 맞는 그 성질에 맞는 로스를 갖다가 쓰시면 됩니다. 근데 여기까지가 이제 디스크리미 테이블 세팅의 로스였어요. 실제로 우리가 딥러닝을 짤 때는 디스크리미니티브 루스는 거의 쓰지 않습니다. 실제로 이 프로버블리시틱 세팅에 해당하는 로스 펑션을 씁니다. 그래서 여기서 손실 함수 확률적 설정이라고 그랬죠 그래서 실제로 딥러닝에서는 앞으로 배울 요 로스를 쓸 거예요. 확률적 설정의 프로버블리스틱 세링의 로스를 쓸 겁니다. 자 여기에서는 그 확률적인 설정에 맞는 로스 펑션으로 시그모이드 로스랑 소프트맥스 로스를 우리가 배우도록 할 거예요. 어 이 먼저 이 시그모이드 로스는요 이 어떤 모델이 한 클래스의 확률인 하나의 점수 y 프라임을 예측하게 돼요. 우리가 모델이 어떤 그 확률을 예측을 한다고 이렇게 가정을 하는 거예요. 그리고 바이너리 클래시피케이션인 경우에 다른 클라스의 확률은 1 마이너스 y 프라임이라고 합니다. 왜냐하면 우리가 확률 분포를 예측하기 때문에 예를 들어서 클라스가 2개만 있어요. 근데 하나의 클라스의 확률은 y 프라임이에요. 그럼 나머지 다른 클라스의 확률은 당연히 1 마이너스 와 프라임이죠. 왜냐하면 모든 확률 분포를 합하면 1이 되기 때문에 그렇죠 그래서 이런 점수 차이에서 시그모이드 함수를 적용하는 게 좋습니다. 그래서 그거에 대한 예로 시그모이드 함수를 뒤에서 배울 거예요. 그리고 소프트맥스 함수는 뭐냐면은 이렇게 아웃풋이 두 개만 있는 경우가 아니고 아웃풋이 여러 개가 있을 때 하나의 그 정답은 1로 하고 나머지는 다 0으로 세팅할 때 로스가 이 소프트맥스 로스입니다. 가령 우리가 어 이거 같은 경우는 1 2 3 4 5 6 7 7개의 클라스가 있네요. 7개의 클라스 중에 딱 하나의 클라스만 맞고 나머지 클라스는 다 틀리게 되겠죠 그래서 어 맞는 클라스의 확률만 1로 남기고 틀리는 클라스는 다 확률을 0으로 세팅을 해 가지고 모델을 트레이닝을 할 때 이 소프트맥스 로스를 많이 씁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 11,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1043,
      "char_count": 1939
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c012_a0482b",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 가령 우리가 어 이거 같은 경우는 1 2 3 4 5 6 7 7개의 클라스가 있네요. 7개의 클라스 중에 딱 하나의 클라스만 맞고 나머지 클라스는 다 틀리게 되겠죠 그래서 어 맞는 클라스의 확률만 1로 남기고 틀리는 클라스는 다 확률을 0으로 세팅을 해 가지고 모델을 트레이닝을 할 때 이 소프트맥스 로스를 많이 씁니다. 그래서 이렇게 확률적 설정을 했을 때 그 아웃풋의 확률 분포를 갖다가 우리가 예측하는 식으로 모델을 만드는데 한번 이제 각각의 로스를 한번 볼게요. 자 이 크로스 엔트로피 로스는 요 앞에서 말씀드린 요 시그모이드랑 크로스 엔트로피 로스를 갖다가 합한 어떤 형태입니다. 그래서 정의를 보시면요. 여기 정보 이론에 따르면 집합에서 추출한 이벤트를 식별하는 데 필요한 평균 비트 수를 측정합니다라고 쓰여 있는데 무슨 말인지 모르실 거예요. 이 식을 보면서 이해를 하시는 게 빠를 거예요. 이 크로스 엔트로피 로스 데피니션은요 이런 식으로 쓰여 있습니다. 그래서 기본적으로 이게 무슨 의미냐면요. yik가 뭐냐면은 아이 번째 데이터의 케이 번째 클래스에 대해서의 정답 값입니다. 이 멀티 레벨 클래시피케이션 테스크에서 예를 들어서 클래스가 캐피탈 케가 있다고 생각을 해봐요. 그러면은 데이터는 엔 개가 있어요. 이 n개의 데이터 중에 아이 번째 데이터에서 케번째 클라스에 해당하는 정답 값이 요 YI케예요. 그리고 y아케의 프라임은 이 아이 번째 데이터의 케번째 클래스에 해당하는 예측 값입니다. 그래서 이 예측 값 앞에다가 로그를 붙이고 그거를 그라운드 트루스 와아케랑 곱한 다음에 다 더한 다음에 델타 수로 나누고 마이너스를 붙였어요. 자 이 시기에 무슨 말이냐 이걸 갖다가 이해하기 위해서 심플 케이스를 볼게요. 예를 들어 우리가 바이너리 클래시피케이션 테스크를 한다고 생각을 해봐요. 그러면은 바이너리니까 요 캐피탈 케이는 뭐가 되겠어요? 이가 되겠죠 클라스가 2개밖에 없으니까 그러면은 요 식은 이렇게 돼요. 그리고 이 바이너리 클래시피케이션에 해당하는 이 크로스 엔트로피 로스를 한번 해석해 볼게요. 기본적으로 이게 바이너리 클래시피케이션에 해당하는 크로스 엔트로피인데 여기에서 어 이 식을 갖다가 해석을 해보면요. 여기가 바이너리 클래시피케이션이니까 요 YI는 1 아니면 0일 거예요. 그쵸? 하나의 클라스가 맞으면 나머지 하나의 클라스는 틀리니까 하나의 클라스에 대해서는 이 클라스는 맞거나 틀린 거 1과 0 하나밖에 없어요. 그래서 예를 들어서 와 아가 0이라고 해봐요. 그러면은 요 앞에 항은 지워지고 이 뒤에 항만 남죠. 그리고 또 YI가 1이라고 해봐요. 그러면은 1 빼기 1이니까 뒤에 항은 지워지고 앞에 항만 남아요. 그래서 결국은 하나의 항만 남아요. 여기 쓴 게 그 말이에요. 모든 클래스에 걸쳐서 합산을 하더라도 단 하나의 k에 대해서만 이 yik가 1이고 나머지는 다 0이기 때문에 이 식은 결국 하나의 항 밖에 안 남아요. 근데 그 하나의 항은 무슨 의미냐 이 하나의 항을 보면요. 기본적으로 LS EC 로그의 yiti에요. 즉 뭐냐면은 모든 샘플에 대한 어떤 올바른 클래스에 대한 예측 확률 앞에다가 로그를 붙인 거예요. 즉 우리가 아까 말했듯이 맞는 클라스에 대한 확률만 이렇게 남는다고 그랬어요. 그래서 올바른 클라스에 대한 이 모델의 예측 확률만 남기고 나머지는 다 지워진 거예요. 즉 이 los란 무슨 의미냐면은요. 이 모델이 올바른 클래스에 대해서 어떤 확률을 가질 것이라고 예측을 한 거에 대한 마이너스 로그 텀입니다. 자 그럼 질문 여기서 왜 마이너스 로그 텀을 붙였을까? 여기서 이제 수학이 나옵니다. 여러분들 고등학교 때 배운 로그 펑션을 생각해 볼게요. 와 콜 로그 스 그게 요 빨간색 그 커브예요. 그리고 와 콜 마이너스 로그스는 뭘까요? 마이너스를 붙였으니까 이거를 x축에 대해서 이렇게 플립한 이 파란색 텀이 될 거예요. 이게 와 콜 마이너스 로그 x예요. 그런데 우리의 로스는 마이너스 로그의 yik의 프라임이죠. 그러니까 이 yik는 확률이에요. 확률 이퀄 0과 1 사이입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 12,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1086,
      "char_count": 2055
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c013_7010e4",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 자 그럼 질문 여기서 왜 마이너스 로그 텀을 붙였을까? 여기서 이제 수학이 나옵니다. 여러분들 고등학교 때 배운 로그 펑션을 생각해 볼게요. 와 콜 로그 스 그게 요 빨간색 그 커브예요. 그리고 와 콜 마이너스 로그스는 뭘까요? 마이너스를 붙였으니까 이거를 x축에 대해서 이렇게 플립한 이 파란색 텀이 될 거예요. 이게 와 콜 마이너스 로그 x예요. 그런데 우리의 로스는 마이너스 로그의 yik의 프라임이죠. 그러니까 이 yik는 확률이에요. 확률 이퀄 0과 1 사이입니다. 그래서 이 파란색의 0과 1 사이의 부분 이 오렌지 박스에 있는 커브만 보면은 이런 슬롯을 가지고 있어요. 즉 우리가 이런 슬롭의 어떤 펑션을 만들기 위해서 로그 펑션을 갖다가 쓴 거예요. 즉 이 yik가 이 가로축이 되겠죠. 이 yik가 1에 가까우면 로스를 0을 주고 이 yik가 0에 가까우면은 무한대의 로스를 주는 거예요. 즉 yik는 뭐예요? 올바른 클라스에 대한 확률이죠. 우리의 모델이 올바른 클라스에 대한 예측 확률을 1에 가까이 주면 줄수록 로스는 적게 주고 올바른 클래스에 대한 예측 확률을 0에 가까이 줄수록 굉장히 큰 무한대에 가까운 로스를 준다는 거예요. 그게 우리가 원하는 거죠. 그래서 여기서 써놨듯이 예측 확률은 0과 1 사이이므로 왼쪽에서 표시된 이 파란색 범위 이 오렌지 박스에 있는 범위의 펑션에 해당됩니다. 그래서 우리가 그 추정치가 1에 가까워지면은 로스가 0이고 0에 가까워지면은 로스가 무한대로 바뀌게 위해서 이렇게 펑션을 갖다가 쓴 것입니다. 대충 이해되시죠? 네 여기에서 수학적으로 다 이해하실 필요는 없습니다. 대충 이 크로스 엔트로피가 이런 마이너스 로그를 갖다가 이런 의미를 가지고 있다만 이해하시면 되겠습니다. 그리고 이 케이엘 다이버전스는 참고 삼아서 넣었어요. 이 케이엘 다이버전스는 뭐냐면요. 저희가 머신 러닝 하면 많이 보게 될 거예요. 그래서 기본적으로 이 케이엘 다이버전스는 뭐냐면 두 개의 확률 분포가 있을 때 그 확률 분포의 유사성을 측정하는 매트릭입니다. 그래서 여기서 피랑 큐가 두 개의 다른 확률 분포예요. 만약에 이런 요 피랑 큐가 디스크릿하면은 이렇게 서메이션을 쓰고 그 컨티뉴어스 화면은 인테그라를 써 가지고 이렇게 케이엘 다이버전스를 계산을 하는데 기본적으로 이 케이엘 다이버전스가 작으면 작을수록 피라는 데이터 디스트리뷰션이랑 큐라는 데이터 디스트리뷰션이랑 비슷하다는 거예요. 근데 이게 완전히 거리의 그 특징을 가지고 있진 않아요. 이렇게 비대칭이라는 것과 트라잉 귤러인 이퀄리티를 만족하지 않다는 점은 거리의 특성을 가지고 있지 않습니다. 하지만 실제 머신 러닝 연구에서 각각의 다른 두 개의 확률 분포를 비교하기 위해서 많이 쓰이는 매트릭입니다. 여기까지가 로스 펑션이고 그 두 번째로 더 중요한 컨셉이 나옵니다. 최적화 이 로스 펑션을 가지고 어떻게 뉴럴넷을 옵티마이제이션 하느냐 이것도 집중해서 잘 따라오시기를 바랍니다. 자 이거 우리가 많이 봤던 슬라이드죠 어떻게 하면은 이 뉴럴넷을 아니면은 어떤 모델을 갖다가 트레이닝을 하겠냐 파라미트릭 어프로치를 이용한 모델을 트레이닝을 하기 위해서는 먼저 매개 변수를 랜덤하게 이니셜 라이즈 한 다음에 학습 데이터 스랑 와를 가지고 와 프라임을 예측을 해서 와 프라임이랑 와의 차이를 계산한 로스 펑션을 이용해서 손실값에 따라 즉 로스 펑션에 따라서 우리 파라미터인 더블유를 갖다가 업데이트를 한다고 그랬어요. 자 질문 어떻게 업데이트를 하냐 이 파트에 대해서 한번 이야기를 해볼게요. 최적화란 뭐냐 최적화란 옵티마이제이션이라고 그러죠. 기본적으로 여기 데피니션 위키피디아에 나온 옵티마이제이션 데피니션이 나와 있는데요. 옵티마이제이션은요 매시메티컬 옵티마이제이션 올 메시메티컬 프로그래밍은요. 셀렉션 오버 베스트 엘리먼트 어 위드 리걸 투 썸 크라이테리아 프롬 썸 셋 오버 어벨러브 얼터네티브라고 되어 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 13,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1066,
      "char_count": 1981
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c014_23905c",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 즉 기본적으로 어떤 크라이테리아를 만족하는 베스트 엘리먼트를 찾는 거예요. 근데 이게 되게 어렵게 보이지만 여러분들 이미 고등학교 때 배운 어떤 컨셉입니다. 저희가 그 고등학교 때 부등식의 영역이라는 것을 배웠잖아요. 기본적으로 어떤 이런 제한 조건이 이루어졌을 때 제한 조건이 제시가 됐을 때 이 제한 조건을 만족하는 어떤 함수를 갖다가 우리가 배우는 그런 부등식의 영역 문제를 풀으셨을 거예요. 그래서 요 그 옵티마이제이션 데피니션에서 베스트 엘리먼트를 찾는다고 그랬죠. 요 부등식의 영역에서도 베스트 엘레멘터인 스랑 와를 찾는 문제를 풀었었고 어 제한 조건 썸 크라이테리아도 이 부등식 영역에서 이렇게 식으로 주어진 어떤 크라이테리아로 나왔습니다. 그래서 저희가 이미 배웠던 컨셉을 좀 더 매스메리컬리 포뮬라이제이션 해 가지고 한번 보도록 하겠습니다. 자 그래서 우리가 여기서 하고자 하는 게 그럼 뭐냐 우리가 궁극적으로 옵티마이제이션을 통해서 뭘 하고자 하냐 이렇게 보면은요. 어떤 조건 하에서 어떤 컨스트레인드 된 조건 하에서 우리가 원하는 어떤 함수의 최소 혹은 최댓값을 찾는 게 우리의 목표입니다. 그래서 우리의 경우에 어떤 로스 값이 우리가 로스 값의 펑션을 배웠잖아요. 어떤 로스 값이 가장 작아지게 하는 로스 값을 미니마이즈 하는 웨잇 값을 찾는 게 우리의 목표예요. 그래서 여러 가지 방법이 있을 거예요. 이렇게 웨이브 값을 찾는 게 첫 번째로 생각할 수 있는 게 완전 탐색 이그저스티브 서치죠. 있는 거 다 해보는 거예요. 그냥 있는 거 다 해봐서 베스트를 찾는 거 당연히 안 좋겠죠. 두 번째는 랜덤 탐색 무작위로 탐색하는 거 당연히 안 좋겠죠. 그리고 시각화도 있을 수 있어요. 이렇게 그림을 그려가지고 이 그림에서 어떤 파라미터가 옵티마이제이션 된 파라미터인지를 찾는 거죠. 그리고 그리디 탐색은 각 변수를 하나씩 해결한다고 쓰여 있는데 기본적으로 여러 가지 변수가 있을 때 여러 가지 어 웨이 파라미터가 있을 때 나머지 웨이 파라미터는 다 고정시키고 하나의 웨이 파라미터만 바꿔가면서 옵티마이제이션 하는 방법입니다. 이거는 잘 안 돼요. 왜냐하면 동시에 2개나 여러 개의 파라미터가 바뀌었을 때 최적화 값이 바뀌었기 때문이죠. 그래서 여기에서는 어 그레디언트 디센트라는 최적화 방법을 한번 배워보도록 할 거예요. 여기 어 곡선을 따라서라고 써놨는데요. 기본적으로 아이디어는 굉장히 간단합니다. 여기 최소화하고자 하는 비용 함수가 있다고 가정을 해볼게요. 이 비용 함수는요 코스트 펑션이라고 하는데요. 여러분들 앞에서 배운 로스 펑션을 갖다가 코스트 펑션 비용 함수랑 같은 개념이라고 생각하시면 됩니다. 가령 우리가 로스 펑션이 있어요. 이 로스 펑션을 우리가 미니마이즈 하고 싶어요. 어떻게 미니마이즈 하냐 우리가 어떤 특정 점에서 기울기를 계산을 해서 아래로 내려가는 방향으로만 움직이면 돼요. 그래서 어 하나 이그 샘플로 생각할 수 있는 게 여러분들 산에 올라갔다고 생각을 해봐요. 산에 올라가서 그 산의 정상에서 혹은 아니면은 산의 중간에서 가장 밑에 있는 부분으로 내려가고 싶어요. 그러면 어떻게 해야겠어요? 그 지점에서 여러 군데 발을 디디어서 그 기울기가 음수인 곳 즉 가파르게 내려가는 방향으로만 내려가는 거예요. 여러분 한번 상상을 해보세요. 여러분들 산에 이렇게 있어요. 여러분들이 여러 가지 방향으로 발을 내닫아요. 그러면서 가장 가파르게 내려가는 방향을 선택하는 거예요. 그리고 그 방향으로 내려가요. 그리고 얼마만큼 내려간 다음에 또 그 위치에서 발을 내딛어서 가파르게 내려가는 방향으로 내려가고 이렇게 계속하다 보면 언젠가는 가장 미니멈의 위치인 산에 도달하게 되겠죠. 이거를 수학적으로 말하면 어떻게 되냐면요. 한 파라미터에 대해서 어떤 랜덤 이니셜 밸류의 파라미터에 대해서 기울기를 계산합니다. 그리고 가장 가파르게 내려가는 기울기 방향으로 움직인 다음에 움직인 위치에서 또다시 기울기를 계산해요. 그런 다음에 내려가는 방향으로 계속 내려가는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 14,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1105,
      "char_count": 2029
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c015_8ae1ca",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그리고 가장 가파르게 내려가는 기울기 방향으로 움직인 다음에 움직인 위치에서 또다시 기울기를 계산해요. 그런 다음에 내려가는 방향으로 계속 내려가는 겁니다. 그게 어 그레디언트 디센트예요. 이거를 수학식으로 써볼게요. 이 세타 뉴는 요 셉타가 파라미터예요. 그 세타 올드가 이니셜 포지션이에요. 예를 들어서 이 여기 이 지점이 세타 올드라고 생각을 해볼게요. 요 셉타 올드에서 우리가 이 코스트 펑션의 기울기를 어 계산을 해요. 예를 들어서 이 기울기는 양수죠. 그러면은 이 기울기랑 어 러닝 레잇을 곱해 가지고 올드 파라미터에서 빼줍니다. 가령 여기가 올드 파라미터라고 그러면은요. 여기에서의 그 어 그 제이라는 코스트 펑션의 그레디언트는 양수가 되겠죠. 그래서 이 양수랑 러닝 레이도 양수니까 이 모든 파라미터가 양수가 될 거예요. 즉 셀타 올드에서 양수의 어떤 값만큼 이렇게 빼지게 돼요. 즉 업데이트된 그 세타 뉴는 이 세타 올드보다 더 왼쪽으로 가게 되겠죠. 마찬가지로 우리가 세타 올드를 여기에서 시작했다고 해봅시다. 그러면은 슬로프는 여기에는 슬로프가 사실은 음수예요. 요 빨간색의 슬로프는 음수죠. 그러면은 요 오른쪽에 있는 모든 파라미터가 다 뭐가 되겠어요? 음수가 되겠죠. 왜냐하면 요 그레디언트 sl이 음수니깐 이게 다 음수가 될 거예요. 그래서 음수 음수에서 플러스가 돼가지고 ST 올드에다가 어떤 파라미터를 더해가지고 stan은 더 오른쪽으로 갈 거예요. 그래서 이렇게 계속 슬로프를 계산해 가면은 기본적으로 이 세타 뉴가 세타 올드보다 더 아래 그러니까 그 코스트 펑션이 더 최소화되는 방법으로 업데이트가 됩니다. 그래서 이거를 갖다가 어 코드로 짜준 게 이 박스 안에 있는 거예요. 여러분들 한번 찬찬히 이 코드를 보시면 이해가 될 겁니다. 근데 이게 이제 그레디언트 디센트라고 불리는 가장 간단한 최적화 방법인데요. 이게 사실은 문제점이 많아요. 안 되는 경우가 많아요. 어떤 경우가 안 되냐면요. 이렇게 저희의 그 코스트 펑션의 서페이스가 볼록하지 않을 수가 있어요. 앞에 봤던 이그잼플은 이 코스트 펑션이 되게 예쁘게 이렇게 곡선을 그 하나의 곡선으로 이루어졌잖아요. 근데 실은 이 코스트 펑션이 이렇게 울퉁불퉁하다고 생각을 해봐요. 그러면은 우리가 이렇게 최적화를 할 때 로컬 최적점 즉 작은 웅덩이 같은 데 빠질 수도 있어요. 아니면 세를 포인트에 빠질 수도 있어요. 세를 포인트는 뭐냐면은 그 사방이 그러니까 이렇게 웅덩이처럼 사방이 높은 건 아니지만 어떤 특정 포인트에서 기울기가 0인 점입니다. 기울기가 0이면 업데이트가 안 되겠죠 이렇게 어떤 최적점이 아닌 곳에 스톱되는 문제가 있을 수도 있어요. 그리고 두 번째 문제는요. 이 코스트 펑션이 미분 불가능할 경우에는 쓸 수가 없습니다. 왜냐하면 이거를 갖다가 업데이트를 하기 위해서 계속 코스트 펑션을 미분을 해야 돼요. 로스 펑션을 미분을 해야 돼요. 근데 로스 펑션이 미분 불가능하다 그러면은 당연히 쓸 수가 없겠죠. 그리고 이 세 번째가 되게 중요한데요. 국부 최소값으로 수렴하는 속도가 상당히 느릴 수 있습니다. 이건 무슨 말이냐면요. 한 특정한 포인트에서 우리가 그 주위에 있는 모든 데이터들에 대해서 미분 값을 계산을 해서 최소의 미분값 방향으로 가야 되잖아요. 그러니까 예를 들어서 산을 내려가는 산에서 내려가는 예제를 생각을 해보면은 우리가 어떤 방향으로 내려가는 게 가장 가파르게 내려가는지를 판단하기 위해서 발을 쥐에 있는 모든 영역에 대해서 디뎌봐야 돼요. 그거랑 똑같은 거예요. 어떤 특정 파라미터에 대해서 쥐의 모든 방향에 해당하는 파라미터의 기울기를 다 계산을 해야 된다는 거죠. 그래서 이 과정이 모든 데이터 포인트를 샘플 해야 되기 때문에 굉장히 시간이 오래 걸립니다. 그래서 실제로는 모든 데이터에 대해서 그래디언트를 계산하지 않고 무작위로 샘플된 하위 집합의 데이터에 대해서만 기울기를 계산합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 15,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 1070,
      "char_count": 1982
    },
    {
      "id": "transcript_ai_math_3강_선형대수_2__linear_classifier_a_c016_a4292b",
      "content": "[AI Math] (3강) 선형대수-2_ Linear Classifier and Softmax Classifier\n\n다. 그래서 실제로는 모든 데이터에 대해서 그래디언트를 계산하지 않고 무작위로 샘플된 하위 집합의 데이터에 대해서만 기울기를 계산합니다. 그거를 스토캐스틱 그레디언트 디센트라고 해요. 기본적으로 모든 샘플에 대해서 파라미터를 업데이트하는 게 아니고 어 뭐 일부만 되는 미니 배치의 데이터만 샘플 해 가지고 거기에 대한 그레디언트를 업데이트하는 겁니다. 그래서 배치 사이즈가 크면 클수록 안정적으로 업데이트가 돼요. 하지만 오래 걸리니까 우리가 배치 사이즈를 조그맣게 해가지고 메모리가 허용하는 범위 내에서 파라미터를 업데이트하는 거죠. 그래서 이거를 이렇게 반복적으로 하면서 우리가 최적화도 배웠어요. 이 스텝을 갖다가 반복적으로 훈련을 하면서 계속 최적화를 하면서 매개 변수를 업데이트를 하고 이 로스가 최소화 되는 매개 변수를 찾아가는 겁니다. 여기서 이제 질문 그러면은 매개 변수 w를 업데이트할 때 로스를 줄이는 거를 어느 시점에서 멈춰야 되냐 이런 문제가 있을 수가 있습니다. 그래서 이 커브는 실제로 어 트레이닝 데이터를 이제 옵티마이제이션 했을 때 나오는 이 러닝 커브인데요. 가로축은 어 학습하면서 본 어떤 데이터의 이그 샘플 개수고 이 세로 축은 트레이닝 로스라고 볼 수가 있습니다. 근데 이 오렌지 커브가 트레이닝 로스 커브가 되겠죠 근데 여기서 보시면 알다시피 계속 마냥 내려가지 않아요. 어떤 포인트에서는 다시 로스가 올라가 올라가기도 하고 다시 내려가기도 하고 이렇게 지글지글지글지글 노이즈가 많은 커브가 생깁니다. 왜 이렇게 노이즈가 많은 커브가 생기나요? 왜냐하면 우리가 미니 배치를 사용해서 그래요 미니 배치를 사용해서 이 그레디언트를 업데이트를 했기 때문에 이 미니 배치가 작을 경우에 그 미니 배치가 전체 트레이닝 셋을 대표하지 못해서 이렇게 다시 로스가 올라가는 형태가 생기게 됩니다. 그리고 또 두 번째 이유는요 우리가 테스트할 때 측정 자체를 전체 이g 샘플에 대해서 테스트하지 않고 작은 테스트 셋에 대해서 테스트했기 때문에 이런 문제가 생기는 겁니다. 그럼 이렇게 지글지글지글 내려가는 로스 커브를 봤을 때 언제 멈춰야 되냐 이 언제 멈춰야 되는 거는 사람마다 다를 수 있어요. 계속 이 커브가 오르락내리락 하기 때문에 여러분들이 이 커브의 그림을 보면서 뭐 인튜에티블이 멈출 수도 있고 아니면은 이렇게 커브가 왔다 갔다 할 때 이 변화의 폭이 거의 안 변하면 멈출 수도 있습니다. 그래서 이런 거는 여러분들이 스스로 정해서 언제 멈춰야 될지 결정을 하셔야 됩니다. 자 그래서 오늘은 기본적으로 머신 러닝 특히 파라미터릭 어프로치의 어 머신 러닝 모델을 갖다가 어 로스랑 최적화 방법으로 옵티마이제이션 하는 방법에 대해서 배웠습니다. 사실 오늘 배운 내용이 가장 중요한 내용일 수가 있습니다. 머신 러닝 부분에서는 그래서 어 제가 굉장히 많은 내용을 빠른 속도로 이렇게 설명드렸는데 이해 안 되는 부분은 다시 비디오를 보시거나 슬라이드를 보시면서 캡처 업해서 다음 시간에 배울 내용들을 잘 이해하는 데 도움이 없도록 하셨으면 좋겠습니다. 어 다음 시간에는 기초 신경망 이론을 드디어 신경망 이론 드디어 뉴럴 네트워크를 배우도록 하겠습니다. 긴 시간 어려운 내용 집중해 주셔서 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier.json",
        "lecture_name": "(3강) 선형대수-2_ Linear Classifier and Softmax Classifier",
        "course": "AI Math",
        "lecture_num": "3강",
        "lecture_title": "선형대수-2_ Linear Classifier and Softmax Classifier",
        "chunk_idx": 16,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f8b9292eb1349255051c3156379d388616e220a6b42d85a505edf77336a6cd"
      },
      "token_estimate": 896,
      "char_count": 1648
    }
  ]
}