{
  "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
  "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
  "course": "MLforRecSys",
  "total_chunks": 13,
  "chunks": [
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c000_0c3e9c",
      "content": "[강의 녹취록] 과목: MLforRecSys | 강의: 4강 | 제목: 변분추론 II\n\n네 안녕하세요 여러분 이번에는 저희 변분 추론에 좀 더 다른 유도 방법 그다음에 심화들을 한번 살펴보도록 하겠습니다. 저희가 민필드 밸리셔 인퍼런스가 무엇인지 한번 살펴봤었는데요. 그거를 우리가 또 다르게도 한번 유도를 해볼 수가 있습니다. 그 내용을 한번 살펴보도록 하고 그다음에 그러한 베레션 인퍼런스가 어떻게 좀 어플리케이션 되는지 그다음에 여러분들께서 사실 EM 알고리즘을 아마 많이 들어보셨을 것 같아요. 그렇죠 익스펙테이션 n 맥시마이제이션 알고리즘으로 불리는 그런 이엠 알고리즘하고 뭐가 다르고 뭐가 같은 건지 두 개가 또 굉장히 또 깊이 관련이 있습니다. 한번 살펴보도록 하겠습니다. 자 그래서 첫 번째 파트는 저희가 mfvi를 좀 더 한 번 더 보도록 하겠습니다. 자 엠프브i를 저희가 유도하는 과정을 앞서서 한번 살펴본 적이 있습니다. 그렇죠 그래서 뭐 로그의 피 틸다를 이용해서 하기도 하고 로그의 피에 로그 익스포넨셜을 같이 써가지고 그런 트릭을 이용해서 그래서 이런 엘보 로어바운드 또는 로그 마시널 라이클루더의 로우 바운드를 우리가 맥시마이즈 하기 위해서 어떤 큐를 잡아야 되는가를 우리가 유도를 했었습니다. 지금 우리가 이걸 계속하고 있는 이유는 뭡니까? 우리는 이런 로어 바운드를 맥시마이즈 하고 싶은 거예요. 그렇죠 그러면 그러한 로어 바운드를 우리가 맥시마이즈 하기 위한 큐는 뭐냐라고 했습니다. 답은 간단해요. 포스테리어입니다. 근데 포스테리어를 우리는 정확하게 구할 수가 없어요. 그래서 우리가 대신에 얘를 맥시마이즈 시키면 우리가 큐가 포스테리어에 가까워질 것이다라고 또는 우리가 포스테리를 잡으면 얘가 로그 마시라이 클로즈에 가까워지지만 여기서는 저희가 포스트를 정확하게 못 구하기 때문에 얘를 맥시마이즈 하는 결국 큐가 뭐냐를 찾고 싶은 거예요. 근데 그 큐가 굉장히 복잡해질 수 있다라고 말씀을 드렸습니다. 언제 여기 나와 있는 큐 제이는 큐 제 제에 좀 더 이제 그 간단화된 버전으로 생각하시면 됩니다. 그걸 그냥 편의상 큐의 제제를 큐 제이라고 그냥 쓴 것뿐입니다. 그러면은 원래는 이제 제가 아니라 q 제라고 하면 제 전체 그러면은 변수가 엄청 여러 개 있을 수 있는 거예요. 여기서 제는 레이턴트 베리어블 즉 스가 아닌 나머지 모든 걸 다 의미하는 거라고 했으니까 우리한테 레이턴트 베이리블이 여러 개인 예제도 하나 봤었죠. 앞에서 레이턴트 디리클레 alocation l d라고 부르는 그러한 것들은 우리가 유도를 못하는 겁니다. 왜 이 q라는 것 자체를 정의를 못해서 그 여러 변수들 간의 조인트 어떤 거는 가우시안 따르고 어떤 거는 또 드리클레 따르고 어떤 건 또 멀티노미아를 따르고 그러한 서로 다른 랜덤 베어러블의 조인트는 무슨 분포냐 말 못 하거든요. 그래서 우리가 각각 쪼개서 보겠다 q 제 1 q 제 2 q 제 3 각각 쪼개서 보겠다라는 게 민 필드 어썸션이었습니다. 가정 사항인 거죠. 어디까지나 그러고 나서 그 각각의 q를 우리가 맥시마이즈 하는 걸 찾겠다라는 게 우리의 목표였습니다. 그리고 그거를 두 가지 방법으로 우리가 유도를 했었어요. 오늘은 그 세 번째 방법 펑셔널 디리버티브를 한번 살펴보도록 하겠습니다. 지금 우리가 볼 거는 펑션 펑셔널 그다음에 펑셔널 또는 베리셔널 디리버티브까지 볼 건데 이거는 지금 우리가 보는 베리에이션 인퍼런스에서뿐만 아니라 굉장히 많은 곳에서 쓰입니다. 그래서 여러분들께서 꼭 기억하고 넘어가시면 좋겠습니다. 자 일반적인 카큘러스랑 뭐가 다르냐 일반적인 여러분들께서 그 배웠을 미적분이랑 뭐가 다르냐 그거는 실수를 인풋으로 받습니다. 그리고 실수가 아웃풋으로 나옵니다. 그런 함수가 있는 거예요. 근데 베리션 칼큘러스는 뭐냐면 함수를 인풋으로 받아요. 함수를 그러고 나서 실수를 내뱉습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 0,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1028,
      "char_count": 1892
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c001_aea7ec",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 지금 우리가 볼 거는 펑션 펑셔널 그다음에 펑셔널 또는 베리셔널 디리버티브까지 볼 건데 이거는 지금 우리가 보는 베리에이션 인퍼런스에서뿐만 아니라 굉장히 많은 곳에서 쓰입니다. 그래서 여러분들께서 꼭 기억하고 넘어가시면 좋겠습니다. 자 일반적인 카큘러스랑 뭐가 다르냐 일반적인 여러분들께서 그 배웠을 미적분이랑 뭐가 다르냐 그거는 실수를 인풋으로 받습니다. 그리고 실수가 아웃풋으로 나옵니다. 그런 함수가 있는 거예요. 근데 베리션 칼큘러스는 뭐냐면 함수를 인풋으로 받아요. 함수를 그러고 나서 실수를 내뱉습니다. 인풋이 함수예요. 그러면 여기 나와 있는 f는 뭐라고 부르면 될까요? 이 f는 FX에 대한 함수인 거잖아요. 여기 f를 우리가 뭐라고 불러요? 여기 나와 있는 f를 이 인풋에 대한 함수 실수에 대한 함수 또는 스에 대한 함수라고 부르죠. 그렇죠 자 근데 여기는 이 프가 프스를 인풋으로 받으니까 프스에 대한 함수인 거예요. 여기 나와 있는 프는 그러면 어떻게 됩니까? 함수에 대한 함수인 거예요. 그거는 우리가 펑셔널이라고 부릅니다. 펑셔널 자 그럼 여기서 한번 보시면 엘보라는 것은 여기서 큐라는 건 우리가 예를 들어 어떤 거라고 했습니까? 가우시안 분포라고 했습니다. 큐 제이는 가우시안 분포예요. 그렇죠 자 그러면은 그러한 큐 제이를 우리가 인풋으로 받아서 함수로 만든 게 엘버입니다. 그렇죠 엘버는 이 q j에 대한 함수인 거잖아요 함수에 대한 함수다. 그러면 여기서 엘버를 우리가 펑셔널이라고 말할 수가 있겠습니다. 그럼 여기서 펑셔널 디리뷰티는 뭐냐면 결국 우리가 여기서 큐 함수를 변화시킬 때 자 결국 여러분 디리버티브라는 건 뭐예요? 우리가 인풋을 변화시켰을 때 아웃풋이 어떻게 변하는지 그 변화량을 우리가 추정하는 것입니다. 그러면 여기서는 펑셔널 디리버티브는 함수가 변했을 때 펑셔널이 어떻게 변하는지를 추정하는 게 펑셔널 디리버티브 또는 베르셔널 디리베티브라고 부릅니다. 아시겠죠? 자 펑셔널 딜리뷰티브를 우리가 가기 전에 좀 몇 가지 준비 과정을 거치고 넘어가도록 하겠습니다. 자 우리가 일반적으로 뭐 이러한 에프엑스에서 최대가 되는 액수를 찾고 싶다 너무 쉽죠 이거는 그냥 4 프라임 x가 0이 되는 포인트를 찾으면 되는 거 아닙니까? 그러한 스텐셔놀리 포인트를 찾으면 끝이네요라고 말하죠. 정답입니다. 맞아요. 그쵸 여기서 최대가 되는 x 찾는다 그러면은 이미 했을 때 0 되는 스텐셔놀리 포인트를 찾고 그게 미니멈인지 맥시멈인지만 우리가 조사하면 되는 거예요. 그러면 게임 끝 그렇죠 자 이게 뭐 매우 해피한 상황인 겁니다. 자 이거를 우리는 함수 버전으로 그냥 하고 싶은 것뿐이에요. 함수 버전으로 아시겠죠? 만약에 예를 들어서 이런 겁니다. 두 개의 점 포인트가 있습니다. 두 개의 포인트가 있어요. 그랬을 때 두 포인트를 지나는 가장 길이가 짧은 함수는 뭐냐 그걸 한번 찾아봐 달라라는 겁니다. 그렇죠 자 그러면은 여기서 한번 보면은 에 포인트가 있고 비 포인트가 있어요. 그럼 여기를 지나는 우리는 함수를 찾고 싶은 거예요. 이런 함수가 좋은지 일직선인 함수가 좋은지 어떤 함수가 가장 짧은지 찾고 싶은 게 목표입니다. 그러면은 여기 나와 있는 이 길이를 우리가 구해야겠죠. 그 길이를 우리가 구하기 위해서 이렇게 단위 길이를 우리가 정의하고 그렇죠 그다음에 단위 길이에 대해서 우리가 적분을 a에서 b까지 해준다. 얘를 아이다라고 생각할 수가 있겠습니다. 그렇죠 그다음에 얘를 우리가 또 다르게 표현하면은 이렇게도 표현을 할 수가 있겠습니다. 자 이런 과정에서 한번 보시면 결국 어떤 말이 되나요? 자 여기서 이 아이라는 거는 펑셔널이 되는 거예요. 왜 결국 우리는 함수를 찾고 싶은 거예요. 함수를 와는 에프엑스라는 함수를 찾고 싶은 겁니다. 그럼 와는 에프엑스라는 게 인풋으로 들어간 함수니까 아이는 펑션에 대한 펑션 펑셔널이라고 우리가 또 정의할 수가 있겠습니다. 펑셔널에 대한 벌써 두 번째 예제를 본 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 1,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1074,
      "char_count": 1984
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c002_f1954a",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 얘를 아이다라고 생각할 수가 있겠습니다. 그렇죠 그다음에 얘를 우리가 또 다르게 표현하면은 이렇게도 표현을 할 수가 있겠습니다. 자 이런 과정에서 한번 보시면 결국 어떤 말이 되나요? 자 여기서 이 아이라는 거는 펑셔널이 되는 거예요. 왜 결국 우리는 함수를 찾고 싶은 거예요. 함수를 와는 에프엑스라는 함수를 찾고 싶은 겁니다. 그럼 와는 에프엑스라는 게 인풋으로 들어간 함수니까 아이는 펑션에 대한 펑션 펑셔널이라고 우리가 또 정의할 수가 있겠습니다. 펑셔널에 대한 벌써 두 번째 예제를 본 겁니다. 앞에서 우리가 엘보가 하나의 펑셔널이라고 배웠고 그다음에 이러한 것도 우리가 펑셔널이라고 할 수가 있습니다. 그렇죠 이런 식으로 우리가 정의를 해볼 수가 있다는 거예요. 그러면은 앞에서 우리가 이렇게 정의했죠. 특정 구간에 대한 적분 값을 통해서 이런 식으로 우리가 여기서는 f에 대한 미분값 f프라임이 여기 들어가 있는 형태네요. 그쵸 즉 우리가 이 i는 f프라임이라는 거를 뭔가 인풋으로 받아서 뭔가를 하는 친구입니다. 그다음에 여기서는 이 엘버에서는 뭡니까? 이 엘버에서는 만약에 q가 그냥 프라고 생각하면은 q에 대한 미분이 아니잖아요. 그렇죠 q 자체입니다. 그러면 q를 만약에 프로 생각하면은 엘버는 에프에 대한 함수라고 생각할 수 있습니다. 즉 이처럼 우리가 지금 다뤄야 되는 펑셔널은 그 프 와는 프x니까 얘를 그냥 프라고 생각하시면 되겠죠. 자 그 프 그다음에 프라임 왜 여기서도 에프 프라임이 있었으니까 결국 프랑 프 프라임을 뭔가 인풋으로 받아서 우리가 함수의 형태로 만든 걸 펑셔널이구나라고 여기서 정리하고 넘어가겠습니다. 여기서 한 가지 주의할 거 그다음에 나중에 볼 거는 한번 보시면 여기서는 엘보에서는 프라임이라는 게 없어요. q 프라임 아니잖아요. 미분형 아닙니다. 그냥 q만 있는 거예요. 그러면 거기서는 여기서 뭐가 될까요? 실제로는 얘가 없는 형태가 되겠죠. 즉 엘보는 이러한 펑셔널 중에서도 좀 비교적 간단한 형태의 펑셔널이다라고 생각하시면 되겠습니다. 아시겠죠? 여러분 그다음에 이러한 바운더리 컨디션도 존재합니다. 왜 여기서도 x1일 때는 y1 지나야지 x2일 때는 y2 지나야지 그러한 컨디션이 붙어서 있는 상황에서 우리가 얘가 가장 맥시마이즈가 되는 포인트를 찾고 싶은 게 우리의 목표라고 할 수가 있겠습니다. 아시겠죠? 그러면 여기서 우리가 결국 찾고 싶은 게 뭐냐면 스테셔너리 포인트를 좀 찾고 싶은 거예요. 그렇죠 결국에는 이러한 아이에 대한 스테이셔너리 포인트를 우리는 찾고 싶은 겁니다. 그래서 가장 크게 만드는 거 만약에 그걸 우리가 y x라고 할게요. y스가 이거 스텐션이 포인트고 그다음에 이러한 바운더리 컨디션도 다 만족을 하는 친구라고 가정하겠습니다. 즉 y스는 익스트리머리인 거예요. 아시겠죠? 즉 yx는 익스트리머이다. 그때 우리가 이러한 바운더리 컨디션을 우리가 해결하기 위해서 이런 형태로 우리가 정리를 한번 해보겠습니다. 즉 y 바라는 걸 우리가 하나 정리를 하고 넘어갈게요. y 바는 yx 더하기 엡실론의 에타 x입니다. 에타 x 그러면은 여기서 x가 x1이 됐다라고 하면은 y x1이 애초에 y 1인 거잖아요 그쵸 그러면은 여기 얘는 애초에 필요가 없는 겁니다. 그렇죠 그래야지 얘가 딱 y1이랑 같아지니까 그러면 얘는 0이 되는 것이고 스투일 때도 y의 스투는 얘가 만약에 y2 얘가 바운더리 컨디션을 만족한다고 하면 얘가 이제 y2가 되면 되는 거니까 얘는 또 여기 스투를 넣으면 0이 되는 형태가 됩니다. 그쵸 자 이렇게 와 바를 우리가 정리하고 넘어가도록 하겠습니다. 그러면은 문제가 어떻게 바뀌나요? 일단 와 바는 이러한 바운더리 컨디션을 다 만족하는 거예요. 이미 와이 바는 왜 엑스원 넣으면은 뭐 와스원만 남게 되니까 이거 0이고 와 엑스원은 우리가 뭐 와원이라고 했잖아요. 왜 와이스가 이미 바운더리 컨디션을 만족한다라고 가정했으니까 그러면 우리가 이렇게 바운더리 컨디션을 제외하고 와 바에 대해서만 이렇게 우리가 쓸 수가 있게 되는 것입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 2,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1084,
      "char_count": 2022
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c003_527247",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 그쵸 자 이렇게 와 바를 우리가 정리하고 넘어가도록 하겠습니다. 그러면은 문제가 어떻게 바뀌나요? 일단 와 바는 이러한 바운더리 컨디션을 다 만족하는 거예요. 이미 와이 바는 왜 엑스원 넣으면은 뭐 와스원만 남게 되니까 이거 0이고 와 엑스원은 우리가 뭐 와원이라고 했잖아요. 왜 와이스가 이미 바운더리 컨디션을 만족한다라고 가정했으니까 그러면 우리가 이렇게 바운더리 컨디션을 제외하고 와 바에 대해서만 이렇게 우리가 쓸 수가 있게 되는 것입니다. 아시겠죠? 그다음에 이건 미분값 그래서 와이바 프레임이라고 우리가 쓴 것뿐입니다. 자 그러면 이렇게 나왔고 결국 우리는 여기서 값을 구하고 싶은 거예요. 결국 얘를 가장 맥시마이즈 시켜주는 값을 구하는 게 우리의 목표입니다. 그쵸 그러면은 다시 한 번 보면은 얘를 우리가 엡실론에 대해서 한번 미분해 볼게요. 왜 결국 이 아이라는 것은 우리가 이렇게 정의가 되는데 y 바 같은 데에다가 지금 엡실론이 다 들어가 있으니까 그래서 우리가 엡실론으로 한번 미분을 한번 해보겠습니다. 아시겠죠? 그러면 어떻게 됩니까? 이 엡실론 미분했을 때 0 되는 포인트를 찾고 싶은 거예요. 그러면은 이 아이라는 것을 우리가 다시 쓰면 이렇게 우리가 쓸 수 있겠죠 얘가 0 되는 포인트를 찾고 싶다라고 우리가 말할 수 있고 자 여기서는 얘랑 얘가 우리가 인터체인지가 가능합니다. 그래서 이 미분 값을 적분 안으로 쏙 넣어요. 그러면은 이렇게 적히게 됩니다. 자 그러고 나면 여기서부터는 우리가 체인룰을 쓰면 되는 거예요. 체인룰 즉 와 바에 대해서도 우리가 체인 롤을 걸고 와바 프라임에 대해서도 체인 룰을 거는 겁니다. 이렇게 아시겠죠? 왜 와바 프라임도 우리가 앱실론에 대한 함수일 수가 있는 거니까 그러고 나서 이 와 바라는 걸 우리가 이렇게 정의했었습니다. 그러면 이 와 바를 엡실론으로 미분하면 뭐예요? 에타만 남죠 그렇죠 자 그다음에 여기서 이 와이바 프라임이라는 거를 우리가 앱실론으로 또 미분하면은 그때는 에타 프라임만 남겠습니다. 그쵸 이타 프레임만 남게 되는 상황이 되는 것이고요. 그러고 나서 그러고 나서 이 부분을 저희가 한번 보게 되면은 얘는 여러분들께서 부분 적분식을 한번 기억을 해 봅시다. 부분 적분식 여기 있는 파트죠 이게 부분 적분에 대한 공식이 됩니다. 그렇죠 그러면은 얘를 우리가 어떻게 계산할 거냐 이거에 대한 적분 값을 우리가 어떻게 계산할 거냐라고 하면은 지금 이런 형태는 여러분 딱 부분 적분 형태랑 굉장히 닮아 있죠 그래서 한번 여러분들께서 이거를 따라서 한번 쑥쑥 써보시면은 여기만 또 남게 됩니다. 이것도 여러분들께서 라인 바이 라인으로 그대로 좀 따라갈 수 있게 모든 과정을 다 자세하게 한번 적어놨으니까 한번 여러분들께서 확인을 한번 해보시면 좋겠습니다. 그럼 딱 이것만 남는 거예요. 그렇죠 그럼 여기 인티그랄이 여기 밖에 있는 거니까 그러면은 우리가 나머지 이 부분을 여기다가 쏙 집어넣을 수가 이제 있게 되는 겁니다. 자 그러고 나서 그러면 우리가 에타를 밖으로 빼낼 수가 있게 되는 것이고 그렇죠 자 그러면 우리가 여기서 한번 살펴보면은 에타는 임의의 함수였어요. 이 조건을 만족하는 그쵸? 이 조건을 만족하는 임의 함수였습니다. 그러한 임의 함수 모든 에타에 대해서 사실은 이 시기 만족을 해야만 하는 거예요. 얘가 0이어야 됩니다. 그러면 뭐 해야 될까요? 에타가 특정한 바운더리 컨디션을 만족할 수 있는 아비트로리 함수인 건데 임의 함수인 건데 그러한 모든 임의 함수에 대해서 이 적분했을 때 0이 된다 그러면 이 안에가 뭐가 돼야 된다 이 안에가 0이 되면 된다. 이 안에가 0이 되면은 그러면은 이 적분값이 0이 되니까 그래서 얘가 0이 되는 값이 우리는 익스트리머이라고 부릅니다. 그렇죠 결론을 우리가 한번 보면은 결론을 한번 봅시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 3,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1031,
      "char_count": 1909
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c004_d8fee2",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 그러한 임의 함수 모든 에타에 대해서 사실은 이 시기 만족을 해야만 하는 거예요. 얘가 0이어야 됩니다. 그러면 뭐 해야 될까요? 에타가 특정한 바운더리 컨디션을 만족할 수 있는 아비트로리 함수인 건데 임의 함수인 건데 그러한 모든 임의 함수에 대해서 이 적분했을 때 0이 된다 그러면 이 안에가 뭐가 돼야 된다 이 안에가 0이 되면 된다. 이 안에가 0이 되면은 그러면은 이 적분값이 0이 되니까 그래서 얘가 0이 되는 값이 우리는 익스트리머이라고 부릅니다. 그렇죠 결론을 우리가 한번 보면은 결론을 한번 봅시다. 결국에 지금 여러분들 한번 보시면 우리가 아이라는 거에 대한 요거에 대한 지금 적분으로 들어 에프가 적분으로 감싸져 있는 이거에 대한 익스트림 포인트를 우리가 찾고 싶은 거였어요. 그렇죠 그러면은 어떻게 구하면 된다 이 안에 있는 에프에 대해서 우리가 잘 미분해서 0되는 값을 찾으면 된다라는 얘기를 하고 있는 겁니다. 아시겠죠? 여러분 그러면 우리가 해결이 된다는 겁니다. 그런데 지금 우리가 다시 한 번 보면은 엘본은 지금 이렇게 생겼죠 엘본은 1번 이렇게 생겼습니다. 그렇죠 그리고 여기 컨스트레인 하나 걸려 있어요. q는 우리가 인테그라 했을 때 1 일단은 이 컨트레인을 우리가 무시하겠습니다. 일단은 왜 큐도 우리가 확률 분포라고 하기로 했으니까 인티그럴 하면 1 돼야죠. 일단은 이거 무시하고 봅시다. 여긴데 지금 얘가 여기서는 프라고 하면은 여기서는 우리가 그 큐라고 보면 됩니다. 그쵸 즉 여기서 와 와도 우리가 큐라고 생각하시면 됩니다. 그럼 그때 여러분 한번 생각해 보면 이게 지금 여기 인티그랄 안에 있는 거를 우리는 이 라지 f라고 볼 수 있겠죠. 인티그랄 안에 있는 거 이거 이거 빼기 요게 라지프라고 볼 수 있습니다. 자 얘는 한번 살펴보시면 큐 프라임이 없어요. q에 대한 함수입니다. 큐프라임이 없습니다. 그 말은 이거는 0이라는 뜻이에요. q프라임이 없는데 이 f를 지금 이 안에를 큐프라임에 대해서 미분해 봤자 0이잖아요. 그럼 이거 없어지는 겁니다. 그럼 뭐만 남는다 이것만 남는다 그러면은 매우 간단하게 이 프 즉 인티그랄 안에 있는 거를 그냥 큐로 미분하면 된다 그랬을 때 그냥 그게 0 되는 걸 찾기만 하면 된다는 거예요. 아시겠죠? 그러면 q로 미분합니다. 이거 q로 미분하면 돼요. 그렇죠 큐롬 미분에서 우리가 0 되는 값을 찾으면 우리는 q가 그쵸 요거라고 말할 수 있겠죠. 즉 익스포넨셜까지 씌운 거 근데 인티그랄 했을 때 1이 돼야 된다라는 조건이 또 우리가 걸려 있는 거지 않겠습니까? 그쵸 그래서 우리가 이 인티그랄 조건까지 딱 걸어주면은 이걸 하기 위해서 노멀라이징 콘텐트를 분모에 쏙 넣으면 이거 적분했을 때 1이 나오는 형태가 됩니다. 그 까닭에 우리가 mfvi를 성공적으로 또 유도를 할 수가 있었던 거예요. 그렇죠 여러분 아시겠죠? 즉 여기까지 보면은 mfvi를 유도할 수 있는 방법 우리가 그냥 세 가지를 배운 겁니다. 지난번에 배웠던 그 로그 피틀다 이용하는 거 그다음에 로그 익스포넨셜을 이용하는 거 그다음에 이번과 같이 펑셔널 디리버티브를 이용해서 그냥 애초에 얘가 맥시마이즈 되는 거를 그라이언트로 찾아버리자라는 겁니다. 그리고 그 과정에서 오늘 저희가 또 어떤 걸 배웠습니까? 펑셔널이라는 게 뭐고 펑셔널 디리v티가 뭔지 배웠습니다. 이거는 굉장히 많이 활용되는 수학 개념이기 때문에 여러분들께서 꼭 기억하고 가시면 좋겠습니다. 자 그러면 지금까지 저희가 변분 추론이라는 게 뭔지 한번 살펴봤는데요. 이거는 우리가 생성 모델을 학습할 때 많이 활용된다고 했습니다. 자 그러면 얘를 실제로 좀 어떻게 활용할 수 있고 이엠 알고리즘하고 어떻게 다른 건지 저희가 한번 보도록 하겠습니다. 자 EM 알고리즘을 먼저 한번 보도록 하겠습니다. EM 알고리즘을 이미 배우신 분들도 있겠지만은 아직 안 배우신 분들도 있기 때문에 EM 알고리즘을 저희가 한번 기초부터 보도록 하겠습니다. 자 이엠 알고리즘을 보기 위해서 우리가 다음과 같은 믹처브 베르누이를 한번 생각해 보겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 4,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1087,
      "char_count": 2032
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c005_109581",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 자 그러면 얘를 실제로 좀 어떻게 활용할 수 있고 이엠 알고리즘하고 어떻게 다른 건지 저희가 한번 보도록 하겠습니다. 자 EM 알고리즘을 먼저 한번 보도록 하겠습니다. EM 알고리즘을 이미 배우신 분들도 있겠지만은 아직 안 배우신 분들도 있기 때문에 EM 알고리즘을 저희가 한번 기초부터 보도록 하겠습니다. 자 이엠 알고리즘을 보기 위해서 우리가 다음과 같은 믹처브 베르누이를 한번 생각해 보겠습니다. 즉 n개의 바이너리 데이터가 있는 겁니다. 스원부터 스n까지 바이너리 입니다. 각각 x1은 01 2도 01 각각 바이너리라고 생각해 주시면 되고요. 자 이런 엔게이 데이터가 k개의 분포에서 나왔다는 겁니다. k개 클러스터에서 엔게이 데이터가 만들어졌다 자 그리고 이 베르누이 분포는 우리가 큐케라는 걸 파라미터로 가진다고 할게요. 즉 쉽게 말해서 우리가 동전을 던진다고 할 때 동전의 앞면이 나올 확률 0.6 뒷면 나올 확률 0.4 그런 확률 값이 있어야 되잖아요 그러한 베르누이 분포에 대한 확률 값입니다. 자 그러면은 그때 xn이라는 게 만약에 뭐 큐케 라는 그 k번체의 베르누이 분포에서 만약에 나왔다라고 하면 우리가 이렇게 표현을 할 수가 있겠습니다. 그러면은 이 xn이라는 데이터가 몇 번째 클래스에서 나왔는지는 우리가 알 수 없지만 만약에 k번째 클래스에서 나왔다고 한다면 그때는 이렇게 qk가 있고 그다음에 뭐 1이 나올 확률이겠죠 얘가 1이 나왔는지 아니면 1 마이너스 qk 0이 나올 확률이겠죠 얘가 0이 나왔는지 1이 나왔는지 우리가 알 수가 있게 됩니다. 즉 이렇게 우리가 모델링을 할 수가 있게 되는 것이고요. 그때 우리가 그러면 PX 전체 데이터에 대해서 우리가 어떻게 모델링 할 수 있냐라고 하면은 이렇게 모델링을 할 수가 있겠습니다. 일단 데이터와 인기가 있어요. n기 그렇죠 데이터 연기가 있고 각각의 이제 엠번트 데이터에 대해서 얘가 케번제 클러스터에서 만약에 나왔다고 하면은 어느 정도의 확률로 얘가 나올지 계산하는 게 있겠죠. 그다음에 실제로 우리는 클러스터가 케 개가 있습니다. 실제로 그렇죠 클러스터가 k개가 있다 보니까 그러면은 k개의 클러스터에 대해서 우리가 서메이션을 해줘야 되는 거예요. 그렇죠 그리고 이 파이케이는 우리가 각 클러스터의 비율로 생각해 주시면 됩니다. 즉 k개 클러스터가 있다면 이제 첫 번째 클러스터는 비율이 한 0.7 두 번째 클러스터는 0.3 이런 식으로 우리가 합이 1이 되는 파이케를 만들 수가 있겠습니다. PX를 우리가 이렇게 정리를 할 수가 있겠고요. 즉 지금 케익의 클러스터가 있고 각 클러스터에서 얘가 나오게 될 확률 그다음에 각 클러스터의 비중 이 나와 있는 것뿐입니다. 여기에다가 로그를 씌우게 되면 로그의 프로덕트니까 얘가 서메이션 로그로 바뀌게 되겠죠 이렇게 바뀝니다. 자 여기서 우리가 추론해야 되는 파라미터가 뭐가 될까요? 우리가 추론해야 되는 우리가 모르는 파라미터 그거는 두 개입니다. 파이케 모르죠 각 클러스터의 비율 왜 우리는 엑셀만 아는 거예요 관측 데이터가 이거밖에 없는 거예요. 엔게이 데이터만 있는데 그 엔게이 데이터가 어떤 분포에서 나왔을까 알고 싶은 거예요. 그래서 이 베르누이 분포의 파라미터를 추정해야 되고 큐를 그다음에 그러한 베르누이 분포의 그러한 베르니의 분포가 케이개가 있다고 할 때 그 케이개를 비중을 우리가 추론해야 되는 겁니다. 자 그래서 우리가 여기에 대해서 얘를 맥시마이즈 할 수 있는 큐랑 파일을 찾는 게 목표예요. 그러면 어떻게 하면 됩니까? 큐를 미분하자는 거예요. 큐를 미분하면 그렇죠 얘를 우리가 큐에 대해서 한번 미분을 한번 해 보면 이렇게 쓸 수가 있겠습니다. 그렇죠 여기 로그가 있으니까 얘가 여기로 가게 되고 그렇죠 그다음에 여기 이제 나오게 되겠죠 그러고 나서 얘가 0이 되는 포인트를 찾으면 되는 거예요. 자 근데 피에 스 바 xn바 큐케가 뭐라고 했습니까? 이거라고 했습니다. 그럼 얘를 집어넣고 q케에 대해서 미분하면 되는 거예요. 그렇죠 그러면은 이렇게 나오게 되는데 얘를 우리는 f라고 정의하고 넘어가겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 5,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1090,
      "char_count": 2038
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c006_340448",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 그렇죠 여기 로그가 있으니까 얘가 여기로 가게 되고 그렇죠 그다음에 여기 이제 나오게 되겠죠 그러고 나서 얘가 0이 되는 포인트를 찾으면 되는 거예요. 자 근데 피에 스 바 xn바 큐케가 뭐라고 했습니까? 이거라고 했습니다. 그럼 얘를 집어넣고 q케에 대해서 미분하면 되는 거예요. 그렇죠 그러면은 이렇게 나오게 되는데 얘를 우리는 f라고 정의하고 넘어가겠습니다. 얘를 f라고 정의합시다. 편의상 얘를 f라고 정의하겠습니다. 자 그러면 우리는 뭡니까? 얘를 우리가 다시 쓰게 되면 이렇게 되겠죠 왜 얘를 적은 게 이거니까 이 빨간색 박스 자리에 얘를 넣으면 되겠죠 그러면 파케에 비프가 나오게 됩니다. 얘가 0 되는 포인트를 우리는 찾는 게 목표입니다. 얘가 0 되는 qk를 찾아야 되는 거죠. 자 그러면은 그걸 우리가 어떻게 찾을 수 있을까 인 건데 지금 당장은 사실 얘를 우리가 큐케를 정확하게 계산하기가 어렵습니다. 왜 이거 0되는 큐케를 찾으려면 여기 안에도 지금 여기에도 큐케가 들어가 있고 그렇죠 여기에만 들어가 있으면 다행인데 q가 지금 여기도 q j 여기 있고 그렇죠 여기도 뭔가 qk가 있고 지금 qk가 여기도 있고 여기도 있고 여기도 있고 엄청 복잡한 형태의 방정식을 풀어야 되는 겁니다. 그래서 여기서 바로 우리가 큐케를 구하기는 어려워요. 그래서 일단은 저희가 다른 방식으로 한번 구해보자는 겁니다. 자 어떻게 구할 거냐면 일단은 우리가 얘를 한번 살펴봅시다. 예 자 얘의 의미를 한번 보겠습니다. 자 여기서 우리가 포스테리어라고 하는 것은 이렇게 모델링을 할 수가 있어요. 포스테리어라는 것 즉 스엔이라는 데이터가 지금 주어진 겁니다. 스앤이라는 데이터가 주어지고 뭔가 파라미터가 주어진 거예요. 여기에 뭐 q도 될 수 있고 파일도 될 수 있는 겁니다. 그때 제 엔케이가 1이 될 확률 값입니다. 얘가 우리가 포스테리어 제트는 우리가 모르는 레이턴트 엑스는 우리가 관측한 거 그러면 q 지바스를 우리가 포스테리어라고 부를 수가 있겠죠. 즉 n번젠 데이터가 관측됐는데 이 n번젠 데이터가 케 번제 클러스트에서 나왔을 확률을 우리가 포스테리어로 정의한 겁니다. 엠 번젠 데이터를 딱 관측을 한 거예요. 그게 주어졌을 때 그게 케번체 클러스터에서 나왔을 확률 즉 제 엔케라는 거는 어사인먼트라고 생각하시면 됩니다. 어사인먼트 즉 엠 번체 데이터가 몇 번째 클러스터에서 왔는지를 계산하는 거예요. 아시겠죠? 그럼 만약에 얘가 z n이 첫 번째 클래스에서 왔다 그러면 z n1이 1이 되는 겁니다. 이런 식으로 아시겠죠? 그러고 나서 우리가 얘를 또 한번 보면은 자 제 엔케가 1이라는 말은 엠 번째 데이터가 케 번째 클래스에서 왔다는 거예요. 그럼 k번체 클러스터에서 이 xn이 생성될 확률은 우리가 이렇게 적을 수가 있겠죠 이렇게 그렇죠 이렇게 적으면 그 베르닐 분포 k 번체 베르닐 분포에서 xn이 샘플링 되는 확률을 생각하는 겁니다. 파이k는 뭐예요? 각 클러스터의 비중입니다. 그러면 이 n 번째 데이터가 k번째 클러스터에서 왔을 확률이라는 말은 우리가 얘를 전체 데이터에 대해서 생각하면은 각 클러스터의 비중으로 또 생각을 할 수가 있겠습니다. 그래서 얘를 우리가 파이키라고 적을 수가 있겠습니다. 자 그러면 얘가 뭐다 얘가 결국 포스테리어가 되는 거예요 포스테리어 그렇죠 자 여러분 근데 여기서 중요한 게 뭐냐면 이 포스테리어를 방금 정확하게 구했어요. 앞에서 우리가 봤던 여러 가지 예제들은 이 포스테리어를 정확하게 못 구하는 게 문제였어요. 그렇죠 포스테리어를 정확하게 못 구하니까 우리가 베르시 인퍼런스를 쓰고 그래도 못 구하니까 민필드 어썸션을 통해서 민필드의 베레션 인퍼런스를 쓴 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 6,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 989,
      "char_count": 1848
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c007_bade90",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 그래서 얘를 우리가 파이키라고 적을 수가 있겠습니다. 자 그러면 얘가 뭐다 얘가 결국 포스테리어가 되는 거예요 포스테리어 그렇죠 자 여러분 근데 여기서 중요한 게 뭐냐면 이 포스테리어를 방금 정확하게 구했어요. 앞에서 우리가 봤던 여러 가지 예제들은 이 포스테리어를 정확하게 못 구하는 게 문제였어요. 그렇죠 포스테리어를 정확하게 못 구하니까 우리가 베르시 인퍼런스를 쓰고 그래도 못 구하니까 민필드 어썸션을 통해서 민필드의 베레션 인퍼런스를 쓴 겁니다. 그렇죠 왜 포스트를 못 구했나요? 이 피의 지바스를 계산하기 위해서는 우리가 보통은 피 지바이스를 계산하려면은 피의 지바스를 구하기 위해서는 우리가 피스분의 피스 콤마 제트를 계산해야 되는데 피스라는 게 뭡니까? 피스라는 거는 우리가 피스 콤마 제트를 또 적분하는 거예요. 제에 대해서 얘가 구하기 어려워서 얘를 우리가 모르니까 그래서 우리가 포스테리어로 추정을 못한다고 했던 건데 여기서는 굉장히 깔끔하게 우리가 포스테리어를 추정을 할 수가 있는 겁니다. 아시겠죠? 그러고 나면은 우리가 얘를 포스테리어로 추정하고 나면은 얘를 우리가 평인상 감마 제nk라고 그냥 부르겠습니다. 편의상 그렇게 부르고 그다음에 얘를 우리가 이 제트 엔케이에 대한 익스펙테이션이라고 봐도 되겠죠. 얘를 얘를 익스펙테이션이라고 봐도 됩니다. 왜냐하면 결국 제 키가 바이너리니까 이게 가능한 거예요. 왜 그러면 이거에 대한 기댓값은 뭐예요? 1 곱하기 이거 뭐 더하기 0 곱하기 이거 하면은 1 곱하기 이것만 남게 되니까 그렇죠 그래서 우리가 기댓값으로 표현을 할 수가 있게 되는 겁니다. 그래서 예나 그다음에 예나 우리가 다 같은 말로 여러분들이 생각해 주시면 되겠습니다. 어찌 됐든 포스테리어를 우리가 구할 수 있다. 정확하게 그게 우리는 중요한 겁니다. 자 그럼 다시 돌아가서 아까 우리가 큐로 미분했을 때 이거 0 되는 값 찾는 게 목표였어요. 그렇죠 q로 미분했을 때 이거 0 되는 값 찾는 게 목표였습니다. 근데 얘가 q가 여기도 있고 여기도 있고 여기도 있고 해서 우리가 정확하게 계산을 못 했던 거예요. 얘를 우리가 앞에서 구했던 포스테리어로 감마로 우리가 치원을 하겠습니다. 감마로 71 그러면 얘를 푸는 거죠 그렇죠 얘를 푸는 겁니다. 물론 얘도 여전히 여기에 큐와 관련된 텀이 있기 때문에 얘가 0이 되는 정확한 큐를 우리가 구할 순 없어요. 그래서 일단 여기까지만 두고 저희가 넘어가겠습니다. 그다음에 파이를 한번 볼게요. 파이 파이에 대해서도 우리가 구해야 되니까 파이도 한번 미분하자는 거예요. 근데 파이는 뭐예요? 비중이 있는 거죠? 비중 파이는 비중이니까 합이 1이 돼야 돼요. 클러스터가 3개가 있으면 파이는 3차원의 벡터입니다. 0.2 콤마 0.3 콤마 0.5와 같이 합이 1이 되는 그래서 합이 1이 되는 걸 보장하기 위해서 우리가 이렇게 라그랑시 멀티플라이어를 우리가 쓰게 됩니다. 그러고 나서 얘가 파이에 대해서 미분했을 때 0 되는 값을 찾고자 합니다. 그러면은 여기 파이에 대해서 미분하면 이렇게 되고 이거 0 되는 값 한번 찾아본다고 가정하면 이거 0 되는 값 한번 찾아본다고 가정하면 파이k를 여기 양변에 곱하고 파이케를 요 여기 분자에 곱해지겠죠. 여기 여기 곱해지겠죠. 파이케가 그다음에 케이에 대해서 서메이션을 돌립니다. 그러면 여기에서 케이에 대한 서메이션이 여기 들어가게 되고 케이에 대한 서메이션이 여기 들어가게 됩니다. 그렇죠 근데 여러분 여기서 한번 보시면 분자 분모가 같아요. 인덱스만 다르지 얘를 그냥 k라고 쓴 거 여기는 j라고 쓴 거지 결국에는 파이 1 PXN 바 q1 파이1 PXN 바 q1 그다음에 2 3 똑같아요. 약분되겠죠 1이라고 약분되겠죠. 1을 n번 더 한다 그러면 n이 됩니다. 그러면은 뭐예요? 얘가 결국엔 n이 되니까 그러면은 여기서 람다가 밖으로 나오면 서메이션 파이케이는 뭐예요? 1이잖아요. 엔 더하기 람다가 0이 된다는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 7,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1055,
      "char_count": 1975
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c008_88ea9d",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 그렇죠 근데 여러분 여기서 한번 보시면 분자 분모가 같아요. 인덱스만 다르지 얘를 그냥 k라고 쓴 거 여기는 j라고 쓴 거지 결국에는 파이 1 PXN 바 q1 파이1 PXN 바 q1 그다음에 2 3 똑같아요. 약분되겠죠 1이라고 약분되겠죠. 1을 n번 더 한다 그러면 n이 됩니다. 그러면은 뭐예요? 얘가 결국엔 n이 되니까 그러면은 여기서 람다가 밖으로 나오면 서메이션 파이케이는 뭐예요? 1이잖아요. 엔 더하기 람다가 0이 된다는 겁니다. 그 람다는 그래서 마이너스 1이 나오는 거예요. 람다 마이너스 그러면 다시 여기다가 마이너스 앤을 집어넣고 양옆에 파일을 곱한 다음에 넘겨줍니다. 그러면 파이가 여기 곱해져 있고 얘는 마이너스 n이었는데 넘어가니까 n이고 파이 곱하니까 파이 n이 나오는 거예요. 그렇죠 자 그러면 얘는 뭐예요? 우리가 앞에서 봤던 거 포스테리어 그렇죠 그럼 포스테리어가 되는 거고 그러면은 우리가 파이케이는 이렇게 우리가 정의할 수 있겠죠. 그러면은 여기서도 물론 이 감마 제테케를 구하려면 파이가 들어가요. 그렇죠 그래서 얘를 파이에 대해서 뭔가 정리해서 구할 수는 없다는 겁니다. 정확하게 대신에 얘를 그냥 우리가 이미 알려져 있는 값으로 그냥 생각하자 그러면 파이는 이렇게 된다라고 우리가 계산할 수 있겠죠. 그러면 어떻게 돼요? q도 우리가 얘를 그냥 알고 있는 값으로 치자는 겁니다. 그러면 이 FQK xn이라는 건 뭔지 우리가 이미 앞에서 봤습니다. 그렇죠 그거는 우리가 뭐라고 하기로 했어요 이거 그러면 q에 대한 뭔가 그 많아봤자 우리가 뭐 분모의 2차 방정식 밖에 안 들어가는 겁니다. 그러면 우리가 큐를 구할 수 있는 거예요. 얘를 0으로 하는 q를 구할 수 있는 겁니다. 어떻게 하면 얘를 마치 우리가 상수 취급해버리면 즉 얘는 마치 q에 대한 함수가 아닌 거예요. 얘는 파이에 대한 함수가 아닌 거예요. 그러면 우리가 파이는 그냥 이겁니다. 그다음에 얘는 q에 대한 함수가 아닌 것처럼 계산하면 q에 대한 함수는 여기만 있는 거고 그럼 얘를 우리가 q에 대해서 잘 정리하면 최적의 큐를 구할 수 있는 거예요. 그러면 큐랑 파이가 나오죠. 그럼 큐랑 파이가 나오면 다시 이 포스테리어를 구할 때 파이랑 큐가 쓰이긴 하니까 여기다 넣어가지고 감마를 업데이트합니다. 그러면 감마가 또 바뀌었어요. 그러면 다시 여기다가 넣으면 q가 나오고 파이가 나요 그러면 q가 나오고 파이가 나오면 다시 여기 넣으면 다시 감마가 나요. 다시 놓고 다시 놓고 이터러티브하게 우리가 업데이트합니다. 이게 EM 알고리즘이에요. 왜 얘가 마치 우리가 디탱크에 대한 익스펙테이션이라고 생각할 수 있어 익스펙테이션 계산하고 그다음에 이거 전체가 맥시마이즈가 되도록 한 거였죠 왜 미분해서 0되는 갖 구한 거니까 얘가 맥시마이즈가 되도록 파일 한 큐를 업데이트해 주고 다시 익스펙테이션 구해주고 맥시마이즈 해주고 그렇기 때문에 우리가 EM 알고리즘이라고 부른다는 겁니다. 자 지금까지 우리가 보면 베르세 인퍼런스를 앞에서 봤었고 그다음에 방금은 이엠을 봤어요. 자 두 개가 뭐가 다를까 뭐가 같을까 한번 지금부터 보도록 하겠습니다. 이러한 이엠과 VI를 지금부터 한번 비교를 해보자면 사실 여러분들께서 이미 이엠을 알고 있던 분들은 이게 뭐 어떻게 연결되는 거지 또 이제 오히려 또 반대로 두 개가 뭐 좀 비슷한 건가 완전 같은 건가라는 착각을 좋아하시는 분들도 굉장히 많습니다. 사실 뭐 이런 거를 그 물어보면 되게 모르는 사람도 되게 많습니다. EM 따로 배우고 VI 따로 배우고 하니까 EM과 VI 가 그래서 어떤 연결 관계가 있고 어쩔 때 EM을 쓰고 어쩔 때 VI를 쓰는 건지 모르는 사람들이 굉장히 사실 많아요. 그거를 저희가 한번 짚고 넘어가도록 하겠습니다. 왜냐하면 저희는 결국 r시스를 잘하게 하기 위한 머신러닝의 알고리즘을 근본을 저희가 학습하는 거기 때문에 그리고 이러한 거는 결국 뭐 렉시스뿐만 아니라 NLP 비전 다 생성 모델 쪽에서 굉장히 많이 활용되고 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 8,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1063,
      "char_count": 2005
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c009_717a1b",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 사실 뭐 이런 거를 그 물어보면 되게 모르는 사람도 되게 많습니다. EM 따로 배우고 VI 따로 배우고 하니까 EM과 VI 가 그래서 어떤 연결 관계가 있고 어쩔 때 EM을 쓰고 어쩔 때 VI를 쓰는 건지 모르는 사람들이 굉장히 사실 많아요. 그거를 저희가 한번 짚고 넘어가도록 하겠습니다. 왜냐하면 저희는 결국 r시스를 잘하게 하기 위한 머신러닝의 알고리즘을 근본을 저희가 학습하는 거기 때문에 그리고 이러한 거는 결국 뭐 렉시스뿐만 아니라 NLP 비전 다 생성 모델 쪽에서 굉장히 많이 활용되고 있습니다. 자 한번 보면은 우리가 베르시 인퍼런스를 다시 한 번 복습을 한번 해보겠습니다. 밸류자 인퍼런스를 유도하는 과정이 우리가 많았는데 그중에 두 번째로 유도하는 과정은 다음과 같이 케일 레벤스를 이용해서 케일 레벤스의 정의를 이렇게 이용해서 하는 방법이 있었습니다. 그래서 우리가 결국 계속 강조 드리는 것은 결국 얘를 우리가 MA시마이즈 할 건데 li를 mx마이즈 할 건데 li를 mx마이즈 한다라는 말은 qig가 이거랑 가까워져서 케엘이 0에 가깝게 된다라는 뜻으로도 해석이 된다는 겁니다. 즉 얘를 맥시마이즈 하면은 우리가 q가 점점 더 포스테리에 가까워진다 또는 반대로 q를 우리가 포스트 에리어에 가깝게 추정할 수만 있다면 얘가 이거에 가까워진다라는 얘기를 하는 겁니다. 왜 얘는 고정인 거니까 데이터에 대한 라이클로드는 우리가 고정된 상황으로 지금 생각을 하는 거니까요. 자 즉 포스테리어 추정이 굉장히 중요했습니다. 왜 포스테리어만 딱 추정된다면 되는 거니까 그래서 아까 우리가 앞에서 윈필드의 베리션 인퍼런스 같은 거 볼 때도 이 q를 우리가 추정하는 것을 목표로 했었습니다. 근데 만약에 우리가 여기서 q를 정확하게 추정할 수 있다면 어떨까요? q라는 거를 이렇게 정확한 포스테리어를 추정할 수 있다면 즉 q가 얘랑 완전히 같아진다면 어떻게 될까요? 그때는 우리가 구한 이 엘보가 마지널 라이클로드 정확하게 로그 마지널 라이키드랑 완전히 같아지는 겁니다. 이 부등호가 아니라 등호가 되는 거죠. 왜 얘가 0이 돼 버리니까 얘랑 얘는 같아지는 겁니다. 만약에 우리가 정확한 포스테리어를 추정할 수만 있다면 그렇죠 그러면은 우리가 또 베니션 인퍼런스를 뭐 이런 식으로 우리가 막 썼었는데 자 그럼 여기서 우리가 q 지랑 피지바스가 완전히 동일하게 되면 어떻게 될까요? 자 여기 피스 콤마 지 자리는 우리가 피지바스 곱하기 피스 그다음에 여기는 우리가 큐 제 자리에 피지바스가 들어가니까 여기 피지바스 피지바스가 들어가는 겁니다. 그러면 우리가 남는 거는 이것만 남겠죠 왜 피지바이스 피스 피지바스니까 빼서 피스만 남게 됩니다. 자 그러면은 얘가 이엠 알고리즘이랑 동일한 거죠. 이엠 알고리즘에서도 포스테리어를 구한 다음에 그 포스테리어를 우리가 이용해서 얘를 맥시마이즈 하는 게 목표였습니다. 그렇죠 자 한번 보시면은 우리가 포스테리어 구한 다음에 얘를 맥시마이즈 하는 우리가 큐랑 파일을 찾는 게 목표였습니다. 그 까닭에 완전히 동일하다라고 볼 수가 있습니다. 즉 무슨 말이냐면 우리가 베르션 인퍼런스를 쓰는 이유는 포스트를 못 구하기 때문에 즉 포스트를 정확하게 구할 수 있는 상황이면 베르션 인퍼런스랑 이엠이랑 완전히 동일해진다. 근데 포스테리어를 많은 경우에는 못 구하니까 이런 베르니 믹스처 모델 같은 경우는 약간 스페셜 케이스 포스테리어를 구할 수 있는 상황이니까 이엠이 된 겁니다. 배려저 인퍼런스는 포스테리를 못 구하는 상황에서 근사하기 위한 알고리즘으로 생각하시면 됩니다. 그리고 포스테리어를 구할 수 있는 상황이면 동일해진다. 이게 그러면 우리가 방금 베르니 믹스처 모델을 봤는데 가우시안 믹스처 모델도 우리가 유사하게 볼 수 있습니다. n개의 데이터가 라지 엔게만큼 존재하고 있고 이 데이터가 클러스터 케이 개 중에서 어떤 클러스터에서 왔는지 제앤이 1인지 제엔이 케인지 얘는 이 데이터가 어떤 클래스에서 왔는지를 나타내는 인디케이터 또는 어사인먼트입니다. 여기까지는 앞에서 봤던 베르니 믹스처 모델 똑같아요. 여기까지도 똑같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 9,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1107,
      "char_count": 2042
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c010_5c0df5",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 그리고 포스테리어를 구할 수 있는 상황이면 동일해진다. 이게 그러면 우리가 방금 베르니 믹스처 모델을 봤는데 가우시안 믹스처 모델도 우리가 유사하게 볼 수 있습니다. n개의 데이터가 라지 엔게만큼 존재하고 있고 이 데이터가 클러스터 케이 개 중에서 어떤 클러스터에서 왔는지 제앤이 1인지 제엔이 케인지 얘는 이 데이터가 어떤 클래스에서 왔는지를 나타내는 인디케이터 또는 어사인먼트입니다. 여기까지는 앞에서 봤던 베르니 믹스처 모델 똑같아요. 여기까지도 똑같습니다. 달라지는 건 우리가 결국에 여기가 결국 민과 코베리언스로 된다는 것만 달라지게 되는 것입니다. 그래서 이렇게 가우시안이 그냥 이제 들어가게 되는 것뿐이에요. 가우시에 들어가는 게 되는 거고 우리가 로그를 통해서 표현하면은 원래는 여기에 베르누이였을 거가 가오시으로만 바뀌었을 뿐입니다. 그리고 앞에서 했던 거랑 똑같아요. 제 앤케에 대한 포스테리어 구하는 것은 앞에랑 똑같습니다. 똑같아요 다만 여기가 이제는 가오션으로만 바뀐 거다 그것만 달라진 거죠. 여기서 포스테리어를 구하고 나면은 그 뒤에는 앞서서 봤던 베르니 믹처 모델과 완전히 동일하게 이거를 결국 맥시마이즈 하는 뮤 시그마 파이만 찾으면 됩니다. 베르뉴와 다른 점은 결국 우리가 가오션이다 보니까 민과 코베레스가 둘 다 필요하다라는 것만 다르겠죠 베르누에서는 확률 값 하나 큐케이만 있었던 반면에 그렇죠 그걸 제외한 나머지는 똑같습니다. 그러면 우리가 최적의 뮤 코베리언스 파이 구할 수 있고 그렇죠 그럼 파 뮤 코베리언스 파이 나왔으니까 다시 우리가 여기서 감마 포스테리어를 우리가 업데이트하고 다시 계산하고 다시 업데이트하고 이런 이엠 알고리즘으로 진행하게 됩니다. 즉 가우지 믹스처 모델도 사실상 포스트 에리어를 정확하게 구할 수 있음으로써 우리가 베리션 인퍼런스를 쓸 필요가 없는 겁니다. 자 근데 여기서 이제 우리가 베이지 가우시안 믹스처 모델을 한번 볼게요. 지금 무슨 말이냐면 여기서 베이지이라고 하는 것은 프라이어를 우리가 도입한다라고 생각하시면 됩니다. 즉 예를 들어서 여기 나와 있는 뷰가 뭐 특정한 범위 안에 있을 것 같다든지 이 파이가 뭐 좀 유니폼에 가까울 것 같다든지 아니면 이 파이가 어느 한쪽에 좀 많이 쏠려 있을 것 같다든지 그러한 우리의 프라이어 놀리지를 모델에 주입할 수 있는 방법 중에 한 가지가 베이지안 방법론입니다. 그럼 여기서 한번 보시면 이 파이에 대해서 뭔가 프라이어가 걸려 있는 거예요. 여기 나와 있는 코베이런스랑 여기 나와 있는 민도 마찬가지입니다. 여기서 이제 람다로 표현했는데 얘는 코베이언스의 그 인버스 프리시즌으로 생각해 주시면 되겠습니다. 자 여기서 그러면 여러분 프라이어라는 걸 왜 도입할까요? 즉 무슨 말이냐면 이 파일을 그냥 우리는 아까 최적화시킬 대상으로 봤습니다. 파이가 업데이트가 되는 거예요. 근데 그게 아니라 뭔가 이 파이는 디리클레 분포라는 데서 나왔을 것 같다. 디리클레 분포는 결국 확률 값을 내뱉어 주는 확률 분포라고 생각하시면 됩니다. 즉 우리가 가오시에서 샘플링 하면은 마이너스 무한에서부터 무한까지의 값이 나오잖아요. 디를 클릭해서 샘플링하면 확률 값 케디맨션의 확률 값이 나온다고 생각하시면 됩니다. 또는 케이 마이너스 1차원의 심플렉스가 나온다라고 생각하셔도 되고요. 자 그런 식으로 우리가 모델링을 여기서 진행할 겁니다. 이것도 마찬가지로 여기 나와 있는 코베레스 또는 여기 있는 인버스 프리세션도 우리가 위샤르트 분포라는 데서 뽑을 거예요. 자 그런 식으로 우리가 이런 프라이어를 주는 이유가 뭘까요? 먼저 첫 번째로 자 우리가 디리클레에서 뽑게 되면 디리클레 알파에서 이 파이어가 나온다 그러면은 이 파이는 합이 1이 되는 게 보장이 돼요. 우리가 이 합이 1이 되도록 보장하기 위해서 어떻게 했습니까? 라그랑지 멀티플레이어 같은 약간 좀 복잡한 과정을 거쳤는데 그럴 필요가 없다라는 점입니다. 또 여기 나와 있는 프리시즌 한번 생각해 봅시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 10,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1088,
      "char_count": 1983
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c011_ae07b8",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 이것도 마찬가지로 여기 나와 있는 코베레스 또는 여기 있는 인버스 프리세션도 우리가 위샤르트 분포라는 데서 뽑을 거예요. 자 그런 식으로 우리가 이런 프라이어를 주는 이유가 뭘까요? 먼저 첫 번째로 자 우리가 디리클레에서 뽑게 되면 디리클레 알파에서 이 파이어가 나온다 그러면은 이 파이는 합이 1이 되는 게 보장이 돼요. 우리가 이 합이 1이 되도록 보장하기 위해서 어떻게 했습니까? 라그랑지 멀티플레이어 같은 약간 좀 복잡한 과정을 거쳤는데 그럴 필요가 없다라는 점입니다. 또 여기 나와 있는 프리시즌 한번 생각해 봅시다. 자 여러분 코베리언스는 가우시안 분포의 코베리언스는 어떤 특징이 있어요? 포스티브 데피닛 함수여야 합니다. 포스티브 데피닛 매트릭스야 돼요 얘는 얘는 이 매트릭스는 그냥 일반적인 매트릭스면 안 되고 그렇죠 포스티브 데피닛 매트릭스여야만 하는데 즉 인버스가 존재하는 인버터블한 매트릭스여야 해요. 근데 우리가 코베런스를 막 그냥 업데이트하는 거예요. 이렇게 그러면은 얘가 인버터블하다라는 보장이 사실 없는 겁니다. 어떻게 해야 될까 우리가 이러한 것을 만약에 위샤르트 품포에서 샘플링 한다라고 치면 위샤르트 품포에서 샘플링 하는 거는 항상 포스티브 데피닛 매트릭스예요. 자 우리가 가우시안 분포에서 샘플링 하는 건 항상 마이너스 무한에서 무한이죠. 베타 분포에서 샘플링하는 건 항상 연결되는 사이죠. 우리가 미셔너트 분포에서 샘플링 하는 거는 우리가 포스트브 데피닛 함수가 매트릭스가 고장이 되는 겁니다. 그런 식으로 우리가 프라이어를 도입하게 되면은 좀 그렇게 우리가 발생할 수 있는 싱귤러리티 즉 여기서 뭐 인버터블 하지 않은 게 업데이트 과정으로 막 나올 수가 있다든지 그러한 문제가 일단 해결이 됩니다. 그다음에 아까 말씀드린 것처럼 나는 이 밍이 대략 어느 범위 안에 있을 것 같다라든지 그러한 프라이어 놀리지를 우리가 도입을 할 수 있게 됩니다. 그래서 베이지안 가우시안 미션 모델은 여기 파이를 그냥 러너블 파라미터로 두는 게 아니라 파이가 특정한 확률 분포를 따르도록 모델링 하는 거예요. 여기 나와 있는 코베리언스의 인버터블 형태의 프리시즌 윈 이것도 그냥 러너블 파라미터로 두는 게 아니라 얘가 어떤 분포에서 나왔는지를 우리가 모델링 하는 것이고 그러면 그때 우리가 학습되는 건 뭐가 학습될까요? 디리클레 분포의 파라미터 정규 분포의 파라미터 위샤르트 분포의 파라미터들이 우리가 이제 업데이트 대상이 됩니다. 원래는 파이 6k 코비란스 k가 러너블 파라미터였는데 학습의 대상이었는데 그러한 랜덤 베리어블들이 생성되는 분포에 대한 파라미터가 이제 업데이트 대상이 되는 겁니다. 그리고 여기서 한번 보시면 레이턴트 베리어블들이 이제 여러 개 그렇죠 그러면 앞에서는 레지턴트 베리이 몇 개였다 1개였다 그러면 QA GB x만 계산하면 되는 거였어요. 그리고 계산이 무지 간단하게 끝났습니다. 정확하게 계산됐어요. 트랙터블 합니다. 여기서는 뭡니까? 뭘 계산해야 돼요? 여기서는 계산해야 되는 게 이거 계산해야 큐에 지콤마 파이 콤마 6콤마를 합니다. 이거에 대한 조인트 계산해야 됩니다. 너무 복잡해요. 그렇죠 그래서 우리가 어떻게 한다 독립으로 쪼갠다. 그럼 여기서 qg qg를 일단 독립으로 쪼갭니다. 그다음에 나머지 얘는 사실 그 볼을 한번 굴러보면 베이스볼 알고리즘에 따라서 여기 있는 파이랑 유랑 여기 나와 있는 람다는 독립이라는 게 애초에 보장이 되어 있기 때문에 그래서 우리가 이렇게 얘는 굳이 가정하지 않아도 베이지블 알고리즘 공글로 가지고 독립성 테스트하는 거 그걸로 우리가 쪼갤 수가 있습니다. 추가적인 가정 없이 그렇죠 자 그러고 나서 우리가 이 각각을 이제 구해야 되는 거예요. 이 각각 그걸 어떻게 구한다 앞에서 봤던 것처럼 밈 필드 밸리저 인퍼런스를 우리가 구하면 가장 최적이 되는 q가 뭔지 각각 우리가 구할 수 있다고 하지 않았습니까? 그렇죠 이걸 이용해서 결국 우리가 구해야 되는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 11,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 1078,
      "char_count": 1978
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_4강_변분추론_ii_c012_2c397f",
      "content": "[MLforRecSys] [MLforRecSys] (4강) 변분추론 II\n\n다. 추가적인 가정 없이 그렇죠 자 그러고 나서 우리가 이 각각을 이제 구해야 되는 거예요. 이 각각 그걸 어떻게 구한다 앞에서 봤던 것처럼 밈 필드 밸리저 인퍼런스를 우리가 구하면 가장 최적이 되는 q가 뭔지 각각 우리가 구할 수 있다고 하지 않았습니까? 그렇죠 이걸 이용해서 결국 우리가 구해야 되는 겁니다. 굉장히 사실 그 컨셉은 간단한데 수식이 좀 이제 여러 계산 과정이 좀 필요한 상황이 되겠죠. 그쵸 결국 여기서는 정리하면은 포스트를 못 구한다 왜 피 제바이스 같은 경우는 이렇게 생겼는데 피스가 뭡니까? 지금 그러면은 이 제트에 대해서 우리가 적분해 줘야 되는데 그렇죠 이렇게 생겼습니다. 피스가 이거 계산 못합니다. 그래서 우리가 뭐 쓴다 이러한 포스테리어를 추정할 수 있도록 하는 q를 잡는다. 베레션 인퍼런스를 통해서 그리고 그러한 q를 어떻게 구한다 아까 얘 이것처럼 우리가 앞에서 봤던 것처럼 그렇죠 즉 베이지안 가우시안 액션 모델만 가도 우리가 베르시 인플런스가 반드시 필요한 EM 알고리즘은 안 되는 상황입니다. 이러한 베니스 인플런스를 우리가 왜 배웠냐 생성 모델 기반의 추천 시스템 연구가 굉장히 활발하게 진행되고 있기 때문에 그래서 이렇게 VA 기반의 추천 시스템이라든지 아니면은 디퓨전 모델 디퓨전 모델이라는 거는 다시 한 번 말씀드리지만 뭐 관점에 따라 다르지만은 베레이션 오토인코더에서 디코더가 그냥 여러 개 있는 걸로 생각하셔도 됩니다. 그래서 우리가 점점 더 그럴듯한 이미지를 점점 더 순차적으로 만들어 나가는 거예요. VA는 이제 한 번에 쫙 그럴듯한 이미지를 만드는데 한 번으로 부족하다. 그러면 디 분들처럼 한 번 만들고 여전히 노이즈가 많으면 한 번 더 만들고 또 한 번 더 만들고 그렇죠 그래서 디션 모델도 사실 배려자 인퍼런스랑 굉장히 큰 연관이 있습니다. 네 여러분 여기까지 해서 저희가 베레션 인플런스를 이제 마치도록 하겠습니다. 고생 많으셨습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (4강) 변분추론 II.json",
        "lecture_name": "[MLforRecSys] (4강) 변분추론 II",
        "course": "MLforRecSys",
        "lecture_num": "4강",
        "lecture_title": "변분추론 II",
        "chunk_idx": 12,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:941676c99a19c11c4105d74a7cb63a16734c1c43935b251d4fc029ba3c367f6d"
      },
      "token_estimate": 533,
      "char_count": 995
    }
  ]
}