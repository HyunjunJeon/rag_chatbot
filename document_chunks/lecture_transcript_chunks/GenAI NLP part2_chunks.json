{
  "source_file": "GenAI NLP part2.json",
  "lecture_name": "GenAI NLP part2",
  "course": "Generative AI",
  "total_chunks": 15,
  "chunks": [
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c000_5f3d71",
      "content": "[강의 녹취록] 과목: Generative AI | 제목: GenAI NLP part2\n\n이번 실습 같은 경우에는 엘엠 이밸류에이션 하니스라는 프레임워크를 통해서 진행이 될 거고요. 이 프레임워크 자체는 뭐 다양한 라즈 랭귀즈 모델 평가 데이터셋들 그리고 다양한 라즈 랭귀즈 모델을 평가할 때 사용하는 프레임워크입니다. 여러분들이 뭐 라즈 랭귀지 모델에 관심이 있으신 분들이라면 흔히들 접했을 오픈소스 엘엘엠 리더보드에서도 해당 프레임워크를 이용해서 평가를 하고 있습니다. 이제 이 실습을 통해서는 결국에 기존 모델 대비 학습된 모델이 어느 정도 성능이 향상되었는지 살펴보고 성능 향상을 살펴볼 데이터셋들이 각각 어떤 식으로 평가를 하고 있고 어떤 능력을 보고 있는지 살펴보도록 하겠습니다. 실습 자료 자체는 원래 엘엠 이벨루에이션 할리스가 만들어진 일루더 AI의 레퍼스토리와 한국어 엘르램들을 활발하게 학습하고 공개하고 계시는 범인 님께서 커스텀에서 사용하시던 코엘엘엠 이벨류에이션 하이닉스 두 가지 레프 스토리를 참고하여 만들었습니다. 우선 왼쪽에 보시는 이 창에서 아실 수 있듯이 여러분들이 다운로드를 받으시면 이 이밸류에이션 파트가 있을 텐데요. 해당 파일을 서버로 옮겨 주시면 됩니다. 이전 실습에서는 쥐 다운을 이용해서 다운로드를 했었는데 현재 이 이밸류에이션 폴더 내부에 파일이 많아서 쥐 다운을 사용해서는 온전히 다운로드를 할 수 없기 때문에 파일을 여러분들의 노트북으로 다운을 받으시고 그 다운로드된 파일을 다시 서버로 옮겨주시는 작업이 필요합니다. 그러면 이 구조부터 설명을 드리자면 lmev과 LM EV 에그 인포에는 저희가 사용할 에엠 이벨루에이션 하니스의 다양한 코드들이 담겨 있습니다. 그래서 사실 저희가 이 코드들을 지금 당장 엄청 자세히 살펴보지 않기 때문에 이것들보다는 이 이제 학습된 모델의 파라미터가 저장되어 있는 경로가 이렇게 트레인드 로라 웨이트 혹은 여러분들은 여러분들 나름대로 있겠죠 이 웨이트와 리졸트 폴더 내부의 평가 결과가 저장이 될 겁니다. 그러면 하나씩 살펴보도록 하겠습니다. 우선 첫 번째로는 서버에 LM 이벨류에이션 할리스를 설치해 주도록 하겠습니다. 뭐 LM 이벨류에이션 할리스를 그냥 이 LM 이벨 폴더와 같이 그냥 이 경로 내부에서 사용하실 수도 있지만 패키지화해서 사용하는 게 훨씬 간편합니다. 그렇기 때문에 저희가 에엠 이뷰를 여기에 있으니 이걸 가지고 리콰이먼트를 설치해 주는 방식으로 설치를 하겠습니다. 저 같은 경우에는 이미 설치가 되어 있기 때문에 이렇게 설치를 눌러도 이미 다 인스토리 세티스 파일이 되어 있다라고 안내문이 뜰 거고요. 그리고 필요한 라이브러리들을 인포트를 해주고 우선 첫 번째로 해볼 건 이 에엠 이밸류에이션 하이닉스 내부에서 어떻게 동작을 하는지 정말 복잡하게 되어 있긴 하지만 아주 간단한 것들 그리고 중요하다고 생각되는 것들에 대해서만 살펴보도록 하겠습니다. 원래 lmev의 평가 과정 자체는 이렇게 주피터 노트북 환경에서 진행이 되는 게 아니라 CLI 환경 즉 우리가 평가하고 싶은 모델의 경로나 다른 여러 가지 요소들을 커맨드 라인으로 쳐서 입력을 하게 됩니다. 하지만 지금 실습 같은 경우에는 엘레 이벨류에이션 하리스가 어떻게 동작하는지 설명을 드리기 위해서 저희가 필요한 코드들 함수들만 가지고 와서 편집을 해서 매우 간략화 시켜 놓은 상태입니다. 에엠 이벨류에이션 하리스 폴더를 열어 보시면 이런 식으로 이벨류에이터 닷 파이라는 게 있습니다. 요 이벨류에이터 닷 파이가 결국 실제로 평가 과정이 어떻게 동작하는지에 대해서 정의가 되어 있는 코드들인데요. 이 코드 내부를 살짝만 살펴보도록 하겠습니다. 그럼 이밸류에이션 할리스에서의 필요한 코드들만 인포트를 해주고 하나씩 살펴보도록 할 텐데요. 우선 첫 번째로는 심플 이벨루에잇이라는 함수입니다. 이 심플 이벨루에잇이라는 함수가 실제로 저희가 평가할 모델 그리고 테스크 그리고 퓨샷이나 배치 사이즈와 같은 여러 가지 요소들을 입력으로 받아서 평가를 진행을 하는 함수다라고 생각을 해 볼 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 0,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1092,
      "char_count": 1986
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c001_4ce9a8",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 요 이벨류에이터 닷 파이가 결국 실제로 평가 과정이 어떻게 동작하는지에 대해서 정의가 되어 있는 코드들인데요. 이 코드 내부를 살짝만 살펴보도록 하겠습니다. 그럼 이밸류에이션 할리스에서의 필요한 코드들만 인포트를 해주고 하나씩 살펴보도록 할 텐데요. 우선 첫 번째로는 심플 이벨루에잇이라는 함수입니다. 이 심플 이벨루에잇이라는 함수가 실제로 저희가 평가할 모델 그리고 테스크 그리고 퓨샷이나 배치 사이즈와 같은 여러 가지 요소들을 입력으로 받아서 평가를 진행을 하는 함수다라고 생각을 해 볼 수 있습니다. 이 코드가 실제 코드와 완전히 동일하지는 않고요. 실제 엘엠 이벨루에이션 하니스는 정말 광범위한 경우의 수들을 모두 고려하기 때문에 코드가 매우 복잡하고 많은 이프문과 엘스문으로 구성이 되어 있습니다. 그중에서 저희는 오늘 평가에 사용할 데이터셋인 코베스트를 평가할 때 실제로 쓰일 수 있는 요소들 그리고 그 외에도 조금 중요하다고 생각하는 것들만 살펴보도록 하겠습니다. 첫 번째로는 여기서 보시면 모델 RGS를 받기도 하고 그다음에 앤 키워즈를 받기도 합니다. 여기서 젠 키워즈는 제너레이션 관련된 즉 생성과 관련된 키워드들입니다. 즉 이 모델이 생성을 할 때 맥스 랭스는 얼마로 할지 아니면은 빔 서치와 같은 제너레이션 스트레터지는 어떤 걸로 할지와 같은 것들을 입력으로 받아서 이거는 다 스트링으로 되어 있었기 때문에 이를 실제 키워드 형태로 변경해 주는 게 해당 코드입니다. 생성 관련 키워드를 받아서 처리를 해줬다면 이제 모델과 관련된 키워드를 받게 됩니다. 뭐 모델이 만약에 bf16으로 되어 있는 모델인지 어 그냥 플로팅 포인트 16으로 되어 있는지 혹은 인트포로 퀀타이제이션이 되어 있는지 혹은 저희가 로라웨이트를 사용을 한다면 이 로라웨이트도 같이 입력을 하게 되는지와 같은 여러 가지 모델 관련 파라미터를 입력으로 받아서 이를 처리해 주게 됩니다. 처리하는 과정에 대해서 간략하게만 설명을 드리자면 보시는 바와 같이 모델의 입력 값이 그냥 문자열이었다면 이 모델의 입력값이 문자열이었기 때문에 이거를 LM 이밸류에이션 하니스 내부에 있는 여러 가지 클래스들을 통해서 실제 모델로 변경을 해주거든요. 이 과정 자체는 사실 그냥 우리가 트랜스포머에서 사용을 하든 뭐 프롬 프리 트레인드 함수 내부에 있는 데서 모델을 불러오는 것과 비슷한 과정을 수행을 해준다라고 생각을 하시면 됩니다. 그리고 이렇게 불러올 때 모델 아규먼트들도 입력을 해서 어 해당 모델을 퀀타이제이션 하면 퀀타이제이션 한 옵션들 그리고 로라 웨이트를 또 추가적으로 쓰는 거라면 추가적으로 사용되는 로라 웨이트들에 대한 것들이 입력이 돼서 엘엠이라는 오브젝트로 저장이 되게 됩니다. 그다음에는 평가 데이터셋 관련 정보들을 가지고 와야 되는데요. 이 평가 데이터들을 이 엘엠 이벨루에이션 하이닉스 내부 코드에서는 테스크라고 얘기를 하고 있습니다. 즉 저희가 오늘 실습에 사용할 평가 데이터셋은 코베스트니까 우리가 테스크라고 하는 건 코베스트 내부에 있는 여러 가지 평가 데이터셋들을 의미를 하게 됩니다. 그래서 이 역시도 보시면 엘레 이벨류에이션 할리스 내부에 테스크스라는 디렉토리로 들어가서 살펴보시면 여기 이렇게 코베스트라는 폴더가 있고 코베스트 내부에 여러 가지 실제 데이터셋들에 대한 양 파일들이 저장이 되어 있습니다. 그러면은 각각의 양 파일로부터 관련 해당 데이터셋에 대한 정보들을 딕셔너리 형태로 변환을 해서 저장을 하는 게 테스크 딕이라는 오브젝트입니다. 그래서 테스크 딕이라는 오브젝트로부터 실제 테스크 와 관련된 여러 가지 데이터셋이 담겨 있고 데이터셋 외에도 뭐 프롬프트가 있다면 프롬프트와 관련된 정보들이 담겨 있는 테스크 오브젝트를 만들게 됩니다. 그리고 해당 테스크에 configuration들도 또 별도로 있기 때문에 해당 컴퓨레들도 이런 식으로 가지고 와서요 어 평가 방식이 여러 개가 있는데 이 평가 방식에 따라서 다른 방식으로 사용을 하게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 1,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1085,
      "char_count": 1974
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c002_d48455",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 그래서 테스크 딕이라는 오브젝트로부터 실제 테스크 와 관련된 여러 가지 데이터셋이 담겨 있고 데이터셋 외에도 뭐 프롬프트가 있다면 프롬프트와 관련된 정보들이 담겨 있는 테스크 오브젝트를 만들게 됩니다. 그리고 해당 테스크에 configuration들도 또 별도로 있기 때문에 해당 컴퓨레들도 이런 식으로 가지고 와서요 어 평가 방식이 여러 개가 있는데 이 평가 방식에 따라서 다른 방식으로 사용을 하게 됩니다. 생성 기반 평가라고 하면 여러분들이 흔히 생각할 수 있는 평가 방식인데요 모델에게 특정 입력 값을 주었을 때 자유롭게 모델이 생성하게 시키고 생성된 결과물을 바탕으로 평가하는 경우입니다. 이 경우에는 제너레잇 언틸이라는 키워드로 사용이 되는데요 그 경우에는 컨피규레이션에서 실제로 모델이 문장을 생성하는 과정이 있을 테니 제너레이션 키워드를 삽입해 주게 됩니다. 그리고 그 외에도 사실 LLM 평가를 할 때 그냥 질문만 넣어 줄 수도 있지만 질문 외에도 에 질문에 대해서는 b라고 대답하고 씨 질문에 대해서는 디라고 대답하는 퓨샷 이그젬플이 컨텍스트 러닝으로 사용을 해서 평가를 하고 싶다라고 하면 넌 퓨샷이라는 옵션이 들어오기 때문에 이 옵션에 맞추어서 configuration 내에 넌 퓨샷을 넣어 주게 됩니다. 이때 주의를 해주어야 될 점은 사실 모든 평가 데이터들이 퓨샷 세팅으로 평가가 가능하지는 않습니다. 왜냐하면 데이터셋에 따라서 이 데이터는 퓨샷으로 평가하면 안 돼라고 하는 데이터셋이 있을 수도 있고 퓨샷 입력이 어떻게 들어가야 되는지 사전에 정의하기 어려워서 그냥 퓨샷으로 평가하지 말라와 같은 경우가 있습니다. 그 경우에는 보시는 것처럼 이 로고를 통해서 이 해당 테스크는 어 임의적으로 퓨사 샘플이 0으로 설정이 되어 있기 때문에 너가 아무리 매뉴얼하게 컴퓨듀리게이션을 넣어도 동작하지 않아라는 프린트를 해주게 되고요. 그게 아니라면 원래 태스크에 있는 넘 퓨샷을 디폴트 값에서 네가 입력한 넌 퓨샷 값으로 변경해 줄게 와 같이 변경이 되게 될 겁니다. 그래서 만약에 여러분들이 향후에 퓨샷으로도 실험을 해보고 싶어서 제로 샷 원샷 4샷과 같이 샷을 바꿔가면서 실험을 했는데 성능 변화가 없다라고 하면은 사실 이 이밸류에이션 과정에서 이런 로깅이 제대로 뜨고 있는지 살펴보셔야 합니다. 그다음에는 이제 실제 평가 요소를 이용을 해서 리조트를 뽑게 됩니다. 리조트를 뽑게 되는 건 현재 실행하고 있는 함수가 심플 이벨류엣 함수인데요. 이 심플 이벨류엣 함수 내부에서도 다른 별도의 이벨류에이트 함수를 통해서 실제 평가가 이루어지게 됩니다. 그런 평가를 할 때 사용되는 건 우리가 사전에 정리한 랭귀지 모델 즉 LLM 그리고 평가하고자 하는 데이터셋 대상인 테스크 d 그리고 리밋이라고 하는 것이 들어가게 되는데요. 리밋이라고 하는 건 평가 데이터가 너무 많아서 뭐 예를 들어서 여러분들이 가지고 있는 현재 서버에서 평가 데이터가 10만 개라 다 평가를 하면은 하루 이상이 걸린다라고 하면은 사실 평가만으로 하루 이상이 걸리면은 온전히 프로젝트 진행이 힘들기 때문에 우리가 전체 데이터 중에 몇 개 뭐 천 건만 쓸게와 같은 정보를 리밋의 인트 값이나 아니면은 소수 값으로 넣어주게 됩니다. 부트 스트레이 미타우스 같은 경우에는 평가를 한 번만 진행하면 사실 이게 제대로 평가되는지 판정하기가 좀 까다로운 측면이 있습니다. 왜냐하면은 랭귀지 모델의 생성 과정 혹은 데이터셋을 어떤 걸 선택했는지에 따라서 잘될 경우가 있고 못 될 경우가 있잖아요. 그래서 n번 평가를 돌려서 n번 평가된 결과값들을 부트스트래픽 합쳐주는 과정에 필요한 게 부트스트랩 이터스라고 해서 몇 번 반복할지 그리고 학습 데이터와 평가 데이터가 나눠져 있었다면 우리가 학습 데이터에 있는 데이터가 평가 데이터에 있으면 안 되니 그것과 관련된 정제 작업을 수행해 주는 디컨테메이션과 관련된 파라미터들이 들어가게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 2,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1058,
      "char_count": 1945
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c003_d75e88",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 왜냐하면은 랭귀지 모델의 생성 과정 혹은 데이터셋을 어떤 걸 선택했는지에 따라서 잘될 경우가 있고 못 될 경우가 있잖아요. 그래서 n번 평가를 돌려서 n번 평가된 결과값들을 부트스트래픽 합쳐주는 과정에 필요한 게 부트스트랩 이터스라고 해서 몇 번 반복할지 그리고 학습 데이터와 평가 데이터가 나눠져 있었다면 우리가 학습 데이터에 있는 데이터가 평가 데이터에 있으면 안 되니 그것과 관련된 정제 작업을 수행해 주는 디컨테메이션과 관련된 파라미터들이 들어가게 됩니다. 그러면 리조트에서는 평가 결과물이 나오게 되는데요. 이제 그러면은 이 평가 결과물로부터 실제로 우리가 저희가 이제 어 이 평가를 할 때 어떤 모델과 어떤 어규먼트들을 사용을 해서 평가를 했었고 실제로 그 결과물은 이 점수와 같아 와 로 점수를 산출해 주는 코드가 이 아래에 있는 코드입니다. 사실 그래서 평가 과정이 다른 일반적인 다운스트림 테스크와 다른 점이 있기 때문에 이런 식으로 복잡하게 구성이 되어 있는데요. 이를 조금만 더 자세히 살펴보도록 하겠습니다. 그리고 조금 더 자세히 살펴보기 위해서는 결국에 이벨루엣 함수가 어떻게 되어 있는지 알아야 되겠죠. 그래서 이벨루엣 함수에 대해서 살펴보자면 이벨루엣 함수는 보시는 것처럼 주어진 라즈 랭귀지 모델에 대해서 입력된 테스크 딕셔널리들 혹은 평가 데이터셋과 관련된 정보가 담겨 있는 딕셔너리를 통해서 평가를 진행하게 됩니다. 네 그럼 실제로 심플 이벨류엔 내에서 실행이 되는 이벨루엣 함수를 통해서 진짜 평가가 어떻게 진행이 되는지 살펴보도록 하겠습니다. 해당 코드 역시 에엠 이벨류에이션 내부에 구현돼 있는 코드를 가지고 오되 단순화하기 위해서 여러 가지 복잡한 코드들은 쳐낸 상태입니다. 이벨루에이션 함수는 앞서 말씀드렸다시피 랭귀지 LLM 자체 그리고 현재 저희가 평가를 수행하고자 하는 태스크들이 담겨 있는 테스크 딕셔너리가 입력되게 됩니다. 네 그러면 하나씩 살펴보면 사실 이 코드 자체는 우리가 실제 평가에 사용될 여러 가지 요소들을 담기 위한 딕셔너리를 생성해 주는 코드들이라서 생략하도록 하고요. 이 아래를 살펴보면 평가가 어떻게 진행되는지 알 수 있는데요. LM 이밸류에이션 평가 같은 경우에는 입력 데이터를 구축해 주는 과정이 먼저 수행이 되고 그렇게 구축된 입력 데이터를 바탕으로 랭귀지 모델에 통과시켜서 결과값을 받아오고 받아 온 결과값을 바탕으로 최종 점수를 산출하는 3단계 정도로 이루어져 있다라고 생각할 수 있습니다. 그럼 하나씩 살펴보도록 하겠습니다. 테스크 디에는 테스크 이름 그리고 해당 테스크에 대한 다양한 정보가 담겨 있는 오브젝트로 구성이 되어 있고요. 그래서 보시다시피 테스크 이름과 테스크 객체를 받아오는 걸 볼 수가 있습니다. 테스크 여러 개를 평가할 때 하나씩 받아오기 때문에 포문이 돌고 있고요. 이 중에서 테스크를 그럼 어떻게 다루는지 살펴보면 퓨샷을 사용을 한다라고 하면은 넘 퓨샷이 이미 컴피그 안에 있을 것이기 때문에 여기서부터 몇 샷을 사용할지를 가지고 오게 되고 그 뒤에 평가 데이터를 전부를 사용을 하지 않고 일부만 사용을 한다라고 하면은 어 평가 데이터 정보에 어 얘 이 평가 데이터에 테스트 데이터가 있니 혹은 밸리데이션 데이터가 있니를 가지고 실제 평가할 데이터를 가지고 오게 됩니다. 만약에 테스트 데이터가 있다라고 하면은 이 실제로 평가를 수행할 데이터는 테스트 데이터가 될 것이고 테스트 데이터가 없다면 밸리데이션 데이터를 가지고 평가를 수행을 할 겁니다. 그리고 리밋 같은 경우에는 아까도 말씀드렸다시피 정수를 넣을 수도 있고요. 그게 아니라 소수로 넣을 수도 있는데요. 정수를 넣으면 만약에 천이라고 넣었다 하면 테스트 데이터 중에 천 건만 평가를 진행한다는 얘기고 소수인 0.1을 넣었다라고 하면은 전체 평가 데이터셋의 10%만 사용한다라는 의미로 해당 함수를 통해서 이를 변환해 주는 과정이 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 3,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1064,
      "char_count": 1938
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c004_9ec5ae",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 만약에 테스트 데이터가 있다라고 하면은 이 실제로 평가를 수행할 데이터는 테스트 데이터가 될 것이고 테스트 데이터가 없다면 밸리데이션 데이터를 가지고 평가를 수행을 할 겁니다. 그리고 리밋 같은 경우에는 아까도 말씀드렸다시피 정수를 넣을 수도 있고요. 그게 아니라 소수로 넣을 수도 있는데요. 정수를 넣으면 만약에 천이라고 넣었다 하면 테스트 데이터 중에 천 건만 평가를 진행한다는 얘기고 소수인 0.1을 넣었다라고 하면은 전체 평가 데이터셋의 10%만 사용한다라는 의미로 해당 함수를 통해서 이를 변환해 주는 과정이 있습니다. 그리고 입력 데이터를 아까 말씀드렸다시피 구축하는 과정이 들어가게 되는데요. 보시는 것처럼 해당 테스크 데이터마다 어 실제로 입력 데이터 구축하는 방식이 다 달라지게 될 겁니다. 그렇기 때문에 테스크 정보 혹은 평가 데이터셋 객체 내부에 있는 빌드 올 리퀘스트 메소드를 통해서 평가 데이터셋을 구축하는 과정이 되겠습니다. 입력 데이터셋을 그러면 이렇게 구축된 데이터에 대해서 이제는 어 리퀘스트 타입을 반환을 해 주게 되는데요. 해당 함수 같은 경우에는 밑에 있는 실제 평가 과정에서 어떻게 사용이 되는지 살펴보도록 하겠습니다. 여기서 한마디만 더 얘기를 해보자면 에엠 이밸류에이션 할리스 내부에서 리퀘스트라고 하는 건 입력 데이터를 의미를 하게 됩니다. 그리고 리퀘스트 타입이라고 하는 건 평가 방식을 의미한다라고 생각을 해 주시면 되겠습니다. 그러면 리퀘스트라는 아이템이 결국에는 어떤 리퀘스트 평가 방식으로 이 데이터를 평가해줘라는 게 있을 테니 평가를 진행하게 될 거고요. 이때 반복 실험을 진행한다고 아까 말씀드렸기 때문에 반복 실험을 위해서 데이터를 케이번 리핏 횟수만큼 증강시켜 주게 됩니다. 이는 뭐 복잡한 어그멘테이션 방법 방식을 쓴다는 게 아니라 단순히 복사를 해서 데이터를 늘린다라고 생각을 해 주시면 됩니다. 그다음에 결국에는 이 리퀘스트 타입에 대해서 결국 리퀘스트를 랭귀지 모델에 입력을 하게 되면 답변을 받아 오게 되는데요. 여기서 리스폰스라고 되어 있기 때문에 엘엘엠에게 질문을 하고 엘엘엠이 무조건 생성을 해서 리스폰스를 받아온다라고 생각을 하실 수도 있겠지만 사실은 그렇게 동작한다고 보장되지는 않습니다. 테스크마다 실제로는 확률값만 반환하는 경우도 있고 어떤 테스크는 실제로 생성을 원하는 경우도 있기 때문에 리스폰스를 그냥 점수 산출을 위한 도구 혹은 입력 데이터를 바탕으로 LLM의 성능을 평가할 수 있는 어떤 결과값이라고만 생각을 해 주시면 될 것 같습니다. 그러면 입력과 출력값이 페어로 되어 있어야 평가가 되니까 이 리퀘스트마다 각각의 출력 값을 복사해서 붙여 넣어 주게 됩니다. 그러면은 사실 다운스트림 테스크랑 이 LLM 평가 과정에서의 가장 큰 차이점이라고 한다면 다운 스림 테스크는 사전에 정의된 여러 가지 지표들이 있을 거고 지표들마다 고정된 방식으로 평가가 진행될 겁니다. 뭐 클래시피케이션이라고 하면 단순하게 어큐러시를 측정할 수도 있고 그게 아니라 제너레이션이라 하더라도 블루나 루제와 같은 여러 NLP 기반의 스코어를 그냥 사용해도 됩니다. 하지만 이 LLM 평가 같은 경우에는 워낙 테스크마다 생성을 해도 되는 문장들 혹은 하면 안 되는 문장들 혹은 그 이후에 어떤 정해진 후 처리들이 있을 수 있습니다. 이와 같은 것들에 대해서도 각 데이터마다 다르기 때문에 테스크별로 후처리 필터를 적용을 해서 출력물에 대해서 후처리를 진행을 해 주게 됩니다. 그러면 이제 출력물에 대한 후처리까지 진행된 결과 값이 테스크 객체 안에 있을 겁니다. 그럼 이를 바탕으로 실제로 각각의 테스크마다 수행된 테스크 결과물을 이용을 해서 점수를 산출하는 코드가 아래와 같은데요. 코드가 얼핏 보면 복잡해 보이는데 사실 이거는 그냥 반복적으로 데이터 셋 내부에 있는 하나하나의 값들을 받아오기 위한 반복 객체를 생성해 주는 거고요. 그러면 여기 닥이라고 되어 있는 게 사실은 데이터 셋 내부에 있는 하나하나의 이그젬플들입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 4,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1092,
      "char_count": 1989
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c005_8201da",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 그러면 이제 출력물에 대한 후처리까지 진행된 결과 값이 테스크 객체 안에 있을 겁니다. 그럼 이를 바탕으로 실제로 각각의 테스크마다 수행된 테스크 결과물을 이용을 해서 점수를 산출하는 코드가 아래와 같은데요. 코드가 얼핏 보면 복잡해 보이는데 사실 이거는 그냥 반복적으로 데이터 셋 내부에 있는 하나하나의 값들을 받아오기 위한 반복 객체를 생성해 주는 거고요. 그러면 여기 닥이라고 되어 있는 게 사실은 데이터 셋 내부에 있는 하나하나의 이그젬플들입니다. 그러면 닭을 가지고 이 다큐먼트와 다큐먼트 내부에 있는 리퀘스트와 리스폰스에 대해서 점수를 산출해 주는 게 매트릭 값이 나오게 됩니다. 이 매트릭이 나오는 과정 역시도 사실 각각의 평가 데이터마다 다르기 때문에 평가 데이터 내부에 정의되어 있을 프로세스 리조트 즉 리퀘스트와 입력 데이터 혹은 레이블 데이터와 어떻게 처리해서 평가를 할지 가 담겨 있는 함수를 통해서 점수를 산출을 해주게 됩니다. 그럼 아까 말씀드렸다시피 이제 각각의 데이터마다 점수는 나와 있으니까 모든 데이터에 그리고 반복 실험된 데이터에 대해서 점수를 합산해 주는 과정이 요 아래 과정입니다. 점수를 합산해 준다라고 하면은 단순히 acls 같은 경우에 각각의 데이터가 천 건이 있었고 천 권의 데이터의 어큐러시로 평균을 내는 것도 가능하지만 그거 외에도 표준편차 역시 산출하고 있습니다. 표준 편차를 산출한다는 건 해당 모델이 해당 점수를 얼마나 신뢰도를 가지고 산출할 수 있는지에 대한 능력이라고 볼 수 있기 때문에 표준 편차를 산출해 주는 코드가 있고 이 코드도 역시 그냥 사전에 정의되어 있는 스탠다드 에러를 산출해 주는 함수를 통해서 이루어지게 됩니다. 그리고 이 뒤에 코드는 사실 크게 설명드릴 부분은 아니고요. 그냥 결과물 자체를 저장을 하기 위한 여러 가지 후처리 코드다라고 생각을 해 주시면 됩니다. 그러면 이렇게 산출된 저작물은 제이슨 파일로 이런 식으로 저장이 되게 됩니다. 해당 파일에 대해서는 이따가 조금 더 자세히 설명을 드리도록 하겠습니다. 그러면 아까 보셨던 코드 중에 사실 하나 코드를 살짝 제가 조용히 넘어간 코드가 있는데요. 여기 보시면 입력 데이터를 구축한다라고만 얘기를 하고 넘어갔던 테스크 내부에 있는 빌드 올 리퀘스트 함수가 있습니다. 이 빌드 올 리퀘스트는 결국에 테스크마다 어떤 식으로 입력 데이터를 구성하고 이 입력 데이터를 바탕으로 어떤 리퀘스트 타입을 보낼지에 대한 정보를 생성하는 함수이기 때문에 이것들을 이 함수 역시도 조금만 더 자세히 살펴보도록 하겠습니다. 빌드 홀 리퀘스트 함수 역시도 제가 조금 간략화해 놓은 함수고요. 해당 함수는 이 코드를 가서 살펴보시면 전체 코드를 살펴보실 수 있습니다. 리퀘스트 함수 역시도 테스트 데이터가 있다면은 테스트 데이터로 테스트 데이터가 없다면 평가 데이터로 진행이 되도록 구성이 되어 있고요. 결국에 인스턴스라는 리스트를 통해서 전체 평가 데이터의 입력 문장들을 저장을 해 주게 되는데 여기서 보시면 퓨샷 컨텍스트를 먼저 만들어 주게 됩니다. 즉 앞서 말씀드렸다시피 제로 샷이 아니라 퓨샷으로 진행이 되는 경우에는 숏 샷으로 진행되기 위한 컨텍스트 자체를 이 전체 문서들 중에서 일부 문서를 가지고 생성을 해 주게 되고요. 그다음에 컨스트럭트 리퀘스트 함수를 통해서 퓨샷 이그잼플과 실제 데이터들을 가지고 인스트럭트를 만들어 주게 됩니다. 사실 그러면은 이 빌드 올 리퀘스트 함수에서 해주는 것 자체는 퓨샷을 사용을 한다면 퓨샷 자체를 생성해 주는 퓨샷 컨텍스트 그리고 그렇게 생성된 퓨샷을 바탕으로 각각의 다큐먼트에 대해서도 실제 평가 대상이 되는 데이터에 대해서 생성을 해주는 컨스트럭트 리퀘스트로 구성이 되어 있는 걸 알 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 5,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1023,
      "char_count": 1862
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c006_082c8e",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 사실 그러면은 이 빌드 올 리퀘스트 함수에서 해주는 것 자체는 퓨샷을 사용을 한다면 퓨샷 자체를 생성해 주는 퓨샷 컨텍스트 그리고 그렇게 생성된 퓨샷을 바탕으로 각각의 다큐먼트에 대해서도 실제 평가 대상이 되는 데이터에 대해서 생성을 해주는 컨스트럭트 리퀘스트로 구성이 되어 있는 걸 알 수가 있습니다. 그러면 컨스트럭트 리퀘스트에 대해서 이제 살펴봐야겠죠. 컨스트럭트 리퀘스트 함수를 살펴보면 이 함수 역시도 해당 경로에 있는 함수인데요. 더 뜯게 되면은 사실 더 자세한 얘기를 할 수 있어 좋을 수도 있겠지만 여러분들이 이해하는 과정에서 이 이상으로 깊어지게 되면 조금 무리이거나 아니면 너무 자세한 내용일 수 있겠더라고요. 그래서 해당 내용을 제외하고 평가 과정 자체가 총 4가지로 구분이 되고 이 네 가지 평가에 대해서 간략하게만 얘기를 해보겠습니다. 일단 이 아웃풋 타입 즉 평가와 리퀘스트 타입이 LG like lewod라고 하면은 입력 문장의 생성 확률 자체를 계산해 주는 겁니다. 그래서 입력 문장이 생성될 확률이 높을수록 LLM이 더 좋은 모델이다 혹은 입력 문장의 생성 확률이 낮을수록 더 좋은 모델이었다와 같은 평가를 할 때는 로그라이클리후드를 사용을 하게 되고요. 로그라이클리후드 롤링이라는 방식은 입력 문장 전체에 대해서 하는 게 아니라 입력 문장을 쪼개서 이렇게 각각의 일부 파트마다 문장 생성 확률을 계산을 해서 이걸 취합해 주는 방식을 로그라이 클리우드 롤링 방식이다라고 얘기를 합니다. 그리고 가장 많이 사용되는 평가 방식이고 오늘 저희 실습에서도 사용될 평가 방식은 멀티플 초이스 방식인데요. 멀티플 초이스 방식은 아래 데이터에서 설명을 드리겠지만 질문이 있고 해당 질문에 정답이 될 수 있는 보기 문장이 2개 혹은 3개 4개와 같이 주어졌을 때 그 보기 문장을 생성할 확률을 가지고 모델이 에 보기 b 보기 시 보기 디 보기를 선택했다라고 간주하는 방식입니다. 그러니까 초이스라고 하는 거는 닥트 초이스로 구성이 되게 되는데요. 이거를 밑에서 보여드리자면 예를 들어서 이 코베스트 데이터 셋인데요. 코베스트 데이터셋 역시 멀티 초이스 방식이기 때문에 얘기를 드리자면 코베스트 데이터 셋 중에서도 센티에그 데이터 셋은 하나의 문장 뚜껑이 잘 열려요와 같은 문장에 대해서 긍정 문장일 수도 있고 부정 문장일 수도 있습니다. 그러면 질문으로 뚜껑이 잘 열려요를 넣고 해당 문장이 긍정인지 부정인지 분류를 해야 되고 LLM에게 이걸 분류하라고 시킬 때는 실제로 긍정입니다. 부정입니다와 같이 생성을 시키는 게 아니라 멀티 초이스 방식은 이 뒤에 뚜껑이 잘 열려요라는 단어 뒤에 올 토큰이 긍정일 확률이 높은지 부정 토큰이 올 확률이 높은지를 계산하는 방식입니다. 그래서 t이스라고 하는 거는 결국 아까 보신 예시에서 긍정 그리고 부정이 되겠죠. 그러면 입력 문장 같은 경우에는 결국에 하나의 입력 문장에 대해서도 뚜껑이 잘 열려요. 긍정 뚜껑이 잘 열려요. 부정과 같이 출력 문장 경우의 수에 따라서 구축이 돼야 되기 때문에 경우의 수만큼 이렇게 리스트로 구성이 되는 걸 볼 수가 있습니다. 결국 리퀘스트 리스트는 하나의 객체로 구성이 되게 되고요. 이 객체 내부에는 다큐먼트와 그 실제 생성의 평가 대상이 되는 문서 그리고 평가를 할 때 사용될 컨텍스트를 바탕으로 구성이 되게 됩니다. 그리고 마지막으로 제너레잇 언틸이 여러분들이 가장 직관적으로 생각을 하실 수 있는 방식인데요. LLM에게 특정 질문을 던졌을 때 실제로 생성되는 LLM의 문장을 보고 평가를 하는 방식입니다. 사실 근데 이 방식은 평가 자체가 조금 까다로운 측면이 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 6,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 980,
      "char_count": 1809
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c007_657c12",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 결국 리퀘스트 리스트는 하나의 객체로 구성이 되게 되고요. 이 객체 내부에는 다큐먼트와 그 실제 생성의 평가 대상이 되는 문서 그리고 평가를 할 때 사용될 컨텍스트를 바탕으로 구성이 되게 됩니다. 그리고 마지막으로 제너레잇 언틸이 여러분들이 가장 직관적으로 생각을 하실 수 있는 방식인데요. LLM에게 특정 질문을 던졌을 때 실제로 생성되는 LLM의 문장을 보고 평가를 하는 방식입니다. 사실 근데 이 방식은 평가 자체가 조금 까다로운 측면이 있습니다. 만약에 예를 들어서 아까 보여드렸던 이 예시로 다시 얘기를 해보자면 엘엘엠에게 다음 문장을 보고 이게 긍정 문장인지 부정 문장인지 얘기해 줘라고 하면은 뚜껑이 잘 열려요라는 문장을 입력을 했을 때 다음 문장은 긍정입니다라고만 대답을 해주면 다음 문장은 긍정입니다라고 얘기했으면 얘가 긍정이라고 판단했구나라고 생각하면 좋겠지만 사실은 이게 평가되는 과정에서 모델이 어떤 문장에 대해서는 다음 문장은 긍정입니다라고 얘기를 하고 어떤 문장에 대해서는 잘 모르겠습니다. 와 같이 정말 생성될 수 있는 문장의 가짓수가 너무 많습니다. 그래서 이 제너레이드 unta 함수를 사용을 할 경우에는 생성된 문장을 가지고 어떻게 실제 점수로 변환을 할지와 같은 과정이 상당히 까다롭다라고 얘기를 할 수 있습니다. 그래서 여러분들이 프로젝트를 하시거나 아니면 향후 현업 혹은 학업에 있어서 엘엘램을 평가를 하는 과정에 있는데 실제로 생성된 문장을 통해서 평가를 하고 싶다라고 하면은 엘엘엠이 생성하는 문장을 최대한 컨트롤하는 방식으로 문장을 생성하도록 유도를 해야 합니다. 어쨌든 그럼 지금까지 설명한 내용들을 다시 역순으로 올라가 보면 LM 이밸류에이션 할리스에서는 우선 입력된 아규먼트들을 바탕으로 랭귀지 모델과 평가할 데이터셋들에 대한 정보를 객체로 생성을 해 주게 되고 데이터셋 객체 내부에는 각각의 데이터에 대해서 리퀘스트 즉 뭐 퓨샷을 넣으면 퓨샷을 앞에 붙여 준다든지 아니면은 평가 방식에 따라서 입력 데이터를 구성을 하는 함수를 통해서 전체 평가 데이터를 실제 모델의 입력 데이터로 변환을 해주게 되고 그렇게 변환된 데이터에 대해서 퓨샷이 붙어야 된다면 퓨샷 컨텍스트를 붙여주는 빌드 올 리퀘스트를 통해서 전체 데이터가 다시 입력 데이터로 변환이 되게 됩니다. 그럼 변환된 데이터를 바탕으로 변환된 데이터를 바탕으로 실제로 모델에게 실제 평가 방식에 따라서 생성을 시키든 확률을 계산하든 결과값을 리스폰스로 받아오고 받아 온 리스폰스에 대해서 점수를 산출해서 저장하는 코드로 구성이 되어 있구나라고 생각을 할 수 있을 것 같습니다. 여기까지 내용만 들으면 사실 에엠 이벨류에이션 하이니스 사용이 너무 복잡하다 너무 어렵다 어떻게 써야 될지 모르겠다라고 생각을 하실 수도 있는데요. 이제 앞뒤에 나올 이제 실습을 통해서는 사실 사전에 이미 엘엠이벨리에이션 하니스 내부에서 정의되어 있는 여러 가지 평가 데이터셋들에 대한 평가는 정말 간단하게 진행을 할 수가 있습니다. 그래서 앞서 제가 얘기했던 내용들을 온전히 이해하실 필요가 있기보다는 아 LLM 평가를 할 때는 기존 다운 스림 테스크와 다르게 뭐 어떤 출력을 가지고 어떤 지표로 어떻게 평가해야 되는지가 되게 모호하기 때문에 각각의 데이터 셋마다 입력 데이터 구성이나 출력된 결과물 통제 혹은 매트릭 산출에 있어서 여러 가지 다 다르게 정의되어 있구나 정도만 이해하셔도 충분할 것 같습니다. 그러면 코베스트 데이터셋에 대해서 하나씩 설명을 해 보겠습니다. 코베스트 데이터셋 내부에는 블큐 코파 블아c 헬라스 웨그 센티네그와 같은 각각의 데이터셋들이 존재하는데요. 해당 데이터셋들은 뭐 없는 데이터셋을 만들었다기보다는 영어 권에서 활발하게 사용이 되고 있던 랭귀지 모델 평가 데이터셋들을 한국어 버전으로 다시 만든 것이다라고 생각을 해 주시면 됩니다. 데이터 셋 예시는 위와 같이 있고 그러면 하나씩 데이터가 어떻게 구성이 되어 있는지 살펴보도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 7,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1086,
      "char_count": 1969
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c008_d0d869",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 그러면 코베스트 데이터셋에 대해서 하나씩 설명을 해 보겠습니다. 코베스트 데이터셋 내부에는 블큐 코파 블아c 헬라스 웨그 센티네그와 같은 각각의 데이터셋들이 존재하는데요. 해당 데이터셋들은 뭐 없는 데이터셋을 만들었다기보다는 영어 권에서 활발하게 사용이 되고 있던 랭귀지 모델 평가 데이터셋들을 한국어 버전으로 다시 만든 것이다라고 생각을 해 주시면 됩니다. 데이터 셋 예시는 위와 같이 있고 그러면 하나씩 데이터가 어떻게 구성이 되어 있는지 살펴보도록 하겠습니다. 우선 첫 번째로 헬라 스웨그 같은 경우에는 여기 보시는 이 문장인데요. 어떤 맥락을 주어지고 이 맥락 뒤에 올 수 있는 문장을 선택하는 데이터 셋입니다. 그래서 원래 영어 버전에서의 헬라 스웨그 자체는 모델이 관련된 주제와 맥락을 고려해서 마지막 문장을 생성할 수 있는지를 평가하는 게 원래 영어 버전의 데이터 셋인데요. 한국어 같은 경우에는 한국어 특성상 주어가 생략되는 경우가 많기 때문에 이 마지막 이 컨텍스트 뒤에 올 문장을 고를 때 주어가 생략된 경우에 고르는 게 상당히 어색해지고 어려워지는 경우가 있습니다. 그래서 그냥 헬라스 테스크 같은 경우에는 컨텍스트를 주고 네 가지 후보들 초이스를 줬을 때 어떤 문장이 실제로 가장 적절한 문장인지 선택하는 방식으로 이루어지게 됩니다. 그러면 앞서 설명드린 LM 이밸류에이션 하이리스 테스크에서의 구현체와의 비교를 해보면 컨텍스트를 입력으로 넣었을 때 엔딩 1에 해당하는 문장이 생성될 확률, 엔딩 2에 해당하는 문장이 생성될 확률 3 4번이 생성될 확률을 각각 계산하고 가장 높은 확률을 가지는 문장이 실제로 어 예측 값이라고 간주를 하고 있습니다. 코파 같은 경우에는 하나의 전제를 주어졌을 때 원인 혹은 결과를 나타내는 문장을 선택을 하도록 합니다. 이것 역시도 프레미스라고 하는 문장을 입력으로 넣게 되고 결과를 뽑아야 되는 상황일 때 전쟁이 시작되었다라고 한다면 이 전쟁이 시작된 것의 결과로서 병사들이 전투에 파견이 된다라는 게 자연스러운 맥락이 되겠죠. 이를 판별하는 테스크라고 할 수 있습니다. BLQ 같은 경우에는 어떤 정보가 주어져 있는 문장들 파라그래프가 입력이 되게 되고 거기에 대해서 질문도 함께 입력이 되게 됩니다. 그러면 엘엘엠은 이 페라 그래프를 보고 질문이 참인지 거짓인지 판별하는 테스크를 수행을 하게 됩니다. 이를 통해서 엘엘엠이 얼마나 주어진 입력 정보들을 이해할 수 있고 이를 바탕으로 퀘스천도 이해를 해서 퀘스천과 패러 게프의 정보를 통합적으로 사용할 수 있는지를 평가하는 데이터셋이다라고 할 수 있습니다. 센티네그는 여러분들이 엔엘피를 공부할 때 가장 처음 접하게 되는 테스크인 센티멘트 애널리시스와 비슷한데요. 문장이 입력으로 들어왔을 때 해당 문장이 긍정적인 감정을 가지고 있는지 부정적인 감정을 가지고 있는지를 평가를 하게 되고 여기서는 센티스 1 ST스 2라고 하지만 사실은 하나의 문장만 들어가게 됩니다. 즉 센티스 1이 입력됐을 때 해당 문장이 긍정인지 부정인지 평가하고 또 다음에 센트스 2가 입력됐을 때 긍정인지 부정인지 평가하게 됩니다. 사실 엘레엔 평가 데이터는 저희가 보기에 되게 직관적이라고 생각하실 수도 있지만 평가하는 과정 자체가 매우 복잡합니다. 왜냐하면 데이터셋 구축을 할 때 엘엠이 어떤 능력을 어떻게 평가해야 될지가 사실 상당히 모호하기 때문인데요. 그래서 그냥 점수만 단순히 산출하고 싶다면 현재 실습을 통해서도 충분히 가능하겠지만 조금 더 LLM이 어떤 식으로 평가가 되어야 되고 한국어 같은 경우에는 실제로 기존 영어권의 LLM 평가 방식과 어떤 게 달라져야 되는지에 대해서 알고 싶으시다면 코베스트 논문을 한번 참고해 보셔도 좋을 것 같습니다. 또 코베스트 데이터셋의 분포를 살펴보자면 트레인 데브 테스트가 모두 있습니다. 하지만 그중에서 저희는 아까 엘엠 이밸루에이션 할리스 코드를 설명하면서 말씀드렸다시피 테스트 데이터를 이용을 해서 평가가 진행이 될 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 8,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1082,
      "char_count": 1973
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c009_8246ab",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 또 코베스트 데이터셋의 분포를 살펴보자면 트레인 데브 테스트가 모두 있습니다. 하지만 그중에서 저희는 아까 엘엠 이밸루에이션 할리스 코드를 설명하면서 말씀드렸다시피 테스트 데이터를 이용을 해서 평가가 진행이 될 겁니다. 그러면은 각각의 데이터 셋들에 대해서 평가를 진행을 하면은 실제로 저희가 사용하게 되는 코드는 여기에 보이시는 것처럼 코베스트 bq에서는 이런 방식으로 평가를 해야 돼라고 하는 파일이 양 파일로 이미 저장이 되어 있습니다. 이건 제가 작성한 것도 아니고 코베스트 저자들이나 아니면 오픈 소스이기 때문에 누군가가 작성을 한 코드인데요. 이 코드를 보고 아실 수 있는 건 데이터 셋 패스 같은 경우에는 이게 허깅페이스의 데이터 세트라고 하는 레포지토리 혹은 데이터 저장소가 있잖아요. 거기에서부터 직접적으로 데이터를 받아오기 위한 이름입니다. 그래서 그룹이라고 하는 건 코베스트 내부에 여러 가지 데이터 셋이 있으니까 이 불큐라고 하는 데이터셋 역시 코베스트 내부에 있는 데이터 셋이고 그중에서도 실제 테스크는 코베스트의 불q 데이터 테스크를 할 것이고, 이 데이터셋 자체는 에티 코베스트 브1이라고 하는 허인 페이스 데이터 셋을 저장소에서 불교 데이터셋으로 평가를 할 거다. 그리고 평가 방식은 멀티 초이스로 평가를 하게 될 건데 학습 데이터 평가 데이터 테스트 데이터는 실제 데이터상 어노테이션으로는 트레인 밸리데이션 테스트로 되어 있고 입력 데이터에 대한 것은 변환을 주어져야 되는데 왜냐하면 왼쪽에서 보시는 것처럼 블큐 데이터는 패러프와 퀘스천이 주어지게 되잖아요. 이때 데이터셋 내부에는 사실 파라그래프라고 하는 게 없고 구한말부터 시작하는 정보가 있을 거고 퀘스천이라고 하는 건 없고 복륜은이라고 시작하는 텍스트만 있을 겁니다. 그거를 모델 입력으로 조금 더 풍부하게 넣어주기 위해서 우선 페러그래프 문장을 넣고 질문 땡땡 이걸 실제 질문 문장을 넣고 그다음에 답변이 무엇인지 도출해라 와 같은 방식으로 변환을 해주는 거를 닥투 텍스트를 통해서 정의를 하게 됩니다. 닥투 타겟 같은 경우에는 레이블이 타겟이 된다라고 하는 얘기가 될 것이고 초이스 방식은 예 혹은 아니오가 됩니다. 즉 페라게프를 보고 퀘스천을 맞춰야 되는데 페러 그래프를 바탕으로 했을 때 퀘스천이 사실이면 얘를 뽑아야 되고 거짓이라면 아니오를 뽑을 확률을 계산을 하게 된다라는 얘기가 됩니다. 그리고 데이터셋마다 평가 매트릭이 다르기 때문에 매트릭에 대한 정보도 이렇게 기재가 되어 있습니다. 이는 불큐뿐만이 아니라 코파나 헬라스, 레그, 센티 네그 그리고 WIC 모두 정의가 되어 있는 방식입니다. 그리고 정의를 할 때 여기 보시면 유트리스라고 되어서 정의가 된 경우가 있는데 이 경우에는 해당 정보를 그냥 별도의 요트스 파일 내부에 저장을 했을 경우에 그렇게 만들고 있습니다. 엘엠 이벨루에이션 하니스 내부에 있는 여러 가지 벤치마크 데이터셋들이 실제로 입력을 어떻게 하고 평가를 어떻게 하는지 살펴보고 싶다라고 하시면 요 에엠 이벨루에이션 하니스 내부에서 테스크스에 들어가시면 실제 벤치마크 데이터셋들이 정말 많이 있습니다. 뭐 코베스트 외에도 실습에서 사용을 하진 않지만 mmlu 역시도 이미 어느 정도 정의가 되어 있는 상태이기 때문에 한 번씩 살펴보셔도 좋을 것 같습니다. 네 그러면은 쭉 진행을 해볼 텐데요. 본래 에엠 이밸류에이션 할리스를 통한 평가 자체는 씨엘아를 통해서 진행이 됩니다. 하지만 지금은 실습이기도 하고 여러분들의 이해를 돕고자 이렇게 주피터 노트북 내부에서 실행을 할 예정이고요. 여기서 실행되는 이 아래 코드들은 여러분들이 씨엘아로 실행을 했을 때 실행되는 코드들을 가지고 온 것이다라고 생각을 해 주시면 됩니다. 우선 함수들을 먼저 정리를 해 주게 되고요. 평가를 해보도록 하겠습니다. 평가를 할 때 저희는 허인 페이스 모델을 사용하기 때문에 모델 타입은 허인 페이스 오토 모델 같은 경우에는 범인 님께서 정말 처음에 구워 주셨던 라마에 대해서 한국어 풀 트레이닝을 진행한 코라마 세븐빌리언 모델을 사용하도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 9,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1113,
      "char_count": 2022
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c010_51dfe4",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 우선 함수들을 먼저 정리를 해 주게 되고요. 평가를 해보도록 하겠습니다. 평가를 할 때 저희는 허인 페이스 모델을 사용하기 때문에 모델 타입은 허인 페이스 오토 모델 같은 경우에는 범인 님께서 정말 처음에 구워 주셨던 라마에 대해서 한국어 풀 트레이닝을 진행한 코라마 세븐빌리언 모델을 사용하도록 하겠습니다. 이 해당 모델 같은 경우에는 저희가 그냥 임의의 모델을 선택했다기보다는 저희가 처음에 다뤘던 파인튜닝 과정에서 썼던 모델을 그대로 사용을 하는 건데요. 그 이유는 파인 튜닝을 하기 전에 성능을 먼저 찍어보고 파인튜닝 하고 나서의 성능을 찍어보면서 성능이 어떻게 변하는지 한번 살펴보도록 하겠습니다. 그리고 모델을 평가할 때 해당 모델에 학습된 웨이트를 그대로 프리시전으로 들고 와야 합니다. 범인 님의 코라마 세븐비 같은 경우에는 플롯 16으로 되어 있기 때문에 디 타입이 그냥 플롯 16인데요. 만약에 최근 모델 이 플롯 16이면 비 플롯 16으로 고쳐주셔야 합니다. 그럼 모델 알규먼트로 이런 식으로 스트링으로 입력을 하면 됩니다. 스트링으로 입력을 한다라는 거는 프리트렌드는 실제로 사용하게 될 허깅 페이스 모델의 이름 그리고 디 콤마 디 타입은 이런 식으로 넣어주게 됩니다. 테스크 같은 경우에는 코베스트 테스크들 중에서 헬라 스웨그, 코파, 블큐 센티에그 총 네 가지에 대해서 어 성능을 찍어 보도록 하겠습니다. 그리고 최종적으로 리포팅 되는 성능을 리조트 디렉토리에 저장해 두도록 하겠습니다. 이때 보시는 것처럼 테스크 같은 경우에도 이렇게 다 쉼표로 연결을 해 놓으면은 한 번에 성능 평가가 이루어지게 됩니다. 그리고 만약에 퓨샷이 가능한 테스크들에 대해서 평가를 한다라고 하면은 보시는 것처럼 n 퓨샷을 입력을 하게 되면 퓨샷으로 평가가 되나 현재 코베스터 데이터 셋 같은 경우에는 제로 샷으로만 평가가 되기 때문에 제로 샷으로만 평가를 해보도록 하겠습니다. 평가 코드를 돌리게 되면 보시는 것처럼 위에는 그냥 현재 테스크들 중에서 등록이 안 된 테스크들이 있다라는 내용이 뜨는 것이고 실제로 돌아가게 되면은 모델을 로드하게 되고 그 이후에 실제 평가 과정이 돌아가게 됩니다. 이때 평가 과정이 돌아가면서 실제 돌아갈 때 사용되었던 여러 가지 컨피규레이션 중에 저희가 알아둬야 되는 내용들은 프린트해서 띄워주게 되는데요. 조금만 기다리면 아마 프린트 되게 될 겁니다. 네 보시는 것처럼 코베스트 각각의 데이터셋들에 대해서 퓨샷이 제로로 되어 있는데 위에서 제가 잠깐 말씀드렸던 것처럼 해당 데이터셋 자체가 이미 제로 샷이기 때문에 아무리 저희가 제로 샷이 아니라 다른 샷을 넣어도 무시된다라는 것을 보여주고 있습니다. 평가 자체는 7600건의 데이터셋에 대해서 돌리기 때문에 오래 걸리지는 않는데 제가 그래도 밑에 돌려놓은 게 있어서 잠깐 정지하고 돌려놓은 결과물로 살펴보도록 하겠습니다. 돌린 결과는 다음과 같습니다. 코베스트 코파 블큐, 헬라 스웨그 센티네그 모두 어큐러시와 F1 그리고 어큐러시 노멀라이제이션 된 결과가 이렇게 나오는데요. 보시게 되면은 VQ 같은 경우에는 50.21%의 정확도, 코파 같은 경우에는 48.4, 헬라스웨그는 23.8 센티, 네그는 49.37의 정확도를 가지고 있습니다. 엘르램 평가 지표들을 살펴볼 때 중요한 점 중 하나는 사실 이게 다운스트림 테스크 성능을 곧이곧대로 의미하지 않는다는 게 중요한 점 중에 하나입니다. 다른 말로 해보자면 불큐에 대해서 50.21%의 성능이 나왔다고 해서 LLM을 패시지와 퀘스천을 주고 얘가 해당 퀘스천이 실제 정답이니 아니냐고 물어봤을 때 항상 50%가 나온다고 보장할 수가 없습니다. 왜냐하면은 저희가 물어볼 퀘스천에 실제 도메인이 무엇이나 되는지 혹은 특수 도메인들 예를 들어서 금융 혹은 의류 도메인과 같은 매우 특수한 도메인의 경우에는 이것보다 성능이 안 나올 가능성도 있고 그것뿐만이 아니라 정보가 모델이 학습된 시점보다 이후에 나온 정보들에 대해서 평가를 하게 되면 또 점수가 달라질 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 10,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1081,
      "char_count": 1997
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c011_1f691e",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 다른 말로 해보자면 불큐에 대해서 50.21%의 성능이 나왔다고 해서 LLM을 패시지와 퀘스천을 주고 얘가 해당 퀘스천이 실제 정답이니 아니냐고 물어봤을 때 항상 50%가 나온다고 보장할 수가 없습니다. 왜냐하면은 저희가 물어볼 퀘스천에 실제 도메인이 무엇이나 되는지 혹은 특수 도메인들 예를 들어서 금융 혹은 의류 도메인과 같은 매우 특수한 도메인의 경우에는 이것보다 성능이 안 나올 가능성도 있고 그것뿐만이 아니라 정보가 모델이 학습된 시점보다 이후에 나온 정보들에 대해서 평가를 하게 되면 또 점수가 달라질 수 있습니다. 그렇기 때문에 실제로 LLM들의 평가 지표들에 대한 점수들은 절대값을 확인하시기보다는 상대적으로 점수가 어떻게 되는지 즉 현재 평가하고 있는 범인 님의 코라마 세롬비 성능이 이 정도 될 때 파인튜닝 하면 성능이 어느 정도 개선이 되는지 그 점수 개선 폭만 보는 것이 좋습니다. 그러면은 이런 식으로 범인 님의 코라마 세븐 비에 대한 성능 평가를 완료하면 결과물이 제이슨 파일 형태로 저장이 되게 되는데요. 아까 잠시 보여드렸던 것처럼 제로샷 성능의 결과가 코베스트 물큐의 성능이 이 정도 코베스트 코파가 이 정도 이런 식으로 각각의 성능이 리포팅이 되고 그거 외에도 평가 시점에서의 컨피규레이션들이 저장이 되는 것을 볼 수가 있습니다. 다른 정보들은 사실 저희가 입력된 정보들을 그대로 사용하거나 아니면 테스크에 있는 정보들을 그냥 여기다가 붙여 놓는 방식이기 때문에 중요하지 않을 수도 있지만 한 가지 중요하게 살펴보셨으면 하는 거는 사실 지금은 그렇진 않은데 어차피 퓨샷을 사용하지 못해서 상관없겠지만 퓨샷을 사용할 수 있을 때 이그잼프를 몇 개를 사서 퓨샷을 구성을 했는지를 살펴보시면 좋을 것 같습니다. 네 여기 보시는 것처럼 코파에서 사용했던 퓨샷의 개수가 0이다라는 걸로 고정이 되어 있는 걸 볼 수가 있는데요. 만약에 제로샷 성능하고 퓨샷 성능을 비교를 하게 되면 올바른 비교는 아닙니다. a 모델의 제로샷, b 모델의 퓨샷 성능을 비교를 하면 a 모델이 점수가 더 높네, b 모델이 더 점수가 높네라고 평가하기가 힘든 것이 퓨샷을 무엇을 구성하는지에 따라서 그리고 몇 개나 더 이그젬프를 넣었느냐에 따라서 성능 편차가 매우 크기 때문에 항상 케 샷은 고정을 하고 성능 기를 비교를 해 보셨으면 좋겠습니다. 그래서 여기도 보이시는 것처럼 제로 샷으로 했을 때 성능이 리포팅이 되고요. 제가 혹시 몰라서 예시로 보여드리려고 원샷하고 포샷도 성능을 뽑아봤는데 보시면 코베스트 자체가 제로샷만 지원을 해주도록 설정이 되어 있기 때문에 원샷을 하든 4샷을 하든 제로 샷과 성능이 정확히 똑같은 걸 볼 수가 있습니다. 왜냐하면 아무리 넘 샷에다가 숫자를 넣어놔도 제로 숏으로 강제되기 때문이죠. 그래서 실제로 평가를 나중에 하실 때도 이게 제로 샷만 되는지 아니면 퓨샷도 가능한 건지 꼭 확인을 하시면서 돌리시길 바라겠습니다. 그러면 범인 님의 코라마 세븐 비 성능은 이 정도에서 마무리가 될 것 같고요. 저희가 학습시킨 모델을 가지고 성능 평가를 해보도록 하겠습니다. 여러분도 실습을 하시면서 학습을 시키고 그 학습된 모델을 가지고 성능 평가를 할 텐데요. 저는 500 스텝을 학습시켰고요. 배치 사이즈 64로 해서 학습이 되어 있습니다. 사실 이런 정보들이 따로 적혀 있으면 더 좋겠지만 적어놓지 않으셨다 하더라도 확인할 수 있는 방법은 있거든요. 이게 트레이너를 사용을 하기 때문에 트레이너 에서 트레이너 스테이트라는 제이슨 파일이 있습니다. 제이슨 파일을 보면 확인을 할 수가 있는데요. 글로벌 스텝은 500이었고 50 스텝마다 이밸류에이션을 진행을 했었다. 그리고 이밸류에이션 결과물들 이건 트레인 노스인데요. 트레인 노스를 보니까 이렇게 쭉 진행이 됐었고 이밸류에이션 결과물을 봤더니 스텝 100에서의 로스가 뭐 이 정도 나왔더라라는 걸 확인을 할 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 11,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1052,
      "char_count": 1940
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c012_8a6d6c",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 사실 이런 정보들이 따로 적혀 있으면 더 좋겠지만 적어놓지 않으셨다 하더라도 확인할 수 있는 방법은 있거든요. 이게 트레이너를 사용을 하기 때문에 트레이너 에서 트레이너 스테이트라는 제이슨 파일이 있습니다. 제이슨 파일을 보면 확인을 할 수가 있는데요. 글로벌 스텝은 500이었고 50 스텝마다 이밸류에이션을 진행을 했었다. 그리고 이밸류에이션 결과물들 이건 트레인 노스인데요. 트레인 노스를 보니까 이렇게 쭉 진행이 됐었고 이밸류에이션 결과물을 봤더니 스텝 100에서의 로스가 뭐 이 정도 나왔더라라는 걸 확인을 할 수가 있습니다. 보시면 트레인 로스가 4.6859에서 시작을 해서 쭉쭉쭉 학습이 진행되면서 4.1166 정도로 지속적으로 학습이 진행이 잘 됐었구나라는 걸 알 수가 있고 결국 최종적으로는 500 스텝 학습이 된 모델을 가지고 와서 평가를 하고 있구나라는 것 역시도 확인을 할 수 있을 겁니다. 배치 사이즈는 64였고요. 아 그러면 500스텝 학습된 모델을 가지고 와서 평가를 하면 어떻게 해야 돼요라고 하시면 아까 학습할 때도 말씀을 드렸다시피 로라로 학습을 하게 되면 모든 모델의 웨이트가 저장되는 게 아니라 어댑터 즉 로라이 웨이트만 저장이 되게 됩니다. 그리고 어댑터에서 랭크가 몇이었는지 즉 알이 몇이었는지 알파가 몇이었는지 드롭 아웃이 얼마였는지도 다 이 어댑터 컴퓨에 저장이 되어 있습니다. 그래서 저희가 해야 될 거는 이 어댑터 컴피그랑 어댑터 모델 세이프 텐서스 파일이 저장되어 있는 이 경로를 패프트는 하고 넣어주시면 됩니다. 여기서 저도 로라패스에다가 트레인드 로라 웨이트 안에 코라마 500까지만 넣어 놓은 걸 보실 수 있죠. 그러면 이거를 기존에 코라마 세븐비 평가하는 거에다가 모델 알이겠스에 추가적으로 페프트는 하고 넣어 놓으시면 엘엠 이벨루에이션 하니스 내부에서 알아서 기본적인 모델은 이거를 로드를 하고 그 이후에 로라 웨이트 이거를 추가적으로 로드해서 사용을 하게 됩니다. 이거 평가 역시도 위에서 말씀드린 것과 나머지 코드는 다 똑같이 진행이 되고요. 평가를 하게 되면 점수가 이렇게 산출이 되게 됩니다. 이거를 1 대 1로 비교를 해보면 좋을 것 같은데요. 얘를 끌고 위로 올려보시면 이게 학습을 안 했던 코라마 세븐비의 기본적인 성능이고 아래가 학습을 했던 500스텝 학습을 한 로라 웨이트에 대한 성능 리포팅입니다. 보시게 되면 사실 이게 숫자가 많아서 조금 보기 어려우실 수도 있는데 하나씩 비교를 해볼게요. 우선 불규 같은 경우에는 어큐러시가 50.21 에프 스코어가 33.43이었습니다. 근데 여기다가 학습을 했다 해서 50.21 33.43으로 똑같은 숫자가 나오는 걸 볼 수가 있는데요. 이 이유는 크게 두 가지가 있을 겁니다. 왜냐하면은 현재 저희가 학습을 했다라는 건 인스트랙션 튜닝 그러니까 입력 데이터로 질문이 들어오면 그에 대한 요청 사항을 출력으로 내뱉도록 학습을 진행을 한 건데 그걸 500 스텝만 학습을 했습니다. 물론 인스트럭션 튜닝이 학습이 많이 필요하진 않지만 500스텝은 모델 크기에 비해서 너무 적은 학습입니다. 세븐빌리언 짜리 모델을 500스텝만 학습을 했으니까요. 그래서 성능 변화가 거의 없는 거라고 생각을 해보실 수 있고요. 코파에 대한 성능을 봤더니 코파는 48.4 48.32의 성능이었는데 49.6 49.54로 1포인트 정도 성능이 오른 걸 볼 수 있습니다. 즉 500 스텝 정도만 학습을 했을 때 1포인트 정도의 성능 개선을 확보할 수 있구나라는 걸 알 수 있고 헬라 스웨그 역시도 조금의 성능 개선이 있는 거는 확인할 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 12,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 947,
      "char_count": 1788
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c013_064928",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 세븐빌리언 짜리 모델을 500스텝만 학습을 했으니까요. 그래서 성능 변화가 거의 없는 거라고 생각을 해보실 수 있고요. 코파에 대한 성능을 봤더니 코파는 48.4 48.32의 성능이었는데 49.6 49.54로 1포인트 정도 성능이 오른 걸 볼 수 있습니다. 즉 500 스텝 정도만 학습을 했을 때 1포인트 정도의 성능 개선을 확보할 수 있구나라는 걸 알 수 있고 헬라 스웨그 역시도 조금의 성능 개선이 있는 거는 확인할 수가 있습니다. 센티네그 같은 경우에는 성능이 저하되는 양상을 보이고 있는데요. 제가 다른 실험들을 해봤을 때도 경험적으로 센티네그가 인스트럭션 팔로잉 능력을 제대로 반영을 하지는 못하는 것 같은 경우가 많이 있더라고요. 아무래도 여기 위에서 예시를 보면서 얘기를 해보자면 센티네그 같은 경우에는 코파나 아니면 불q 와 다르게 단순하게 입력 데이터로 문장 하나가 들어왔을 때 긍정 부정 두 가지만 뱉으면 되기 때문에 사실 이게 인스트랙션 팔로잉과 조금 다른 방식의 동작이지 않겠습니까? 예를 들어서 퀘스천이 주어진다거나 아니면 입력된 게 전장이 시작되었다. 그다음에 올 문장이 병사들이 집으로 돌아왔다. 병사들이 전투에 파견되었다. 둘 중에 무엇인지 알아내는 거는 사실 입력 데이터로부터 여러 가지 정보를 취합을 해야 되는데 센티에그는 정말 짧은 문장을 가지고 긍정인지 부정인지만 분류하면 되기 때문에 사실 요 부분에서 센티네그가 인스트럭션 튜닝 성능을 제대로 리포팅을 반영이 되지 않는 건 아닌가라는 생각을 해볼 수가 있을 것 같습니다. 왜 이렇게 센티네에 대해서 자세히 얘기를 드리고 나머지 세 가지 데이터셋에 대해서는 설명이 자세하지 않냐면 사실 LLM의 성능 평가가 한 가지 지표로 완전히 이루어질 수는 없습니다. 그러니까 학습 인스트럭션 팔로잉 로라 튜닝을 안 했을 때랑 튜닝을 했을 때 성능을 봤을 때 센티네그 성능이 감소했으니까 인스트럭션 팔로잉이 제대로 안 되는 거 아니야 혹은 파인 튜닝이 잘못된 거 아니야라고 생각하기는 너무 성급한 생각이고요. 여러 가지 데이터셋의 성능을 종합적으로 살펴봤을 때 전반적으로 성능이 개선되더라 혹은 성능이 개선되지 않더라 하는 것들을 같이 살펴봐야 됩니다. 그래서 이번에 평가 과정에서도 한 가지 데이터만 하는 게 아니라 코베스트에 있는 네 가지 데이터셋으로 진행을 한 거고요. 또 한 가지 더 얘기를 해보면 좋을 부분은 엘엠 이밸류에이션 할리스를 사용하는 다양한 리더보드들이 있을 겁니다. 근데 해당 리더 모델에서 성능이 높은 모델들이 무조건 좋은 모델은 아닙니다. 왜냐하면은 원래 인스트럭션 팔로잉 튜닝이라 하는 거는 다양한 인스트럭션에 대응할 수 있는 튜닝이 되어야 하는데요. 보시는 것처럼 블큐의 경우에는 파라 그래프를 넣고 이 파라 그래프에 대한 정보를 바탕으로 퀘스천에 대한 게 사실인지 아닌지를 판단하는 테스크니까 이런 형식으로 데이터를 끌어서 튜닝을 하면 불큐에 대한 성능이 높아질 수 있거든요. 그리고 그런 데이터를 충분히 만들 수 있고요. 근데 그렇다고 해서 브q의 성능이 높아지는 그 모델이 과연 정말로 인스트럭션 팔로잉이 잘 되느냐라고 하면은 그렇게 얘기하기는 조금 힘들 겁니다. 왜냐하면은 당연하게도 우리가 원하는 엘엘엠에게 무엇이든 물어봤을 때 무엇이든 답해주는 능력은 이 형식만 따른다고 잘 되는 게 아니니까요. 그래서 LLM 리더 보드들을 살펴보시고 어떤 모델을 사용해야 되지라는 고민을 하실 때 조금 더 유의념해서 살펴보셨으면 좋겠는 건 전반적인 어 데이터셋들에서 모두 일관된 성능 개선이 보이고 있는지가 가장 중요할 것이고 그 외에도 이 모델을 튜닝을 했을 때 즉 파인 튜닝을 한 모델이라면 파인 튜닝 할 때 사용했던 데이터 셋들이 어떤 포맷을 따르고 있는지 그리고 그 포맷이 너무 평가 데이터셋들의 오버핏 되도록 포맷을 의도적으로 구성되어 있지 않는지 같은 것들도 함께 살펴보시면 좋을 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 13,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 1055,
      "char_count": 1939
    },
    {
      "id": "transcript_generative_ai_genai_nlp_part2_c014_ac39d4",
      "content": "[Generative AI] GenAI NLP part2\n\n다. 네 아마 여기까지가 전반적인 내용이 될 텐데요. 여러분들이 프로젝트를 하시면서 실제로 모델을 평가하실 때는 굳이 이렇게 지피트 노트북 상에서 제가 했던 것처럼 이벨류의 모델 함수를 위해서 선언을 해서 사용하실 필요는 없고요. lmebl 패키지를 맨 처음에 말씀드린 것처럼 f을 통해서 설치를 하셨다면 이와 같은 명령어를 CLI에 입력을 하시면 위에서 제가 해 놓았던 것과 동일한 과정을 수행을 할 수가 있습니다. 그래서 많이들 하시는 것처럼 CLI에 입력을 배시 스크립트로 작성을 해서 입력하는 등의 방법으로 LM 이벨을 한번 사용해 보시는 걸 추천드리겠습니다. 또 한 가지 해보면 좋을 것 같은 건 현재 실습에서는 베이스 모델하고 500 스텝을 학습한 모델만 평가를 하고 있는데 과연 300스텝, 500 스텝 1스텝 즉 학습을 더 하면 할수록 성능이 어떻게 변화하는지도 한 번씩 살펴보시면 좋을 것 같습니다. 네 여기까지 진행하도록 하겠습니다. 라즈 랭귀지 모델의 파인 튜닝 그리고 평가 과정에서 데이터 셋 구성이나 평가, 학습 데이터를 어떻게 전처리하고 실제로 학습이 진행되고 저장이 되는지, 그리고 저장된 모델에 대해서 평가를 할 때는 다운스트림 테스크마다 어떤 식으로 평가 과정이 이루어지는지에 대해서 살펴보았습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "GenAI NLP part2.json",
        "lecture_name": "GenAI NLP part2",
        "course": "Generative AI",
        "lecture_num": "",
        "lecture_title": "GenAI NLP part2",
        "chunk_idx": 14,
        "total_chunks": 15,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:5e2377922e6ecf875f21935c1e35124f500eed9a0c8f5e26e6fa2c5eecd2b4bb"
      },
      "token_estimate": 353,
      "char_count": 672
    }
  ]
}