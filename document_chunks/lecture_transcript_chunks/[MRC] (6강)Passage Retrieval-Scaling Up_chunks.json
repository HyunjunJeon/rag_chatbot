{
  "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
  "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
  "course": "MRC",
  "total_chunks": 8,
  "chunks": [
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c000_7a78c5",
      "content": "[강의 녹취록] 과목: MRC | 강의: 6강 | 제목: Passage Retrieval-Scaling Up\n\n네 안녕하세요 6강 시작하도록 하겠습니다. 6강의 주제는 이제 저희가 4강에서 5강 까지 패시지 리트리버를 어떻게 스파스 인베딩과 댄스 인베딩을 할 수 있을지 알아봤는데요. 이제는 댄스 인베딩 같은 경우는 스케일업을 할 때 좀 고려해야 할 점들이 많습니다. 그래서 어떻게 우리가 뭐 10개 100개 천개 정도 수준의 패시지의 개수에서 리트을 하는 것이 아니라 그걸 억 단위 또는 경우에 따라서는 조 단위까지 갈 수 있는 그런 전략을 오늘 알아보도록 하겠습니다. 그래서 오늘의 강의는 총 4개의 섹션으로 구성돼 있고요. 먼저 스머널티 서치에 대해서 다시 한 번 복습하면서 기본적인 개념을 알아보는 시간을 가지도록 하고요. 섹션 2에서는 이런 검색 방법론을 어떻게 우리가 어프록스메이션을 할 수 있을지 그리고 그 어프록스메이션을 통해서 더 빠른 속도로 가장 가까운 가장 유사한 문서를 찾을 수 있을지에 대해 알아보도록 하고요. 세 번째 섹션에서는 좀 더 구체적으로 파이스라는 라이브러리를 활용해서 어떻게 우리가 스케일업 할 수 있는지 알아보고 네 번째로는 실제로 실습을 해보면서 스케일업을 시도해 보도록 하겠습니다. 네 첫 번째 섹션입니다. 복습인데요. 저희가 4강 5강에서 말씀을 나눈 것처럼 임베딩을 통해서 리트리버 하려고 한다면은 퀘스천 쪽에 인코더가 있고 마찬가지로 패스 쪽에 인코더가 있습니다. 퀘스천 쪽 같은 경우는 질문이 들어올 때마다 인코딩을 해줘야 되는 그런 부분이고요. 패시지 쪽 같은 경우는 미리 패시지를 확보한 상태라면 오프라인으로 전부 다 연산을 해놔서 저장을 해놨다가 새로운 질문이 들어올 때마다 기존 패시지들과 비교를 해서 가장 유사도가 높은 패시지를 내보내는 방식을 취하게 됩니다. 여기서 저희가 인베딩 스페이스 상에서 질문이 들어오게 되면 그 질문에 가장 가까운 패시지를 벡터 스페이스에서 본다고 말씀을 드렸는데요. 다만 벡터 스페이스에서 리얼스 네이버 서치럼 보는 것과 그리고 이너 프로덕 스페이스에서 가장 하이스트 더 프로덕트 스코어로 보는 것과 좀 방법은 다르지만 기본적으로는 비슷하다고 볼 수 있겠습니다. 결국 가장 큰 문제는 저희의 패세이지의 개수가 늘어날수록 여기에 파란색 점들이 엄청 많아지게 되는 건데요. 기본적인 저희의 스코어를 계산하는 방식은 질문과 각각의 패시지 임베딩을 다 프로덕트 하는 것이고 다 프로덕트조차도 디멘션이 커지면 꽤나 부담스러운 연산이 될 수 있는데 이 파란색 점이 정말 많아지게 되면은 어떻게 효율적으로 가장 가까운 문서를 또는 가장 가까운 벡터를 찾을 수 있을까가 오늘 주제의 가장 중요한 골자입니다. 그리고 이 과정을 저희가 일반적으로는 시뮬레이티 서치라고 부르는데요. 말 그대로 검색을 상당히 빠르게 하는 것이죠. 기본적으로는 리얼스 네이버 설치 대신에 보통 현재 이쪽 분야에서는 이너프 서치가 조금 더 많이 쓰이고 있습니다. 간단한 배경을 설명을 드리자면은 뉴스 네이버 설치 같은 경우는 좀 개념적으로는 설명을 드리기가 쉬운 것 같아요. 결국에는 어떤 위치 기반으로 가장 가까운 걸 찾는다고 표현을 하면 조금 더 사람의 사람들의 개념으로 좀 쉽게 와 닿을 수 있는데 실질적으로 학습할 때나 아니면 여러 가지 효율성 차원을 봤을 때는 뉴스 네이버 같은 l2 유클리디언 디스턴스를 재는 것보다 두 개의 벡터에 다 프로덕트를 계산해 가지고 가장 높은 즉 맥시멈 값을 찾는 문제로 보는 게 조금 더 수월합니다. 그래서 지난번 렉처와 마찬가지로 기본적으로 저희가 어떤 가장 가까운 벡터를 찾겠다라는 말을 말씀을 드리면 맥시멈 이널 프로덕트를 찾겠구나라고 생각을 해 주시면 좋습니다. 다만 개념적으로는 좀 더 l2 같은 유클리던 스페이스에서의 어떤 거리를 생각하시는 게 조금 더 개념 정리하시거나 좀 상상을 하시는 데는 조금 더 도움이 되겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 0,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 1043,
      "char_count": 1913
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c001_c6121d",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 그래서 지난번 렉처와 마찬가지로 기본적으로 저희가 어떤 가장 가까운 벡터를 찾겠다라는 말을 말씀을 드리면 맥시멈 이널 프로덕트를 찾겠구나라고 생각을 해 주시면 좋습니다. 다만 개념적으로는 좀 더 l2 같은 유클리던 스페이스에서의 어떤 거리를 생각하시는 게 조금 더 개념 정리하시거나 좀 상상을 하시는 데는 조금 더 도움이 되겠습니다. MI스라고도 부르는 맥시멈 이n프로덕 설치는 결국은 연산으로는 상당히 간단한데요. 두 개의 벡터를 한 쪽을 트랜스포즈 한 다음에 멀티플리케이션을 통해서 스케일러 밸류를 내보내게 되고 여기서 이 스케일러 밸류가 스코어라 불릴 수 있는데 가장 높은 스코어를 내주는 벡터 즉 아이번즈 벡터를 찾는 게 립스 문제의 정의입니다. 저희가 5강에서 봤던 부분들 또는 4강에서 봤던 부분들은 이걸 연산하기 위해서 모든 벡터들을 다 하나씩 고오버 하면서 연산을 하고 브루 포스로 가장 높은 값을 찾는 방식을 저희가 알아봤습니다. 하지만 이게 결국 개수가 많아지면 많아질수록 비효율적이 되겠죠. 도면상으로 보면 결국 방대한 문서의 집합이 있고 이 문서의 집합에 대응되는 벡터 리스트를 오프라인으로 저장을 해 놓은 다음에 이 저장된 값들을 리얼 타임으로 쿼리가 들어올 때마다 해당되는 벡터를 찾아주는 방식인 건데요. 지금까지는 좀 인코딩 방법론을 봤다면 저 벡터로 변환해 주는 방법론이죠. 그리고 제가 4강 같은 경우는 스파스 인베딩 그리고 5강 같은 경우는 댄스 인베딩 방법론을 봤다면 오늘은 저 인코딩을 하고 난 다음에 저 검색하는 과정 조금 더 이제 블루 퍼스로 한다면 심플하다고 볼 수 있는 과정을 어떻게 스마트하게 더 효율적으로 할 수 있을지를 보는 거라고 또 말씀드릴 수 있겠습니다. 실제로 검색해야 할 데이터가 워낙 방대하다 보니까 문서의 개수로 봤을 때는 위키피디아는 500만 개의 문서가 있고요. 웹 상으로는 훨씬 더 많은 개수 억 단위 또는 조 단위까지 가기도 하고 또한 저희가 문서 단위로만 검색하는 것이 아니라 문서보다 더 작은 단위 패시지나 8급 단위로 볼 수도 있기 때문에 실제로 검색해야 할 총 대상은 정말로 방대합니다. 하지만 유저가 어떤 질문을 던졌을 때 그거에 대한 답을 줘야 되는 실질적인 저희의 타임 리밋은 상당히 짧죠. 예를 들어서 여러분이 검색 엔진에 질문을 던졌는데 그거에 대한 답이 내일 또는 일주일 후에 나온다면은 상당히 불편하겠죠. 따라서 이렇게 방대한 문서를 아주 짧은 시간 내에 훑으면서 가장 가까운 문서, 가장 가까운 벡터를 찾는 알고리즘이 상당히 중요하다고 할 수 있는데요. 결국에는 여러 가지의 트레이드 오프를 좀 고려를 해야 됩니다. 첫 번째로는 얼마큼 빨리 찾을 것인가인 건데 결국에는 당연한 얘기겠지만 벡터의 개수 즉 문서의 개수가 많으면 많을수록 스피드가 더 느려지겠죠. 더 찾는 데 오래 걸릴 수밖에 없고 그리고 이 벡터들을 저장할 공간이 필요할 텐데 웹에 올려놓는 경우는 상당히 편하긴 하겠지만 빠르기도 하고 용량이 커지면 커질수록 엠도 부족하게 될 거고요. 결국 디스크로 바꿔야 되겠지만 또 디스크 같은 경우는 계속 불러와야 한다는 부분이 또 효율성을 속도를 느리게 합니다. 마지막으로 정확성인데요. 브루 폴스 같은 경우는 정확성이 정해져 있겠죠. 하지만 오늘 저희가 알아볼 어프록시메이션 알고리즘들은 결국 정확성을 어느 정도 sc 파이스를 하면서 속도를 증가시키기 때문에 이 둘 사이의 트레이드 오프가 정말 중요해집니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 1,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 938,
      "char_count": 1729
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c002_385b3c",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 마지막으로 정확성인데요. 브루 폴스 같은 경우는 정확성이 정해져 있겠죠. 하지만 오늘 저희가 알아볼 어프록시메이션 알고리즘들은 결국 정확성을 어느 정도 sc 파이스를 하면서 속도를 증가시키기 때문에 이 둘 사이의 트레이드 오프가 정말 중요해집니다. 그래서 기본적으로는 이런 세 가지 문제를 접근하는 방법론이 각각 하나씩 있는데요. 결국에는 eqc를 올리는 부분에 대해서는 어떻게 하면 검색을 더 효율적으로 할 것인가에 대해서 저희가 고민을 해봐야 되고요. 그리고 메모리 같은 경우는 조금 더 줄이기 위해서는 컴프레션을 고민을 해봐야 되고 어떻게 저희가 더 적은 용량으로 저장할 수 있을 것인가 그리고 마지막으로는 설치 스피드를 더 올리기 위해서는 인퍼런스 타임 때 프로닝 같은 메커니즘을 활용을 해서 어프록시메이션을 더 많이 하는 대신 경우에 따라서 정확성을 좀 잃더라도 저희가 속도를 필요한 속도를 확보하는 것이 중요하다고 할 수 있겠습니다. 실제로 이런 엔진을 만들고 저희가 벤치마크를 해보면은 네 속도랑 실제 정확성이라 불릴 수 있는 리콜레이트의 상관관계가 항상 보이고요. 더 정확한 검색을 하려고 한다면 더 오랜 시간이 소모되는 것을 아마 여러분이 실제 실습이나 또는 마지막 프로젝트를 하실 때도 보시게 될 겁니다. 결국 콜퍼스가 커지면 커질수록 많은 것들이 어려워진다고 결론을 낼 수 있을 것 같아요. 탐색 공간이 커져서 검색도 어려워지고 저장해 둘 메모리 스페이스 또한 많이 요구가 되고 그리고 뭐 스파스 인베딩 같은 경우 이런 게 좀 더 심각하긴 하겠지만은 어쨌든 컴프레션을 통해서 이걸 줄일 수 있고요. 스파스 인베딩 같은 경우는 댄스 인베딩도 결국 저장해 둘 공간이나 탐색 공간이 커지는 부분은 똑같은 문제가 있습니다. 그래서 예를 들면 저희가 각 문서를 768차원의 벡터로 표현할 경우에 아무런 컴프렉션 없이 모든 문서를 저장한다고 하면은 문서의 개수가 10억 개를 넘어가는 순간 3 테러바이트가 필요하고 이 정도면 위키피디아의 개수랑 거의 큰 차이가 없고요. 10억 개면 1테라 개수 즉 이제 1조 개를 넘어가는 순간 페타바이트 수준의 용량이 필요해지게 됩니다. 다행히 저희가 이제 실습할 때나 챌린지 할 때는 이 정도로 크진 않겠지만 그래도 밀리언 단위나 또 기가 단위까지 갈 테니 어 오늘 강의 내용을 잘 숙지하시고 프로젝트에 잘 활용하시길 바라겠습니다. 네 그 섹션 2는 결국 이런 문제점들을 해결하기 위해서 특히 속도 측면에서 문제를 해결하기 위해서 어프록시메이션을 어떻게 활용할 수 있을지에 대한 얘기를 해보도록 하겠습니다. 먼저 1차적으로는 숫자를 컴프레션 하는 방법론입니다. 저희가 보통 어떤 숫자를 저장을 할 때는 플롯 32라는 4바이트짜리 체계를 활용을 하죠. 하지만 실제 이널 프로덕트 서치를 할 때는 4바이트까지 필요한 경우가 많지가 않습니다. 1바이트로 어느 정도 어프록스메을 한 다음에 저장을 하더라도 상당히 정확한 경우가 많고요. 이제 이런 프로세스를 스케일러 퀀타이제이션이라고 부르는데 말 그대로 각각 수치를 퀀타이즈를 해 가지고 퀀타이즈를 한 값에 대해서 용량을 줄일 수 있도록 해주는 겁니다. 그래서 압축하는 알고리즘이라고 보시면 될 것 같고요. 그래서 기본적으로 플롯 32 4바이트에서 4배를 압축할 수가 있습니다. 다음으로는 프루닝 쪽에서 저희가 좀 알고리즘을 살펴볼 텐데요. 이제 간단하게 말씀을 드린다면은 프루닝이라 하면은 이 점들을 정해진 클러스터로 정해진 클러스터로 소속을 시켜서 군집을 일으키는 형태를 말합니다. 그래서 이렇게 군집이 이루어진 상태에서 저희가 쿼리가 들어왔었을 때 쿼리가 모든 군집을 다 모든 클러스터를 다 비슷하는 것이 아니라 이 쿼리에 저 사각형이 쿼리죠. 저 오른쪽 밑에 다 그램의 사각형이 쿼리인데 저 쿼리에 가장 근접한 클러스터만 보는 방식을 택합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 2,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 1042,
      "char_count": 1917
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c003_5337dc",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 다음으로는 프루닝 쪽에서 저희가 좀 알고리즘을 살펴볼 텐데요. 이제 간단하게 말씀을 드린다면은 프루닝이라 하면은 이 점들을 정해진 클러스터로 정해진 클러스터로 소속을 시켜서 군집을 일으키는 형태를 말합니다. 그래서 이렇게 군집이 이루어진 상태에서 저희가 쿼리가 들어왔었을 때 쿼리가 모든 군집을 다 모든 클러스터를 다 비슷하는 것이 아니라 이 쿼리에 저 사각형이 쿼리죠. 저 오른쪽 밑에 다 그램의 사각형이 쿼리인데 저 쿼리에 가장 근접한 클러스터만 보는 방식을 택합니다. 실제로 클러스터링이 잘 돼 있다고 한다면은 멀리 있는 클러스터는 아예 볼 필요가 없겠죠. 왜냐하면 실제 그 클러스터 내에 있는 점들은 아무리 가깝다 하더라도 tk 내에 들어갈 일은 없을 테니까요. 그래서 결국은 가장 근접해 있는 클러스터만 그 방문함으로써 이 경우는 3개가 있겠죠. 저 동그라미로 표시된 저 3개의 군집을 방문을 하고 3개의 군집 내에 있는 포인트들에 대해서는 이그저 스티브 서치로 다 직접 비교를 합니다. 예를 들면 저희가 데이터 포인트가 100만 개인데 클러스터가 천개라고 한다면 그리고 각 쿼리 타입마다 10개의 클러스터만 비슷한다고 한다면 원래는 천 개 전부 다 클러스터를 비슷해서 100만 개의 포인트를 봐야 한다고 했을 때 100분의 1인 10개만 보고 실제 속도 같은 경우도 100분의 1로 줄일 수 있겠죠. 이런 개념으로 보시면 되고요. 클러스터링 같은 경우 보통 가장 자주 활용되는 방법론은 케이 미스 커스트링입니다. 이런 케이미스 커러스트링을 통해서 클러스터를 먼저 정의를 하고 그다음에 각 각 다큐멘트 벡터들을 클러스터의 가장 가까운 클러스터에 속하게 한 다음에 군집이 형성되면은 쿼리 타임 때 가장 가까운 클러스터만 순차적으로 방문함으로써 속도를 비약적으로 늘릴 수 있습니다. 그래서 이런 방법론을 인벌드 파일이라고 부르는 이유도 실제로 각 클러스터에 속해 있는 포인트들을 역으로 인덱스로 가지고 있기 때문인데요. 그래서 이거를 인벌리 리스트 업 스트럭처라고 부르고요. 인벌리 리스트 업 스트럭처라고 부르고 각 클러스터의 센트로이드 아이디와 해당 클러스터의 벡터들이 연결이 되어 있는 형태라고 보시면 될 것 같습니다. 그래서 보시면은 특정 벡터 파에 어떤 것들이 연결되어 있는지를 쫙 리스트가 돼 있죠 벡터 에도 그렇고요. 각각 벡터마다 연결돼 있는 센트로이드들이 리스트 되는 방식으로 데이터 구조가 변경되는 거고요. 자 이렇게 되면은 저희가 클러스터를 찾은 다음에 클러스터에 속해 있는 벡터들을 빠르게 확보가 가능함으로써 실제로 서치 스페이스를 비약적으로 빠른 시간 내에 줄일 수 있습니다. 네 실제로 이런 개념을 저희가 이제 코드 상에서 적용을 해볼 텐데요. 저희가 주로 사용할 라이브러리는 파이스라는 라이브러리입니다. 파이스는 페이스북에서 모니터링하는 그리고 만든 패스트 어프스메이션을 위한 라이브러리고요. 모든 것이 오픈 소스 되어 있고 따라서 이제 사용하기도 편하고 실제로 라지 스케일에 상당히 특화되어 있어 갖고 저희가 스케일업 할 때 상당히 용이하게 활용할 수 있습니다. 기존 백본은 시플 풀로 되어 있는데 래핑은 파이썬으로 돼 있어서 파이썬만 쓰시는 여러분들도 저희도 쉽게 활용할 수 있습니다. 그래서 파이스는 실제로 아까 저희가 보여드렸던 다이그램에서 인덱싱 쪽을 도와준다고 보시면 되고요. 인코딩 쪽은 그 도움을 주고 있는 부분은 아닙니다. 그래서 저희가 어떤 벡터들을 확보를 하게 되면은 이걸 먼저 학습을 할 거예요. 이 벡터들을 학습을 왜 하냐 저희가 파이스를 활용하려면 아까 보여드렸던 푸닝 즉 클러스터들을 이제 확보를 해야 되는데 당연히 완전히 랜덤하게 클러스터를 저희가 랜덤하게 지정을 하게 되면은 상당히 비효율적이겠죠. 그래서 실제로 그 클러스터들은 어떤 데이터 포인트들의 분포를 보고 적절한 데 클러스터를 지정을 해야 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 3,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 1054,
      "char_count": 1932
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c004_19d31c",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 그래서 저희가 어떤 벡터들을 확보를 하게 되면은 이걸 먼저 학습을 할 거예요. 이 벡터들을 학습을 왜 하냐 저희가 파이스를 활용하려면 아까 보여드렸던 푸닝 즉 클러스터들을 이제 확보를 해야 되는데 당연히 완전히 랜덤하게 클러스터를 저희가 랜덤하게 지정을 하게 되면은 상당히 비효율적이겠죠. 그래서 실제로 그 클러스터들은 어떤 데이터 포인트들의 분포를 보고 적절한 데 클러스터를 지정을 해야 됩니다. 그래서 이 클러스터를 지정하기 위해서 학습 데이터가 필요하고요. 또한 그뿐만 아니라 스케일러 퀀타이즈 하는 과정에서도 즉 플롯 31을 인트8로 바꿔주는 과정에서도 사실상 그 퀀타이즈 과정이 아주 레인지가 큰 플롯 넘버를 인터저 0부터 255 사이로 압축시키는 거라 볼 수가 있는데 그렇게 되면 결국에 저희가 그 플롯 넘버의 맥스가 얼마인지 그리고 미니멈이 얼마인지 그래서 얼마로 스케일하고 얼마큼을 업셋을 할 것인지에 대해서 저희가 파악을 할 필요가 있기 때문에 파이스를 인덱스를 빌딩을 할 때 학습 단계가 필요합니다. 즉 학습 단계에서는 이런 클러스터들과 스케일러 퀀타이제이션을 하는 비율과 이제 업셋을 계산을 하게 되고요. 이 학습 데이터를 통해서 클러스터랑 sq8이 정의가 되면 이제 그다음으로는 실제로 이 클러스터와 이 클러스터 내의 벡터들을 투입하게 됩니다. sq8 형태로요. 퀀타이즈 앤 형태로 투입하게 되는데 그래서 트레인 단계가 있는 것이고 그래서 에딩하는 단계가 있는 것입니다. 이 부분을 헷갈리시지 않도록 잘 구분을 하셨으면 좋겠습니다. 다만 많은 경우에는 학습할 데이터와 더하는 데이터를 따로 하지 않고 근데 다르게 하는 경우는 보통은 더할 데이터가 너무 많아서 이 전부를 학습하기엔 비효율적일 때 에드할 데이터의 일부를 샘플해서 학습 데이터로 활용합니다. 정말 데이터가 커지는 경우는 대충 한 40분의 1 정도의 크기나 이 정도로 샘플해서 쓰는 경우도 있습니다. 이렇게 파이스 인덱스가 만들어지면 실제로 인퍼런스 타임 때는 쿼리가 들어오게 되고 검색을 한 다음에 가장 가까운 클러스터들을 실제로 방문을 해서 그 클러스터 내에 있는 벡터들을 다 일일이 비교를 함으로써 tk 가장 가까운 문서 벡터들을 뽑아주게 되고요. 물론 제가 문서 벡터라고 표현을 하는 건 이해를 돕기 위해서 하는 것이고 파이스라는 라이브러리는 아주 일반적인 그런 라이브러리이기 때문에 어떠한 종류의 벡터든지 그 유사성을 잴 수가 있습니다. 그래서 예를 들면 가장 가까운 10개의 클러스터를 비슷한 다음에 sq8 베이스로 서치를 하고 이제 탑 케겔을 서치 리졸트로 내보내 주게 되는 형태인 거죠. 네 실제로 한번 코드 예제를 보도록 할게요. 먼저 슬라이드로 보고 실습으로 넘어가도록 하겠습니다. 저희가 브루트 포스로 모든 벡터와 쿼리를 비교하는 가장 단순한 인덱스를 먼저 만들어 볼 텐데요. 이제 보시다시피 랜덤 넌 파일을 이용해서 이렇게 매트릭스를 만들 수가 있고요. 그 인덱스를 만드는 과정은 상당히 간단합니다. 파이 스타 인덱스 플랫 l2라는 걸 활용해서 먼저 디멘션을 정의를 해 주고 여기다가 더해주면 되는데요. 그럼 여기서 물어보실 수 있어요. 왜 여기는 학습을 하지 않냐 학습이 필요가 없는 이유는 저희가 아까 보여드렸던 퀀타이제이션 즉 푸루닝과 스케일러 퀀타이제이션을 여기서 활용하지 않기 때문에 학습이 필요가 없는 것입니다. 퀀타이제이션을 활용할 때만 그 두 가지 타입의 컨타이제이션을 활용할 때만 학습이 필요한 점을 유의해 주시고요. 그래서 인덱스 닷 트레이는 없고 인덱스 닷 애드만 있습니다. 그다음에 바로 검색으로 넘어가는데요. 탑 케이크에 몇 개를 리튜브 할지 케를 정해주고 그다음에 인덱스에 서치를 해서 해당하는 쿼리를 넣어주고 케도 같이 넣어줘서 디랑 아이가 나오게 되고요. 여기서 d 같은 경우는 쿼리와의 실제 거리 즉 점수인 거죠. 점수 스코어하고 아이 같은 경우는 실제 그 각각의 패시지의 인덱스 아이디가 되겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 4,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 1071,
      "char_count": 1970
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c005_420b3a",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 퀀타이제이션을 활용할 때만 그 두 가지 타입의 컨타이제이션을 활용할 때만 학습이 필요한 점을 유의해 주시고요. 그래서 인덱스 닷 트레이는 없고 인덱스 닷 애드만 있습니다. 그다음에 바로 검색으로 넘어가는데요. 탑 케이크에 몇 개를 리튜브 할지 케를 정해주고 그다음에 인덱스에 서치를 해서 해당하는 쿼리를 넣어주고 케도 같이 넣어줘서 디랑 아이가 나오게 되고요. 여기서 d 같은 경우는 쿼리와의 실제 거리 즉 점수인 거죠. 점수 스코어하고 아이 같은 경우는 실제 그 각각의 패시지의 인덱스 아이디가 되겠습니다. 자 여기서 저희가 플랫 투 l2를 활용을 해서 아주 그 브리포스 서치 알고리즘을 만들어 봤는데 바로 직전에 말씀드렸던 퀀타이제이션을 통해서 저희가 어프록시메이션을 해야겠죠 어프로스메이션을 할 수 있는 퀀타이저를 만드는 방법은 이제 간단합니다. 먼저 한번 푸닝을 알아볼게요. 푸닝은 ibf라는 이름으로 이제 파이스 내에서 찾을 수 있는데 먼저 저희가 퀀타이저를 만들 거예요. 이게 이 퀀타이저 같은 경우는 클러스터에서 거리를 잴 때 어떻게 잴 것이냐에 대한 거고요. 클러스터끼리 거리를 재는 방법론은 저희가 l2 방식을 취할 거기 때문에 즉 거리를 재는 거죠. 거리를 재는 직접 이그저스티브하게 거리를 잴 거기 때문에 인덱스 플랫 l2를 활용을 해서 퀀타이저를 만들어 주게 되고 클러스터와 쿼리의 거리를 재는 방법론을 정의를 했으면 이 퀀타이저를 활용을 해서 파이스 닷 인덱스 ibf 플랫이라는 이제 클래스로 인스텐쉐이트 해 주게 됩니다. 이거 같은 경우는 이 퀀타이저를 활용해서 이제 클러스터들을 만들겠다라는 의미고요. 이게 중요한 파라미터가 엔 리스트가 있는데 결국엔 클러스터 개수를 몇 개로 할 것이냐 100개로 할 것이다 이렇게 정의를 해주는 것이죠. 그다음에 저희가 트레인을 하게 되면은 이 트레인 과정 중에서 인덱스 ibf 플랫이 학습 데이터 XB를 활용을 해서 엔 리스트 개수만큼 100개만큼의 클러스터를 케이인지 알고리즘으로 생성을 합니다. 좀 시간이 걸리고요. 보통 그다음에 다시 더해주죠. index TT 에드 스비를 하게 되는데 아까 말씀드린 것처럼 이 XB가 꼭 다를 필요는 없고 보통은 같습니다. 하만 다만 스비가 너무 큰 경우 학습할 때 너무 오래 걸릴 수도 있으므로 스비를 일부 샘플에서 학습을 한 다음에 더해주는 건 당연히 다 더해줘야겠죠. 저희가 ritive 하고 싶은 대상을 뺄 수는 없으니까 임의로 그다음에 이제 쿼리가 들어왔었을 때 서치를 하는 방식을 똑같이 취하게 됩니다. 이렇게 되면은 가장 가까운 몇 개의 클러스터만 방문을 해서 답을 내오게 되고요. 압축 기법 중에 sq 말고도 또 pq라는 압축 기법이 있는데요. sq보다 조금 더 압축량을 높일 수 있고 자세하게는 파이스 웹 페이지에서 보시는 걸 추천을 드리고 저희 오늘 렉처에서는 다루지 않겠습니다마는 에스큐랑 마찬가지로 학습이 필요하시고요. 그리고 또한 학습을 하는 방법론은 상당히 비슷합니다만 훨씬 더 줄일 수도 있습니다. q 같은 경우는 4b를 1바이트로 줄일 정도였다면 pq는 경우에 따라서 이제 768개 디멘션의 벡터의 사이즈가 768 곱하기 4라고 한다면은 그거를 예를 들면 한 100바이트 정도로 줄일 수 있는 즉 배수로 치면은 4배가 아니라 한 뭐 20배 정도로 줄일 수도 있는 거죠. 예를 들면 임의의 크기로 줄일 수가 있기 때문에 이런 것들은 챌린지에서 필요하다고 생각하시면 활용하시면 좋을 것 같고요. 일단 그렇다 하더라도 기본적으로 먼저 큐 스케일러 컨타이제이션을 먼저 해보시고 나중에 피큐가 필요하다고 생각이 들면 넘어가는 거를 추천드립니다. 마지막으로 이제 그 파이스의 연산 속도를 더 높이기 위해서는 GPU를 활용할 수가 있고요. GPU를 활용할 경우에 GPU의 모든 벡터를 올리는 것이기 때문에 장점 같은 경우 esr 스티브 서치를 상당히 빠르게 할 수 있다는 거지만 단점 같은 경우는 GPU 메모리의 제한에 따라서 벡터의 개수가 제한이 될 거고요. 그리고 또한 어쨌든 그 GPU에서 할 수 있는 부분들이 좀 한정적입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 5,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 1081,
      "char_count": 2036
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c006_c29279",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 마지막으로 이제 그 파이스의 연산 속도를 더 높이기 위해서는 GPU를 활용할 수가 있고요. GPU를 활용할 경우에 GPU의 모든 벡터를 올리는 것이기 때문에 장점 같은 경우 esr 스티브 서치를 상당히 빠르게 할 수 있다는 거지만 단점 같은 경우는 GPU 메모리의 제한에 따라서 벡터의 개수가 제한이 될 거고요. 그리고 또한 어쨌든 그 GPU에서 할 수 있는 부분들이 좀 한정적입니다. 사용할 수 있는 라이브러리들이 보시다시피 인덱스 플랫 l2만 활용할 수가 있고 다른 라이브러리 다른 클래스 같은 경우 다른 서치 알고리즘을 활용하기 어려운 경우가 많습니다. 그리고 또한 여러 개의 GPU로도 활용할 수 있는데요. 이도 마찬가지로 먼저 싱글 GPU를 활용해 보시고 더 빠르게 하는 게 필요하다고 생각이 드시면 멀티 GPU를 활용을 생각해 보시되 대부분의 경우는 멀티 GPU까지는 가실 필요가 없을 것이라 예상합니다. 네 오늘 6강 여기서 마치도록 하겠습니다. 감사합니다. 네 6강 실습 시작하도록 하겠습니다. 이제 먼저 런타임부터 오늘은 저희가 GPU를 일단은 먼저 요청을 하고요. 커넥트를 해줍니다. 오늘 같은 경우는 저희가 5강에서 썼던 이제 댄스 벡터 서치 댄스 인베딩 서치를 그대로 파이스에서 해보려고 해요. 그래서 거기서 썼던 모델을 그대로 가져와서 검색을 해볼 텐데 다만 5강에서 저희가 스케일을 상당히 좁혀서 낮춰 가지고 뭐 해봐야 10개 20개 정도의 벡터의 개수에서 가장 가까운 벡터를 찾아오는 걸 했다면 그 개수를 오늘은 늘려보도록 하겠습니다. 파이셀을 활용해서요. 그래서 이 노트북의 맨 초반 여러 개는 사실 5강에서 그대로 복사해 온 것이기 때문에 설명을 크게 안 하고 넘어가도록 할게요. 그래서 5강 내용이 여기까지는 전부 다 이거랑 이거 네 인포트를 해주고요. 매뉴얼 시드를 박아주고요. 그리고 다음으로 데이터 셋을 가져오고요. 다운로드를 해줍니다. 데이터 셋을 그리고 필요한 프리 프로세싱을 다음으로 해주고요. 학습을 학습에 필요한 코드 그대로 가져와서 디파인 해 주고 아규멘트 정의해 주고요. 그리고 모델 실제로 불러오도록 할 거고요. 이제 다운을 받겠죠. 모델을 그다음에 학습을 진행을 하도록 하겠습니다. 학습을 진행하도록 하고요. 그다음에 학습이 끝난 후 네 여기부터 조금 달라지게 되는데요. 일단은 서치 코퍼스를 정의를 해 주고 960개의 코퍼스가 그 문서가 있는 걸 확인을 했고요. 자 이제 인베딩을 생성하도록 합니다. 이 960개에 대해서 좀 시간이 걸리죠. 960개를 전부 다 보려고 하니까 네 그래서 보시면 저희가 마지막으로 나온 매트릭스는 지난번과 달리 960개의 모든 문서에 대한 768 디멘션 짜리 임베딩이 나온 걸 볼 수가 있고요. 이게 저희의 그 검색할 대상 문서들의 임베딩이라고 보시면 되고 쿠션 쪽은 상당히 비슷하게 진행을 합니다. 기존과 다만 스케일이 늘어난 것을 확인하실 수 있죠. 네 똑같은 방식이고요. 다만 5개를 뽑았어요. 저희가 퀘스천 개수를 그래서 이거를 저희가 또 퀘스천 5개가 있으니까 이걸 하나의 매트릭스로 묶어 줄게요. 자 그럼 저희 쿼리 매트릭스는 5개의 쿼리에 각각 768개의 디멘션이 있는 매트릭스고요. 저희가 검색한 대상은 960개 768 디멘션 짜리 문서 벡터입니다. 그러니까 저희가 이제 파이스를 활용해서 어떻게 패시지 리트리버 할 수 있는지 보기 전에 이거를 GPU 상에서도 할 수 있는 걸 좀 보여드릴게요. 그래서 저희가 먼저 쿠다를 GPU 상에 모두 이제 필요한 것들 다 올려놓고요. 이렇게 이 인베딩들 저희가 방금 계산했던 그 매트릭스들을 쿠다 상에 올려놓은 다음에 GPU 상에 올려놓은 다음에 간단하게 매트릭스 멀티플리케이션을 해 주시면 됩니다. 그럼 시간을 좀 재볼게요. 저희가 보시면은 시작 시간을 먼저 스탬핑을 하고 다 프로덕트를 구해요. 이 두 개의 사이에 그다음에 랭킹을 구하는 것까지 한 다음에 얼마나 걸리는지 시간을 보는 건데요. 상당히 빠르죠 GPU 상이어서 960개 문서에 대해서 0.01초 만에 가장 가까운 문서를 유저스티브 서치로 찾았습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 6,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 1074,
      "char_count": 2037
    },
    {
      "id": "transcript_mrc_mrc_6강passage_retrieval_scalin_c007_13a426",
      "content": "[MRC] [MRC] (6강)Passage Retrieval-Scaling Up\n\n다. 그럼 시간을 좀 재볼게요. 저희가 보시면은 시작 시간을 먼저 스탬핑을 하고 다 프로덕트를 구해요. 이 두 개의 사이에 그다음에 랭킹을 구하는 것까지 한 다음에 얼마나 걸리는지 시간을 보는 건데요. 상당히 빠르죠 GPU 상이어서 960개 문서에 대해서 0.01초 만에 가장 가까운 문서를 유저스티브 서치로 찾았습니다. 실제로 이거에 대한 답변을 한번 보시면 잘 나왔는지를 보시면 사실 이 경우는 그렇게 잘 나오지 않았지만 어쨌든 상당히 그 문서들이 나오는 걸 볼 수가 있어요. 네 이렇게 GPU를 활용해서 검색을 해봤는데요. 사실은 비슷하게 저희가 파이스를 활용해서 이젠 CPU 상에서 검색을 해볼 거고요. 한번 속도를 비교해 보도록 하겠습니다. 네 파이스 같은 경우는 렉처에서 본 강의에서 보여드린 것처럼 파이스를 먼저 인포트 해 온 다음에 이제 클러스터 개수 ibf 프로닝을 위한 개수를 설정을 하고 그다음에 몇 개의 클러스터를 볼지 같은 부분들 그리고 몇 개를 최종적으로 가져올지를 저희가 설정을 합니다. 그리고 클러스터링을 먼저 진행하기 위해서 클러스터를 위한 인덱스를 만들어 주고요. 플랫 l2로 그다음에 실제로 클러스터링을 진행을 합니다. 이렇게 사실 이렇게 진행을 해도 되고 학습을 바로 해서 진행하셔도 됩니다. 일단 좀 이거를 풀어서 보여드리려고 해서 클러스터링을 따로 진행을 했고요. 파이스로 클러스터링을 하신 다음에 이 센트로이드들을 확보를 해서 이 센트로이드를 결국 저희 퀀타이저에 더해줌으로써 ibf에 필요한 퀀타이저를 완성하게 됩니다. 네 됐고요. 상당히 빨랐죠. 이제 그다음에 이 클러스터를 학습한 후에 만들어진 퀀타이저를 활용해서 ibf 인덱스를 만들고요. sq8까지 같이 하도록 하겠습니다. 그래서 그다음에 그 벡터들을 똑같이 넣어주도록 할게요. 자 그다음에 저희가 이제 검색을 해볼 텐데요. 한번 똑같은 걸 검색을 해보도록 하겠습니다. 인덱스가 만들어진 후에 보시면 똑같은 방식으로 서치를 하죠. 다만 다 프로덕트를 계산을 GPU 상에서 했던 거와 달리 여기서 서치라는 메소드를 활용해서 똑같은 큐 인베딩 매트릭스를 가지고 탑 k를 구하는 알고리즘을 이제 수행을 하고 실제로 얼마나 걸리는지 한번 보도록 하겠습니다. 네 0.0221초가 나왔죠. 아까 GPU를 활용했던 검색의 속도가 0.01초였고 지금 CPU의 파이스를 활용한 어프록시메이션 이런 퀀타이제이션을 활용한 속도가 0.0002였습니다. 그러니까 대략 거의 5배 이상의 차이가 나는 거죠. 게다가 어프록시메이션 CPU를 활용을 했고 방금 위저스트립 서치 같은 경우는 GPU를 활용했는데 훨씬 더 지표가 빠름에도 불구하고 이런 어프록시네이션 알고리즘들이 얼마큼 우리가 검색을 빨리 하게 해주는지를 좀 아실 수가 있을 것 같아요. 실제 디스턴스들을 구해 볼 수도 있고요. 네 이렇게 디스턴스 나오는 걸 보실 수가 있고 실제로 검색 결과를 마찬가지로 비주얼라이즈를 해볼 수도 있습니다. 똑같은 코드죠 비주얼라이즈 하기 위한 봤을 때 똑같이 나오는 걸 볼 수가 있고 네 이렇게 보셨다시피 파이스를 활용을 하셔서 더 빠르게 검색을 하실 수 있고 거기다가 GPU 같은 이제 비싼 엑설레이터를 활용하지 않고도 저희가 검색을 상당히 빠르게 할 수 있는 거를 확인할 수 있습니다. 그래서 최종 챌린지 프로젝트에서도 파이스를 적극 활용하셔가지고 리얼 타임으로 퀘션 엔서링 시스템을 만들 수 있도록 하시면 좋겠습니다. 네 그러면은 6강 실습 여기서 마치도록 하겠습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (6강)Passage Retrieval-Scaling Up.json",
        "lecture_name": "[MRC] (6강)Passage Retrieval-Scaling Up",
        "course": "MRC",
        "lecture_num": "6강",
        "lecture_title": "Passage Retrieval-Scaling Up",
        "chunk_idx": 7,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c61ff883d1adc5b42d1399ce13e6719486f2d96c9c5ad38eddd0eea411b0d1ef"
      },
      "token_estimate": 936,
      "char_count": 1756
    }
  ]
}