{
  "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
  "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
  "course": "MLforRecSys",
  "total_chunks": 16,
  "chunks": [
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c000_e957cf",
      "content": "[강의 녹취록] 과목: MLforRecSys | 강의: 6강 | 제목: 마르코프 연쇄 몬테카를로 방법 II\n\n네 안녕하세요 이번에는 저희 마르코프 연쇄 몬테카를로 방법에 대해서 구체적인 알고리즘을 살펴보도록 하겠습니다. 자 저희가 지난 시간까지 해서 우리가 지금 샘플링이 필요한 이유들에 대해서 한번 살펴봤습니다. 그렇죠 우리는 pg라는 곳에서 만약에 샘플링을 하고 싶거나 익스펙테이션을 우리가 구하는 데 활용하고 싶거나 그런 과정을 하고 싶은데 pg라는 정확한 거는 알지 못하고 pg 대신에 PT 다시 즉 노멀라이징 콘서트는 우리가 모르는 상황들이 많았습니다. 그래서 우리가 예를 들어서 피지가 다음과 같이 지분의 뭐 우리가 앞에서 봤던 로테이션으로는 이렇게 쓸 수가 있었죠. 자 이런 상황에서 실제로 노멀라이징 콘센트를 구하는 건 매우 어렵다라고 했습니다. 그래서 분자만 우리가 어떻게 좀 알고 있을 때 예를 들어서 가우시안 피디프에서 그 루트 이파이 시그마 제곱 말고 그거 모르고 우리가 그 위에 익스포넨셜 텀만 알고 있을 때 어떻게 우리가 거기서 좀 샘플링도 하고 익스펙테이션도 구하고 할 거냐까지 봤습니다. 자 그런데 문제가 있었죠 인퍼턴 샘플링 리잭션 샘플링 너무나 좋은 알고리즘이긴 하지만 디멘션이 커지면 커질수록 문제가 많이 발생했었습니다. 자 그러면 우리가 좀 더 효과적으로 할 수 있는 방법이 없을까 한번 살펴보도록 하겠습니다. 그래서 오늘은 저희가 MCMC에 대한 기본적인 내용과 그에 대한 원리 왜 이 엠시엠시가 작동할 수 있는지 그거를 저희가 한번 살펴보고 그다음에 그림으로 예시까지 한번 살펴보도록 하겠습니다. 그다음에는 엠씨엠씨 심화입니다. 자 여기서는 저희가 헤밀토니안 몬테카를로를 아주 짤막하게 다룰 거예요. 자 엠씨엠씨 심화로 치엠c라는 알고리즘을 저희가 볼 건데 그 에이치엠씨라는 알고리즘은 여기 슬라이드에는 저희가 좀 여러분들께서 궁금하신 분들이 혹시 있을까 싶어서 제공은 하지만 이 엠시엠시의 단점이 뭐고 그러한 단점을 어떻게 해결해 줄 수 있는지 정도까지만 저희가 보고 넘어가도록 하겠습니다. 네 향후에 이 부분과 관련해서 궁금하신 분들은 제가 따로 나중에 심화 특강이 있으면 그때 그 특강을 통해서 아니면은 저에게 메일을 주시면은 제가 그 메일 아니면 슬랙의 질문을 통해서 저희가 해결을 한번 같이 보도록 하겠습니다. 자 첫 번째 엠씨엠씨에 대해서 저희가 한번 보도록 하겠습니다. 결국 우리가 찾고 싶은 거는 대부분의 경우에는 포스테리어이긴 해요. 우리가 그렇죠 우리가 q지라는 걸 찾고 싶다고 할 때 q지를 우리가 구하려면은 옵티멀한 q지는 피 제바이스라는 거였습니다. 그렇죠 여러분 기억나시죠? 자 q지라는 거를 우리는 옵티멀한 큐지는 뭐였습니까? 피의 제바 스였습니다. 근데 문제는 뭐였어요? 피 제바 스를 정확하게 구할 수가 없다. 왜냐하면 얘는 우리가 피스분의 피 제 콤마 스라고 우리가 생각할 수가 있는데 피스를 구하기가 너무 어렵기 때문입니다. PX라는 것은 결국 우리가 다르게 쓰면은 p의 z 콤마 x를 모든 z에 대해서 다 적분한 형태이기 때문에 얘를 구하는 것은 굉장히 어렵다 까지 봤습니다. 그 까닭에 우리가 이러한 노멀라이즘 컨스텀 때문에 너무 고통받고 있는 거예요. 지금 그런 고통에서 좀 더 우리가 회피할 수 있는 방법 베리션 앰퍼런스를 배웠지만 또 다른 방법을 보도록 하겠습니다. 그렇죠 사실 인포턴스 샘플링과 리젝션 샘플링도 마찬가지로 회피해 주는 이걸 해결해 주는 방법이긴 했어요. 그렇죠 이 노멀레신 콘서트를 모를 때에도 우리가 여기서 뭐 샘플링을 할 수 있는 방법이었으니까 다만 그게 좀 많이 비효율적이었기 때문에 보다 더 효율적인 방법을 찾고자 하는 것뿐입니다. 자 MCMC에 대해서 한번 살펴보겠습니다. 자 리젝션 샘플링과 인포턴 샘플링은 좋아요 좋은데 여러 가지 리미테이션이 있습니다. 예를 들어서 우리가 하이드미션에서 즉 아까 봤던 이러한 제의 차원이 굉장히 커지게 되면 잘 작동하지 않는다는 겁니다. 반면 엠씨엠씨는 상대적으로 물론 이 엠씨엠씨도 뒤에서 말씀드리겠지만 디멘션이 커지면 커질수록 어려워요. 잘 작동 안 합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 0,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1103,
      "char_count": 2018
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c001_c1d5ae",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 자 MCMC에 대해서 한번 살펴보겠습니다. 자 리젝션 샘플링과 인포턴 샘플링은 좋아요 좋은데 여러 가지 리미테이션이 있습니다. 예를 들어서 우리가 하이드미션에서 즉 아까 봤던 이러한 제의 차원이 굉장히 커지게 되면 잘 작동하지 않는다는 겁니다. 반면 엠씨엠씨는 상대적으로 물론 이 엠씨엠씨도 뒤에서 말씀드리겠지만 디멘션이 커지면 커질수록 어려워요. 잘 작동 안 합니다. 그래서 그거를 좀 해결하기 위해서 우리가 다양한 샘플링 방법들로 변형들이 나타나고 있거든요. 하지만 기본적으로는 기존에 있던 인포턴 샘플링과 리젝션 샘플링 대비해서는 훨씬 낫다라는 얘기를 하고 있는 겁니다. 그리고 그러한 MCMS 샘플링은 마찬가지로 위에서 봤던 리젝션 샘플링 인포턴스 샘플링과 사실 결이 비슷해요. 좀 더 효율적이라는 것뿐입니다. 근데 마찬가지로 프로포절에서 뽑아요. 큐라는 프로포절을 마찬가지로 제안을 합니다. 큐라는 프로포절을 우리가 하나 설정을 해요. 근데 그 프로포셜이 계속 이동하는 거예요. 현재 우리가 뽑은 샘플에 기반해서 우리가 그다음에는 어디서 샘플을 뽑을 것인지 우리가 결정해 나아간다는 겁니다. 그래서 우리는 결국에 이러한 샘플들의 시퀀스를 우리는 만들어 나가는 과정입니다. 왜 그러한 샘플들의 시퀀스는 우리가 잘 설계만 해놓으면 이런 샘플들의 시퀀스는 결국 어디로 수렴을 할 거다 우리가 원래 찾고자 했던 포스테리어로 수렴을 할 거다라는 말입니다. 예를 들어서 그림 이런 식의 그림을 한번 보겠습니다. 우리가 인포턴 샘플링과 리젝션 샘플링 같은 경우는 결국에 특히 리젝션 샘플링 같은 경우는 이런 뭐 피가 있다고 할 때 아 뭐 여기서 뭐 피틸다라고 생각하셔도 됩니다. 자 피틀다가 있다고 할 때 우리가 큐라는 것을 뭐 잡아요? 큐라는 걸 잡는데 뭐 q라는 걸 잡고 케도 잡죠 이렇게 커지게 하기 위해서 그렇죠 자 그때 우리가 q를 잘못 잡으면 애초에 q를 잘못 잡으면 케이를 굉장히 크게 해야지만 우리가 이렇게 다 커버할 수 있겠죠. 왜 여기서는 지금 굉장히 작은 값이니까 점점 더 작은 값이니까 케이가 뭐 거의 뭐 10만 100만까지 가야 됩니다. 굉장히 커야 돼요. 자 그러면은 우리가 어떻게 돼요? 여기에서는 케이가 그러면 쫙 이렇게 쫙 엄청 컸다가 이렇게 되는 게 되겠죠 여기서는 대부분 리젝션이 된다라는 문제점도 있습니다. 일단 그 문제점도 하나 있고 q를 잘못 잡으면 굉장히 이렇게 고통스럽다. 그다음에 두 번째로는 우리가 q를 잘못 잡았다 하더라도 이걸 기반으로 계속해서 q를 만약에 이동해 나갈 수만 있다면 좋을 텐데 근데 그게 쉽지가 않은 거죠. 왜 여기서는 q가 고정되어 있다라고 많이 생각을 하니까 자 그래서 우리가 큐를 현재의 상황에 현재의 샘플에 기반해서 큐를 계속해서 이동해 나갈 수 있으면 좋지 않을까라는 내용입니다. 이렇게 지금 현재 제1이 우리의 샘플이라고 하면은 이걸 기반으로 자 그다음에 제 2를 어디서 뽑지 만약에 여기서 뽑혔다고 하겠습니다. 그러면 또 이 제 2를 기반으로 해서 우리가 그다음 지트3를 어디서 뽑지 결정하는 거예요. 물론 여기서도 어셉과 리젝이 존재합니다. 그런 면에서는 리젝션 샘플링과 좀 많이 유사하다고 볼 수가 있겠죠 하지만 차이점은 우리는 이 제1에 기반해서 제 2가 샘플링 되고 제2에 기반해서 또 제 3가 샘플링 되는 겁니다. 그렇죠 여러분 뭐가 떠오르나요? 마코브 체인이 떠오르지 않나요? 제 1에서 제 2가 나오고 제 2에서 제 3가 나오고 그쵸 이렇게 우리가 점점 더 발전시켜 나가는 과정입니다. 자 그러면은 이러한 엠씨엠씨를 이해하기 위한 기본 개념들을 저희가 쌓아 나가도록 하겠습니다. 자 여기서 우리가 살펴볼 것은 결국에 어떠한 환경에서 우리의 이러한 마크 오브 체인이 우리가 원하는 분포로 수렴할 것인가입니다. 무슨 말이냐면 우리가 여기서 z1을 샘플링하고 그죠 그다음에 이 제트을 기반으로 해서 제 2 샘플링하고 그다음에 제 3 샘플링하고 이렇게 쫙 나갑니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 1,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1070,
      "char_count": 1984
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c002_c27198",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 자 그러면은 이러한 엠씨엠씨를 이해하기 위한 기본 개념들을 저희가 쌓아 나가도록 하겠습니다. 자 여기서 우리가 살펴볼 것은 결국에 어떠한 환경에서 우리의 이러한 마크 오브 체인이 우리가 원하는 분포로 수렴할 것인가입니다. 무슨 말이냐면 우리가 여기서 z1을 샘플링하고 그죠 그다음에 이 제트을 기반으로 해서 제 2 샘플링하고 그다음에 제 3 샘플링하고 이렇게 쫙 나갑니다. 그랬을 때 이거를 그러면 무수히 많이 해 나가면 우리가 원하는 분포로 수렴을 하는 건가라는 생각이 드는 거예요. 이제 그게 어떠한 상황에서든 항상 되는 건 아니고요. 그게 가능한 상황을 저희가 만들어 가야 되는 거 즉 무슨 말이냐면은 자 우리가 만약에 예를 들어서 자 제1이 이렇게 있다고 하겠습니다. 그리고 제2가 나왔어요. 그럼 이때 이 제2를 어셉을 하면 그러면 이제 그다음 스테이트로 넘어가는 거예요. 제 2라는 스테이트에서 제 3를 또 샘플링해서 얘를 어셉할지 리젝할지 정하는 겁니다. 근데 이 제2를 우리가 어셉을 할지 리젝을 할지 이런 걸 어떻게 정할 건지 그러 내용에 대해서 어떻게 우리가 정해야지 이론적으로 결국 우리가 원하는 분포로 수렴해 나갈 것인지 살펴보도록 하겠습니다. 자 그걸 위해서 먼저 마코브 체인을 저희가 한번 정리하고 넘어가겠습니다. 자 마코브 체인은 다음과 같이 제1 제2 제3 이런 이제 랜덤 베리어블들의 시리즈로 우리가 정리를 할 수가 있는데요. 그런데 그중에서도 다음의 조건을 만족하는 시리즈를 우리가 마코브 체인이라고 부릅니다. 자 이걸 한번 보시면 n 플러스 한 번째의 z에 영향을 미치는 것은 우리가 뭐가 될까요? 첫 번째부터 n 번째까지가 될 겁니다. 근데 그 모든 게 다 필요한 게 아니라 바로 직전의 값만 있어도 된다라는 게 여기에 나와 있는 마법체입니다. 즉 결국 랜덤 베리블들의 연속적인 값들인 건데 시리즈인 건데 그렇죠 그중에서 즉 랜덤 베리어블의 시퀀스라고 생각하면 되겠죠 시퀀스인 건데 그냥 시퀀스가 아니라 이러한 성질을 만족하는 시퀀스를 우리는 마코브 체인이라고 부른다는 겁니다. 아시겠죠 즉 쉽게 생각해서 제엠에서 엠 플러스 1로 넘어가는 걸로 얘를 생각합시다. 그래서 우리가 얘를 다음과 같이 트랜지션 매트릭스로 표현도 할 거예요. 즉 zm에서 우리가 z n 플러스 1로 갈 수도 있고 또 다른 데로도 갈 수 있겠죠 그렇죠 그때 zm에서 zn 플러스 1로 갈 확률 를 이렇게 정의하도록 하겠습니다. 그리고 똑같이 얘를 t라고 정의할게요. 그리고 n 번째 시점 이라는 뜻에서 TM 트랜지션이라는 뜻에서 t로 정의하겠습니다. 그때 제엠에서 제엠 플러스 1로 넘어갈 확률은 그 과거의 것들은 우리가 중요하지 않다. 현재의 것만 있으면 된다라는 뜻입니다. 그걸 우리가 마콥 체인이라고 정의하겠습니다. 이런 성질을 만족하는 제1 제2 제3 zm들을 우리가 마콥 체인이라고 할게요. 이러한 마콥 체인은 결국 두 가지만 있으면 만들 수 있는 거예요. 일단 처음 제트 제로는 있어야겠죠 처음 스테이트는 만들어 놓고 그다음에 그 스테이트에서 우리가 그다음에는 또 어떤 스테이트로 넘어갈 건지 정하는 이런 트랜지션 매트릭스 또는 트랜지션 프로볼리티가 정의가 되면 우리는 이러한 마코브 체인들을 만들어 나갈 수가 있는 겁니다. 왜 우리가 여기서 트랜지션 매트릭스 또는 트랜지션 프로볼리티가 바로 직전의 스테이트에만 의존하도록 설정했기 때문에 그럼 마코브 체인이 만들어지는 거예요. 여기까지 마코브 체인 아시겠죠? 자 여기서는 우리가 마코브 체인이 그 호모지니어스 하다라고 우리가 가정을 하겠습니다. 왜 그러냐면 모든 타임 스텝에 따라서 일단은 우리가 동일하다라고 가정할 거기 때문이에요. 즉 여기서 보시면 여기서 마크업 체인은 뭔가 시간에 따라서 또 달라질 확률이 달라지거나 그러진 않아요. 즉 우리가 시간에 따라서 zm에서 zm 플러스 1로 갈 확률하고 그다음에 좀 시간이 지나서 얘가 달라질 확률이 또 달라질 수가 있습니다. 자 그런 거는 일단 우리가 무시하고 가장 간단한 세팅에서 한번 엠씨엠씨를 배워보겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 2,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1096,
      "char_count": 2034
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c003_fc611d",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 왜 그러냐면 모든 타임 스텝에 따라서 일단은 우리가 동일하다라고 가정할 거기 때문이에요. 즉 여기서 보시면 여기서 마크업 체인은 뭔가 시간에 따라서 또 달라질 확률이 달라지거나 그러진 않아요. 즉 우리가 시간에 따라서 zm에서 zm 플러스 1로 갈 확률하고 그다음에 좀 시간이 지나서 얘가 달라질 확률이 또 달라질 수가 있습니다. 자 그런 거는 일단 우리가 무시하고 가장 간단한 세팅에서 한번 엠씨엠씨를 배워보겠습니다. 그래도 쉽지 않을 수 있어요. 하지만 최대한 쉽게 우리가 한 줄 한 줄 따라가 보겠습니다. 자 그러면은 이러한 엠씨엠씨를 우리가 학습하기 위해서 필요한 개념 지식 일단 마코브 체인을 하나 저희가 배웠고요. 그다음 마지널 프로버빌리티를 하나 짚고 넘어가도록 하겠습니다. 자 여기 마지널 프로블리티라는 거는 뭐 특별한 거는 여러분 아니고 이미 여러분들께서 알고 계셨을 사항입니다. 즉 이거를 뭐 다르게 생각하면은 우리가 뭐 피에 스콤마 세트가 있는데 얘를 우리가 뭐 제에 대해서 제가 가질 수 있는 모든 경우의 수에 대해서 다 서메이션 하면 뭐가 되나요? PX가 됩니다. 그렇죠 그러면은 이런 걸 우리가 마지널lis라고 말하잖아요. 그렇죠 즉 이것처럼 여기서도 마찬가지입니다. 똑같은 얘기를 그냥 하고 있는 것뿐이에요. 자 한번 살펴보면은 자 제 엠 플러스 1에 우리가 속할 확률은 지금 뭐라는 거예요? 일단 제엠 이 있고 zm에서 z n 플러스 1로 트랜지션 될 확률로 생각하면 되겠죠. 그러면 z n 플러스 1이라는 게 딱 있는데 여기에 도달하는 확률은 zm이라는 거에서 zm 플러스 1로 오는 확률 값을 곱하면 됩니다. 근데 zm이 여기 있을 수도 있고 여기 있을 수도 있고 여기 있을 수도 있고 그렇죠 그럼 zm이 여기 있으면 여기에서 zm 플러스 1로 오는 zm이 여기 있으면 여기에서 zm 플러스 1로 올 확률 그거를 우리가 모두 다 고려만 해주면 된다라는 얘기입니다. 그렇죠 그걸 우리가 마지널l PRV리티라고 부릅니다. 그리고 우리는 특정한 분포가 인베리언트 또는 스테이셔널하다라고 말합니다. 특정한 마코브 체인에 대해서 여기서 특정한 마코브 체인이라 함은 특정한 트랜지션 매트릭스 또는 특정한 트랜지션 프로버빌리티에 대해서만은 아닙니다. 자 예를 들어서 이런 식을 한번 볼게요. 자 호모지니어스 마코브 체인이 나왔습니다. 이건 앞에서 본 것처럼 우리가 이러한 트랜지션이 이제 시간에 따라서 바뀌지 않는 거였습니다. 그렇죠 그래서 m이라는 걸 떼버렸어요. z에서 제 프라임으로 만약에 뭐 밥을 안 먹었는데 밥을 먹는 스테이트로 바뀌는 거는 항상 확률이 90%다. 뭐 새벽이 됐든 밤이 됐든 아침이 됐든 항상 동일하면 우리는 m을 떼고 호모 시니어스 마크 체인이라고 부르겠습니다. 그때 우리가 인베리언트 또는 스테셔널리하다라는 말은 얘가 스테셔널하다라는 말은 이 조건을 만족할 때라는 뜻입니다. 이 조건이 뭔지 한번 살펴보면은 여러분 피스타 지는 여기서 티의 제 프라임 지입니다. 즉 제 프라임에서 제로 넘어갈 확률이라는 거예요. 그렇죠 제 프라임에서 제로 넘어갈 확률 그다음에 얘는 뭐예요? 제 프라임에 지금 위치할 확률인 겁니다. 인스테이션이 그렇죠 그러면은 이거에 대한 서메이션이 다시 피스타 지가 되면은 우리는 얘를 스테이션을 하다 또는 인베리언트 하다라고 말합니다. 모든 지에 대해서 자 얘를 우리가 간략하게 쓰면 결국 얘예요. 피스타가 있고 걔는 피스타에다가 티를 곱해도 다시 피스타가 된다는 겁니다. 즉 얘는 바뀌지 않는다는 거예요. 쉽게 말해서 우리가 트랜지션을 다시 곱하더라도 바뀌지 않는 상황을 우리는 베리 그러한 분포를 우리는 인베리언트 또는 스테이셔너리 분포라고 말합니다. 결국 얘가 우리가 원하는 상황인 거예요. 어떤 상황 이 피스타가 결국 뭐가 되면 좋을까요? 자 마코브 체인은 우리가 마코브 체인 몬테칼로 엠씨엠씨는 뭐예요? 제트를 계속 우리가 샘플링 해 나갈 거예요. 그쵸 연속적으로 제1 샘플링하고 그 기반으로 제 2 샘플링하고 그다음에 제 제 3 샘플링하고 계속 샘플링 해 나갑니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 3,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1093,
      "char_count": 2039
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c004_05e75a",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 즉 얘는 바뀌지 않는다는 거예요. 쉽게 말해서 우리가 트랜지션을 다시 곱하더라도 바뀌지 않는 상황을 우리는 베리 그러한 분포를 우리는 인베리언트 또는 스테이셔너리 분포라고 말합니다. 결국 얘가 우리가 원하는 상황인 거예요. 어떤 상황 이 피스타가 결국 뭐가 되면 좋을까요? 자 마코브 체인은 우리가 마코브 체인 몬테칼로 엠씨엠씨는 뭐예요? 제트를 계속 우리가 샘플링 해 나갈 거예요. 그쵸 연속적으로 제1 샘플링하고 그 기반으로 제 2 샘플링하고 그다음에 제 제 3 샘플링하고 계속 샘플링 해 나갑니다. 근데 결국에는 얘가 계속해서 바뀌면 우리가 그거는 우리가 원하지 않는 거예요. 어느 순간에는 수렴하길 원해요. 그렇죠 수렴한다라는 말은 우리가 여기서 트랜션 매트릭스를 곱한다고 하더라도 우리가 동일하게 유지가 되는 걸 원하는 겁니다. 그게 여기 나와 있는 스테이셔너리고요. 그러면 이 스테이셔너리가 결국 뭐가 되면 좋을까요? 우리가 궁극적으로 추정하고 싶은 타겟 디스비션이 되면 좋습니다. 우리가 앞에서 봤던 생성 모델의 예시에서는 얘가 포스테리얼이 되겠죠 피제트바이스가 되면 좋겠다는 거였죠. 그게 우리의 스테이셔널 분포로 되길 원합니다. 즉 이 피스타가 우리의 타겟 분포가 되면은 우리가 계속해서 트렌징을 해 나가면은 어느 순간에는 우리가 이러한 스테셜링 분포로 수료하게 된다. 그러면은 그러한 스텐셔널 분포는 우리의 타겟 분포니까 우리가 원하는 타깃 분포를 잘 추정하게 된 겁니다. 여기서 또 한 가지 의문점이 들 수가 있어요. 굉장히 예리한 친구들은 어떤 의문점이 들 수 있냐면 그래요 알겠습니다. 선생님 그러면은 여기서 우리가 계속해서 이 피스타를 우리의 타깃으로 설정한다는 거는 좋은 전략 같긴 합니다. 그런데 이 피스타가 스테이셔너리 분포가 존재하지 않을 수도 있는 거 아닌가요? 즉 스테이셔너리 분포가 일단 존재를 해야지 그 스테이셔널의 분포가 우리의 타겟 분포가 되도록 설정을 해서 우리가 계속해서 샘플링을 하게 되면은 스테이셔널리 분포 또는 우리의 타겟 분포로 수렴한다라는 거는 말이 됩니다. 근데 애초에 스테셜링 분포가 존재하지 않는 마크업 체인도 있을 수 있는 거 아닌가요라고 의문점을 표현할 수가 있어요. 굉장히 예리합니다. 하지만 다행히도 최소 하나의 스테이션의 분포는 있어요. 이러한 디스크리단 마크업 체인에 대해서 자 여기서 디스크리단다라는 말은 카운터블까지 포함한 겁니다. 즉 카운터블 하다라는 말은 정수까지는 우리가 또는 유리수까지는 카운터블하다고 말해요. 실수부터는 우리가 언카운터블이라고 하지만 유리수까지는 괜찮아요. 우리가 가질 수 있는 이런 스테이트가 유리수까지는 괜찮다. 그때는 우리가 그러한 디스크나 마크업 체인에서는 우리가 항상 최소 1개 이상은 있다라는 겁니다. 그리고 이 리더 서버를 하고 어피어틱한 상황에서는 그러한 스텐셜링 분포가 딱 하나만 존재합니다. 자 예를 들어서 우리가 뭐 이런 상황이 아닌 이드 서버를 하지 않고 어필라디 하지 않은 상황을 가정할게요. 이게 뭔지는 뒤에서 볼 겁니다. 만약에 트랜지션 매트릭스가 아이덴티티라고 할게요. 그러면 어떠한 우리가 어떠한 분포든 스텐셔널이에요. 애초에 트렌지션 매트릭스가 아이덴티티하다라는 말은 어떤 걸 이 아이를 곱하면 어떤 걸 곱하든 비스타는 그냥 비스타예요. 그러면은 모든 분포가 스텐셜을 한 겁니다. 우리가 원하는 게 아니에요. 가장 우리가 원하는 거는 유니크한 즉 스텐셜의 분포가 딱 하나 있는데 그러면 우리의 타겟 분포를 그 스텐션을 분포로 딱 만들 수 있는 상황은 뭐예요? 언제 일류 서버라고 어필을 하면 일단 스텐셜의 분포를 만들 수가 있다는 겁니다. 존재한다는 거예요. 그거를 그러면 그러한 스테셜의 분포를 우리의 타겟 분포로 만드는 거는 좀 이따가 볼게요. 일단은 우리가 어떻게 구체적으로 할 건지는 저희가 다시 한 번 뒤에 나올 겁니다. 그거에 대해서 일단 대략적인 개념만 보고 다시 한 번 이 일 주 소블과 AP로딕이 뭔지 보도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 4,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1094,
      "char_count": 1997
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c005_1d0ff3",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 존재한다는 거예요. 그거를 그러면 그러한 스테셜의 분포를 우리의 타겟 분포로 만드는 거는 좀 이따가 볼게요. 일단은 우리가 어떻게 구체적으로 할 건지는 저희가 다시 한 번 뒤에 나올 겁니다. 그거에 대해서 일단 대략적인 개념만 보고 다시 한 번 이 일 주 소블과 AP로딕이 뭔지 보도록 하겠습니다. 여기서 우리가 앞에서 봤던 스테이셔너리를 우리가 다시 한 번 정리하면 이렇게 나와요. 그냥 정리하면 이렇습니다. 결국 우리가 트랜션 매트릭스를 곱하더라도 우리가 유지가 되는 상황인 건데 근데 그거는 얘를 만족하면 된다 얘를 만족하면 걔는 스테이셔널 하다라는 게 알려져 있습니다. 즉 이거는 스테이셔널이 분포에 대한 정의인 거고요. 이거는 얘를 만족하는 것은 스테이셔널이 분포다라는 성질을 의미하는 겁니다. 즉 얘를 만족시키는 분포를 우리는 찾으면 돼요. 또는 다르게 말하면 우리의 타겟 분포가 여기 있는 스텐션의 분포가 되길 원하잖아요. 그러면 우리의 타겟 분포가 얘를 만족하게 하면 된다는 겁니다. 그걸 만족하도록 하는 티를 잡으면 되는 거예요. 자 얘를 우리는 디테일드 밸런스 이퀘이션이라고 부를 겁니다. 왜 그러냐면 여기서 한번 생각을 해봅시다. 자 여기서 디테일리 밸런스 이퀘이션을 만족하면 왜 걔는 스텐셔널이 되냐 그거를 우리가 간략하게 한번 살펴보면 여기 여기에서 우리가 제트 프라임에 대해서 서메이션 한번 해보겠습니다. 제 프라임 서메이션 그러면 어떻게 됩니까? 우리가 디테일로 밸런스 이퀘이션을 만족하니까 그러면 어떻게 돼요? 얘는 지금 얘가 지금 여기 적혀 있는 거예요. 제 프레임에 대해서 서메이션을 했는데 얘랑 지금 같은 게 뭐예요? 얘예요 이렇게 쓸 수 있다는 겁니다. 그러면 어떻게 돼요? 피스타지가 밖으로 나올 수가 있겠죠 제 프라임이 여기 남아 있고 그럼 뭐가 돼요? 피스타 지가 됩니다. 얘가 피사시가 된다는 거예요 얘가 뭐예요? 앞에서 봤던 스테셜리 분포에 대한 정의였어요. 즉 디테일의 밸런스 이퀘이션만 만족하면 우리는 스텐셜의 분포라고 말할 수 있다는 겁니다. 즉 무슨 말이냐면 어떠한 분포가 우리는 스테셜의 분포인지 아닌지 확인하고 싶다. 그러면 디테일드 밸런스 리케이션을 만족시키는지 검증을 하면 된다는 겁니다. 또는 반대로 우리가 딱 특정이 원하는 타겟 분포가 있는데 그 타겟 분포가 스테이셔널이 분포가 되게 만들고 싶다라고 하면 어떻게 하면 된다는 겁니까? 이 시기 만족하도록 트랜지션 프로벌리티를 정하면 된다는 겁니다. 그래서 우리가 원하는 타겟 분포가 스테이셔너리가 되도록 하고 싶다면 이 티를 잘 정하면 된다 얘를 만족하도록 그렇죠 지금 만약에 t를 우리가 정할 수가 없는 상황이다. 그러면 그때 특정한 분포가 스테이셔널인지 아닌지 확인하고 싶다 그러면 여기 넣어서 확인하면 되고요. 만족하는지 근데 애초에 우리가 t를 정할 수 있는 상황이에요. 지금 왜 이런 거 샘플링 할지 리액션 할지 우리가 마음대로 정하면 되잖아요. 마치 우리가 신이 된 것처럼 정하면 되는 겁니다. 그 까닭에 t를 우리가 정할 수 있는 거예요. 그럼 t만 잘 정하면 우리는 문제가 해결된다라는 얘기를 하고 있는 겁니다. 여러분 지금까지 저희가 살펴본 것은 결국에 특정한 트랜지션 매트릭스를 저희가 잘만 활용한다면 이렇게 스테셔널리 분포를 우리의 타겟 분포로 만들 수 있다 라는 겁니다. 그렇죠 왜 이 성질만 만족하면 우리는 스테셜링 분포라고 할 수 있으니까 여기다가 나중에 우리가 원하는 타겟 분포를 끼워놓고 그랬을 때 어떻게 이 트랜션 매트릭스를 설정해야지 이 밸런스 이퀘이션에 만족할지 그것만 우리가 좀 고민해 주면 된다는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 5,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 991,
      "char_count": 1821
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c006_55f877",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 여러분 지금까지 저희가 살펴본 것은 결국에 특정한 트랜지션 매트릭스를 저희가 잘만 활용한다면 이렇게 스테셔널리 분포를 우리의 타겟 분포로 만들 수 있다 라는 겁니다. 그렇죠 왜 이 성질만 만족하면 우리는 스테셜링 분포라고 할 수 있으니까 여기다가 나중에 우리가 원하는 타겟 분포를 끼워놓고 그랬을 때 어떻게 이 트랜션 매트릭스를 설정해야지 이 밸런스 이퀘이션에 만족할지 그것만 우리가 좀 고민해 주면 된다는 겁니다. 대신에 여기까지의 가정은 일단 우리가 마코브 체인을 계속 돌렸을 때 체인을 계속 돌렸을 때 결국에는 우리의 그러한 샘플들이 이러한 스텐셜의 분포로 일단 수렴한다는 가정이 있는 거예요. 그렇죠 수렴을 해야지 그때 우리가 이러한 스페셜의 분포를 우리의 타겟 분포로 바꿔 끼우고 그러면 아 우리의 샘플들이 결국에 우리의 타깃 분포로 수렴을 하는구나라고 말을 할 수가 있는 것이니까요. 그러면은 과연 어느 상황에서 아니면 항상 스테이션의 분포로 수렴을 하는 건가 아니면 어떤 상황에서 수렴하는 건가라는 말입니다. 자 이 리더 서벌하고 어필로딕 할 때 우리는 마크 체인이 결국 스텐셜 분포로 수렴한다라고 말합니다. 무한히 많이 돌린다면 즉 무슨 말이냐면 여기서 이 위주 서블이 뭔지 한번 저희가 짚고 그다음에 어피어딕이 뭔지 짚고 넘어가겠습니다. 첫 번째 마크오브 체인은 우리가 이 리즈 서블하다고 말합니다. 언제 그러냐면 모든 이런 a b에 대해서 다음이 만족할 때라는 뜻입니다. 즉 무슨 말이냐면은 마우크 체인이 결국에 모든 스테이트가 여기서 모든 스테이트에 대해서 이게 가능해야 됩니다. 즉 여기서는 우리가 도달이 가능해야 되는 거예요. 그렇죠 모든 스테이트가 결국 이렇게 서로 우리가 왔다 갔다 할 수 있는 상황을 우리는 이 wide subl하다라고 말합니다. 왜 a에서 b로 가는 확률이 0이 아닌 거잖아요 그렇죠 그래서 우리가 몇 번의 스텝은 더 걸릴 수 있지만 당장은 아니더라도 몇 번의 스텝을 더 거쳐서 얘가 조달할 수 있는 확률이 0이 아닌 상황 을 의미합니다. 자 어피어리딕하다라는 말은 이러한 조건이 만족할 때라는 뜻입니다. 얘는 오랜만에 이제 보는 기호들이 나오는데요. 이게 좀 바로 와닿지 않을 수가 있기 때문에 이거 어필로딕은 저희가 뒤에 예시를 통해서 한번 보도록 하겠습니다. 자 여기 앞에 있는 이 위드 서블은 앞서 말씀드린 것처럼 우리가 도달할 수 있을 때 라는 뜻이 되는 것이고요. 어피어딧 같은 경우는 다음과 같이 결국 최대 공약수를 우리가 의미하는 것인데 결국 제1에서 zm 까지 즉 우리가 지금 스텝에서 a가 있는데 나중에 n 번째 타임 스텝에서 다시 a가 나온다는 거죠. 결국 자기 자신으로 다시 돌아오는 것을 의미합니다. 그렇죠 그러면 자기 자신으로 돌아오는 게 뭐 여러 스텝에 걸쳐서 다시 자기 자신으로 돌아올 수도 있고 바로 두 번째 돌아올 수도 있고 아니면 그것도 유니크하지는 않겠죠 두 번째 돌아왔다가 다섯 번째 돌아왔다가 그렇죠 그렇게 될 겁니다. 자 그때 그러한 모든 앱들을 우리가 모아놔요. 그것들의 최대 공약수가 1이 될 때 우리는 어피어리딕 하다라고 말합니다. 만약에 예를 들어서 이러한 그림을 우리가 한번 두 가지 그림을 보도록 하겠습니다. 여기서 1에서 2로 무조건 가는 거예요. 2에서 1로 무조건 가는 겁니다. 아시겠죠? 자 그렇다고 하면 얘는 이 리즈 서버를 합니까 안 합니까? 얘는 이 리즈 서버를 해요. 왜 1에서 2로 가고 2에서 1로 가고 어느 스테이트는 다 왔다 갔다 할 수 있잖아요 그렇죠 리처블하죠 근데 얘는 어필로디 하지 않습니다. 왜 1에서 자기 자신으로 다시 돌아오는 거는 두 번째 또 네 번째 여섯 번째 여덟 번째입니다. 즉 그것들의 최대 공약수를 구하면 2가 되기 때문에 어필러딕 하지는 않습니다. 자 근데 여기를 한번 보게 되면 여기서는 뭐예요? 이 리즈 서버를 합니다. 언제 피랑 q가 0이 아닐 때 0보다 클 때 그래야지 왔다 갔다 한다는 말이잖아요 그렇죠 자 그다음에 그때 이 리드서버하고 어피어를 합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 6,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1083,
      "char_count": 2019
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c007_21afc1",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 왜 1에서 자기 자신으로 다시 돌아오는 거는 두 번째 또 네 번째 여섯 번째 여덟 번째입니다. 즉 그것들의 최대 공약수를 구하면 2가 되기 때문에 어필러딕 하지는 않습니다. 자 근데 여기를 한번 보게 되면 여기서는 뭐예요? 이 리즈 서버를 합니다. 언제 피랑 q가 0이 아닐 때 0보다 클 때 그래야지 왔다 갔다 한다는 말이잖아요 그렇죠 자 그다음에 그때 이 리드서버하고 어피어를 합니다. 언제 마찬가지로 피랑 q가 0보다 클 때 왜 만약에 p가 0보다 크다라고 하면 p도 0보다 크고 q도 0보다 크다고 하겠습니다. 그러면은 1에서 자기 자신으로 돌아올 때는 어떻게 돼요? 큐로 갔다가 여기 2로 갔다가 다시 올 수도 있고 또 갔다가 올 수도 있고 그러면은 두 번째 네 번째 여섯 번째입니다. 근데 자기 자신으로 이렇게 올 수도 있어요. 그러면 첫 번째 세 번째 다섯 번째도 되는 거예요. 그렇죠 그 까닭에 이때는 이 m에 대한 최대 공약수가 1이 나오는 겁니다. 그래서 어피어리드 하다라고 말합니다. 지금까지 보면은 결국 우리는 뭘 한 겁니까? 이즈 서버라고 어패러딕하다라는 가정이 있으면 우리의 마코브 체인은 무한히 계속된다고 하면은 스테셔너리 분포로 가요. 그렇죠 그러한 스테셔너리 분포는 그러면은 우리가 뭐에 대해서 이러한 트랜지션 매트릭스에 대해서 얘가 스테이션의 분포라고 한다고 했습니다. 언제 이러한 디테일드 밸런스 이퀘이션을 만족할 때 그러면 지금 우리가 하고 싶은 게 뭐예요? 우리의 타겟 분포를 여기다가 집어넣고 싶은 거예요. 즉 우리의 타겟 분포가 이러한 디테일드 밸런스 이케이션을 만족하도록 만들고 싶다는 겁니다. 아시겠죠? 우리의 타겟 분포가 저렇게 만듭니다. 그러면 우리가 어떻게 해야 될까인 겁니다. 한번 가장 좀 비교적 단순한 형태를 보도록 하겠습니다. 여기서는 우리가 어떻게 할 거냐면 메트로폴리스 알고리즘이라고 부르는 건데요. 다음과 같은 어셉턴스 확률을 한번 가정해 보도록 하겠습니다. 즉 무슨 말이냐면 지금 우리가 제타워가 현재의 샘플이에요. 현재의 샘플이고 이걸 기반으로 그다음 샘플의 후보 제 스타를 뽑는다고 하겠습니다. 이 제스타는 아직 후보예요. 왜 어셉할지 리제 갈지 정하지 않았으니까 아시겠죠? 자 그러면은 이 제트 세터를 우리가 뽑는데 뽑고 나서 얘를 우리가 어셉할 확률을 이렇게 정하겠다는 겁니다. 즉 어셉할 확률은 최대 1이겠죠 최대 100%입니다. 근데 이것까지도 된다는 거예요. 왜 미니멈이니까 그렇죠 그럼 얘가 1보다 크면 항상 어셉인 거고 얘가 얘보다 작으면 이거에 비례해서 우리가 어셉을 한다는 겁니다. 이 확률에 비례해서 이렇게 나와 있는 이러한 어셉턴스의 조건이 트랜지션 매트릭스 역할을 하게 되는 겁니다. 왜 현재 제타운인데 여기서 제 스타로 넘어갈지 안 넘어갈지 결국 이 확률에 의해서 정해지게 되는 것이니까요. 아시겠죠? 즉 그다음 시점에서의 샘플은 z 스타가 될 수도 있고 아니면 얘가 리젝이 되면 그때는 제 타운 현재의 상태가 그대로 유지가 될 수 있는 겁니다. 여기에서 한 단계 더 나아가서 메트로폴리스 헤스팅 알고리즘을 보겠습니다. mh 알고리즘으로 많이 부르기도 하고 엠시엠시의 가장 대표적인 알고리즘입니다. 아시겠죠? 자 여기서는 우리가 어떤 형태로 어셉턴스를 결정할 거냐면 이런 형태로 어셉턴스를 결정할 겁니다. 우리가 프로포절을 하나 정합니다. 프로포절을 자 프로포절을 정하는데 그때 프로포절의 그 평균값이 프로포절을 만약에 가오션으로 정한다. 그때 가오샤의 민을 현재의 샘플로 우리가 두는 거예요. 그리고 그다음에 샘플을 우리가 샘플링하겠다는 겁니다. 지금 한번 보시면 우리가 알고 있는 거는 뭐라고 말씀드렸습니까? 우리가 알고 있는 거는 피틸드까지만 우리가 알고 있는 거예요. 그렇죠 피틸드까지만 쓴 거예요 피 안 썼습니다. 피틸다를 쓴 거예요 자 피틀다는 뭡니까? 피틸다는 노멀라이징 콘서트가 생략된 피라고 생각하시면 된다고 했습니다. 그렇죠 자 그때 우리가 이러한 확률에 의해서 어셉을 시키겠다가 메트로폴리스 헤스팅 알고리즘입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 7,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1097,
      "char_count": 2028
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c008_a5b46a",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 그때 가오샤의 민을 현재의 샘플로 우리가 두는 거예요. 그리고 그다음에 샘플을 우리가 샘플링하겠다는 겁니다. 지금 한번 보시면 우리가 알고 있는 거는 뭐라고 말씀드렸습니까? 우리가 알고 있는 거는 피틸드까지만 우리가 알고 있는 거예요. 그렇죠 피틸드까지만 쓴 거예요 피 안 썼습니다. 피틸다를 쓴 거예요 자 피틀다는 뭡니까? 피틸다는 노멀라이징 콘서트가 생략된 피라고 생각하시면 된다고 했습니다. 그렇죠 자 그때 우리가 이러한 확률에 의해서 어셉을 시키겠다가 메트로폴리스 헤스팅 알고리즘입니다. 아시겠죠? 즉 한번 보시면 현재 제타운까지 샘플이 뽑혔어요. 그 상황에서 우리가 제 스타일을 뽑았습니다. 그다음 샘플을 뽑았어요 어디를 통해서 여기를 통해서 제 타워가 밍인 가우시안 분포에서 제 스타를 뽑았습니다. 그때 그 제 스타가 과연 우리가 어셉을 시킬지 리젝을 시킬지 어떻게 정한다 다음과 같은 어셉턴스 확률을 통해서 정한다는 겁니다. 자 저희가 지금 이러한 어스턴스 렛이 이렇게 나온다라는 것을 계속해서 반복해서 저희가 보고 있습니다. 자 메트로폴리스 헤스팅에서 왜 이렇게 좀 복잡하게 생긴 어셉턴스 레잇을 우리가 갖고 있느냐 그거를 저희가 다시 한번 살펴보도록 하겠습니다. 자 결국 그 이유는 이러한 디테일 밸런스 이퀘이션을 우리가 만족시키도록 만들고 싶은 겁니다. 즉 우리가 어셉턴스 레잇을 이렇게 정해야지 그때 디테일드 밸런스 이퀘이션이 만족되게 됩니다. 자 이 어셉턴스 레을 우리가 다시 한번 살펴보면은 제 타우라는 게 현재의 샘플이죠. 타우라는 시점에서의 현재의 샘플 이걸 기반으로 우리가 제 스타라는 걸 만들어 냅니다. 그렇죠 자 그랬을 때 그 z 스타가 어셉 될 확률을 지금 a라고 나타낸 겁니다. 그러면 이거를 저희가 한번 보도록 하겠습니다. 이런 어셉턴스 레잇은 결국 1 이거나 즉 항상 어셉하거나 또는 예입니다. 그렇죠 지금 여기서 제 타워가 현재 시점에서의 샘플 제 스타가 우리가 새로 뽑은 샘플 그래서 얘를 어셉 할지 말지 우리가 정하는 확률 값입니다. 자 그러면 우리가 디테일 리밸런스 유케이션을 우리가 쓰면 이렇게 쓸 수 있습니다. 왜 결국 이러한 트랜지션이라는 거는 우리가 제를 기반으로 제 프라임을 샘플링하고 그다음에 그 제 프라임이 어셉이 될 확률을 우리가 곱하면 되기 때문입니다. 그렇죠 자 그러면 얘를 한번 보면은 자 우리가 피지 그렇죠 현재 우리가 지라는 샘플이 있다고 하겠습니다. 그럼 거기서 우리가 지프라임이라는 샘플을 만들었고 거기서 우리가 어셉할 확률 이거를 곱하면은 위에 있는 좌변을 우리가 모델링 하게 됩니다. 자 그러면은 우변은 결국 뭐냐면 이 억셉된 확률이 1 또는 이거라고 하지 않았습니까 그럼 1일 때는 이렇게 되겠죠 그다음에 얘가 되면 얘랑 피랑 큐케랑 곱하면 얘가 됩니다. 그렇죠 자 이거를 한번 살펴보면은 자 일단 여기 나와 있는 거는 얘가 1이면 당연히 1 곱하기 피지 곱하기 q 지 프라임 바 지 곱하기 1을 곱하니까 이게 오는 거고 뒤에 있는 텀은 결국 이 PC랑 qk가 있는데 AK 자리에 이제 여기 나와 있는 뒤에 있는 텀이 들어가는 거예요. 그렇죠 그런데 여기 z 스타라는 게 뭐예요? 제 프라임이겠죠 여기서 새로 나온 샘플 그렇죠 그다음에 여기 나와 있는 제타워가 현재의 샘플이니까 지입니다. 그렇죠 그러면 여기 얘랑 얘가 같은 거예요. 그러면 약분됩니다. 약분 약분되고 분자에 이제 큐만 남는 거예요. 피지 그다음에 큐케에 지바 지 프라임이 남는 겁니다. 이렇게 자 근데 여기는 왜 피지 프라임이냐 한번 생각해 보시면 자 여기서 우리가 실제로는 틸다를 알고 있는 거죠. 노말라이징 콘서트는 모르고 틸다만 아는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 8,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 996,
      "char_count": 1843
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c009_c49fbc",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 그렇죠 그러면 여기 얘랑 얘가 같은 거예요. 그러면 약분됩니다. 약분 약분되고 분자에 이제 큐만 남는 거예요. 피지 그다음에 큐케에 지바 지 프라임이 남는 겁니다. 이렇게 자 근데 여기는 왜 피지 프라임이냐 한번 생각해 보시면 자 여기서 우리가 실제로는 틸다를 알고 있는 거죠. 노말라이징 콘서트는 모르고 틸다만 아는 겁니다. 그렇죠 근데 뭐예요? 여기서 제피가 나눠져 있고 여기서 제피가 나눠져 있는 걸로 우리가 생각하면 제피가 뭔지 모르지만 우리가 위아래 동시에 제피를 나눠주면 식은 똑같은데 이 피틀다라는 게 결국 피라고 우리가 생각할 수 있는 거예요. 왜 노멀라이징 콘서트가 어차피 위아래 똑같이 우리가 곱해주면 똑같이 있다라고 생각해도 우리가 위아래 1을 곱해준 거니까 그렇죠 똑같다고 생각할 수 있겠죠. 그럼 위아래다 우리가 지피라는 걸 나눠줬다고 치면 틸다가 없어진 걸로 생각할 수 있는 거예요. 그렇죠 자 그 상황에서 아래는 뭐예요? 아래는 피지 위에는 피지 프라임입니다. 그럼 피지랑 피지랑 또 약분돼서 피지 프라임이 나온 거예요. 그래서 여기가 피지 프라임에 큐케의 지바치 프라임입니다. 왜 결국 이 피지는 얘랑 약분돼서 분자만 살아남게 되고 그다음에 얘는 또 얘랑 약분돼서 분자만 살아남게 되니까 큐의 지바시 프라임이 나오게 되는 것입니다. 아시겠죠? 자 그다음에 미니멈이니까 미니멈의 에 콤마 비는 미니멈의 비 콤마 인이랑 같잖아요. 그렇죠 순서를 변경해도 같습니다. 그까닥하게 순서 변경해 주는 것뿐이고요. 그다음에 얘를 우리가 다시 순서만 바꿔서 다시 쓰면 이렇게 써지는 거죠. 왜 지금 이렇게 세 개의 꼴이 이렇게 써진다고 했잖아요. 그렇죠 그러면 지금 얘는 반대로 이제 지 프라임에 대한 걸로 우리가 이렇게 쓸 수가 있습니다. 그렇죠 왜 여기 피지니까 여기 미니멈에 피지가 오고 그렇죠 그렇게 된 건데 그럼 여기서 미니멈에 여기 피지 프라임이 있으니까 여기 피지 프라임이 오게 되고 즉 여기서 이렇게 넘어갔던 과정을 반대로 여기서 이렇게 넘어가면 이 식이 나오게 되는 것입니다. 자 그럼 어떻게 되냐 결국 우리가 어스턴스 레잇을 이렇게 잡게 되면은 디테일드 밸런스 위케이션을 만족하게 된다. 즉 메트로폴리스 헤스팅 알고리즘은 이러한 어셉턴스 내에서 게 잡는데 그 이유는 이렇게 잡으면 디테일드 밸런스 위케이션을 우리가 만족하게 되니까 결국 우리의 스테셔놀리 분포는 우리가 원하고자 하는 이 타겟 분포로 우리는 수렴시킬 수 있다라는 얘기를 하고 있습니다. 그래서 얘가 좀 약간 이런 특이한 형태가 나오게 된 것이고요. 자 그래서 얘를 가지고 우리가 한번 실제로 한 번 데모를 해보겠습니다. 자 그러면은 현재 제21이 있고 그다음에 우리가 제21에서 제2가 나왔다고 가정을 한번 해볼게요. 그러면 여기서 z2는 우리가 어셉이 사실 될 가능성이 높습니다. 왜냐하면은 이거를 한번 우리가 계산을 해보면 자 우리가 현재 z1이었어요. 그리고 얘는 이제 z 2였습니다. 그러면 z1과 z2가 일단 두 개 쯤 이제 p에서 차지하는 비율이 그래도 크게 다르지 않아요 비슷합니다. 그럼 얘가 이제 비슷하고 그렇죠 큐에서도 애초에 제원에서 제2가 나올 확률이 꽤 또 이 확률들이 다 높기 때문에 우리는 얘를 어셉하겠습니다. 근데 여기서 z3가 나왔다고 할게요. z2가 나왔고 z2를 중심으로 해서 z3를 우리가 만든다고 가정하겠습니다. 그러면 이때는 우리가 어떻게 해야 되나요? 이때는 제3가 리젝이 될 가능성이 크다는 겁니다. 왜 그러냐면 자 이 확률을 보게 되면 이 제 스타가 지금 제 3라고 생각하시면 됩니다. 자 그러면은 제 3 이 피 틸다 에서 이 z3가 차지하는 여기 나와 있는 PDF 값은 굉장히 작아요. 얘가 굉장히 작습니다. 반면에 이전 샘플은 어때요? z2는 차지하는 게 좀 비교적 높아요. 그렇죠 이전에는 샘플의 좀 퀄리티가 괜찮았는데 지금은 또 샘플의 퀄리티가 엄청 엉망이 됐다는 겁니다. 그럼 지금 샘플이 좀 이상한 거 아니니라고 얘는 제 3는 못 받아주겠다 이제 가겠다는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 9,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1085,
      "char_count": 2023
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c010_21ae25",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 왜 그러냐면 자 이 확률을 보게 되면 이 제 스타가 지금 제 3라고 생각하시면 됩니다. 자 그러면은 제 3 이 피 틸다 에서 이 z3가 차지하는 여기 나와 있는 PDF 값은 굉장히 작아요. 얘가 굉장히 작습니다. 반면에 이전 샘플은 어때요? z2는 차지하는 게 좀 비교적 높아요. 그렇죠 이전에는 샘플의 좀 퀄리티가 괜찮았는데 지금은 또 샘플의 퀄리티가 엄청 엉망이 됐다는 겁니다. 그럼 지금 샘플이 좀 이상한 거 아니니라고 얘는 제 3는 못 받아주겠다 이제 가겠다는 겁니다. 그러면 제 2에서 우리가 다시 샘플링해요. 그럼 그걸 우리가 이렇게 샘플링이 되면 제4가 나오는 이런 확률로 우리가 어셉바리젝을 하게 되면은 뭐가 된다 결국에는 디테일리 밸런스 이케이션을 만족하게 되고 우리가 원하는 얘가 트랜지션 얘가 결국엔 디테일 밸런스 케이션 만족하니까 얘가 스테이션의 분포가 된다. 우리가 맥코브 체인을 무수히 많이 하게 되면 결국 여기로 수렴하게 된다라는 얘기가 됩니다. 자 하지만 이러한 메트로 폴리스 헤스팅 알고리즘도 사실은 잘 작동하지 않는 경우가 실제로는 많습니다. 자 이러한 메트로폴리스 헤스팅도 결국 프로포셜 큐를 우리가 정해줘야 돼요. 그렇죠 q를 정해줘야 됩니다. 그 말은 우리가 q 만약에 가우시안으로 정했다고 하겠습니다. 보통 가오션으로 많이 정하니까요. 커런트 스테이트를 평균으로 한 자 그때 우리가 베리언스를 어떻게 정하느냐에 따라서 사실은 또 굉장히 수렴 속도 같은 게 많이 달라지게 돼요. 만약에 우리가 베리언스를 만약에 크게 잡았다고 할게요. 베리언스를 크게 잡았다 그러면은 여기서 밸런스를 엄청 크게 잡은 겁니다. 그러면은 지금 이 부근에서 노는 게 아니라 갑자기 막 이상한 게 뽑힐 수도 있는 거예요. 그러면 리젝션이 엄청 리젝션 될 가능성이 커지겠죠. 근데 밸런스가 만약에 작다 여기 제 2 부근에서 베런스를 엄청 작게 잡았어요. 그럼 이 부근에서만 샘플링 되니까 그럴듯한 샘플이 많이 나오게 될 거고 그러면은 리섹션은 레이스는 작습니다. 그러면은 어 그러면은 밸런스를 작게 하면 좋은 거 아닌가요 라고 할 수도 있는데 그럼 배리언스를 작게 하면 이게 언제까지 여기까지 나갈 거예요? 그리고 그게 우리가 고차원으로 가면은 디멘전마다 또 배리언스가 다 제각각인데 그럼 보통 우리는 리액션을 좀 적게 하기 위해서 각 디멘션마다의 그 밸런스를 다 조사해서 밸런스가 비교적 좀 작은 것을 가져와서 우리가 프로포절로 잡습니다. 근데 그러면은 베리언스가 또 우리가 작게 잡으면 좀 탐색을 많이 해야 되는 디벤션에서는 우리가 또 무수히 많은 체인을 돌려야지만 가능한 겁니다. 자 여기서 엠씨엠씨는 앞서 말씀드린 것처럼 하이디맨션에서는 또 굉장히 잘 작동하지 않는 경우가 많기 때문에 우리는 얘를 차원마다 아예 별도로 샘플링하자라는 깁스 샘플링을 이제 권하게 됩니다. 자기 샘플링은 비교적 우리가 하이디맨션에서도 비교적은 MCMC보다는 기본적인 m 기반의 MCMC보다는 잘 작동할 수 있는 MCMC의 변형 방법론입니다. 즉 우리가 제1을 샘플링 하는데 나머지가 다 기분으로 주어진 상황에서 제원을 샘플링해요. 그러면 제1은 체인이 하나 더 연결된 거예요. 그렇죠 그러면은 그거 넣어주고 나머지는 아직 과거의 체인입니다. 그걸 기반으로 제 2를 샘플링하고 또 넣어주고 제3 샘플링하고 또 넣어주고 그렇게 해서 우리가 각각의 이제 코디네 와이즈하게 다 이제 우리가 디멘션별로 샘플링을 한다라는 뜻입니다. 이것도 한번 생각해 보시면 결국 여기서는 우리가 이거를 샘플링 할 때 이거 외적인 나머지 모든 거를 기본으로 해서 사실 우리가 샘플링을 지금 진행을 하고 있는 것으로 해석을 할 수가 있습니다. 그쵸 사실 프로포절이 결국 얘가 된다라고도 생각을 할 수 있겠죠. 여기서 우리가 샘플을 뽑으니까 그렇죠 그 까닭에 얘도 여러분들께서 한번 생각해 보시면 얘는 앞에서 봤던 그 프로포절 얘를 우리가 이러한 형태로 표현을 하게 되면은 얘는 어셉턴스가 항상 1이에요. 무조건 어셉을 하게 되는 형태로 우리가 생각할 수 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 10,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1100,
      "char_count": 2035
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c011_b8e218",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 이것도 한번 생각해 보시면 결국 여기서는 우리가 이거를 샘플링 할 때 이거 외적인 나머지 모든 거를 기본으로 해서 사실 우리가 샘플링을 지금 진행을 하고 있는 것으로 해석을 할 수가 있습니다. 그쵸 사실 프로포절이 결국 얘가 된다라고도 생각을 할 수 있겠죠. 여기서 우리가 샘플을 뽑으니까 그렇죠 그 까닭에 얘도 여러분들께서 한번 생각해 보시면 얘는 앞에서 봤던 그 프로포절 얘를 우리가 이러한 형태로 표현을 하게 되면은 얘는 어셉턴스가 항상 1이에요. 무조건 어셉을 하게 되는 형태로 우리가 생각할 수 있게 됩니다. 자 근데 여기서 또 한 가지 또 의문점이 들 수 있습니다. 아니 그러면 이 변수가 엄청 많아지면 이거에 대한 컨디셔널 분포를 우리가 어떻게 계산하냐라고 생각을 할 수가 있는데 그거는 마크 블랭킷을 활용할 겁니다. 마크모 블랭킷은 얘가 얘를 만약에 다른 것들이랑 완전히 분리시키고 싶다라고 하면은 그때는 어떻게 하면 되냐면 얘의 부모 얘의 자식 얘 자식의 부모까지 기분으로 주어지면은 그것까지 우리가 기본으로 주어지면 그때는 나머지가 다 주어진 거랑 동일한 환경이다 입니다. 지금 무슨 말이냐면 스포 기은 스1 스2 스7 스9 스6면 나머지 스5는 주어지든 안 주어지든 상관없다 그거는 독립이다라는 뜻입니다. 즉 얘만 주어지면 얘는 나머지랑 다 조건부 독립이 되는 거예요. 그렇다면 우리가 이 전체에 대한 거를 구해야 된다고 하더라도 이 전체를 실제로 다 구할 필요 없이 얘에 대한 부모 그다음에 얘의 자식에 대한 부모만 또 우리가 얘 자식과 얘 자식에 대한 부모만 우리가 고려하면 된다라는 얘기를 지금 하고 있는 겁니다. 자 여기까지 해서 우리가 MCMC가 뭔지 살펴봤고 그러한 MCMC의 가장 대표적인 알고리즘인 메트로폴리스 헤스팅 알고리즘에 대한 이론적인 분석까지도 봤습니다. 하지만 이러한 메트로폴리스 헤스팅 알고리즘이 강력하지만 그 차원에서도 잘 작동하지 않을 수가 있기 때문에 왜 애초에 샘플이 어셉 될 확률이 굉장히 낮기 때문에 샘플을 항상 어셉시킬 수 있는 우리 여기 방법 딥 샘플링을 한번 살펴봤습니다. 그렇죠 자 그다음에 갭 샘플링은 실제로 이 모든 거를 다 우리가 컨디션을 줄 필요 없이 마크 오브 블랭킷으로만 컨디션을 주면 된다까지도 우리가 봤고요. 자 여러분 저희가 지금까지 메트로폴리스와 그다음에 깁스 쪽을 봤는데요. 이런 MCMC의 보다 더 심화된 알고리즘인 HMC에 대해서 한번 살펴보겠습니다. 이런 HMC 알고리즘은 굉장히 심화적인 알고리즘이라서 복잡하지만 하지만 또 굉장히 중요하고 많이 쓰이는 파트이다 보니까 저희가 또 이거를 그냥 생략하고 갈 수는 없습니다. 그 까닭에 에이치엠씨가 굉장히 복잡한 알고리즘이지만 그거에 대한 핵심적인 내용들을 저희가 좀 중요하게 빠르게 한번 살펴보고 넘어가는 시간을 가지도록 하겠습니다. 그래서 뒤에서 한번 슬라이드를 한번 보시면 굉장히 자세하게 슬라이드가 구성이 되어 있긴 합니다. 라인 바이 라인으로 여러분들께서 한번 읽어보시면 따라갈 수 있는 수준으로 굉장히 자세하게 적긴 했지만 저희가 또 이거를 그 강의를 진행할 때는 워낙 심화된 내용이다 보니까 핵심을 위주로 저희가 한번 진행을 하도록 하겠습니다. 그래서 강의를 한번 듣고 궁금하신 분들은 이 강의 자료를 한 번 더 살펴보시면 좋을 것 같고 그래도 만약에 잘 이해가 안 간다 그러면 언제든지 저한테 질문을 주시면 좋겠습니다. 자 이러한 치엠시 알고리즘은 앞선 알고리즘과 뭐가 다르냐라고 하면은 결국 운동량이라는 것을 제한합니다. 즉 만약에 우리가 여기서 이 아래쪽으로 가고 싶다고 할게요. 그럼 우리가 만약에 여기 있다 그러면 우리가 빨리 가야겠죠 그렇죠 많이 가야 됩니다. 그리고 우리가 만약에 여기 있다 그러면 여기까지 가기 위해서 이제 천천히 가도 되는 거예요. 그렇죠 자 이렇게 우리가 결국 운동량이라는 걸 도입해서 우리가 프러포즈를 좀 더 잘 잡을 수 있다면 더 빨리 수렴할 수 있지 않을까라는 컨셉입니다. 결국 우리가 그라디언트 정보를 이용해서 좀 더 잘 좋은 프로포즈를 한번 구해보자라는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 11,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1111,
      "char_count": 2044
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c012_608533",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 즉 만약에 우리가 여기서 이 아래쪽으로 가고 싶다고 할게요. 그럼 우리가 만약에 여기 있다 그러면 우리가 빨리 가야겠죠 그렇죠 많이 가야 됩니다. 그리고 우리가 만약에 여기 있다 그러면 여기까지 가기 위해서 이제 천천히 가도 되는 거예요. 그렇죠 자 이렇게 우리가 결국 운동량이라는 걸 도입해서 우리가 프러포즈를 좀 더 잘 잡을 수 있다면 더 빨리 수렴할 수 있지 않을까라는 컨셉입니다. 결국 우리가 그라디언트 정보를 이용해서 좀 더 잘 좋은 프로포즈를 한번 구해보자라는 겁니다. 즉 치엠씨의 큰 컨셉은 뭐냐면 우리가 보다 더 좋은 프로포즈를 잡자 그리고 어셉턴스 레이을 우리가 잘 잡아가지고 항상 어셉이 가능하도록 우리가 만들어주자라는 얘기를 하고 있습니다. 아시겠죠? 즉 메트로폴리스 에스트 알고리즘에서는 큐에서 바로 뽑아요. 그렇죠 그 큐라는 게 뭔가 속도가 달라지거나 그러지 않습니다. 자 근데 에치엠씨에서는 이제 뭐가 있냐면 운동량이라는 게 하나 도입됩니다. 운동량 블라는 뒤에서 이제 우리가 오메가라도 표현할 겁니다. 자 이러한 운동량이라는 걸 우리가 도입을 해서 결국 우리가 어느 정도의 속도로 프로포즈를 옮겨 나가야 되는지 그거를 함께 모델링 할 겁니다. 그리고 그를 위해서 다음과 같은 헤밀토니안 다이나믹스를 우리가 모델링 할 겁니다. 즉 무슨 말이냐면 여기 있는 이러한 헤밀토니안은 우리가 여기 나와 있는 위치 에너지와 운동 에너지의 합으로 우리가 표현할 겁니다. 즉 우리가 에너지 보존의 법칙에 따라서 운동 에너지가 만약에 많아지면 위치 에너지는 작아지고 반대로 위치 에너지가 커지게 되면 운동 에너지는 또 작아지는 것을 우리는 생각할 수가 있습니다. 이러한 헤밀토니안의 관계를 우리가 이용해서 모델링을 진행할 겁니다. 자 그런데 어떻게 진행할 거냐 결국 우리가 여기서 모델링 해야 되는 거는 두 개예요. 즉 오브젝트의 위치와 그다음에 모멘텀 즉 오브젝트의 위치 그다음에 그 오브젝트의 위치를 우리가 또 어느 정도의 속도로 우리가 변경시켜서 다른 곳으로 옮겨 나갈 건지를 우리가 모델링 해야 되는 것입니다. 아시겠죠? 즉 우리가 여기서 샘플을 뽑을 건데 그 샘플의 위치랑 그다음에 또 우리가 그 다음번 샘플은 어디서 뽑을 건지 그러면 우리가 어느 방향으로 어느 속도로 나아가야 되는지를 모델링을 한다는 겁니다. 자 그걸 하기 위해서 다음과 같은 우리가 헤밀토니안 이퀘이션을 모델링 합니다. 즉 변화량 시간에 따른 변화량을 우리가 이렇게 정의하는 거예요. 즉 시간에 따라서 오브젝트도 변하고 모멘텀도 변할 거야. 그러면 시간에 따른 변화량을 우리가 어떻게 정의할 거냐 이렇게 정의할 거랍니다. 왜 이렇게 정의하냐 이렇게 정의하면은 뭐가 되냐면 시간에 따른 이러한 헤밀토니안의 변화량 에너지의 변화량은 0이 됩니다. 즉 시간에 따라서 에너지가 변하지 않는다 즉 에너지가 보존된다라는 얘기를 하는 겁니다. 왜 얘를 이러한 헤밀토니안을 우리가 t로 미분한다고 가정하겠습니다. 그러면 체인 룰에 따라서 우리가 이렇게 오메가 우리가 모멘텀 앞에서 우리가 더블로 부르기도 했던 그 모멘텀을 우리가 이렇게 미분하기도 하고 이렇게 포지션 위치에 대해서 미분할 수도 있습니다. 그렇죠 자 근데 여기 나와 있는 디티분의 디블 자리에 얘를 넣고 디티분의 디스 자리에 얘를 넣으면 그때는 이 왼쪽과 오른쪽이 같아지면서 0이 됩니다. 즉 이렇게 모델링 하면은 우리가 마치 에너지가 보존되는 것과 같은 효과를 낼 수 있다라는 겁니다. 에너지가 보존돼서 뭐 할 건데 뒤에서 우리가 이걸 가지고 어스턴스 렛이 항상 100%가 나오는 모델링을 살펴볼 겁니다. 자 여기까지만 봤을 때는 결국은 뭐냐면 HMC는 이렇게 물체의 위치랑 운동량을 같이 모델링 한다. 그다음에 그러면 이 물체의 위치랑 운동량이 시간에 따라서 변화해야 될 거 아니냐 그래야지 우리가 지금 시점에서는 이 샘플 그 다음 시점에서는 이 샘플 그 다음 시점에서는 또 이 샘플 이렇게 모델링 해 나가야 될 거 아닙니까? 그렇죠 그러면 이런 시간에 따른 변화량을 어떻게 모델링 할 거냐 이렇게 모델링 할 거다라는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 12,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1119,
      "char_count": 2053
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c013_3134af",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 자 여기까지만 봤을 때는 결국은 뭐냐면 HMC는 이렇게 물체의 위치랑 운동량을 같이 모델링 한다. 그다음에 그러면 이 물체의 위치랑 운동량이 시간에 따라서 변화해야 될 거 아니냐 그래야지 우리가 지금 시점에서는 이 샘플 그 다음 시점에서는 이 샘플 그 다음 시점에서는 또 이 샘플 이렇게 모델링 해 나가야 될 거 아닙니까? 그렇죠 그러면 이런 시간에 따른 변화량을 어떻게 모델링 할 거냐 이렇게 모델링 할 거다라는 겁니다. 자 그러고 나서 이런 과정들이 이 자세한 과정은 저희가 약간은 좀 생략을 하고 넘어가겠습니다. 자 여기서는 어떤 얘기를 하고 있냐면 결국에 운동 에너지라는 걸 어떻게 정의할 거냐 그 얘기를 하고 있어요. 운동 에너지라는 것은 우리가 앞에서 이런 이제 k라고 적기도 했고 여기서 우리가 v라고도 적을 건데 이러한 운동 에너지는 결국 우리가 어떻게 정리할 거냐면 이렇게 즉 모멘텀은 우리가 이러한 곳에서 샘플링이 된다라고 가정할 겁니다. 그다음에 여기 나와 있는 위치 에너지는 결국 우리가 구하고 싶은 타겟 분포로 우리가 모델링을 할 겁니다. 거기다가 마이너스 로그 붙이는 걸로 실제로는 우리가 파이라는 걸 모르고 파이에 이제 비례하는 r 왜 노말라이징 콘서트를 우리가 모르니까 계속 우리가 베리션 인퍼런스도 하고 엠시엠스도 한 거지 않습니까? 즉 우리가 실제로도 파이는 모르고 파이에 비례하는 알이라는 텀은 알겠죠 노멀라이징 콘서트는 제외하고 그렇죠 우리가 모르니까 자 그러고 나서 우리가 파일도 정의하고 그다음 v도 정의하면은 우리가 h라는 걸 정의하게 됐고 h가 정의되면 뭡니까? h가 정의되고 나면 이제 시간에 따른 위치의 변화량 시간에 따른 모멘텀의 변화량이 정의가 되니까 결국 치엠씨라는 것은 이렇게 우리가 모멘텀이라는 것을 도입을 하게 됩니다. 즉 앞에서 우리가 에이치엠씨를 하기 위해서 이러한 위치 에너지와 운동 에너지라는 걸 우리가 정의를 해야 했습니다. 그러면 그때 위치 에너지는 우리가 결국 구하고 싶은 파이가 우리의 타겟입니다. 실제로 우리는 파이는 모르고 파이에 비례하는 알만 알겠죠 노멀레진 콘서트를 제외한 그렇죠 노멀레진 콘서트를 우리가 모르니까 엠씨엠씨든 브아든 계속 하고 있는 거니까 실제로 우리는 파이를 알고 싶지만 우리가 알고 있는 건 알이에요. 파이에 비는 그러면은 우리가 위치 에너지라는 것은 이렇게 우리가 모델링하고 실제로는 뒤에서 이제 알로 우리가 쓸 겁니다. 위치 에너지 이렇게 운동 에너지는 이렇게 우리가 정의할 겁니다. 자 그러면 그랬을 때 우리가 모멘텀이라는 거는 이런 데서 우리가 샘플링을 진행을 할 겁니다. 즉 모멘텀이라는 건 가우시안 멀티베리의 가우시안 분포에서 우리가 뽑는 걸로 생각해 주시면 됩니다. 그러면 여기 나와 있는 v라는 거는 여러분 그 가우시안 분포의 PDF 보면 익스포넨셜의 마이너스 2 시그마 제곱 분의 x 마이너스 m의 제곱 그런 거 있죠 그게 여기 나와 있는 v가 이제 되게 되는 것이겠죠. 익스포넨셜 안에 있는 거 그렇죠 즉 이렇게 부위를 정하고 그다음에 우리가 이렇게 u 결국 위치 에너지와 운동 에너지를 우리가 다 정하고 나면은 헤밀톤이라는 게 정리됐습니다. 그럼 헤밀톤이 정리되면 뭐예요? 그럼 우리가 변화량을 정할 수 있어요. 즉 우리는 지금 할 게 뭐냐면 오브젝트랑 모멘텀이 이제 두 개가 있는 겁니다. 이 두 개가 시간에 따라서 어떻게 변화하는지 모델링 한 거예요. 그렇죠 그럼 시간에 따라 막 변화합니다. 시간에 따라서 막 변화해서 스티 그다음에 오메가 티가 나왔어요. 티라는 시점이 지나서 그러면 그때 우리가 얘를 어셉을 할 거냐 말 거냐를 이러한 확률로 우리가 정하겠다라는 뜻입니다. 근데 이 확률은 항상 1위에요. 왜 우리가 에너지 보존의 법칙을 활용하기로 했잖아요. 그러면 분자랑 분모가 이 에너지라는 게 시간에 따라서 안 변하니까 분자랑 분모가 같은 겁니다. 에너지가 그러면 약분 되면 1이잖아요 그러면 항상 어습시킨다 라는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 13,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1077,
      "char_count": 1987
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c014_bdf33f",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 시간에 따라서 막 변화해서 스티 그다음에 오메가 티가 나왔어요. 티라는 시점이 지나서 그러면 그때 우리가 얘를 어셉을 할 거냐 말 거냐를 이러한 확률로 우리가 정하겠다라는 뜻입니다. 근데 이 확률은 항상 1위에요. 왜 우리가 에너지 보존의 법칙을 활용하기로 했잖아요. 그러면 분자랑 분모가 이 에너지라는 게 시간에 따라서 안 변하니까 분자랑 분모가 같은 겁니다. 에너지가 그러면 약분 되면 1이잖아요 그러면 항상 어습시킨다 라는 겁니다. 근데 왜 굳이 이런 확률을 적어놨냐 항상 어셉인데 우리가 실제로 여러분 파이썬으로 구현하면 실제로는 우리가 이러한 뉴메리컬한 오차가 발생하면서 실제로는 1이 아니라 0.99 이렇게 나올 수 있는 거예요. 그렇죠 그 까닭에 우리가 이러한 텀을 작성하게 된 것입니다. 즉 치엠시라는 건 다시 말씀드리지만 일단 심화 과정이기 때문에 여러분들이 이해를 못하셔도 괜찮습니다. 핵심은 우리가 알고 가자는 거예요. 자 핵심은 뭐냐면 결국 치엠씨라는 건 우리가 위치 에너지랑 운동 에너지를 이용해서 프로포절을 우리가 계속해서 변화해 나갈 겁니다. 근데 그 프로포션을 우리가 어떻게 변화시켜 나갈 건지 즉 우리가 샘플을 결국 우리의 타겟 분포로 수렴시키기 위해서 우리가 얼마나 빠른 속도로 이동해 나가면서 샘플을 뽑아야 되는지를 지금 모델링하고 있는 게 메트로폴리 시스템과 에이치엠씨의 차이라고 우리가 볼 수가 있습니다. 그렇죠 자 결국 에치엠씨라는 거는 우리가 모멘텀이라는 게 결국 존재하기 때문에 설사 우리가 어스텝이 여기서 안 된다고 하더라도 모멘텀은 계속 바뀌는 거예요. 그러면 어떻게 돼요? 그러면 설사 어스텝이 안 되더라도 모멘텀은 계속 바뀌니까 속도는 우리가 계속해서 바뀌게 되는 겁니다. 즉 상황에 적응하면서 여기 있는 뒤에 있는 과정들은 여러분들께서 한번 라인 바이 라인으로 한번 살펴보세요. 근데 뒤에 있는 과정은 결국 뭘 모델링 하고 싶었던 거냐면요. 우리가 뭐 유라는 거를 실제로 이렇게 이제는 파일을 모르고 알을 아니까 이렇게 우리가 모델링 할 거고 그다음에 운동 에너지는 우리가 앞에서 그 가우시안 분포 이용하기로 했으니까요. 그래서 뭔가 가우지 분포의 그 익스포넨셜 위에 있는 부분 안에 있는 부분 그렇죠 그 부분으로 우리가 이렇게 모델링 하자라고 치자는 겁니다. 자 그러면 u가 나왔고 그다음에 v가 나왔으니까 헤밀톤이라는 걸 우리가 정의할 수 있게 돼요. 그러면 치라는 걸 우리가 정의했으니까 그에 대해서 우리가 미분만 이렇게 시켜주면 즉 시간에 따른 샘플의 변화량 그다음에 시간에 따른 모멘텀의 변화량을 우리가 모델링 할 수 있게 되는 겁니다. 그렇죠 이렇게 이렇게 모델링 하게 돼요. 자 그러면은 근데 실제로 지금 보면은 시간에 따른 샘플의 변화량을 보니까 모멘텀이 걸려 있고 시간에 따른 모멘텀의 변화량을 보니까 또 여기에 결국 뭐가 걸려 있게 되냐면 이런 현재 샘플의 위치가 또 걸려 있게 됩니다. 서로 얽히고 얽히는 거예요. 즉 이러한 미분 방정식을 우리가 풀어야 되는데 미분 방정식을 어떻게 풀 거냐 미분 방정식이라는 게 뭐냐 애초에 그럼 잠깐 말씀드리면 결국 미분이 들어가 있는 식을 우리가 미분 방정식이라고 합니다. 즉 y 한 번 미분했을 때 y랑 같고 y 제로는 1입니다. 와는 뭔가요? 이런 거 푸는 게 미분 방정식이에요. y는 익스포넨셜의 x면 인식 만족하잖아요. 그렇죠 이런 거 y 두 번 미분하고 y 더하면 0입니다. 그때 y 뭔가요? 이거면 되잖아요. 이런 거 푸는 겁니다. 자 이런 걸 푸는 방법은 뭐 굉장히 많습니다. 여기서 우리가 일단 가장 대표적인 오일러 메소드만 볼게요. 오일러 메소드는 결국 미분의 정의를 이용하는 거예요. 자 여기 블 또는 오메가 티 플러스 엡실론 빼기 얘를 왼쪽으로 넘기면 오메가 티가 되잖아요. 그다음에 엡실론으로 나눠 주겠습니다. 그러면 뭐가 돼요? 엡실론을 우리가 0으로 보내면 그게 바로 엡실론 분의 오메가 티 플러스 엡실론 마이너스 5메가니까 미분의 정의입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 14,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 1083,
      "char_count": 2002
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_6강_마르코프_연쇄_몬테카를로_방_c015_9cf624",
      "content": "[MLforRecSys] [MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II\n\n다. 그때 y 뭔가요? 이거면 되잖아요. 이런 거 푸는 겁니다. 자 이런 걸 푸는 방법은 뭐 굉장히 많습니다. 여기서 우리가 일단 가장 대표적인 오일러 메소드만 볼게요. 오일러 메소드는 결국 미분의 정의를 이용하는 거예요. 자 여기 블 또는 오메가 티 플러스 엡실론 빼기 얘를 왼쪽으로 넘기면 오메가 티가 되잖아요. 그다음에 엡실론으로 나눠 주겠습니다. 그러면 뭐가 돼요? 엡실론을 우리가 0으로 보내면 그게 바로 엡실론 분의 오메가 티 플러스 엡실론 마이너스 5메가니까 미분의 정의입니다. 엡실론 0으로 보내면 그렇죠. 그 미분의 정의에 의해서 나온 식이에요. 즉 오메가라는 거를 우리가 정확하게 딱 한 번에 못 구하니까 일터러티브하게 계속 우리가 이 스텝을 밟아 나가면서 우리가 문제를 풀겠다라는 겁니다. x도 마찬가지. 그리고 그거를 이제 2분의 시노스텝만큼 가는 또 다른 방식이 리플로그 메소드라는 게 또 다른 방식으로 있는 것뿐입니다. 이거는 저희가 생략할게요. 즉 여러분 한번 보시면 오늘날 메소드를 보면 오메가는 우리가 이렇게 업데이트하면 되고 x라는 건 이렇게 업데이트해 나가면 된다는 겁니다. 그리고 엡실론에 1 넣으면 우리는 뭐가 된다? 엡실론에 1 넣으면 우리는 이제 앞에서 봤던 것처럼 x 제로랑 오메가 제로를 우리가 랜덤하게 샘플링하고 그러면은 이거에 따라서 엡실론 1 넣으면 오메가 1이 나오고 엡실로 1이 나오고 오메가 1이 나오고 엑스원이 나오고 그렇죠. 또 오메가 2가 나오고 스투가 나오고 쭉쭉쭉 우리가 나오게 되는 것을 볼 수 있게 됩니다. 그러면 우리가 결국 뭐가 된다 이런 xt 오메가t를 우리가 구하게 되고 그거를 샘플링 할지 말지 어셈 할지 말지는 이 어셈 확률로 정하는데 이 어셈 확률이라는 것은 애초에 1이다. 왜 우리가 시간에 따른 변화량이 0이라고 앞에서 봤으니까 사실은 0이 되도록 얘를 정리했으니까. 즉 HMC라는 건 요약하면 모멘텀이라는 게 도입된 엠시엠씨다. 그쵸 그리고 그러면 우리가 이러한 모멘텀에 의거해서 뒤에서 한번 보시면 샘플의 위치가 결국 달라지는데 샘플의 위치가 이 모멘텀의 의해서 샘플의 위치가 달라지는 것을 볼 수가 있습니다. 그렇죠 그다음 시점에서의 샘플은 현재 시점에서의 샘플 그리고 모멘텀에 뭔가 우리가 의존해서 모멘텀에 디펜던트 한 이 텀을 기반으로 바뀌게 되는 겁니다. 그렇죠 그래서 우리가 속도에 따라서 우리가 더 빠르게 좋은 샘플을 뽑을 수가 있게 되는 겁니다. 이게 HMC 알고리즘이고요. 그리고 이런 HMC 알고리즘은 수렴한다라고 알려져 있습니다. 왜 디테일드 밸런스 이퀘이션은 만족하거든요. 그래서 디테일 밸런스 유케이션이 만족한다라는 거는 이제 여기서 또 저희가 증명을 해 놨고요. 라인 바이 라인으로 다 증명이 되어 있습니다. 하지만 이 부분은 또 저희가 심화 과정이라서 이 부분은 저희가 생략을 하고 넘어가겠습니다. 하지만 호기심이 굉장히 많은 친구분들이 계실 수가 있기 때문에 저희가 이제 슬라이드로 라인 바이 라인으로 적어놨던 거고요. 이거는 필요하신 분들 한번 보시고 궁금하시면 질문 주시면 되겠습니다. 네 여러분 고생 많으셨습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II.json",
        "lecture_name": "[MLforRecSys] (6강) 마르코프 연쇄 몬테카를로 방법 II",
        "course": "MLforRecSys",
        "lecture_num": "6강",
        "lecture_title": "마르코프 연쇄 몬테카를로 방법 II",
        "chunk_idx": 15,
        "total_chunks": 16,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:830148043d1ece811bd8a37198a62f84a9cde4df945871cf7ca9fd117ba5542b"
      },
      "token_estimate": 851,
      "char_count": 1584
    }
  ]
}