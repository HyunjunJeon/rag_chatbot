{
  "source_file": "(8-2강) Generative Models.json",
  "lecture_name": "(8-2강) Generative Models",
  "course": "8-2강",
  "total_chunks": 6,
  "chunks": [
    {
      "id": "transcript_8_2강_8_2강_generative_models_c000_02a6c4",
      "content": "[강의 녹취록] 과목: 8-2강 | 강의: 8-2강 | 제목: Generative Models\n\n네 지금까지는 이제 제니티오 모델들에 대한 개발에 대해서 살펴봤었는데요. 최근에 많은 성공 사례들을 보여준 디퓨전 모델을 중심으로 해서 제네이티브 모델의 활용 사례들에 대해서 살펴보도록 하겠습니다. 첫 번째로 소개할 연구는 프롬프트 프롬프트 이미지 에디팅입니다. 어 이 프롬프트 프롬프트라는 기술에서는 텍스트를 기반으로 한 이미지 에디팅 기술을 제안을 하게 되는데요. 여기 보면은 이렇게 이런 이미지를 우리가 생성할 때 이제 포터오버 캣 라이딩 어 바이스쿨이라는 그런 텍스트가 주어진 상태로 이런 이미지를 생성할 수 있을 텐데요. 그때 이 바이스큐를 카로 바꿈으로써 이제 원래 이미지를 최대한 보존하면서 여기에 타고 있는 것만 자동차 형태로 바꿔주는 이런 형태의 기술입니다. 그래서 이제 프롬프트 프롬프트는 보통 오리지널 한 그런 이미지의 컴포지션과 스트럭처는 최대한 유지하면서 이제 텍스트를 기반으로 이제 이미지를 에디팅하는 그런 유용한 테크닉이라고 볼 수가 있고요. 그다음에 여기서 이렇게 워드를 리플레이스 하는 것뿐만 아니라 이렇게 어떤 단어를 빼거나 또는 추가적인 어티뷰트를 추가를 해서 이미지에 스타일링을 하는 등의 사용을 할 수가 있습니다. 이 프롬프트 프롬프트를 소개하는 이유는 어 디퓨전 모델의 어떤 큰 성공 사례들과 더불어서 이제 디퓨전 모델을 보통 구성을 할 때 사용하는 어 컨디셔널 유닛 구조에서 크로스 어텐션을 굉장히 많이 사용하는데 그 크로스 어텐션을 우리가 어떻게 인터프리테이션 하고 어떻게 활용할 수 있는지를 좀 보여드리려고 이 연구를 첫 번째로 소개를 하게 되었습니다. 그래서 여기서의 키 옵토베이션은 어 중간에 디퓨전 모델에서 디노이징을 하면서 발생하고 있는 그런 크로스 언텐션 맵을 보면은 거기서 스페셜 레이아웃이라든지 어떤 지오메트리가 전부 다 다 묻어 있는 것을 좀 확인할 수가 있습니다. 예를 들어서 현재 생성하고 있는 이미지가 이런 거라고 하면은 굉장히 초반 레이어에서도 이런 에버리지 어텐션 맵을 한번 구성을 해봤을 때 그때의 이제 곰에 대한 어떤 단어가 들어왔을 때 그것과의 크로스텐션 맵이 굉장히 클리어하게 어 여기 영역은 곰하고 연관이 있어라는 거를 가르치고 있고요. 여기 보면 벌드가 있는데 이 벌드의 위치도 여기에 어텐션이 일어나는 것을 볼 수가 있습니다. 그래서 이와 같이 각각 단어와 그다음에 각 스페셜 레이아웃과의 연관 관계가 크로스 어텐션에서 고스란히 담아나는 것을 볼 수가 있고요. 그다음에 추가적으로 이 이거는 이제 전체적인 디노이징 프로세스의 평균을 낸 그런 어텐션 맵인데요. 평균이 아니라 어 샘플링의 시작에서부터 최종적으로 위쪽에 타임 스텝까지 가서 이미지가 완전히 디코딩 됐을 때를 쫙 비교를 해 봤을 때 굉장히 초반 얼리터레이션에서도 얼리 타임 스텝에서도 어 이런 크로스 어텐션이 굉장히 분명하게 드러나는 것을 볼 수가 있습니다. 이 얘기는 굉장히 극 초반에도 이게 굉장히 노이즈하게 보일지라도 실제로 스페이셜 레이아웃은 훨씬 더 그 사전에 잡힌다라는 얘기예요. 그래서 시멘틱한 그런 의미가 이렇게 공간상에서 드러나기 시작하는 것은 우리가 눈으로 이런 어떤 객체를 클리어하게 퍼셉션을 하지 못하는 상황에서도 이렇게 잘 나오고 있다라는 것을 우리가 알 수가 있습니다. 네 크로스 어텐션에 대해서 조금 더 이해하기 위해서 이제 크로스 어텐션이 어떤 식으로 유도되는지 한번 살펴보도록 하겠습니다. 여기서 먼저 중간 레이어에서 이제 픽셀 피처가 이렇게 주어지면은 이 각각의 픽셀에 대해서 피처들이 이렇게 존재를 하는 상태인 거죠. 이거를 먼저 픽셀 쿼리로 트랜스포메이션을 합니다. 그런 다음에 이제 픽셀 쿼리가 트랜스포메이션이 되면은 여기에 이제 토큰 키하고 토큰 밸류를 미리 계산을 해 놓습니다. 여기서는 이제 각각의 토큰마다 이제 키로 매핑을 해서 프로젝션을 한 그런 텍스트 인베이딩 벡터라고 생각을 하면 될 것 같고요. 이 각각의 키마다 요 픽셀 쿼리하고 이너 프로덕트를 해서 이 어텐션 맵 하나를 이렇게 만들게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(8-2강) Generative Models.json",
        "lecture_name": "(8-2강) Generative Models",
        "course": "8-2강",
        "lecture_num": "8-2강",
        "lecture_title": "Generative Models",
        "chunk_idx": 0,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8fe6b9199ffefdc6a8dd0da004fb557cb3c9bcd0d5581e428a095fe3c3d1cbb1"
      },
      "token_estimate": 1107,
      "char_count": 2005
    },
    {
      "id": "transcript_8_2강_8_2강_generative_models_c001_ac258d",
      "content": "[8-2강] (8-2강) Generative Models\n\n다. 여기서 먼저 중간 레이어에서 이제 픽셀 피처가 이렇게 주어지면은 이 각각의 픽셀에 대해서 피처들이 이렇게 존재를 하는 상태인 거죠. 이거를 먼저 픽셀 쿼리로 트랜스포메이션을 합니다. 그런 다음에 이제 픽셀 쿼리가 트랜스포메이션이 되면은 여기에 이제 토큰 키하고 토큰 밸류를 미리 계산을 해 놓습니다. 여기서는 이제 각각의 토큰마다 이제 키로 매핑을 해서 프로젝션을 한 그런 텍스트 인베이딩 벡터라고 생각을 하면 될 것 같고요. 이 각각의 키마다 요 픽셀 쿼리하고 이너 프로덕트를 해서 이 어텐션 맵 하나를 이렇게 만들게 됩니다. 그다음에 또 다른 토큰에 대해서 이 픽셀 쿼리를 어텐션을 해서 이렇게 하나씩 만들어서 어 토큰 개수에 맞게끔 요 어텐션 맵들이 이렇게 나온다라고 생각을 할 수가 있을 것 같고요. 그다음에 여기다가 이제 어 토큰의 키 형태가 아닌 밸류 형태로 트랜스포메이션 된 것과 어텐션 맵을 매트릭스 프로덕트를 통해서 아웃풋 피처를 다시 리컨스럭션 하게 됩니다. 이 아웃풋 피처가 리컨석션 된 거를 조금 더 자세하게 보면은 이 각각의 어텐션 맵에 의해서 이제 토큰 밸류가 어디에 배치가 돼야 되는지를 결정한다라고 이해할 수 있을 것 같은데요. 예를 들어서 어텐션 맵의 웨이트가 큰 부분에 이 토큰 밸류가 같이 곱해지면서 그 위치의 이 토큰 밸류가 스캔될 수 있도록 유도가 되는 거라고 이해할 수가 있습니다. 즉 다시 말하면은 이 어텐션 맵에 따라서 이 토큰 밸류가 어디에 배치돼야 되는지 조정을 하고 있고 이 어텐션 맵에서 레이아웃을 다 결정을 해서 이 아웃풋에다가 어떤 피처가 어디에 배치돼야 되는지를 이렇게 배치하는 그런 메커니즘이다라고 이해할 수가 있을 것 같습니다. 그래서 이런 특징을 가지고 나서 우리가 이 스페셜 레이아웃을 컨트롤하고 우리가 매니플레이션을 하기 위해서 이 어텐션 맵을 우리가 조작을 하면 되는데요. 네 그러기 위해서 이 프롬프트 프롬프트 알고리즘은 이런 식으로 구현이 되어 있습니다. 먼저 이제 소스 프롬프트하고 우리가 변경할 타겟 프롬프트 피스타를 이렇게 인풋으로 주어지게 되면은 그다음에 우리가 랜덤 시드에서부터 이렇게 스에서부터 시작을 하게 됩니다. 그럴 때에 이제 처음에 가우시안에서부터 레이턴트를 샘플링을 하고요. 그다음에 여기에서 이제 가우시안에서 샘플링 된 이 랜덤 노이즈를 똑같이 복사를 해서 이제 d스타 t라는 형태로도 만들어 놓습니다. 그래서 똑같은 시작점에서 시작을 하는 거죠. 그다음에 이제 디퓨전 모델을 두 개를 동시에 패러럴하게 돌립니다. 어떻게 돌리냐면 처음에 라지 티 라지 캐피탈 티에서부터 시작을 해서 그 타임 스탬프에서부터 먼저 이제 디노이징을 시작을 하게 되는데요. 그때 요 어텐션 맵을 같이 출력을 하게 만듭니다. 근데 여기서 이렇게 패럴하게 돌아가고 있는 이 디퓨전의 인풋을 보면은 각각의 레이턴트 코드가 입력으로 들어가고 그다음에 여기에 프롬프트가 소스 프롬프트 타겟 프롬프트에 대해서 이렇게 병렬적으로 어 어텐션 맵을 구하게 돼요. 그랬을 때 이 어텐션 맵이 2개가 나오면 그 두 개의 차를 비교 분석을 할 수가 있겠죠. 이거를 이제 좀 더 제너럴한 에디팅 메커니즘까지 포괄하기 위해서 여기 에디팅 펑션이라는 형태로 이 두 개의 입력을 넣습니다. 여기서 이제 사람이 원하는 대로 메뉴플레이션을 해서 에디팅을 하는 그런 새로운 크로스 어텐션 맵을 출력을 하게끔 만듭니다. 그다음에 이 변형된 크로스 어텐션 맵으로 이 타겟 프롬프트가 주어진 그 디노이징 디퓨전 프로세스에다가 이거를 대체를 해서 껴 넣습니다. 그래서 어텐션 맵을 그냥 갈아 끼는 거예요. 그렇게 함으로써 디노이징을 해서 이렇게 한 스텝 디노이징을 하게 됩니다. 그런 다음에 이 디노이징 된 요 중간에 레이턴트와 그다음에 이 레이턴트 스타 이 두 개를 활용해서 다음 스텝을 또 진행을 패럴럴하게 진행을 하고 에디팅을 하고 이거를 반복하면서 어 디노이징을 하게 됩니다. 그렇게 해서 최종적으로 수렴한 어 요 이미지들에 대해서 최종적으로 아웃풋 이미지를 여기에서부터 디코딩을 해내게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(8-2강) Generative Models.json",
        "lecture_name": "(8-2강) Generative Models",
        "course": "8-2강",
        "lecture_num": "8-2강",
        "lecture_title": "Generative Models",
        "chunk_idx": 1,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8fe6b9199ffefdc6a8dd0da004fb557cb3c9bcd0d5581e428a095fe3c3d1cbb1"
      },
      "token_estimate": 1102,
      "char_count": 2010
    },
    {
      "id": "transcript_8_2강_8_2강_generative_models_c002_570080",
      "content": "[8-2강] (8-2강) Generative Models\n\n다. 그래서 어텐션 맵을 그냥 갈아 끼는 거예요. 그렇게 함으로써 디노이징을 해서 이렇게 한 스텝 디노이징을 하게 됩니다. 그런 다음에 이 디노이징 된 요 중간에 레이턴트와 그다음에 이 레이턴트 스타 이 두 개를 활용해서 다음 스텝을 또 진행을 패럴럴하게 진행을 하고 에디팅을 하고 이거를 반복하면서 어 디노이징을 하게 됩니다. 그렇게 해서 최종적으로 수렴한 어 요 이미지들에 대해서 최종적으로 아웃풋 이미지를 여기에서부터 디코딩을 해내게 됩니다. 네 방금 전에 설명한 오퍼레이션들 중에서 에디팅이라는 굉장히 제너럴한 텀이 있었는데 이 연구에서는 이 에디팅을 좀 몇 가지 사례에 대해서 제안을 했었습니다. 먼저 이제 워드를 변경하는 경우에는 이런 어텐션 자체의 맵을 이제 통째로 이렇게 변경을 해서 리플레이스를 하는 식으로 오버라이딩 하는 식으로 우리가 에디팅을 구현을 할 수도 있고요. 그다음에 또 다른 방식으로는 새로운 프레이즈를 우리가 추가를 해서 어트리뷰트랑 형용사 같은 걸 추가를 하는 경우에는 이렇게 중간중간에 삽입을 하는 식으로 인젝션을 하는 식으로 어텐션을 매니플레이션 하기도 합니다. 또 가끔씩은 우리가 어떤 콘셉트에 대해서 뭐 앰플리피케이션을 하거나 또는 어테뉴에이션을 해서 굉장히 줄인다든지 톤다운 한다든지 이런 경우도 있을 수 있는데요. 그 정도에 맞춰서 우리가 새로운 웨이팅으로 리웨이팅을 해서 어텐션을 조작하는 그런 방법도 제안을 했습니다. 그래서 이러한 방식으로 어텐션을 조작을 해서 이렇게 이미지를 에디팅을 하는 그런 접근을 어 했고요. 결과를 보면 이 인풋에 대해서 이 결과들이 각각 이제 모터사이클이나 자동차로 바꾸거나 이제 에어플레인으로 바꾸거나 이런 형태가 되게 되는데 여기서 이제 어텐션을 풀 인잭션을 해서 어텐션을 최대한 유지할수록 스페이셜 레이아웃이 잘 유지되는 것을 확인할 수가 있고 그다음에 어텐션 인젝션을 거의 하지 않으면 완전 다 바뀌는 것을 확인할 수가 있습니다. 스페셜 레이아웃이 유지가 되지 않고 그다음에 그 중간 단계들도 이렇게 확인할 수가 있죠. 그래서 이 크로스 텐션에 이제 스페셜 레이아웃 정보가 꽤 많이 묻어 있고 이 크로스 텐션을 우리가 좀 해석을 해서 인터프리터블하게 에디팅하는 데 활용을 할 수가 있다라는 사실을 알게 되었습니다. 근데 이 프롬프트 프롬프트 테크닉이 좀 유용해 보이면서도 막상 이거를 실제 실용적으로 적용을 하려고 하면 좀 한계점이 많습니다. 먼저 사실 제가 언급하지 않았지만 프롬프트 프롬프트에 적용을 하려고 하면은 이미지에 대한 풀 디스크립션이 먼저 주어져야 됩니다. 그다음에 그 풀 디스크립션에서 특정 단어를 우리가 교체를 해야 되는 거죠. 그래서 이 풀 디스크립션을 잘 찾아야 되는 그런 이슈가 한 가지 있습니다. 그리고 또 이제 샘플링부터 시작을 하면서 이미지를 에디팅 해야 되기 때문에 우리가 직접 찍은 실제 이미지에서부터 적용을 하려고 하면은 어 이 레이턴트 코드로 매핑하는 정확한 인버전부터 필요합니다. 그래서 이 레이턴 이미지에서부터 레이턴트로 변경하는 인버전 프로세스가 어큐레이트 하지 않은 경우에는 우리가 에디팅의 성공률이 굉장히 떨어진다라고도 할 수가 있겠죠. 그래서 실제로 이 프롬프트 프롬프트의 인풋 상황에 잘 맞는 형태의 어떤 실용적인 시나리오를 찾기가 좀 어렵습니다. 그래서 지금 소개해 드릴 인스터트 픽스트 픽스 같은 경우는 어 이런 프롬프트 프론트의 어떤 한계를 좀 더 개선한 그런 연구라고 볼 수가 있겠는데요. 여기서 먼저 인풋은 뭐냐 하면은 하나의 이미지가 그냥 주어져 있고요. 그다음에 그걸 어떻게 변경할지 그 텍스트 인스트럭션이 이렇게 인풋으로 주어집니다. 그래서 이 텍스트 인스트럭션을 컨디션으로 해서 이 이미지를 바로 변경을 할 수가 있는 거죠. 이건 뭐 인벌전도 필요하지 않고 그냥 인스트럭션만 하면 되고 또 이제 풀 디스크립션도 필요하지 않습니다. 우리가 인풋 이미지에 대해서 이런 풀 디스크립션을 잘 찾아서 표현을 하는 것도 굉장히 어려운 문제 중의 하나인데요. 이런 것도 필요하지 않습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(8-2강) Generative Models.json",
        "lecture_name": "(8-2강) Generative Models",
        "course": "8-2강",
        "lecture_num": "8-2강",
        "lecture_title": "Generative Models",
        "chunk_idx": 2,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8fe6b9199ffefdc6a8dd0da004fb557cb3c9bcd0d5581e428a095fe3c3d1cbb1"
      },
      "token_estimate": 1106,
      "char_count": 2004
    },
    {
      "id": "transcript_8_2강_8_2강_generative_models_c003_c68326",
      "content": "[8-2강] (8-2강) Generative Models\n\n다. 그래서 이 텍스트 인스트럭션을 컨디션으로 해서 이 이미지를 바로 변경을 할 수가 있는 거죠. 이건 뭐 인벌전도 필요하지 않고 그냥 인스트럭션만 하면 되고 또 이제 풀 디스크립션도 필요하지 않습니다. 우리가 인풋 이미지에 대해서 이런 풀 디스크립션을 잘 찾아서 표현을 하는 것도 굉장히 어려운 문제 중의 하나인데요. 이런 것도 필요하지 않습니다. 그래서 지금 보면은 이런 에펠 타워가 있을 때 거기다가 이제 파이어를 추가해 라는 식으로 명령을 하니까 여기에 이렇게 불꽃놀이가 추가가 되고 그다음에 다른 케이스 같은 경우도 여기서 해바라기를 이제 장미로 바꿔줘라고 하니까 해바라기를 이제 장미로 바꿔주는 형태로 이 명령을 잘 따르는 그런 에디팅이 되는 것을 확인할 수가 있습니다. 네 인스터트 픽스터 픽스는 어 이 전체 인스트럭션 베이스 이미지 에디팅이라는 문제를 슈퍼바이즈 러닝 프로그램으로 접근을 했습니다. 즉 인풋과 어떤 인풋 조건과 그 다음에 아웃풋 페어 데이터셋을 잘 만들어 놓고 거기다가 트레이닝을 시킨 거죠. 그러기 위해서는 굉장히 많은 데이터가 필요한데요. 그 데이터 컨스트럭션을 어떻게 했는지 한번 보도록 하겠습니다. 먼저 사람한테 이렇게 라이언 캡션 데이터를 줍니다. 그래서 이미지가 있을 때 그 이미지에 대응하는 캡셔닝 데이터셋을 이용을 해서 이렇게 캡션이 주어져 있을 때 우리가 어떤 부분을 변경하고 싶은지 인스텍션을 사람한테 만들도록 하고요. 그다음에 이 인스석션에 따라서 이 캡션을 에디팅을 하도록 이렇게 텍스트 에디팅을 먼저 수행을 합니다. 이걸 가지고 한 700개 정도의 굉장히 적은 데이터를 만드는 게 이제 시작점이고요. 그래서 이제 텍스트 레벨에서만 어떻게 보면 사람한테 어노테이션을 받았다라고 이해를 할 수가 있을 것 같습니다. 이 700개의 이 잼플을 가지고 그다음에 뭘 하냐면은 gpt3를 파인 튜닝 합니다. 그래서 gpt3를 파인 튜닝 한 다음에 지금과 같이 라이언 캡션 데이터를 주고 나서 이거에 대한 인스트럭션과 그 인스트럭션에 맞는 캡션 에디팅을 자동으로 생성하게끔 해서 이렇게 한 45만 개 이상의 에디팅 데이터를 이렇게 만들게끔 합니다. 아무래도 텍스트이기 때문에 랭귀지 모델을 사용을 해서 이렇게 에디팅 된 이기제프를 만들기는 굉장히 쉽겠죠 이런 식으로 오토메이션을 한 게 어떤 키 아이디어 중에 하나입니다. 그래서 보면은 또 지피티3를 파인튜닝해서 만들다 보니까 굉장히 크리에이티브한 접근도 하기도 하고 이런 캡션도 굉장히 잘 만들어지는 것을 이제 확인할 수 있었다라고 합니다. 네 이렇게 만들어진 텍스트 데이터를 가지고서는 정말 종합적인 이미지와 인스트럭션 그리고 아웃풋 출력에 대한 그런 데이터 페어를 만들어서 이제 이 디퓨전 모델 하나를 학습을 하게 됩니다. 순차적으로 살펴보면요. 먼저 지피티3를 어 파인 튜닝을 한 걸로 방금 전에 설명한 것처럼 인스터션과 그거에 대한 에디티드 된 캡션을 생성을 합니다. 이걸로 이제 데이터셋을 많이 만들고 나서 그다음에 지금 만들어진 이 에디팅 된 캡션과 그다음에 오리지널 인풋 캡션을 이용해서 이제 프롬프트 프롬프트 방법을 적용을 해서 이미지를 에디팅을 합니다. 우리가 이 인풋 캡션과 에디팅 된 그 캡션에 페어가 있기 때문에 우리가 프롬프트 프롬프트를 자동으로 적용하기가 참 쉽겠죠. 그래서 이렇게 데이터 셋의 어떤 이미지 페어도 이렇게 만들어 놓습니다. 그런 다음에 이제 본격적으로 이 인스트럭션과 그다음에 이 프롬프트 프롬프트에 어떤 페허된 그 아웃풋을 다 연결을 해서 이렇게 45만 개 이상의 트레인 된 데이터셋 페어를 이렇게 만듭니다. 그래서 오리지널 이미지 그거에 대한 인스트럭션 그다음에 슈퍼비전으로 사용할 에디팅 된 이미지를 이렇게 슈퍼바이즈 트레이닝 데이터셋으로 만들어 놓은 거죠. 그다음에는 간단합니다. 이 인스터트 픽스 픽스라는 그 모델을 그냥 우리가 파인 튜닝을 해서 트레이닝을 하도록 합니다. 그래서 이런 인스터션을 주고 이미지를 주고 이런 식으로 바꾸는 형태로 슈퍼바이즈 러닝을 해서 인스트럭션 팔로잉 디퓨전 모델을 만들게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(8-2강) Generative Models.json",
        "lecture_name": "(8-2강) Generative Models",
        "course": "8-2강",
        "lecture_num": "8-2강",
        "lecture_title": "Generative Models",
        "chunk_idx": 3,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8fe6b9199ffefdc6a8dd0da004fb557cb3c9bcd0d5581e428a095fe3c3d1cbb1"
      },
      "token_estimate": 1107,
      "char_count": 2021
    },
    {
      "id": "transcript_8_2강_8_2강_generative_models_c004_f39212",
      "content": "[8-2강] (8-2강) Generative Models\n\n다. 그래서 오리지널 이미지 그거에 대한 인스트럭션 그다음에 슈퍼비전으로 사용할 에디팅 된 이미지를 이렇게 슈퍼바이즈 트레이닝 데이터셋으로 만들어 놓은 거죠. 그다음에는 간단합니다. 이 인스터트 픽스 픽스라는 그 모델을 그냥 우리가 파인 튜닝을 해서 트레이닝을 하도록 합니다. 그래서 이런 인스터션을 주고 이미지를 주고 이런 식으로 바꾸는 형태로 슈퍼바이즈 러닝을 해서 인스트럭션 팔로잉 디퓨전 모델을 만들게 됩니다. 그래서 이렇게 학습된 모델은 또 인퍼런스 타임에 굉장히 제널라이제이션이 잘 된다고 해요. 그래서 보면은 이렇게 뭔가 선글라스 같은 거를 좀 씌워봐 그리고 이제 만화 캐릭터 형태로 만들어줘 그다음에 뭐 굉장히 좀 호러처럼 이렇게 만들어줘 라는 그런 명령어들을 굉장히 잘 따라서 이 레이아웃을 그대로 유지하면서 정말 적재적소에 적용해야 되는 어떤 인스트럭션만 잘 따르는 형태로 학습이 된 것을 볼 수가 있습니다. 그래서 당연하게도 어 이제 우리가 크로스 어텐션을 뭐 매뉴플레이션 한다든지 그렇게 우리가 조작을 하는 게 아니라 학습 데이터를 가지고서는 모델을 학습한 거기 때문에 어 피드 포워드 방식으로 잘 작동하는 것을 확인할 수가 있습니다. 그래서 결과도 보면은 이런 씬에 대해서 이제 에디팅이나 스타일라이제이션도 전반적으로 잘 일어나는 것을 확인할 수가 있고요. 그다음에 적당하게 레이아웃도 잘 유지가 되는 것을 확인할 수가 있습니다. 다음으로는 이제 기존의 스테이블 디퓨전과 같이 잘 학습된 거의 파운데이션 모델 레벨의 어떤 생성 모델을 다른 테스크에 확장 적용하는 그런 트렌드에 대해서 살펴보려고 합니다. 어 그 트렌드에 대해서 이 메리골드라는 연구가 있는데요. 이 메리골드는 스테이블 디퓨전 모델의 그 잘 학습된 이미지 널리지를 뎁스 에스티메이션에 사용한 연구입니다. 그래서 이 데스티메이션을 사용을 할 때에 어떤 식으로 했냐면 스테이블 디퓨전을 단순하게 파인 튜닝을 해서 이제 데스티메이션에 적용을 했는데요. 그때 이제 데스티메이션 된 결과를 보면은 정말 말도 안 될 정도로 디테일들이 이렇게 살아있는 것을 볼 수가 있습니다. 그래서 일반적인 우리가 그라운트루스 뎁스 맵보다도 훨씬 더 정교한 느낌이 드는 정도의 그런 뎁스 퀄리티를 보여주는 것을 확인할 수가 있습니다. 예 그럼 이런 파인 튜닝을 어떻게 했는지 한번 살펴보도록 하겠습니다. 이제 마리골드의 파인 튜닝은 굉장히 심플하면서도 리소스 이피션트하게 일어나게 되는데요. 먼저 이제 프리트레인 된 그런 레이턴트 디퓨션 모델 스테이블 디퓨션 모델을 이제 이미지 컨디셔널한 뎁스 에디터로 만들기 위해서 이런 전체 파이프라인을 밟습니다. 첫 번째로는 기존에 스테이블 디퓨전에서 사용했던 그런 레이턴트 엔코더를 그대로 사용합니다. 그래서 레이턴트 엔코더를 그대로 사용을 해서 이미지를 레이턴트화하고요. 그걸 지스라고 부르고 그다음에 뎁스도 그냥 이미지로 보고 이거를 3채널로 불립니다. RGB 이미지처럼 3채널로 불려서 레이턴트 코드에 그냥 넣어줘요. 그럼 레이턴트 엔코더에서 이거를 이제 CD라는 레이턴트 형태로 이렇게 매핑을 하게 됩니다. 그다음에 이 지디에 대해서만 우리가 디노이징 디퓨전을 좀 더 파인튜닝 하고 싶은 거예요. 그래서 이 지디를 레이턴트로 보고서는 여기다가 가오샷 노이즈를 어 티 스텝에 대해서 좀 더해줘서 이렇게 퍼터베이션을 시키게 됩니다. 그다음에 이렇게 노이징 된 인풋과 그다음에 여기에 이 이미지에서 온 레이턴트를 2개를 컨캐터네이션 합니다. 콘캐터네이션 하는데 얘는 노이징이 돼 있고 얘는 노이징 돼 있지 않고 굉장히 클린한 그런 레이턴트가 그대로 들어오게끔 이렇게 만듭니다. 그래서 이렇게 인포 컨디션을 통해서 이 이미지를 컨디셔널한 그런 데센스티메이션을 할 수 있도록 유도를 한다라고 이해를 할 수가 있을 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(8-2강) Generative Models.json",
        "lecture_name": "(8-2강) Generative Models",
        "course": "8-2강",
        "lecture_num": "8-2강",
        "lecture_title": "Generative Models",
        "chunk_idx": 4,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8fe6b9199ffefdc6a8dd0da004fb557cb3c9bcd0d5581e428a095fe3c3d1cbb1"
      },
      "token_estimate": 1045,
      "char_count": 1893
    },
    {
      "id": "transcript_8_2강_8_2강_generative_models_c005_a79a43",
      "content": "[8-2강] (8-2강) Generative Models\n\n다. 그다음에 이렇게 노이징 된 인풋과 그다음에 여기에 이 이미지에서 온 레이턴트를 2개를 컨캐터네이션 합니다. 콘캐터네이션 하는데 얘는 노이징이 돼 있고 얘는 노이징 돼 있지 않고 굉장히 클린한 그런 레이턴트가 그대로 들어오게끔 이렇게 만듭니다. 그래서 이렇게 인포 컨디션을 통해서 이 이미지를 컨디셔널한 그런 데센스티메이션을 할 수 있도록 유도를 한다라고 이해를 할 수가 있을 것 같습니다. 당연하게도 이렇게 컨캐터네이션을 해서 인풋으로 넣어주게 되면은 이 레이턴트 디퓨전 유닛의 입구 부분에 인풋 레이어 부분을 이제 어 8채널 즉 여기 4채널, 여기 4채널 해서 총 8채널을 담을 수 있는 형태로 인풋 레이어를 변경을 해줘야 되는데요. 그때 이제 제로 컨볼루션이라든지 그런 구조를 활용할 수가 있을 것 같습니다. 그렇게 해서 이 레이턴트 디퓨션 유닛이 노이즈를 프리젝션 하는 형태로 이제 디노이징 디퓨션을 트레이닝을 시킵니다. 이때에 이 길은 이제 레이턴트 디퓨션 모델에서부터 시작을 해서 이제 파인튜닝 하는 형태로 적용이 되게 되는 거죠. 그렇게 학습을 하고 나면은 인퍼런스 할 때는 먼저 스탠다드한 그냥 가우시안 노이즈에 대해서 샘플링을 하고요. 그다음에 이제 이미지 컨디션을 여기다 컨캐터네이션을 시켜줘서 이제 레이턴트 디퓨션 유닛에다가 태워서 이제 노이즈를 프리딕션 하는 식으로 디노이징을 계속 반복하게 됩니다. 그렇게 디노이징 된 최종적인 레이턴트 코드를 이제 레이턴트 디코더에다가 입력으로 넣어줘서 이런 식으로 뎁스로 디코딩을 하게 됩니다. 여기서 레이턴트 디코더도 이제 스테이블 디퓨전에서 사용했던 그 레이턴트 디코더를 그대로 활용합니다. 즉 이제 파인튜닝 된 부분은 이 레이턴트 디퓨저 유닛을 최소한으로 파인튜닝을 하게 되는데요. 그때에 이제 사용한 데이터셋은 굉장히 신테틱 데이터들과 좋은 퀄리티의 데이터셋을 다 활용을 해서 이제 파인튜닝을 하도록 만듭니다. 이렇게 튜닝된 이제 마리골드를 보면은 어 신트틱 데이터에 학습되었음에도 불구하고 이 신트틱 데이터가 굉장히 고퀄리티이기도 하고 그다음에 스테이블 디퓨전이 애초에 이미지에 대한 정보와 그런 널리지가 굉장히 충분하다 보니까 어 실제로 언신 데이터에 대해서도 제로샷 트랜스퍼 퀄리티가 굉장히 좋은 것들을 이렇게 확인할 수가 있습니다. 보면 여기에 이 의자에 살 같은 부분도 굉장히 디테일하게 잘 나오는 것들을 볼 수가 있고요. 그리고 여기 엔아유2 데이터셋에 대해서도 리얼 데이터임에도 불구하고 굉장히 좋은 퀄리티의 결과가 나오는 것을 확인할 수가 있습니다. 네 그리고 마리골드는 레이턴트 엔코더하고 레이턴트 디코더를 곧바로 사용하기 때문에 어떤 이미지 도메인에 대한 그런 어 레인지를 가지고 있고 실제 앱솔루트 스케일의 뎁스를 가지고 있지 않습니다. 그래서 약간 랠러티브한 형태의 뎁스 맵을 디코딩을 하게 되는데요. 그럼에도 불구하고 이 메리골드가 이제 그라운트루스하고 비교를 했을 때 굉장히 파인 디테일도 가질뿐더러 이제 레이아웃에 대해서도 적절하게 잘 추정하는 그런 결과들을 보여주고 있습니다. 이렇듯이 이제 스테이블 디퓨전과 같이 파운데이션 레벨에 어떤 굉장히 좋은 제네이티브 모델이 있으면 그 모델과 연관된 다른 모델리티 또는 다른 테스크로도 확장해서 적용이 가능합니다. 그래서 이런 유용한 스테이블 디퓨전을 리소스 이피션트한 그런 형태의 파인튜닝을 통해서 굉장히 빠르게 자기만의 테스크 그다음에 우리가 필요한 어떤 서비스 응용 사례에 적용할 수 있는 게 무엇이 있을지 한번 고민해 봐도 좋을 것 같습니다. 네 이것으로 오늘 강의 마치도록 하겠습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(8-2강) Generative Models.json",
        "lecture_name": "(8-2강) Generative Models",
        "course": "8-2강",
        "lecture_num": "8-2강",
        "lecture_title": "Generative Models",
        "chunk_idx": 5,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8fe6b9199ffefdc6a8dd0da004fb557cb3c9bcd0d5581e428a095fe3c3d1cbb1"
      },
      "token_estimate": 993,
      "char_count": 1795
    }
  ]
}