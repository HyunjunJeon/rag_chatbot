{
  "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
  "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
  "course": "MRC",
  "total_chunks": 8,
  "chunks": [
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c000_c81016",
      "content": "[강의 녹취록] 과목: MRC | 강의: 4강 | 제목: Passage Retrieval-Sparse Embedding\n\n네 안녕하세요 네 오늘 4강 시작하도록 하겠습니다. 오늘 4강은 패세지 리트리버 그중에서도 스파스 인베딩 부분을 다루도록 하겠습니다. 오늘은 크게 세 가지 파트로 구성이 돼 있습니다. 첫 번째로는 패시지 리트리벌이라는 게 어떤 것인지 좀 알아보고요. 그다음에는 패쇄지 인베딩을 구하는 방법 특히 오늘 같은 경우는 스파스 인베딩을 구하는 방법을 알아보도록 하고요. 세 번째로는 스파스 인베딩 중에서도 가장 널리 쓰이고 있는 tfidf에 대해서 알아보도록 하겠습니다. 네 먼저 패시지 리트리벌에 대한 개론입니다. 패세지 리트리벌이란 이제 질문이 주어졌을 때 저희가 질문에 맞는 문서를 찾는 것을 의미합니다. 그래서 보시다시피 저희가 데이터베이스가 있고 그 데이터베이스 상에 찾고 싶은 어떤 문서나 어떤 데이터 조각이 있다고 하셨을 때 질문이 들어오면 그 조각을 찾아서 내보내 주는 것을 의미하죠. 물론 이 데이터베이스의 형태가 여러 가지 다양할 수가 있습니다. 뭐 좀 더 구조화된 형태일 수도 있겠지만 저희가 이번 수업에서 다루고 있는 주제 같은 경우는 주로 위키피디아 같은 문서를 다루고 있기 때문에 실제로도 위키피디아 또는 좀 더 넓게 해석을 하자면 일반적인 웹상에 존재하는 모든 문서를 볼 수가 있겠고요. 그리고 이 문서들 중에서 저희가 넣은 질문에 관련이 있는 문서를 찾아서 내보내는 방식을 취한다고 보시면 되겠습니다. 그래서 저희가 실제로 목표로 하는 것은 저런 질문 토트넘이 들어왔었을 때 패시지 위트래블이라는 문제는 이 웹 상에 또는 위키피디아 상에 관련된 문서를 가져오는 시스템을 의미를 합니다. 왜 저희가 이런 시스템을 만드는 것에 관심이 있을까요? 사실 그 이유는 이런 시스템을 저희가 1강 2강 3강에서 논의했던 MRC 시스템과 연결하게 되면은 오픈 도메인 퀘스천 엔서링을 할 수 있게 되기 때문입니다. 저희가 MRC에서는 지문이 주어졌다고 가정을 하고 그 지문에서 답변을 찾는 형태의 모델을 만드는 것을 다뤘죠. 근데 당연히 그 지문이라는 것이 누군가가 주어져야 답변을 할 수 있는 거기 때문에 지문을 주는 모델도 마찬가지로 필요한 것입니다. 그래서 패시지 리트리벌이라는 첫 번째 단계에서는 질문에 관련된 또는 질문을 질문에 대한 답을 포함하고 있을 것 같은 지문을 찾아서 MRC 모델에 넘기게 되고 MRC 모델은 그 지문을 보고 또 질문을 다시 읽어봄으로써 아주 정확한 답변을 낼 수 있는 결국 투 스테이지의 파이프라인이라고 보시면 될 것 같아요. 실제로 그러면 어떻게 패시지 리트리버를 저희가 인플멘테이션을 접근하면 될까요? 일반적으로는 저희가 인베딩 스페이스에서 검색을 하려고 합니다. 그래서 어떤 방식이냐 질문이 들어왔었을 때 질문을 어떤 벡터 스페이스로 인베딩을 하게 됩니다. 그리고 마찬가지로 같은 벡터 스페이스에 패시지를 인베딩을 하게 되는데요. 다만 패시지 또는 문서 쪽의 인베딩은 저희가 질문이 들어왔었을 때 그때그때 하는 것이 아니라 미리 해둠으로써 효율성을 도모를 하고요. 그다음에 들어온 질문에 대해서 각 패시지에 임베딩과 서로 스뮬레이티를 스코어를 재게 됩니다. 이 스뮬레이티 스코어라는 건 여러 가지 방법이 있긴 한데 보통 일반적으로 많이 쓰이는 스코어는 리얼스 네이버 같은 경우 즉 고차원 스페이스에서 서로의 거리가 얼마나 되는지를 보거나 또는 이너 프로덕트 같은 서로의 다 프로덕트를 계산해 가지고 가장 높은 스코어가 되는 그런 패시지를 찾는 방식을 취하게 됩니다. 이 시뮬레이티 스코어가 무엇이 되었든 그 시뮬레이티 스코어를 잰 다음에 모든 패시지에 관해서 잰 다음에 이 스코어들을 랭킹을 해서 가장 높은 문서 순서대로 내보내 주는 방식을 취하게 됩니다. 다음으로는 저희가 패쇄지 인베딩을 스파스한 방법으로 하는 방법을 알아보도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 0,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 1037,
      "char_count": 1908
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c001_78fc7d",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 이 시뮬레이티 스코어가 무엇이 되었든 그 시뮬레이티 스코어를 잰 다음에 모든 패시지에 관해서 잰 다음에 이 스코어들을 랭킹을 해서 가장 높은 문서 순서대로 내보내 주는 방식을 취하게 됩니다. 다음으로는 저희가 패쇄지 인베딩을 스파스한 방법으로 하는 방법을 알아보도록 하겠습니다. 일단 먼저 저희가 인베딩 스페이스에 대한 이해도가 있어야 되는데요. 결국에는 저희가 패시지를 벡터 스페이스로 매핑을 하려고 하는 것이고 이 벡터 스페이스라 함은 사실 결국에는 고차원 여러 개의 숫자로 이루어져 있는 포인트들이 모여 있는 그런 아주 고차원적인 추상적인 공간이라고 보시면 될 것 같아요. 저희가 이걸 3차원 스페이스에서 좀 비주얼라이즈를 한다면은 저 그림에서 보시다시피 어떤 일종의 3차원 상의 점이 될 텐데 이제 결국은 어떤 두 개의 문서의 유사도 또는 어떤 문서와 질문의 유사도를 저 벡터 스페이스 상에서 거리로 계산을 한다고 보시면 되겠습니다. 물론 거리뿐만 아니라 아까 말씀드렸던 그런 이너 프로덕트 같은 유사도를 통해서도 두 개의 관련성을 저희가 계산을 할 수가 있고요. 이제 저희가 오늘 렉처에서 특히나 좀 자세하게 알아볼 부분은 스파스 인베딩인데요. 스파스하다는 의미는 댄스의 이제 반대어로서 어떤 숫자가 0이 아닌 숫자가 상당히 적게 있음을 의미합니다. 그래서 가장 대표적인 게 백업 월드라는 스파스 인베딩 방법론인데요. 이제 보시다시피 저는 문서가 주어졌을 때 이 문서를 인베딩 스페이스로 매핑하기 위해서 각 문서에 존재하는 단어들을 1이나 0으로 존재하면 1 없으면 0으로 표현하여 아주 긴 벡터로 표현하는 것을 의미를 합니다. 물론 이런 경우에는 특정 단어가 존재하는지 하지 않는지로 표현하다 보니까 벡터의 길이는 전체 우리가 고려하고 있는 보케블러리의 사이즈와 동일하게 되겠죠. 예를 들면 우리 보케블러리의 40만 개의 단어가 있다 한다면 모든 문서가 30만 벡터 스페이스로 매핑을 하게 됩니다. 그리고 각각의 디멘션은 하나의 단어에 해당하게 되는데 이 밑에 보시다시피 예를 들면 잇 워스 더 베스트 워스트 타임스라는 단어가 저희 보케 빌러이라고 한다면은 각 단어가 해당 문서 다큐먼트 1이랑 2에 있는지 없는지에 따라서 1과 0이 표시된 걸 볼 수가 있죠. 물론 이 경우는 보케블러리가 상당히 적은 경우고 또 문서 또한 단어가 상당히 적은 편이라 할 수 있겠습니다만 실제로는 보케블러의 사이즈가 몇십만 또는 몇 백만이 될 수도 있고요. 그리고 문서의 길이도 마찬가지로 저렇게 한 6개 7개의 단어가 아니라 수백 개 또는 수천 개의 단어로 이루어질 수도 있습니다. 물론 백업 월즈를 구성하는 방법은 이제 가장 기본적인 방법은 유니그램 즉 하나의 단어만 보는 방법인데요. 그래서 방금 전에 보셨던 예제처럼 잇 워스 더 베스트 타임즈라는 문서를 저희가 유니그램으로만 구성을 한다면 잇 워즈 더 베스트 오브 타임 이렇게 나눠서 하나하나 단어로 보는 방법이 있고요. 이 경우는 당연히 그러면 벡터의 크기가 보케블러리 사이즈와 동일하게 되겠죠. 하지만 조금 더 어드벤스 된 방법으로는 바이그램 또는 엔그램 백업 월즈를 형성할 수가 있는데요. 예를 들어서 이제 바이그램 백업 월즈를 형성하게 되면 기존 유니그램처럼 하나의 단어만 보시는 게 아니라 단어 두 개씩 묶어서 그 두 개를 하나의 단어로 보고 보케블러리를 형성하게 됩니다. 즉 이런 경우는 예를 들면 유니그램 베이스의 보케블러리 사이즈가 이제 10만이라고 한다면 모든 가능한 바이그램의 보케블러의 사이즈는 그거의 제곱인 10만 곱하기 10만 즉 100억이 되겠죠. 즉 엔그램에서 엔이 늘어나면 늘어날수록 기하급수적으로 보케블러의 사이즈가 늘어나기 때문에 사실 항상 DSB 한 건 아니고요. 이 높게 하려고 하는 게 다만 바이그램 정도까지는 보통 활용을 하는 편이고 경우에 따라서는 트라이그램도 활용하는 경우가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 1,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 1053,
      "char_count": 1944
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c002_d68326",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 즉 이런 경우는 예를 들면 유니그램 베이스의 보케블러리 사이즈가 이제 10만이라고 한다면 모든 가능한 바이그램의 보케블러의 사이즈는 그거의 제곱인 10만 곱하기 10만 즉 100억이 되겠죠. 즉 엔그램에서 엔이 늘어나면 늘어날수록 기하급수적으로 보케블러의 사이즈가 늘어나기 때문에 사실 항상 DSB 한 건 아니고요. 이 높게 하려고 하는 게 다만 바이그램 정도까지는 보통 활용을 하는 편이고 경우에 따라서는 트라이그램도 활용하는 경우가 있습니다. 그리고 이런 경우는 백업 월즈 가장 베닐한 백업 월즈 같은 경우는 텀이 문서 내에 등장하는지 안 하는지만 보도록 돼 있고 즉 바이너리 0이나 1로 이제 표현이 되는 건데 사실 이런 방법은 당연히 상당히 바닐라 한 방법이고 더 잘 할 수가 있겠죠. 그래서 조금 더 잘 할 수 있는 방법이 텀이 몇 번 등장하는지를 보는 방법이 있고 거기서 더 나아가서 tfidf 같은 조금 더 어드벤스 된 방법론이 있겠습니다. 이거에 대해서는 저희가 오늘 강의 중 조금 이따가 설명을 하도록 할게요. 그래서 사실 좀 유의해야 될 점은 스파스 인베딩에선 결국 디멘션은 텀의 개수에 의해서 총 텀에 가능한 개수 즉 보캡의 사이즈에 의해서 결정이 되는데요. 즉 보케블러리의 크기가 커지면 커질수록 이 벡터의 크기가 늘어나겠죠. 예를 들면 유니그램 케이스 같은 경우 보케블레의 사이즈가 30만이면 벡터의 크기도 30만이 되겠죠. 근데 또 말씀드렸던 것처럼 저희가 엔그램 텀을 고려하기 시작하면 n이 또 커지면 커질수록 또 벡터의 크기가 커집니다. 엔이 2인 경우는 이제 제곱이 될 수도 있는 거고요. n이 3인 경우는 이제 세 제곱이 되어서 훨씬 더 큰 크기의 벡터가 될 수도 있습니다. 또 스파스 인베딩의 특징 중 하나는 어떤 TM이 오버랩이 되는지 안 되는지를 파악하기엔 정말 좋다는 겁니다. 그래서 저희가 검색에 활용을 할 때는 저희 검색 단어가 실제 그 문서에 들어가 있는지 없는지를 볼 때 정말 유용한데요. 다만 좀 반대로 안 좋은 점 중 하나는 의미가 비슷하지만 다른 단어인 경우에는 이런 백업 vs 방법론으로는 전혀 비교를 할 수가 없기 때문에 이런 경우는 저희가 다음 강의에서 볼 댄스 인베딩의 활용을 해야겠습니다. 그래서 밑에서 보신 것처럼 저희가 질문이 w PD RR 챗 디스커버리라는 쿼리를 날리게 되면은 여기서 등장했던 단어들이 존재한 문서를 찾는 데는 아주 백업 버즈가 유용한 것을 알 수가 있습니다. 자 이젠 tfidf에 대해서 소개를 해보도록 할게요. 방금 보셨던 것처럼 백업 월즈라는 방법론은 아주 단순하게 특정 단어가 문서에 존재하는지 안 하는지를 보고 0과 1로 나눠서 벡터화를 하는 것으로 이제 소개를 드렸는데요. tfidf 같은 경우는 조금 더 나아가서 특정 털의 단어의 등장 빈도를 보는 방식으로 진행을 하게 됩니다. 그리고 그 단어의 등장 빈도만 고려하는 것뿐만이 아니라 단어의 단어가 제공하는 정보의 양 또는 반대로 얘기하자면 단어가 얼마큼 덜 등장하는지를 봐서 이 두 개의 숫자를 곱해줌으로써 이제 최종 수치를 재게 되는데요. 이게 어떻게 해석하시면 되냐면은 어떤 문서 내에 단어가 많이 등장을 했는데 그 단어가 전체 문서에서는 등장하는 경우가 별로 없더라 그러면 상당히 그럼 그 단어는 중요한 거겠죠. 그 문서 내에서는 특히나 그래서 그런 단어들한테 좀 더 점수를 준다고 보시면 되겠습니다. 예를 들면 아까 예제로 보여드렸던 문장 잇 워스 더 베스트 업 타임즈 같은 경우는 잇 워즈 더 오브 같은 단어들은 사실은 정보를 담고 있는 단어들은 아니고요. 비교적 베스트나 타임즈 같은 경우는 조금 더 정보를 담고 있기 때문에 이런 단어들이 아이디프 스코어가 조금 더 높게 됩니다. 먼저 그럼 텀 프리퀀스를 재볼 텐데요. 텀 프리퀀스를 재는 방식은 아주 간단합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 2,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 1017,
      "char_count": 1913
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c003_1518ff",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 예를 들면 아까 예제로 보여드렸던 문장 잇 워스 더 베스트 업 타임즈 같은 경우는 잇 워즈 더 오브 같은 단어들은 사실은 정보를 담고 있는 단어들은 아니고요. 비교적 베스트나 타임즈 같은 경우는 조금 더 정보를 담고 있기 때문에 이런 단어들이 아이디프 스코어가 조금 더 높게 됩니다. 먼저 그럼 텀 프리퀀스를 재볼 텐데요. 텀 프리퀀스를 재는 방식은 아주 간단합니다. 보시다시피 이제 먼저 특정 단어가 해당 문서에 몇 번 나오는지를 재게 되고요. 이게 좀 결정적으로 배어버즈랑 다른 점이긴 하죠. 백어버즈 같은 경우는 0이나 1이었던 반면에 일단 먼저 텀 프리퀀시는 특정 단어가 두 번 등장을 하면 1이 아니라 2가 되는 거고요. 근데 여기서 끝나는 것이 아니라 저희가 이걸 노멀라이즈를 해 줍니다. 보통은 이제 그 러 카운트를 그대로 쓰지는 않고 여기서 이제 실제 문서 내에서 총 단어의 개수가 몇 개인지를 봄으로써 비중을 비교적 비율로 나타나게 되고 그리고 총 합이 1 이하일 수 있도록 하는 거죠. 물론 다른 베리언트도 여러 가지 있습니다. 저희가 수업에서 다루지는 않겠지만 로그 노멀레이션 같은 방법론도 있고요. 실제로 여러분이 다른 tfif 패키지를 보실 때 로그 노멀라이제이션을 보시게 되면 아 턴 프리퀀시에서 활용된 방법론이구나라고 생각하시면 되겠습니다. 그다음으로는 IDF인데요. IDF는 계산은 아래와 같습니다. 먼저 총 다큐멘터의 개수 n을 구하고요. 그다음에 특정 의 다큐먼트 프리퀀시를 구하게 됩니다. 즉 특정 티가 텀이 등장한 다큐멘트 개수인 거죠. 이제 어떻게 해석하시면 되냐면 예를 들면은 der나 is 같은 아주 자주 등장하는 단어에 같은 경우는 df t가 상당히 높겠죠. 거의 대부분의 문서에 저게 들어가 있기 때문에 사실상 저희가 문서가 100개라고 한다면은 100개 모두 더가 한 번쯤은 들어갈 겁니다. 그럼 디프 스코어는 100이 되겠죠. 그리고 엔 같은 경우도 100이 되기 때문에 100 나누기 100은 1이 되고 여기에 로그를 씌워주시게 되면은 0이 되겠죠. 즉 저와 같은 단어 즉 모든 문서에 등장하는 단어 같은 경우는 아이디프 스코어가 빵점이 됩니다. 근데 예를 들면은 어떤 단어 같은 경우는 한 문서에만 등장할 수도 있겠죠. 그런 경우는 엔은 똑같이 100인데 DFT 같은 경우 1이 되기 때문에 n 나누기 DFT는 100이 되고 이 숫자를 로그하게 되면은 어 2진법을 썼을 때는 저희가 지금 암산할 수는 없겠지만 10진법을 썼을 때는 로그 10에 100은 2가 되겠죠. 그래서 꽤나 큰 수치가 TF에 곱해지게 되는 것입니다. 그래서 여기서 중요한 점은 TF랑 다르게 IDF 같은 경우는 각 t 즉 각 t에 특정되고 문서에는 무관하다는 점입니다. 그래서 좀 이따 보시면 아시겠지만 TF 점수 같은 경우는 펑션이 t랑 d 즉 특정 과 그 이 해당되는 문서까지 그 표시를 해줘야 되지만 IDF 같은 경우는 털마다 유니크하고 문서마다 다르지가 않습니다. 그리고 최종 tfidf 스코어는 이 두 개의 수치를 곱한 수치로 정의가 됩니다. 그래서 앞서 말씀드렸듯이 a나 b와 같은 관사는 상당히 낮은 tfidf 스코어를 갖게 되고요. 사실상 0이 될 겁니다. 거의 반대로 자주 등장하지 않는 고유 명사 같은 경우 사람 이름 이런 경우는 보통 전체 문서 콜퍼스에서 사실 1개 또는 2개에서만 나올 수도 있기 때문에 상당히 높은 IDF가 나오게 되고요. 그래서 TF가 낮다고 하더라도 IDF가 높기 때문에 최종 tfidf 스코어는 높게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 3,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 913,
      "char_count": 1775
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c004_a0ba95",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 그리고 최종 tfidf 스코어는 이 두 개의 수치를 곱한 수치로 정의가 됩니다. 그래서 앞서 말씀드렸듯이 a나 b와 같은 관사는 상당히 낮은 tfidf 스코어를 갖게 되고요. 사실상 0이 될 겁니다. 거의 반대로 자주 등장하지 않는 고유 명사 같은 경우 사람 이름 이런 경우는 보통 전체 문서 콜퍼스에서 사실 1개 또는 2개에서만 나올 수도 있기 때문에 상당히 높은 IDF가 나오게 되고요. 그래서 TF가 낮다고 하더라도 IDF가 높기 때문에 최종 tfidf 스코어는 높게 됩니다. 한번 저희가 수업 중에 간단하게 한번 수치를 구해볼게요. tfidf 수치를 구해볼 텐데 만약에 저희가 문서가 이렇게 4개가 있고요. 음식 운동 영화 음악이라는 문서가 있고 각 문서에 이런 내용들이 있다고 쳤을 때 저희가 먼저 tfidf를 계산하기 위해서는 토크나이즈를 해야 되고요. 그래야 저희가 어떤 그런 털들을 정의를 할 수가 있겠죠. 일단 저희가 유니그램으로만 생각할게요. 바이그램은 고려하지 않고 이제 그렇게 했었을 때 저희가 그 기본적으로 그 각 털의 문서화에 몇 번씩 등장하는지를 잴 수가 있겠죠. 물론 이 경우는 한 번씩만 등장하다 보니까 1.0 이외에는 숫자가 없습니다. 0과 1 위에는 하지만 저희가 이렇게 트 피퀀스를 재볼 수 있고 각 문서마다 그 음식이라는 문서 같은 경우는 주어는 과자를 좋아한다라는 단어가 각각 한 번씩 등장을 하고요. 운동 같은 경우는 주연은 좋아한다. 농구와 축구를 이렇게 각각 한 번씩 등장하는 것을 보실 수가 있습니다. 다음으로는 이제 IDF를 계산을 해야 되는데요. IDF 같은 경우는 아까 말씀을 드렸듯이 특정 단어가 몇 개 문서에 나왔는지가 중요하고요. 이제 이 경우는 사실 좀 달라지는 부분은 이제 보시면 이 IDF 같은 경우 주어원이라는 단어는 모든 문서에 등장을 하다 보니까 n 나누기 df가 1이 되고 로그를 취하게 되면 0이 되고요. 최종 숫자가 좀 높게 나온 단어 같은 경우 예를 들면 과자를 같은 경우는 실제로 등장하는 문서가 하나만 있다 보니까 좀 높게 나오는 것을 보실 수가 있습니다. 가장 같은 경우는 모든 문서에 나오지 않지만 그래도 몇 개 두 개 이상의 문서에 나오다 보니 좀 숫자가 낮은 거 아실 수가 있고요. 그다음에는 마지막으로 이 두 개의 테이블을 그대로 곱해주면 됩니다. 그러면 tfidf 스코어가 되고요. 이제 그렇게 되면은 보시면은 숫자가 조금씩 변경된 걸 아실 수가 있고 이거를 벡터화를 하게 되면 각 문서에 TF아디 벡터가 됩니다. 이제 이렇게 문서에 tfidf 벡터들을 구해놨으면은 마지막으로는 저희가 질문 쪽에서 똑같은 방식으로 질문을 문서라 생각을 하고 tfidf 스코어를 구한 다음에 그다음에 두 개의 벡터를 유사도를 구해주면 됩니다. 유사도를 구하는 방식은 여러 가지가 있는데 보통 tfidf 같은 경우는 코사인 디스턴스를 구하게 되고요. 사실상 노멀라이즈를 하고 난 다음에는 두 개를 이너 프로덕트를 하게 되면은 코사인 드 스탠스가 됩니다. 이거는 저희가 실습 때 좀 더 자세히 다뤄볼 예정이고요. 그래서 말씀드렸던 것처럼 그 이널 프로덕트를 함으로써 스코어를 계산하는 공식을 보실 수가 있고요. 그래서 실제로 이렇게 계산을 하게 되면은 가장 가까운 단어는 저희가 음악이라는 문서를 찾는 거를 실습에서 보실 수가 있습니다. 마지막으로 저희가 tfidf를 좀 많이 다뤄봤지만 이 bm 25라는 좀 더 자주 쓰이는 백업 월즈와 유사한 또는 스퍼스 인베딩 방법론이 있습니다. 간단하게 설명을 드리자면 tfidf의 개념을 바탕으로 거기다 플러스 문서의 길이까지 고려한 점수 매기는 방식입니다. TF 값에 이제 이 경우는 한계를 지정해 두어 가지고 이제 일정한 범위를 유지하도록 하고요. 이제 평균적인 문서의 길이보다 더 작은 문서에서 단어가 매칭된 경우에 즉 작은 문서에 더 가중치를 부여하는 방식을 취하고 있습니다. 그리고 이런 방법론은 실제로는 티프아데프보다 조금 더 실질적으로는 검색 엔진이나 추천 시스템 등에서 많이 활용되고 있고 아직도 상당히 많이 활용되고 있는 방법론입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 4,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 1074,
      "char_count": 2046
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c005_25af34",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 간단하게 설명을 드리자면 tfidf의 개념을 바탕으로 거기다 플러스 문서의 길이까지 고려한 점수 매기는 방식입니다. TF 값에 이제 이 경우는 한계를 지정해 두어 가지고 이제 일정한 범위를 유지하도록 하고요. 이제 평균적인 문서의 길이보다 더 작은 문서에서 단어가 매칭된 경우에 즉 작은 문서에 더 가중치를 부여하는 방식을 취하고 있습니다. 그리고 이런 방법론은 실제로는 티프아데프보다 조금 더 실질적으로는 검색 엔진이나 추천 시스템 등에서 많이 활용되고 있고 아직도 상당히 많이 활용되고 있는 방법론입니다. 저희가 저희 강의 중에는 자세하게 들어가지 않을 텐데 여러분이 챌린지를 하실 때 만약에 조금 tfidf를 조금 개선시키고 싶다고 한다면은 이 공식을 활용을 해서 개선을 시키는 것도 좋은 방법일 것 같고요. 그리고 또한 이 공식을 그대로 쓰는 것이 아니라 또 튜닝을 해서 더 계산하는 것도 하나의 방법이 되겠습니다. 네 오늘 본 강 마치도록 하겠습니다. 감사합니다. 네 안녕하세요. 4강 실습 시작하도록 하겠습니다. 오늘 실습은 tfidf를 활용해서 실제로 콜 코드라는 데이터 셋에서 패시지를 리트리브 하는 거를 한번 해보도록 할게요. 먼저 여느 때와 같이 이제 런타임을 설정을 해 주셔야 되는데 오늘은 저희가 GPU를 활용하지 않을 거기 때문에 런 타임을 설정을 CPU로 하도록 하겠습니다. 그래서 실제로 디폴트이기 때문에 놔두셔도 되긴 해요. 그래서 논으로 설정을 하시면 되고 그다음에 세이브를 하신 다음에 이제 커넥트를 하시게 되면 CPU에 연결이 되면서 이제 노트북을 사용할 수 있으시게 됩니다. 그다음에 이제 두 개의 패키지를 인스톨을 해 주실 텐데요. 똑같은 패키지이긴 해요. 데이터셋 활용을 하기 위해서 데이터셋으랑 트랜스포머를 설치를 해 주시겠습니다. 네 마찬가지로 콜코드 가져오도록 할게요. 네 그리고 저희가 한번 이번에는 문서들만 따로 빼서 가져와 보도록 할게요. 저희 콜퍼스를 정의를 할 텐데 이 콜퍼스가 저희가 tfif를 활용하는 데 쓰일 콜퍼스죠. 결국에 어떤 콜퍼스냐면은 각 이그젬플에서 컨텍스트를 가져오고요. 학습 데이터에 있는 것만 가져오도록 하겠습니다. 여기서 셋을 한 다음에 리스트를 다시 바꿔주는 이유는요. 중복된 컨텍스트가 있기 때문에 콜 코드 내에 그 한 컨텍스트에 해당되는 질문이 여러 개 있다 보니까 그 중복이 되겠죠. 컨텍스트가 그래서 셋을 해줬다가 다시 리스트로 변경을 해주는 거고요. 이렇게 해주신 다음에 실제 콜퍼스의 그 텍스트 개수를 보면은 약 9600개인 걸 아실 수가 있고 하나 한번 가져와 볼까요? 콜퍼스의 첫 번째 걸 가져오시게 되면은 보시다시피 텍스트 문서 지문을 저희가 볼 수가 있습니다. 자 그다음에 저희가 토크나이제이션을 할 텐데요. 가장 기본적인 토크나이제이션은 그냥 와이 스페이스 토크나이저겠죠 좀 더 복잡한 토크나이저를 쓸 수 있겠지만 오늘은 와이 스페이스만 쓸게요. 그래서 이 토크나이제이션 펑션을 제가 정의를 해 주고요. 스페이스로 토크나이즈를 하는 거고요. 그다음에 이 펑션을 활용해서 저희가 첫 번째 텍스트를 하게 되면은 그리고 뭐 저희가 첫 번째 10개만 볼게요. 토큰을 그러면 대략 이렇게 되는 걸 알 수가 있습니다. 그다음에 저희가 tfidf를 연산하기 위해서 실제로 저희가 이거를 직접 하지 않고 사이킷런의 티프아디프 벡터라이저라는 그런 라이브러리를 활용을 할 거예요. 그래서 사이클런 같은 경우는 여기 콜 앱에 이미 설치가 돼 있습니다. 그래서 저희가 인포트를 해올 텐데 피처 익스트랙션 텍스트라는 패키지에 있고요. 그리고 이 벡터라이저를 정의를 하도록 하겠습니다. 어떻게 텍스트를 벡터로 바꿔줄지 텍스트를 벡터로 바꿔주는 그런 클래스라고 보시면 될 것 같아요. 토크나이저를 아까 저희가 정했던 토크나이저로 정의를 해 주고요. 그리고 엔그램 같은 경우는 저희가 유니그램만 이제 보는 걸로 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 5,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 1050,
      "char_count": 1952
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c006_29cdd7",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 그래서 저희가 인포트를 해올 텐데 피처 익스트랙션 텍스트라는 패키지에 있고요. 그리고 이 벡터라이저를 정의를 하도록 하겠습니다. 어떻게 텍스트를 벡터로 바꿔줄지 텍스트를 벡터로 바꿔주는 그런 클래스라고 보시면 될 것 같아요. 토크나이저를 아까 저희가 정했던 토크나이저로 정의를 해 주고요. 그리고 엔그램 같은 경우는 저희가 유니그램만 이제 보는 걸로 하겠습니다. 이가 인크sive 하지 않아요 벡터라이저를 정의를 해줬고요. 일단 이 벡터라이저를 아까 그 보셨겠지만 아디프를 구하기 위해선 문서 전체를 보셔야 돼요. 그래서 문서 전체에 대해서 문서 전체를 보고 아디프에 해당되는 값들을 구해줘야 됩니다. 그리고 텀들을 정의를 해야겠죠 문서 전체에 등장하는 텀들을 다 보케브러리로 만들어 줘야 되기 때문에 이거를 이제 f이라는 표현을 쓰는데 사이클론에서는 학습한다고 보시면 되고요. 일종의 다만 이제 뉴얼렛 학습하는 거랑 약간 좀 다른 개념이죠. 그래서 아까 저희가 만들었던 콜퍼스를 피트하게 되고 이것은 좀 시간이 걸리죠. 살짝 이게 큰 퀄퍼스라서 이제 그다음에 이 벡터라이저는 일종의 펑션이니까 즉 문서를 벡터로 바꿔주는 펑션이고 이 펑션을 이제 이 핏을 통해 정의를 했으면 실제로 바꿔줘야겠죠. 콜퍼스를 그래서 스퍼스 메이트릭스를 벡터 라이즈를 통해서 옵테인 하게 됩니다. 트랜스폼이라는 펑션을 이용을 해서 콜퍼스를 바꿔주게 되죠. 그러면 이 스파스 매트릭스는 각 문서에 해당되는 스파스 벡터를 갖고 있는 하나의 큰 매트릭스입니다. 그래서 보시면 스파스 매트릭스의 셰이프를 보시면 9606 그리고 꽤 큰 숫자인 걸 보실 수가 있는데 이 9606이 결국 아까 저희가 봤던 그 텍스트 지문의 개수이고요. 이제 정확하게 숫자가 127만 2768인 걸 볼 수가 있는데 이게 유니그램과 바이그램 둘 다 고려한 값입니다. 그러니까 유니그램만 봤을 때 이렇게 많을 수 없는데 바이그램까지 포함을 해서 이렇게 많은 걸 아실 수가 있습니다. 그리고 다음으로는 이제 저희가 이걸 테이블을 잘 핸들 하기 위해 판다스를 활용하도록 할게요. 그래서 이건 제가 카페인 페이스트를 해올 텐데 간단하게 좀 비주얼라이즈 하기 위한 용도고요. 그래서 저희가 여기서 스파스 매트릭스의 첫 번째 것 즉 첫 번째 문서에 대한 해당하는 벡터만 가져와가지고 한번 테이블 화해서 좀 보게 된다면은 어 이렇게 이렇게 이루어져 있는 걸 알 수가 있죠. 보시다시피 각 단어에 해당하는 tfid 벡터 값들이 이렇게 옆에 볼 수 있는 것을 알 수가 있습니다. 이게 스파스 매트릭스의 일부고요. 에스피 매트릭스의 0 번째 걸 가져왔죠 이렇게 보시다시피 그럼 이제 벡터를 만들어서 리트리버를 해볼 텐데요. 그 다음으로는 저희가 한번 한번 저희가 학습 데이터 셋에서 하나의 질문을 고른 다음에 한번 랜덤하게 구해 볼게요. 그래서 랜덤을 임포트하고요. 넌 파일을 가져오고 벡터를 다루기 위해서 일단 그냥 시드를 하나 정해 놓은 다음에 하나 그 이젠플을 찾도록 하겠습니다. 여기서 트레인을 쓰는 이유는 저희가 지문이 겹치지 않을 수 있기 때문에 사실 학습에서 꼭 찾으셔야 돼요. 대부를 가시면 지문이 겹치지 않아서 정답 부심을 가져올 수가 없겠죠 좀 다르죠 기존 셋업하고는 그다음에 쿼리는 결국 데이터 셋에서 해당 샘플 ID x를 액세스하고 퀘스천을 가져오시면 되고 그다음에 이거에 해당되는 지문은 마찬가지로 샘플 아이디스에서 컨텍스트가 되겠죠 이렇게 정의를 해 주시고요. 자 이제 그다음에 쿼리 벡터를 변경을 하시면 됩니다. 같은 벡터 라이즈를 활용을 해서 자 그러면 쿼리 벡터를 쉐이프를 보시면은 보시면 앞에 1이라는 숫자를 볼 수가 있는데 이거는 편의상 저희가 매트릭스로 표현하기 위해서 항상 앞에 배치 사이즈를 보셔도 되긴 하는데 1이 붙어 있는 거고 실제로는 벡터인 거죠. 원디맨셔널 벡터인 거고 127만 디멘션의 벡터가 나온 걸 볼 수가 있습니다. 이제 그다음에 마지막으로는 결과를 계산하게 될 텐데요. 결과는 심플하게 이너 프로덕트를 계산하시면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 6,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 1089,
      "char_count": 2016
    },
    {
      "id": "transcript_mrc_mrc_4강passage_retrieval_sparse_c007_3eae64",
      "content": "[MRC] [MRC] (4강)Passage Retrieval-Sparse Embedding\n\n다. 같은 벡터 라이즈를 활용을 해서 자 그러면 쿼리 벡터를 쉐이프를 보시면은 보시면 앞에 1이라는 숫자를 볼 수가 있는데 이거는 편의상 저희가 매트릭스로 표현하기 위해서 항상 앞에 배치 사이즈를 보셔도 되긴 하는데 1이 붙어 있는 거고 실제로는 벡터인 거죠. 원디맨셔널 벡터인 거고 127만 디멘션의 벡터가 나온 걸 볼 수가 있습니다. 이제 그다음에 마지막으로는 결과를 계산하게 될 텐데요. 결과는 심플하게 이너 프로덕트를 계산하시면 됩니다. t는 트랜스포즈죠 그다음에 리조트 세이프를 보시게 되면은 9606개의 숫자가 나온 거죠. 즉 각각의 지문과 현재 질문의 유사도를 9606개의 숫자로 나타낸 거고요. 그다음엔 당연히 여기서 가장 높은 숫자가 나온 걸 찾으면 되겠죠 그래서 저희가 솔트를 하고요. 여기서 클수록 좋다 보니까 저희가 오름차순으로 솔트를 할 거면 마이너스로 이렇게 앞에 어플라이를 해줘야 되고요. 그다음에 실제 스코어 같은 경우는 이 솔트 리졸트가 인덱스이기 때문에 인덱스를 다시 그 데이터에 인덱싱을 해서 스코어를 가져오게 되고 다 아이디즈 같은 경우는 이렇게 구할 수 있습니다. 그러면 여기서 예를 들면 저희가 탑 3개만 한번 볼까요? 탑 3개를 보시게 되면 k를 3로 한번 하시고 d 스쿼즈에서 k를 가져오고 d 아디즈를 3개를 가져오게 되면 보시면 첫 번째 세개의 문서는 아이디가 7405번 6315번 그리고 5135번 그리고 1165번인 걸 알 수 있고 각각 스코어가 0.18 0.036 0.033인 걸 알 수가 있습니다. 그다음에 이제 저희가 이거를 한번 프린트를 해볼 텐데요. 프린트 하는 건 제가 그냥 카피 앤 페이스트에서 프린트하는 루틴을 보여드리도록 할게요. 이제 저희가 실제로 해당 쿼리에 해당하는 문서가 어떤 문서인지 내용까지 같이 보여주는 그냥 코드 스니펫인데 보시면은 저희가 방금 냈던 결과에 대한 해석이죠. 저희가 퀘스천을 날렸던 거는 호메르스 참가율 신통계에 비해 간결한 서사로 간주한 사람은 누구인가라는 질문을 날렸고 실제로 tfidf를 통해서 retive 된 문장을 보면은 탑 1이 정확하게 맞췄죠. 고전 시대 신화에서는 티탄들의 패배 이후라는 지문을 가져왔고 이게 탑 1 패시지인데 실제로 저희가 정답으로 원래 알고 있던 패세지도 똑같은 패시지인 걸 알 수가 있습니다. 이처럼 tfif가 잘 작동했다는 걸 알 수가 있고요. 네 오늘 실습은 여기서 마치도록 하겠습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (4강)Passage Retrieval-Sparse Embedding.json",
        "lecture_name": "[MRC] (4강)Passage Retrieval-Sparse Embedding",
        "course": "MRC",
        "lecture_num": "4강",
        "lecture_title": "Passage Retrieval-Sparse Embedding",
        "chunk_idx": 7,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3f8e384e1f78256f3a539f6db5cc935f806cf3707897edb35b7b29725d2995db"
      },
      "token_estimate": 648,
      "char_count": 1246
    }
  ]
}