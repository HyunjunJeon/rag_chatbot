{
  "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
  "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
  "course": "AI Math",
  "total_chunks": 13,
  "chunks": [
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c000_65261c",
      "content": "[강의 녹취록] 과목: AI Math | 강의: 2강 | 제목: 선형대수-1_RegressionandNNClassifier\n\n여러분 안녕하세요 어 김수경입니다. 저번 시간에 저희가 엠의 라이프 사이클에 대해서 개괄적인 아이디어를 배웠습니다. 머신 러닝 모델을 어떻게 만들고 그 모델을 어떻게 실제 프로덕트의 인플리멘트에서 배포하는 데까지를 배웠습니다. 오늘은요 본격적으로 이제 머신 러닝 모델의 테크니컬한 부분에 대해서 배우도록 하겠습니다. 머신 러닝 모델의 가장 기초가 되는 모델 중에 어 선형 대수에 기반이 된 회귀 모델과 클래스 파이어 모델이 있는데요. 어 이번 시간에서는 그 가장 기초가 되는 모델인 리니어 리그레션 모델이랑 그 리니어 리그레션 모델을 확장시켜서 뉴럴넷 컴포넌트를 합한 그 뉴럴넷 클래스 파이어 모델을 배우도록 하겠습니다. 먼저 리니어 리그레션 모델부터 시작하도록 하겠습니다. 리그레션이란 무엇일까요? 영어로 리그레스라는 거는요 회귀라는 의미를 가집니다. 그래서 리그레스라는 건요 기본적으로 과거의 상태로 돌아가는 것이라는 의미를 가집니다. 근데 이 리그레션 모델은요 실은 영국의 어떤 유전학자인 프랜시스 골턴이라는 분께서 부모의 키랑 자식의 키에 어떤 상관관계를 연구하는 데에서부터 그 기원을 가지고 있습니다. 그래서 요 그 과학자분께서 부모의 키가 자녀의 키랑 상관관계가 있을 거라는 가정 하에 이 리그레션 모델을 갖다가 만들었습니다. 그래서 부모의 키랑 자녀의 키가 사실은 선형적인 관계가 있다라는 가정을 세웠습니다. 부모의 키가 크면 자식의 키도 크고 부모의 키가 작으면 자식의 키도 작고 이렇게 선형적인 관계가 있다고 가정을 하고 모델을 만들었는데 실은 그 연구의 결과는요 키가 커지거나 작아지는 그런 관계보다는 사실은 그 자식의 키가 전체 키의 어떤 평균으로 돌아가려는 경향이 있다는 그런 발견을 했습니다. 부모 키랑 엄마랑 아빠의 키가 있잖아요. 그 키의 평균으로 자식이 이렇게 연관성이 있다라는 결과를 재미있는 결과를 발표를 했습니다. 그래서 이렇게 기원을 둔 어떤 선형적 관계를 가정으로 인풋과 아웃풋 즉 어떤 그 이 여기서는 인풋은 부모의 키가 되겠죠 그리고 아웃풋은 자식의 키가 되겠죠. 이 인풋과 아웃풋의 관계를 선형적인 관계로 모델 시키는 게 요 리니어 리그레션 모델입니다. 자 그래서 이 리그레션 어널리시스에 대해서 다시 한번 요 컴포넌트들을 이렇게 분석해 보면요. 회귀 분석 리니어 어널리시스라는 건요. 기본적으로 어떤 우리가 관찰된 우리가 옵저브한 어떤 연속적인 변수에 대해서 연속적인 파라미터에 대해서 두 변수 사이에 모형을 구한 다음에 그거의 적합도를 측정하는 분석 방법이라고 생각하면 됩니다. 즉 쉽게 말해서 인풋과 아웃풋의 관계를 갖다가 학습하는 과정입니다. 인풋이 있고 아웃풋이 있을 때 인풋과 아웃풋은 어떤 관계가 있을지 이거를 갖다가 리그레션 하는 거예요. 이거를 분석하는 거예요. 그래서 우리가 이렇게 어려운 말들을 쓰고 있는데 변수 변수란 기본적으로 변하는 숫자예요. 값이 변하는 데이터의 요소나 속성 변하는 수를 변수라고 그래요. 예를 들어서 아까 그 부모의 키랑 자식의 키에 대한 연관관계를 분석하는 케이스에 대해서 변하는 값은 뭘까요? 그러니까 부모의 키랑 자식의 키가 되겠죠. 부모의 키가 변하면 자식의 키도 변한다 이게 변하는 수죠. 그게 변수가 되겠고 이 변수는 독립 변수랑 종속 변수가 있어요. 그래서 소위 말하면 이렇게 독립변수 종속 변수 하면은 말이 어렵지만 독립 변수는 인풋이고 종속 변수는 아웃풋입니다. 그래서 독립 변수는 결과의 원인이라고 써 있는데 x라고 되어 있죠 인풋입니다. 인풋이 독립 변수고 그다음에 종속 변수는 아웃풋이에요. 독립 변수에 따라서 달라지는 이 아웃풋을 종속변수 y라고 합니다. 그래서 실제로 텍스트 북 같은 데 보면요. 독립변수 종속변수 이런 말은 잘 안 쓰고 그 x랑 와 이렇게 표기를 합니다. 인풋 x를 통해서 y 아웃풋을 예측을 하는 게 리그레션입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 0,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 1062,
      "char_count": 1942
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c001_c4a930",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 그래서 독립 변수는 결과의 원인이라고 써 있는데 x라고 되어 있죠 인풋입니다. 인풋이 독립 변수고 그다음에 종속 변수는 아웃풋이에요. 독립 변수에 따라서 달라지는 이 아웃풋을 종속변수 y라고 합니다. 그래서 실제로 텍스트 북 같은 데 보면요. 독립변수 종속변수 이런 말은 잘 안 쓰고 그 x랑 와 이렇게 표기를 합니다. 인풋 x를 통해서 y 아웃풋을 예측을 하는 게 리그레션입니다. 그래서 다시 이제 또 정의로 돌아가면은요 리니어 리그레션이라고 나와 있는데 아까 우리가 리그레션 모델을 배웠잖아요. 여기 앞에 리니어가 붙었어요. 리니어는 뭐예요? 선형 모델이죠 그래서 리니어 리그레션은요 기본적으로 선형 모델을 이용해서 리그레션 모델을 푸는 겁니다. 간단하죠. 그래서 한국말로 하면은 이 선형 획이라고 그러는데 이 리뉴얼 리그렉션은 이 종속 변수 하나와 독립 변수 하나의 관계를 모델링하는 그러니까 종속 변수 하나 이상이라고 그랬죠. 그러니까는 인풋 x가 하나 이상입니다. 인풋 x가 뭐 두 개가 될 수 있고 3개가 될 수 있고 한 기드가 될 수도 있어요. 그리고 독립 변수는 하나죠 와는 하나죠. 그래서 인풋에 따라서 아웃풋의 관계를 모델링하는 이 종속 변수에 따라서 독립 변수의 관계를 모델링하는 통계적 방법을 리니어 리그레션이라고 합니다. 여기서 키가 되는 건 리니어예요. 선형 모델을 이용해 가지고 인풋과 아웃풋의 관계를 우리가 모델링 하는 거죠. 자 그래서 여기 쓰인 것과 같이 독립 변수의 값을 기반으로 우리가 종속 변수의 값을 예측을 하게 됩니다. 즉 스를 갖다가 넣어 가지고 와를 예측하게 되는 거죠. 우리가 이 리니어 모델을 만들면 어떤 스가 들어와도 우리가 와를 예측을 할 수가 있겠죠. 그래서 독립 변수의 값을 기반으로 종속 변수의 값을 예측을 한다. 그래서 여기 그 예시를 보면 좀 더 이해가 쉬워지실 거예요. 집의 크기랑 위치랑 침수수 같은 특성을 기반으로 집값을 예측하는 테스크를 생각을 해볼게요. 집값은 우리가 이제 기본적으로 생각할 때 집의 크기가 크면 집값이 비싸겠죠 그리고 위치가 좋으면 집값이 비싸겠죠 그리고 침실 수가 많으면 집값이 비싸겠죠 그래서 여기서 보면 이 독립 변수는 뭐겠어요? 인풋은 기본적으로 여기 그 아웃풋인 집값에 영향을 주는 어떤 변수들 여기에서 이그 샘플에서 보는 거는 집의 크기랑 위치랑 침실 수가 이 종속 변수 x가 되겠죠. 그리고 y는 뭐예요? 집 값이죠 그렇죠 이렇게 인풋을 통해서 아웃풋을 예측을 할 수가 있습니다. 근데 이 리니어 모델이기 때문에 이렇게 리니어하게 이 관계가 선형적인 특성을 가진다고 가정을 하는 거예요. 자 그러면 이제 리니어 모델 얘기가 나왔으니까 이 모델의 그 그 수학적인 시의 모양을 한번 보도록 하겠습니다. 우리가 예전에 그 고등학교 때 어 그 선형 모델을 배웠을 거예요. 와 콜 엠스플러스 비라는 선형 모델 기본적인 선형 모델을 생각할 때 이거를 갖다가 2차원 평면에서 표현하면 어떻게 될까요? 예를 들어서 이 가로축이 x고 이 세로축이 y라고 그러면은 yec mx 플러스 b는 뭐예요? 여러분 다 아시겠지만 이 m은 기울기죠 요 선의 기울기 그리고 비는 뭐예요? 비는 절편이죠. 여기 절편 y 절편 왜냐 스가 0이면은 와 콜 비니까 그래서 0 콤마 비를 항상 지나게 될 거예요. 그래서 이렇게 어떤 선형적인 식으로 표현을 한 게 이 하나의 선으로 표현이 됩니다. 그래서 여기서 보여준 것처럼 y 이퀄 종속 변수 목표 즉 아웃풋이 되겠죠. 그리고 m은 요 y 이콜 엠스 플러스 비에서 m은 이 직선의 기울기라고 그러죠. 그래서 한국말로는 회기 개수라고도 부릅니다. 그리고 또 스는 인풋 독립 변수가 되겠고 비는 y 절편 상수항이 됩니다. 그래서 이 식을 이용해서 모델링을 할 거예요. 선형 모델이니까 근데 이 선형 모델을 만들 때 이 네 가지 가정을 만족시켜야 돼요. 선형성 독립성 등분산성 정규성 하나씩 보도록 할게요. 먼저 선형성이 제일 중요합니다. 즉 종속 변수와 독립 변수 간의 관계가 선형적이어야 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 1,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 1075,
      "char_count": 2012
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c002_58f9de",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 그리고 또 스는 인풋 독립 변수가 되겠고 비는 y 절편 상수항이 됩니다. 그래서 이 식을 이용해서 모델링을 할 거예요. 선형 모델이니까 근데 이 선형 모델을 만들 때 이 네 가지 가정을 만족시켜야 돼요. 선형성 독립성 등분산성 정규성 하나씩 보도록 할게요. 먼저 선형성이 제일 중요합니다. 즉 종속 변수와 독립 변수 간의 관계가 선형적이어야 됩니다. 즉 y랑 스의 관계가 선형적으로 표현이 될 수 있어야 돼요. 예를 들어서 아까 말씀드린 것처럼 부모님의 키랑 자식의 키는 선형적인 관계가 있을 거라고 생각이 되잖아요. 부모가 키가 크면 자식도 키가 크고 부모가 키가 작으면 자식도 키가 작을 것이다. 이런 선형적인 관계가 있을 것이라고 생각이 되잖아요. 그래서 이런 선형성을 가정을 해야지 우리가 리니어 모델을 모델을 모델링하는 데 쓸 수가 있어야 돼요. 그래서 가장 중요한 가정은 이 종속 변수와 독립 변수 와랑 스 관계의 관계가 선형적인 관계가 있어야 된다 이게 첫 번째 가정입니다. 자 두 번째는 독립성입니다. 즉 관측값들은 서로 독립적이어야 됩니다. 여기서 관측 값들은 뭐냐면은 어 스랑 와의 어떤 하나하나의 값들입니다. 부모의 키랑 자식의 키 이런 게 관측 값이 되겠죠 여기서 잔차라고 그러는데 이 잔차가 뭐냐면 영어로 레지듀얼이에요. 그래서 오차의 추정치라고 생각을 하시면 됩니다. 우리가 관측하는 y가 있을 거 아니에요 자식의 키 아까 그 앞에 이그 샘플에 의하면 자식의 키가 되겠죠. 그리고 그 예측 값은 모델로 예측한 자식의 키 가 됩니다. 그래서 이 잔차는 예측값과 실제 그라운드 트루스 실제 값의 차이입니다. 근데 여기 이렇게 써 있죠 잔차들이 무작위로 분포되어 있어야 합니다. 시간의 흐름에 따라 잔차가 특정 패턴을 보이면 독립성 가정이 위배됩니다. 즉 무슨 말이냐 하면은 우리가 어떤 관측값을 관측할 때 시간의 영향을 받으면 안 된다는 거예요. 그렇죠 그래서 예를 들어서 뭐 시간에 영향을 받는 게 어떤 게 있을까요? 뭐 온도의 변화 이런 게 있을 수 있겠죠 그래서 온도의 변화는 당연하게 우리가 그 온도를 갖다가 뭐 시간 단위로 관측을 하면은 시간을 1시에 측정을 하고 2시에 측정하면은 2시에 측정한 온도가 1시 측정에 한 온도에 당연히 디펜드 되잖아요. 이렇게 독립성이 가정되지 않습니다. 이렇게 시간적인 어떤 잔차의 패턴이 있으면 안 됩니다. 기본적으로 관측 값들은 서로 독립적이어야 되고 그다음에 등분산성 이건 뭐냐 하면은 오류의 분산이 일정해야 됩니다. 즉 잔차의 분산이 일정한 분포를 보여야 됩니다. 즉 우리가 이렇게 그 레지듀얼을 요 잔차를 갖다가 데이터에 따라서 이렇게 디스트리뷰션을 그려보면은 특정 구간에서 이 레지듀얼이 분산이 커지거나 작아질 수도 있는데 그렇게 되면은 요 등분산성에 위반이 됩니다. 뒤에서 그래프를 보여드리면서 좀 더 설명드릴 거예요. 그리고 이 정규성은 이 잔차들이 노멀 디스트리뷰션을 따라야 됩니다. 이 레지듀얼이 우리가 이 레지듀얼를 갖다가 그러니까 오차 그 오차를 갖다가 디스트리뷰션으로 매핑을 해보면은 그 매핑이 노멀 디스트리뷰션을 따라야 됩니다. 근데 예를 들어서 우리가 어떤 시간적인 코릴레이션이 있다거나 이 관측 값들이 독립되지 않으면 이 잔차들도 노멀 디스트리뷰션을 따르지 않게 돼요. 이게 이건 우리가 이제 관측 값을 우리가 데이터를 측정을 해 가지고 모은 다음에 분석을 해보면은 이것들이 다 맞아떨어지는지를 확인을 할 수 있습니다. 그래서 이게 맞아떨어지면은 우리가 모델을 갖다가 리니어 모델로 리니어 리그레션 모델로 가정을 하고 모델링을 해도 된다는 거죠. 자 그래서 그래프를 보여드리면은 먼저 선형성 요 x랑 y가 여기서 가로축이 x고 세로 축이 y죠. 가로축이 인풋이고 세로축이 아웃풋인데 그거의 관계가 이렇게 선형적인 특성을 가져야 되고 이 잔차들이 시간에 따라 바뀌지 않고 예를 들어서 여기 이 독립성의 그래프는 요 가로축이 시간이고 y축이 관측 값입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 2,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 1072,
      "char_count": 1978
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c003_3b0800",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 그래서 이게 맞아떨어지면은 우리가 모델을 갖다가 리니어 모델로 리니어 리그레션 모델로 가정을 하고 모델링을 해도 된다는 거죠. 자 그래서 그래프를 보여드리면은 먼저 선형성 요 x랑 y가 여기서 가로축이 x고 세로 축이 y죠. 가로축이 인풋이고 세로축이 아웃풋인데 그거의 관계가 이렇게 선형적인 특성을 가져야 되고 이 잔차들이 시간에 따라 바뀌지 않고 예를 들어서 여기 이 독립성의 그래프는 요 가로축이 시간이고 y축이 관측 값입니다. 이 관측 값이 시간에 따라 영향을 받지 않고 이렇게 스토캐스틱하게 랜덤하게 분포해야 된다는 거고 등분산성은 우리가 레디 듀얼를 갖다가 볼 때 예를 들어서 이 그래프에서 x축은 인풋이고 즉 어 가로 축은 스고 y축은 아웃풋입니다. 저희의 그 예측 값이죠. 이거를 갖다가 플롯을 했을 때 우리가 원하는 그 모델 실제 그 관측 값은 이 점들이고 그리고 모델로 매핑한 거는 이 일직선이죠. 그러면은 잔차는 뭐겠어요? 이 실제의 관측값과 우리가 예측한 값 간의 차이가 되겠죠. 이 점과 일직선 사이의 거리 이 거리들이 등 분산을 가지고 있어야 된다는 거죠. 이게 어떤 구간에서는 크고 작지가 않고 이렇게 고르게 잔차가 분포해야 된다는 거죠. 그리고 정균성은 또 그 잔차들이 노멀 디스트리뷰션을 가지고 있다는 거죠. 그래서 잔차를 갖다가 플롯을 해보면은 이거는 이제 y축이 어 이 세로축이 잔차가 되고 이 x축이 관측한 시간이 되겠죠. 그거를 이렇게 플롯 했을 때 이렇게 랜덤하게 분포해야 된다는 거죠. 그래서 실제로 이 가정들이 맞아떨어지는지를 볼 수 있는 가장 좋은 방법은 이런 식으로 데이터를 갖다가 시각화하는 방법입니다. 이렇게 시각화하면은요 각각의 가정들을 패스했는지 우리가 쉽게 알 수가 있어요. 자 그래서 이런 과정을 통과했다 그러면은 이 모델을 어떻게 튜닝할 건데라는 질문이 있을 수가 있습니다. 이렇게 모델을 어떻게 학습할 건데 그거는 이제 우리가 어 여러 가지 방법들을 여러 가지 옵티마이제이션 최적화 방법들을 쓰는데 다음 시간에 배우게 될 거예요. 근데 여러 방법들 중에 어떤 것들이 있는지 어떤 것들이 많이 이용되는지 여기서 간략하게만 보여드릴게요. 먼저 이 최소 제곱법 방법이 많이 쓰입니다. OLS라고 쓰여있는데 odnr 리스트 스퀘어라는 거예요. 그래서 기본적으로 이 YI YI는 우리의 관측 값입니다. 우리가 관측한 어떤 데이터죠? 그리고 이 두 번째 텀 mx 플러스 b가 우리의 선형 모델이죠. 즉 이거는 우리 모델이 예측한 값이 될 거예요. 그래서 실제 값 빼기 관측 값을 해가지고 그 차이의 제곱을 해서 다 더합니다. 그렇죠 그래서 그러면은 이게 이 값은 어떤 의미를 가지게 되겠어요? 실제 관측값이 우리의 예측값과 얼마나 틀린지를 어떻게 메이저 하는 그런 어떤 정량적인 어떤 어떤 매트릭이 되겠죠. 그래서 이 숫자를 갖다가 최소화하는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 3,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 775,
      "char_count": 1449
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c004_c4d8fc",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 우리가 관측한 어떤 데이터죠? 그리고 이 두 번째 텀 mx 플러스 b가 우리의 선형 모델이죠. 즉 이거는 우리 모델이 예측한 값이 될 거예요. 그래서 실제 값 빼기 관측 값을 해가지고 그 차이의 제곱을 해서 다 더합니다. 그렇죠 그래서 그러면은 이게 이 값은 어떤 의미를 가지게 되겠어요? 실제 관측값이 우리의 예측값과 얼마나 틀린지를 어떻게 메이저 하는 그런 어떤 정량적인 어떤 어떤 매트릭이 되겠죠. 그래서 이 숫자를 갖다가 최소화하는 겁니다. 즉 여기 코스트라고 그랬죠. 이 코스트라는 것을 이 코스트 이퀄 우리의 실제 관측값 빼기 예측값의 제곱을 다 합한 거 그거를 갖다가 우리가 미니마이즈 하는 거예요. 그렇게 미니마이즈 하면은 이 선이 이 빨간 선 우리의 모델이 우리가 관측한 이 노란 점들 이 노란 점에 가깝게 매핑되는 식으로 모델이 트레이닝이 되겠죠. 즉 모델이 트레이닝 되는 거는 이 매개 변수인 엠이랑 비를 우리가 정하는 거겠죠. 그렇죠 이렇게 함으로써 우리가 요 m이랑 b의 최저 값을 정해서 모델을 만들 수 있는 거죠. 이 다중 선형 회귀 모델은 뭐냐면요. 멀티플 리니얼 리그레션이라고 그러는데 아까랑 똑같아요. 아까랑 똑같은 선형 모델인데 인풋만 여러 개 있는 거예요. 아까 앞에서 봤던 예시처럼 우리가 집 값을 갖다가 모델링할 때 여러 가지 파라미터들이 영향을 줄 수가 있어요. 예를 들어 집 크기라든지 침실 수 집값 이런 여러 가지 그러니까 이 집 값을 예측하는 데 집 크기뿐만 아니고 침실 수까지 영향을 가질 수가 있죠. 이거 같은 경우에는 인풋이 두개죠. 집 크기랑 침실수 이게 x1이고 x2라고 이렇게 매핑을 할 수 있겠죠. 그리고 집 값은 y죠. 이런 경우에 y는 하나인데 x는 여러 개예요. 그러면은 여전히 우리가 선형 모델을 이용해 가지고 아까랑 똑같은 형태로 y 이퀄 b 플러스 mx처럼 b 플러스 m이 여러 개죠. 이거 같은 경우에는 x가 여러 개니까 이 x의 각각의 파라미터 앞에 다른 그 매개 변수를 곱해 가지고 다중 선형 회귀 모델 멀티플 리니얼 리그레션 모델로 만들 수가 있다라는 겁니다. 자 이렇게 모델을 평가를 했으면은 이 모델을 갖다가 평가를 해야 되잖아요. 우리가 아까 앞에서 머신 러닝 라이프 사이클에서도 배웠듯이 모델만 만들면 끝 끝이 아니고 이 모델이 실제로 어떻게 잘 동작하는지 우리가 모델 평가 지표를 정해야 돼요. 그렇게 평가 지표를 정하는 여러 가지 방법들을 또 알려드리도록 하겠습니다. 첫 번째는 평균 절대 오차 민 앱솔루트 에러입니다. 이거는 별개 없어요. 기본적으로 우리 와아 우리의 측정값과 그리고 와아 프라임 요게 와 아이 헤이네요. 여기에서는 와아 햇이 예측 값입니다. 실제 값과 예측 값이 얼마나 차이가 나는지를 그냥 그것들을 서로 이렇게 차이를 계산해 가지고 절대값을 씌워줍니다. 그러면은 실제 값과 예측값의 차이가 이렇게 음수가 되지 않게 이렇게 나오겠죠. 그래서 그것들을 평균을 내주는 거예요. 그게 그냥 말 그대로 민 앱솔루트 에로가 되겠죠. 즉 예측값이 실제 값과 얼마나 차이가 나는지 절대 값으로 계산해서 평균화한 지표입니다. 그래서 이거의 장점은요. 모든 오차를 동일하게 고려하기 때문에 해석이 간단해요. 그쵸 그리고 단위가 종속 변수와 동일합니다. 종속 변수 y의 유닛과 요 에러의 유닛이 동일하기 때문에 해석이 쉽죠. 그리고 또 다른 방법은 윈 스케어드 에러라는 게 있어요. 이건 뭐냐면은 말 그대로 평균 제곱 오차입니다. 위랑 똑같이 에러를 갖다가 평균을 내되 에러의 제곱을 해가지고 평균을 합니다. 이 식을 보시면 알 수 있죠. 요 에러에 와아 빼기 와아 햇에 예측값과 실제 값의 차이를 제곱을 해가지고 평균을 냅니다. 이거의 장점 이거의 장점은 뭐냐면요. 오차가 크면 클수록 제곱을 해주니까 에러가 훨씬 커져요. 그렇죠 예를 들어서 2를 제곱하면 4가 되지만 3을 제곱하면 9가 되고 10을 제곱하면 100이 되잖아요 이렇게 에러가 크면 클수록 제곱을 하면서 그 에러를 훨씬 더 크게 뻥 튀겨 가지고 이렇게 계산을 하게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 4,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 1079,
      "char_count": 2025
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c005_1f8883",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 위랑 똑같이 에러를 갖다가 평균을 내되 에러의 제곱을 해가지고 평균을 합니다. 이 식을 보시면 알 수 있죠. 요 에러에 와아 빼기 와아 햇에 예측값과 실제 값의 차이를 제곱을 해가지고 평균을 냅니다. 이거의 장점 이거의 장점은 뭐냐면요. 오차가 크면 클수록 제곱을 해주니까 에러가 훨씬 커져요. 그렇죠 예를 들어서 2를 제곱하면 4가 되지만 3을 제곱하면 9가 되고 10을 제곱하면 100이 되잖아요 이렇게 에러가 크면 클수록 제곱을 하면서 그 에러를 훨씬 더 크게 뻥 튀겨 가지고 이렇게 계산을 하게 됩니다. 그래서 그렇게 함으로써의 장점이 뭐냐면은요 모델이 오차가 크면 클수록 더 큰 패널티를 갖게 돼요. 그래서 모델이 큰 오차를 줄이는 데 유용을 합니다. 오차가 작으면 작은 패널티를 주고 오차가 크면 더 큰 패널티를 주게 되는 거죠. 그런데 이거의 단점은 뭐냐면요 우리가 y의 제곱을 갖다가 계산을 하잖아요. 그래서 그 에러의 유닛이 그 y의 유닛과 다릅니다. 출력 값의 유닛과 에러의 유닛이 다릅니다. 그래서 단위가 제곱 탭 형태라서 실제 값과 그 오차의 유닛이 다를 수가 있다라는 단점이 있습니다. 자 다른 평가 지표들도 많은데요. 실제 그 머신 러닝 모델에서 이 두 개가 많이 쓰이기 때문에 살펴봤고 이제 본격적으로 이제 그 분류 모델인 클래스 파이어 모델을 몇 개를 보도록 할게요. 어 클래식 파이어 모델을 어 제가 가르칠 때 가장 먼저 가르치는 게 요 리얼리스트 네이블 클래식 파이어입니다. 가장 어 굉장히 그 컨벤셔널 한 머신러닝 모델 중에 가장 기본적인 머신 러닝 모델 중에 하나이고 이거의 단점을 보완한 게 뉴럴넷 클래스 파이어입니다. 그래서 먼저 리얼리스트 네이버 클래스 파이어부터 보도록 하겠습니다. 이 리얼리스트 네이버 클래시파이어는요 되게 되게 간단해요 한 페이지로 설명이 가능합니다. 자 기본적으로 어 제가 질문을 드릴게요 여기 그 세 가지의 그 점들이 이렇게 나와 있죠 첫 번째 문제 첫 번째 문제 이렇게 점들이 이렇게 분포할 때 여기 퀘스천 마크가 있죠 만약에 이 퀘스천 마크에 해당하는 데이터 셋을 어떤 색이라고 물어보면 뭐라고 대답하실 거예요 파란색이죠 당연히 왜냐하면 여기 파란색이 모여 있으니까 그렇죠 여기 자 여기는 여기는 뭐예요? 당연히 빨간색이겠죠. 왜냐하면 여기 빨간색들이 모여 있으니 그리고 여기는 또 초록색일 가능성이 많다고 생각이 되죠. 왜냐하면 여기 주위에 초록색이 많으니까 자 그럼 이제 두 번째 좀 더 어려운 문제를 볼게요. 자 여기에서 요 퀘스천 마크 여기에 있는 점이 무슨 색이라고 물어보면 뭐라고 대답하실 거예요? 이거는 뭐 여기서 보여주시는 것처럼 근처에 파란색도 있고 빨간색도 있고 초록색도 있죠 그래서 애매하단 말이에요. 그래서 이런 거 같은 경우에는 그럼 가까이에 있는 거를 하나만 보지 말고 예를 들어 3개를 보자 그러면은 제일 가까운 점이 예를 들어 이 파란색 빨간색 빨간색이라고 해봐요. 그럼 빨간색이 2개 있잖아요 그럼 얘는 이렇게 빨간색으로 판단이 되는 겁니다. 그렇죠 자 그럼 가장 어려운 케이스 이 세 번째 걸 볼게요. 여기에서 이 점이 이 퀘스천 마크에 있다면은 얘는 뭘까요? 파란색이죠. 왜냐하면 가까이 있는 게 파란색 점이니까 그렇죠 만약에 이 퀘스천 마크에 점이 있다면 얘는 무슨 색이라고 대답하실 거예요 아마도 빨간색 왜냐하면은 빨간색에 가까이 있으니까 자 어려운 거 이거가 여기 퀘스천마크에 점이 있다면 뭐가 될까요? 이 점은 실은 빨간색과 파란색과 초록색이 다 같은 거리에 있어요. 그럼 이건 애매하단 말이에요. 이렇게 제가 이렇게 질문하고서 이제 각각의 퀘스천 마크에 있는 데이터 포인트들의 색깔을 물어봤는데 여러분들이 이 문제를 풀면서 했던 어떤 그 그 띵킹이 사실은 이 리얼리스트 네이버 클래스 파이어 알고리즘이다. 기본적으로 리얼리스트 네이버 클래스 파이어는요 말에서 보는 것처럼 그대로 리얼리스트 네이버를 봅니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 5,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 1061,
      "char_count": 1963
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c006_7c6053",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 기본적으로 리얼리스트 네이버 클래스 파이어는요 말에서 보는 것처럼 그대로 리얼리스트 네이버를 봅니다. 즉 어 그 어떤 데이터셋이 퀘스천 마크에 로케이트 되어 있을 때 이 데이터셋의 클래스를 클래스 파이게 하기 위해서 우리가 뭘 하냐면요. 가장 가까운 g의 k개의 데이터를 봅니다. 예를 들어 그 케이는 하나가 될 수도 있고 2개가 될 수도 있고 3개가 될 수 있죠. 그 g에 있는 가장 가까운 k개의 데이터를 보고 그 케이의 데이터의 레이블 중에 가장 다수결로 많은 데이터의 레이블을 갖다가 내 레이블이라고 택하는 겁니다. 다시 이 리얼리스트 네이버 클래시 파이어는요 어떤 쿼리 어떤 테스트 데이터 포인트에 대해서 가장 가까운 가장 가까운 케이계의 학습 데이터 포인트를 보고 그 케이계의 학습 데이터 포인트가 가지고 있는 레이블을 이용해서 그 레이블의 다수결로 나의 레이블을 예측을 하는 겁니다. 그래서 한 예로 예를 들어서 뭐 여기 어려운 케이스 중간에 있는 퀘스천 마크를 봅시다. 여기에서 케이가 3이라고 생각을 해봐요. 가장 가까운 3개의 데이터를 볼 거예요. 얘랑 얘랑 얘 그러면은 얘는 파란색 빨간색 빨간색이죠. 그러면은 다수결이 뭐예요? 빨간색이잖아요. 그래서 다수결이니까 이 퀘스천 마크에 있는 데이터는 빨간색이라고 예측을 하는 거예요. 간단하죠. 네 그래서 얘를 이제 코드로 한번 써볼게요. 그 머신 러닝 알고리즘은요. 기본적으로 두 개의 페이지로 나누어서 이루어집니다. 먼저 트레이닝 페이지가 있고 그다음에 프리딕트 페이지가 있어요. 트레이닝은 모델의 레이블드 된 데이터를 이용해서 모델을 학습시키는 과정이고 프리딕트는 완전히 트레이닝이 다 된 모델을 가지고 와서 여기 있는 모델의 인풋이죠. 모델과 인풋을 가지고 와서 그 인풋을 모델을 이용해 가지고 프리딕션 레이브를 예측을 하는 게 프리딕션 어 페이지입니다. 자 이 리얼리스트 네이버 클래스 파이어 같은 경우에 이 트레이닝 페이지에 뭘 할까요? 딱히 할 게 없어요. 왜냐면은 이 트레인이라는 거는 사실 모델을 학습시키는 건데 리얼리스트 레니b 클래스 파이어가 하는 일은 가장 가까운 케이크의 학습 데이터만 보는 거잖아요. 그래서 이 학습 과정에서 실제로 리얼리스트 네이버 클레이스 파이어가 하는 일은 모든 데이터랑 레이블 페어를 기억하는 것밖에 없어요. 그래서 이 리얼리스트 네이버 클래스 파이어를 레이지 메소드라고 합니다. 왜냐하면 트레이닝 과정에서 학습 과정에서 할 일이 없기 때문에 그저 그냥 인풋과 아웃풋을 갖다가 페어로 만들어서 저장하는 일만 하죠. 하지만 이 프리딕션 타임에는 할 일이 많아요. 왜냐 이 학습 단위에서 인풋과 아웃풋을 다 기억을 한 다음에 이 프리딕션 타임에서 쿼리 이미지가 들어오면은 이 이미지를 갖다가 모든 학습 데이터를 스캐닝을 하면서 가장 가까운 그 학습 데이터를 찾아야 되죠. 그래서 이 프리딕션 타임에는 가장 유사한 훈련 데이터에 가장 시뮬러한 트레이닝 데이터를 의 라벨을 출력하는 과정을 가지게 됩니다. 그래서 이 뉴얼리스트 네이버 클래스 파이어는 트레이닝 과정에서 할 일이 없지만 프리딕션 과정에서 할 일이 많습니다. 자 그래서 오케이 이렇게 하면은 좀 어느 정도 클래시피케이션이 잘될 것 같아요. 하지만 리얼리즘 네이버를 실제로 쓰는 경우는 거의 없습니다. 왜냐하면 굉장히 많은 문제가 있기 때문입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 6,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 914,
      "char_count": 1681
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c007_869f0b",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 그래서 이 뉴얼리스트 네이버 클래스 파이어는 트레이닝 과정에서 할 일이 없지만 프리딕션 과정에서 할 일이 많습니다. 자 그래서 오케이 이렇게 하면은 좀 어느 정도 클래시피케이션이 잘될 것 같아요. 하지만 리얼리즘 네이버를 실제로 쓰는 경우는 거의 없습니다. 왜냐하면 굉장히 많은 문제가 있기 때문입니다. 먼저 하나의 이그 샘플로 하나의 이그 샘플 테스크로 우리가 어 그 이미지를 클래시피케이션 하는 그런 이미지 분류 테스크를 한번 생각을 해볼게요. 이 테스크에서 우리가 하고자 하는 거는 이렇게 예를 들어서 예쁜 고양이 이미지가 들어왔을 때 이 고양이 이미지를 고양이라고 분류하는 클래시피케이션 하는 이런 테스크를 할 거예요. 자 그러면은 이거를 갖다가 리얼리스트 네이버 클래시피케이션 알고리즘을 이용해서 한번 해볼게요. 그러면 우리가 하는 일은 이렇게 이미지가 들어왔을 때 이 쿼리랑 가장 가까운 트레이닝 이그젬프를 찾아 가지고 그게 어떤 레이블로 되어 있는지를 알아야 되죠. 그렇죠 그게 리얼리스 레이블 클래시피케이션이니까 그러면은 이 이미지를 가지고 와서 이 예를 들어서 이게 트레이닝 데이터셋이라고 그러면은 트레이닝 데이터 셋에 있는 이미지들이랑 이 고양이 이미지랑 가장 유사한 이미지를 찾아야 돼요. 그렇죠 그러면은 이 쿼리 요 고양이 이미지랑 가장 유사한 이미지는 이 트레이닝 데이터에서 도대체 뭐예요? 그걸 어떻게 정의할 거예요? 그게 굉장히 힘듭니다. 사실 그런 매트릭이 정의하기가 어렵고 사실 없기 때문에 이 리얼리스트 네이버가 문제가 되는 거예요. 가령 예를 들어서 그 고양이 이미지랑 비행기 이미지는 뭐 예를 들어 0.003만큼 비슷하다. 고양이랑 차랑은 0.085만큼 시뮬러하다 비슷하다. 그리고 비로소 고양이랑 고양이가 이렇게 비교가 되면은 굉장히 높은 시뮬리티 0.4 정도 이렇게 시뮬러리티를 간다. 이런 시뮬러리티 매트릭이 필요해요. 이런 유사도 매트릭이 필요해요. 하지만 어떻게 해요? 이게 되게 어려운 문제라는 겁니다. 시뮬러리티 매트릭을 갖다가 디자인하는 게 어려운 문제예요. 그래서 사람들이 생각한 게 사실 실은 이미지가 정수로 이루어진 2차원의 행렬이라고 생각을 해보면은 그 이미지랑 이미지 간의 그 거리라는 거는요. 매트릭스와 매트릭스 사이의 거리랑 같다고 이렇게 생각을 해볼 수가 있겠죠. 이런 방법을 쓰기로 해요. 사실 이미지는 우리가 딱 눈으로 봤을 때 이런 이미지지만 실제로 컴퓨터가 보는 이 이미지의 모습은 RGB 레드 그린 앤 블루 그 빨간색 초록색 파란색으로 이루어진 어떤 행렬이라는 거죠. 행렬 데이터라는 거죠. 그래서 기본적으로 이런 행렬 데이터랑 이런 행렬 데이터가 있을 때 행렬 데이터는 숫자로 이루어진 어떤 데이터잖아요. 이 데이터와 데이터 간의 디스턴스를 구할 수가 있다는 거죠. 그래서 디스턴스를 구하는 방법이 여러 가지가 있는데 먼저 이제 몇 가지를 볼게요. l1 거리를 한번 써볼게요. l1 거리라는 건 뭐냐면요. 어떤 그 행렬과 행렬이 있을 때 그 행렬에 어떤 앱솔루트 디퍼런스를 구합니다. 절대적인 차이를 구해요. 그래서 예를 들어서 a가 이런 행렬이고 b가 이런 행렬이라면은 그냥 각각의 그 엘레먼트를 차이를 구합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 7,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 872,
      "char_count": 1611
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c008_7ac2e4",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 절대적인 차이를 구해요. 그래서 예를 들어서 a가 이런 행렬이고 b가 이런 행렬이라면은 그냥 각각의 그 엘레먼트를 차이를 구합니다. 각각의 그 1 콤마 0 콤마 0에 있는 데이터의 차이를 구하고 0 콤마 1에 있는 데이터 간의 차이를 구하고 이렇게 해가지고 각각의 행렬과 행렬의 엘레멘트에 해당하는 거예요. 차이를 구한 다음에 그 차이를 갖다가 다 더해 주는 거죠. 그게 엘 1 디스턴스라고 그래요 앱솔런트 디스턴스 자 그리고 엘투 디스턴스는 뭐냐면요 아까 앞에 거랑 똑같은데 그 차이를 계산한 다음에 제곱해서 루트 씌워준 거예요. 똑같아요 거의 그냥 계산 방법만 차이에 제곱해서 루트 씌워준 거랑 다르죠 이렇게 이런 식으로 그 이미지랑 이미지 사이에 거리를 갖다가 측정을 할 수 있어요. 그러면은 만약에 이미지가 서로 비슷하다면 각각의 픽셀들이 서로 비슷한 분포를 가지고 있다면은 아니면 각각의 그 이미지들이 같은 오브젝트 뭐 고양이면 고양이 말이면 말 이런 오브젝트를 가지고 있다면요. 이미지가 유사하면 유사할수록 이 거리가 더 가까울 거라는 것을 생각할 수가 있어요. 그렇죠 그래서 이미지의 유사도 이퀄 거리가 작은 거라고 이해할 수가 있겠죠. 그래서 이 디스턴스를 가지고 이제 리얼리스트 네이버 클래시 파이어를 한번 구현해 볼게요. 아까 말씀드린 것처럼 머신 러닝 방법은 트레인과 프리딕션의 단계로 이루어지는데 먼저 이 리얼리스 네이버 클래스 파이어의 트레이닝은 레이지 메소드이기 때문에 그냥 모든 레이블을 기억하는 과정이라고 그랬어요. 그래서 이미지랑 레이블이라는 이 한 페어의 트레이닝 데이터 셋이 들어오면은 이 이미지랑 레이블을 갖다가 저장을 하는 과정이 이 트레이닝 과정입니다. 자 이제 프리딕션이 중요합니다. 이 프리딕션은 테스트 이미지 즉 쿼리가 들어왔을 때 이 테스트 이미지에 대해서 가장 비슷한 가장 디스턴스가 작은 트레이닝 데이터를 찾고 그 데이터의 레이블을 갖다가 출력해 와야 돼요. 그래서 그 코드가 이거예요. 먼저 그러니까 간략하게만 이 코드를 설명드리면요. 그 민 디스턴스를 미니멈 디스턴스를 처음에 이니셜라이즈 할 때는 초기 값으로는 가질 수 있는 가장 큰 디스턴스를 윈 디스턴스로 지정을 해 놓고 모든 트레이닝 이미지를 갖다가 훑어가면서 요 포루프가 모든 트레이닝 이미지를 훑어가는 거예요. 그 트레이닝 이미지에 있는 해당 이미지랑 이 쿼리 간의 디스턴스를 계산합니다. 이 디스트가 그 쿼리 이미지랑 트레인 데이터 사이의 디스턴스예요. 만약에 그 디스턴스가 미니멈 디스턴스보다 작으면은 미니멈 디스턴스를 업데이트하고 그 미니멈 디스턴스를 가지는 이미지의 인덱스를 갖다가 출력을 해가지고 이 포 루프를 계속 돌다 보면은 미니멈 디스턴스를 가지는 인덱스는 나랑 가장 즉 쿼리랑 가장 유사한 즉 디스턴스가 가장 작은 트레이닝 데이터 셋의 인덱스가 됩니다. 그래서 그 인덱스에 해당하는 레이블을 갖다가 출력하는 기본적으로 리얼리스트 네이버 클래시피케이션 알고리즘을 구현한 거예요. 테스트 이미지가 들어왔을 때 그 테스트 이미지를 전체의 모든 트레이닝 데이터 셋과 비교를 해가면서 가장 가까운 트레이닝 데이터셋의 레이블을 출력하는 알고리즘입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 8,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 876,
      "char_count": 1600
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c009_d09061",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 그래서 그 인덱스에 해당하는 레이블을 갖다가 출력하는 기본적으로 리얼리스트 네이버 클래시피케이션 알고리즘을 구현한 거예요. 테스트 이미지가 들어왔을 때 그 테스트 이미지를 전체의 모든 트레이닝 데이터 셋과 비교를 해가면서 가장 가까운 트레이닝 데이터셋의 레이블을 출력하는 알고리즘입니다. 자 이 알고리즘의 타임 콤플렉스티 시간 복잡도를 한번 볼게요. 먼저 이 트레이닝 펑션을 보세요. 이 훈련을 할 때 즉 트레이닝 펑션을 실행할 때 타임 컴플렉시티는 뭘까요? 자 여기서 하는 일은 이미지랑 레이블을 갖다가 받아서 그냥 저장하는 거예요. 하나의 이미지랑 레이블 페어를 받아서 저장하는 게 요 펑션에서 하는 일이죠. 그래서 타임 컴플렉시티는 1이 될 거예요. 그렇죠 이미지 레이블 하나 딱 저장하니까 자 이제 이 프리딕션 테스크를 볼게요. 요 프리딕션 테스크 보면요. 하나의 테스트 이미지에 대에다가 모든 트레이닝 데이터셋을 훑어가면서 디스턴스를 계산하는 과정이 있어요. 요 포루프를 돌잖아요. 근데 이 포 루프는 만약에 트레이닝 데이터셋이 엔 개가 있다고 그러면은 즉 엔개의 훈련 샘플이 있다고 생각하면은 이 포 루프를 엔번을 돌아야 되겠죠 그러니까 타임 콤플렉스 티는 엔이 될 거예요. 그렇죠 굉장히 굉장히 안 좋은 문제입니다. 굉장히 비효율적인 거예요. 왜냐 사실 우리는 트레이닝 타임 컴플렉시티는 되게 커도 돼요. 왜냐하면은 머신 러닝 모델을 만들 때 훈련은 한 번 하고 예측은 그 훈련된 데이터를 가지고 와서 여러 번 할 수도 있기 때문이죠. 예를 들어서 유튜브 레코멘데이션 시스템을 만든다고 할 때 모델 트레이닝은 딱 한 번만 하면 돼요. 그러면은 트레이닝 된 모델은 딱 준비가 된 거예요. 하지만 실제로 우리가 유튜브 레코멘데이션을 할 때는 이 트레잉된 데이터를 갖다가 실제 커리에 대해서 무수한 번 반복해 가면서 테스트를 한다는 거죠. 그래서 우리는 사실 훈련 컴플렉시티는 커도 되지만 예측 어 타임 컴플렉시티는 최소한 하고 싶어요. 하지만 이 리얼리스트 네이버 클래스 파이어를 보세요. 예측할 때 모든 트레이닝 데이터 셋을 봐야 되잖아요. 그러면 이게 얼마나 비효율적이에요 예를 들어서 유튜브 레코멘데이션을 볼 때 어떤 사람이 들어와서 비디오를 추천해 줘 할 때 그 수억 개의 비디오를 다 훑어가면서 디스턴스를 계산해야 된다는 거잖아요. 그러니까 이거는 실질적으로 디플로이 할 수 없는 굉장히 비효율적인 알고리즘이라는 거죠. 그래서 실제로 현업에서 이 리얼리스트 클래시 파이어는 실제로 쓰이지는 않습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 9,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 700,
      "char_count": 1290
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c010_014131",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 자 이제 디시전 바운더리를 볼게요. 요 리얼리스트 클래시 파이어를 배웠으니까 어 예를 들어서 이렇게 이 점으로 나와 있는 데이터가 트레이닝 데이터예요. 이 파란색 빨간색 초록색 노란색 보라색 이 점들이 트레이닝 데이터라고 할 때 만약에 테스트 데이터가 어떤 임의의 어 장소에 이렇게 어 로케이트 됐을 때 그 테스트 데이터가 어떤 클래스로 매핑이 될지를 이 리얼리스트 네이버 클래식 파이어로 예측을 한 다음에 이 디시전 바운더리를 그려본 거예요. 즉 예를 들어서 어떤 랜덤한 데이터가 여기 이렇게 로켓인 됐다 이 보라색 섬 안에 로켓이 됐다 그러면 걔는 무조건 보라색이라고 클래스 파이가 되는 거고 노란색 이 가운데 섬 있죠 이 섬에 로케이트가 됐다 그러면 얘는 노란색이라고 클래스 바이가 되는 거예요. 자 이제 케미어리스트 레이버를 갖다가 한번 이용을 해볼게요. 앞에 나온 이거는요. 케이가 1일 경우에 디시전 바운더리예요. 자 이제 케이를 갖다가 1에서 3으로 3에서 5로 이렇게 늘려가면서 디시전 바운더리가 바뀌는 거를 보도록 할게요. 먼저 케이가 1일 때는 기본적으로 가장 가까운 하나의 트레이닝 셋만 보고 나의 클래스를 정하는 게 되겠죠. 그래서 이런 식으로 디시전 바운더리가 나왔는데 여기에 문제가 있어요. 가장 가까운 것만 보니까 예를 들어서 가운데 이게 노란색이죠. 이 노란색의 트레이닝 데이터가 있을 때 예를 들어 샘플이 여기 어 테스트 샘플이 여기에 이렇게 로케이트 된다고 그러면은 당연히 노란색 트레이닝 데이터가 가장 가까운 데이터니까 여기에 있는 것들은 다 노란색이라고 클래식 파이가 될 거예요. 근데 실제로 이렇게 데이터가 분포할 때 여기 보세요. 이 가운데 있는 건 다 초록색인데 얘만 노란색이잖아요. 이거는 아마도 이 노란색이 잘못된 데이터일 경우가 많아요. 실제로 데이터가 굉장히 지저분하거든요. 그래서 뭐 파란색인데 노란색으로 기록될 수도 있고 초록색인데 노란색으로 기록될 수도 있어요. 근데 여기 보면은 다른 거 옆에 있는 건 다 초록색인데 얘만 노란색이잖아요. 그거는 정확히는 모르지만 아마도 이 노란색 데이터가 잘못 레이블 될 가능성이 있다는 거죠. 그런데 그거를 잡아내지 못해요. 왜냐면은 가까운 그 네이버 하나만 보고 클래식 파이 하니까요. 근데 만약에 이 케를 갖다가 3으로 늘리면은요 만약에 어떤 테스트 데이터가 여기에 로켓이 됐다고 하더라도 주에 가장 가까운 트레이닝 데이터를 3개를 보고서 걔를 다수결로 레이블을 예측하기 때문에 주위에 있는 다른 포인트들도 참고를 합니다. 이 노란 데이터랑 이 두 개의 초록 데이터 그럼 다수결로 하면 초록 데이터가 2개고 노란 데이터가 하나니까 여기에 있는 점은 그쵸 초록색으로 예측이 되겠죠 그래서 이 k의 값을 늘려가면 늘려갈수록 이렇게 노이지 한 데이터를 잡아내는 데 유리합니다. 그쵸 그래서 그 증거로 이렇게 케이를 늘려가면 늘려갈수록 이 디시전 바운더리가 스무스해지는 걸 볼 수 있어요. 여기는 디시전 바운더리가 이렇게 울퉁불퉁했죠. 근데 케이를 늘려가면 늘려갈수록 이 디시전 바운더리가 스무스해지고 훨씬 더 이런 노이지한 데이터에 대해서 어 잘 잡아낼 수 있는 알고리즘이 만들어지게 됩니다. 자 여기에 이 하얀 색깔들이 있죠 이 하얀 색깔은 뭘까요? 이 하얀 색깔은요 어 여기에 어떤 데이터가 있을 때 어 그게 어떤 클래스인지 알 수 없는 겁니다. 왜냐 가까운 거를 이렇게 세 개를 뽑았을 때 그게 동률인 거죠. 빨간색이 될 수도 있고 초록색이 될 수도 있는 거죠. 그래서 이렇게 하얀색으로 표현을 했습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 10,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 967,
      "char_count": 1784
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c011_2cacad",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 자 여기에 이 하얀 색깔들이 있죠 이 하얀 색깔은 뭘까요? 이 하얀 색깔은요 어 여기에 어떤 데이터가 있을 때 어 그게 어떤 클래스인지 알 수 없는 겁니다. 왜냐 가까운 거를 이렇게 세 개를 뽑았을 때 그게 동률인 거죠. 빨간색이 될 수도 있고 초록색이 될 수도 있는 거죠. 그래서 이렇게 하얀색으로 표현을 했습니다. 그래서 k를 늘리면 늘릴수록 알고리즘이 리그로스 해져요. 즉 노이즈 한 데이터를 잘 잡아낸다는 거죠. 근데 그만큼 또 그 컴퓨팅 타임이 오래 걸리니까 왜냐면은 어 그 케이의 만큼의 네이버를 보고서 우리가 다수결 투표를 해야 되기 때문에 훨씬 더 프리딕션 타임은 오래 걸린다는 것을 알 수가 있어요. 이거는 l1 거리랑 l2 거리를 이렇게 네이버를 측정할 때 두 개의 다른 디스턴스 매트릭을 이용했을 때 어떻게 디스전 바운드가 달라지는 겁니까? 들를 보여주고 있는데 실제로 큰 의미는 없습니다. 비슷비슷한 그런 퍼포먼스를 가지고 있습니다. 자 이제 그 니어리스 네이버 클래스 파이어의 그 이슈들 문제점들을 한번 보도록 하겠습니다. 우리가 어 이 강의에서는 특히 컴퓨터 비전을 중심으로 즉 시각화 문제를 중심으로 머신러닝 알고리즘을 갖다가 학습을 할 텐데요. 여기 앞에서도 그렇지만 이그 샘플로 보고 있는 게 이미지 클래시피케이션 테스크죠. 이미지를 보고서 그 이미지가 뭔지를 클래스피케이션 하는 테스크 근데 이런 시각적 정보를 처리하는데 리얼리스트 네이버 클래스 파이어는 굉장히 굉장히 안 좋아요. 왜냐 픽셀 거리에는 k 리얼리스트 네이버가 사용되지 않습니다. 왜냐 픽셀의 거리 매트릭은 정보를 제공하지 않는다고 되어 있죠. 그 말은 무슨 말이냐면요. 우리가 아까 엘투 디스턴스랑 엘1 디스턴스를 가지고 각각의 이미지 사이의 거리를 갖다가 측정을 했어요. 근데 그 거리가 도대체 무슨 의미가 있을까요? 사실 그 거리라는 거는요 어떤 시멘틱한 정보도 제공하지 않습니다. 시멘틱한 정보라는 거는요 그 그림이 가지고 있는 어떤 의미를 말합니다. 자 여기 이그 샘플이 있어요. 이게 원본 이미지예요. 여기 여자 이미지가 있죠 이 이미지를 갖다가 몇 픽셀 이렇게 오른쪽으로 밀어볼게요. 사실 이렇게 보기에는 별로 달라진 게 없지만 요 오른쪽 이미지는 왼쪽 이미지를 시프트 한 거예요. 오른쪽으로 자 이제 이 이미지를 갖다가 눈과 입을 가려볼게요. 이렇게 박싱을 했어요. 자 이 이미지를 이렇게 틴트 해 가지고 다른 톤으로 바꿨어요. 근데 이 변형한 모든 이미지는 결국 이 여자 이미지잖아요. 같은 시멘틱을 가지고 있어요. 그 이미지의 내용은 똑같아요. 이 여자예요. 하지만 요 원본 이미지랑 이 3개의 변형된 이미지의 엘투 디스턴스는 다 똑같다고 나와요. 즉 이 이미지랑 틴트 된 이미지의 엘투 디스턴스는 이 이미지랑 요 박싱된 이미지의 엘투 디스턴스와 같고 또 이 이미지랑 오른쪽으로 시프트 된 이미지랑 l2 디스턴스가 같아요. 근데 사람이 생각하기에 직관적으로 이 왼쪽에 그 박싱된 이미지는 이 여자에 대한 정보를 하나도 담지 못하잖아요. 이렇게 눈과 입을 가리면은 이 얼굴을 가지고 어떻게 이 여자라고 우리가 예측을 할 수가 있겠어요. 그리고 이 시프트 된 이미지는 사실 이 여자랑 거의 똑같은 이미지죠. 그래서 사람이 봤을 때 이 여자는 시프트 된 이미지랑 훨씬 더 유사하고 요 박싱된 이미지랑은 훨씬 덜 유사해야 되는데 실제로 픽셀 간의 거리를 계산했더니 하나도 차이가 없고 똑같은 엘투 디스턴스를 가지고 있다는 거예요. 즉 이렇게 우리가 디스턴스 매트릭을 쓸 때 이 픽셀을 가지고 디스턴스 매트릭을 쓰면은 우리가 어떤 그런 시멘틱한 정보를 갖다가 표현하는 데 역부족이 됩니다. 그래서 특히 이 컴퓨터 비전 같은 경우에는 리얼리스트 네이버 클래시 파이어는 절대 쓰지 않습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 11,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 1021,
      "char_count": 1892
    },
    {
      "id": "transcript_ai_math_2강선형대수_1_regressionandnnclassi_c012_4f51e7",
      "content": "[AI Math] (2강)선형대수-1_RegressionandNNClassifier\n\n다. 그래서 특히 이 컴퓨터 비전 같은 경우에는 리얼리스트 네이버 클래시 파이어는 절대 쓰지 않습니다. 그리고 두 번째 두 번째 문제가 요 컬스 오브 디맨저널리티라는 게 있어요. 차원의 저주죠. 우리나라의 말로 하면 컬스 오브 디맨저널리티 차원의 저주 그 말은 고차원의 데이터의 경우 그 데이터의 그 숫자가 기하급수적으로 증가한다는 거예요. 여기 이렇게 점이 4개가 있죠. 이게 예를 들어서 같은 수준의 이 디스턴스를 데이터 픽셀 포인트 간의 같은 수준의 디스턴스를 유지하기 위해서 필요한 데이터 포인트 개수가 이 4개라고 해요. 얘를 갖다가 2차원으로 하면은 이 4개 곱하기 4개가 되니까 16개의 포인트가 돼요. 3차원으로 하면은 4 곱하기 4 곱하기 4니까 64 포인트가 돼요. 이렇게 데이터의 차원이 이렇게 늘어날수록 필요한 그 데이터의 숫자는 기하급수적으로 늘어난다는 거예요. 근데 우리가 이미지를 생각해 봤을 때 이미지는 사실 3차원이거든요. 우리가 이미지를 딱 봤을 때 투 디 2차원이라고 생각하지만 이 이미지가 색깔이 있기 때문에 RGB 채널이 있어요. 그 색깔별로 채널이 다 3개가 있어요. 그러니까 사실 이미지는 3차원 벡터라는 거예요. 그래서 이미지 하나를 이루기 위해서 굉장히 많은 데이터 포인트들이 하나의 이미지를 이루어요. 그런데 디스턴스를 계산하기 위해서는 그 수많은 데이터 포인트들 간에 거래를 갖다가 계산을 해야 된다는 거예요. 그래서 기본적으로 우리가 이미지를 처리하기 위해서 디스턴스 매트릭을 쓴다면 그 디스턴스를 구하기 위해서 굉장히 많은 픽셀 밸류 간의 거리를 계산해야 되겠죠. 그래서 이거는 사실 컴퓨테이셔널 코스트를 생각했을 때 수지 타산이 맞지 않아요. 그래서 실제로 클래스 파이어를 우리가 디자인한다 할 때 가장 우리가 생각하지 말아야 될 게 이 리얼리스트 네이버 클래스 파이어입니다. 그러면 우리가 그 클래스 파이어 뭘 써야 되냐 리얼리스트 네이버 이렇게 안 좋고 여러 가지 문제가 있는데 뭘 써야 되냐 그래서 다음 시간에는 그 니어리스트 네이버 클래스 파이어의 단점을 보완한 리니어 클래스 파이어랑 소프트맥스 클레이스 파이어를 배우도록 하겠습니다. 그래서 이 리니어 클래스 파이어랑 소프트 맥스 클레이스 파이어를 배우면은 요 어 이 두 개의 모델을 통해서 자연스럽게 뉴럴넷 모델로 연결이 됩니다. 그래서 다음 시간에는 어 뉴럴넷 신경망 모델의 기초가 되는 이 두 개의 클래스 바이어 모델을 배우도록 하겠습니다. 네 긴 시간 어려운 내용 집중해 주셔서 감사합니다. 다음 시간에 뵙겠습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(2강)선형대수-1_RegressionandNNClassifier.json",
        "lecture_name": "(2강)선형대수-1_RegressionandNNClassifier",
        "course": "AI Math",
        "lecture_num": "2강",
        "lecture_title": "선형대수-1_RegressionandNNClassifier",
        "chunk_idx": 12,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d669cffe782e3fb1addbbeba22f7aa94221031fbdc69280280f140eec51a9fb7"
      },
      "token_estimate": 698,
      "char_count": 1299
    }
  ]
}