{
  "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
  "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
  "course": "Deep Learning Basic",
  "total_chunks": 10,
  "chunks": [
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c000_a09666",
      "content": "[강의 녹취록] 과목: Deep Learning Basic | 강의: 5강 | 제목: 기초 신경망 이론 -2 - Back Propagation\n\n네 안녕하세요 여러분 어 이전 시간에 4강에 거쳐서 저희가 머신 러닝 라이브 사이클에 대해서 쭉 배워보고 있습니다. 바로 전 시간에서는요 저희가 뉴럴 네트워크를 만들고 그거를 어떻게 트레이닝 하는지에 대해서 배워봤는데요. 오늘은 본격적으로 뉴럴넷을 트레이닝하는 데 가장 중요한 개념인 백프로파게이션 에 대해서 수학적으로 다 배워보면서 예시를 한번 따라가 보도록 하겠습니다. 그래서 오늘 배울 거는요 기본적으로 백프로파게이션은 뭔지 그리고 이그 샘플을 보면서 우리가 실제로 백프로파게이션을 실제로 해보는 시간을 갖도록 하겠습니다. 자 그럼 시작하도록 하겠습니다. 저희가 이전 시간에 뉴럴넷을 트레이닝 하기 위해서는 먼저 모델을 만들고 모델로부터 예측 값을 구하고 예측 값이랑 실제 정답 값이랑 비교를 해가지고 로스 텀을 구한 다음에 그 로스를 어 모델의 파라미터에 대해서 미분을 하는 과정을 거쳐서 옵티마이제이션 최적화를 한다고 배웠어요. 요 백프로파게이션은요 기본적으로 이 노스를 웨이 파라미터에 대해서 미분하는 과정입니다. 그 과정을 쉽게 하는 과정이고요. 어 뉴럴넷 트레이닝 하는 데 있어서 굉장히 중요한 그런 테크닉이니까 오늘 집중해서 잘 들으셨으면 좋겠습니다. 자 먼저 저희가 이전까지 리니어 클래식 파이어에 대해서 쭉 배웠었는데요. 기본적으로 이 리니어 클래시 파이어는 이 FX블 같은 식으로 나타낼 수 있다고 배웠어요. 그래서 이렇게 함수 f가 있으면은 인풋 스랑 우리가 배우는 웨이 파라미터 더블유에 대해서 식이 이렇게 나타내 줄 수 있을 것 같아요. 더블x 더하기 비 비는 바이러스죠 그래서 이 식을 갖다가 우리가 컴퓨테이션을 그래프로 나타낼 수 있어요. 즉 이 파라미터들 스랑 블랑 비 이런 거를 인풋으로 하고 또 그 수확식 오퍼레이터 를 이렇게 노드로 보내가지고 컴퓨테이션을 그래프를 그릴 수 있어요 이런 식으로 됩니다. 그래서 인풋은 x랑 w 그리고 b는 우리가 배우려는 파라미터죠. w랑 b는 이렇게 들어오면은 먼저 이 식에 따라서 하면은 요 블랑 엑스를 먼저 이렇게 곱하기를 하고 그다음에 거기다가 바이어스 텀인 비를 이렇게 더해 가지고 우리가 이 프라는 식의 아웃풋을 나타낼 수 있죠. 그냥 이 식을 보고 그래프로 이렇게 나타낸 거예요. 그리고 백프로파게이션은 이 두 가지 페이스가 있어요. 첫 번째는 폴드 패스라고 해가지고 주어진 x랑 w랑 b에 대해서 이 컴퓨테이션을 그래프를 갖다가 식을 솔브 함으로써 아웃풋을 예측하는 과정 이 앞에서부터 뒤로 가는 과정을 포워드 패스라고 그래요. 그리고 백 프로파게이션은 반대예요. 우리가 로스를 계산해서 뒤에 있는 로스를 계산해서 그 로스를 앞에 인풋 단까지 쭉 그레디언트를 구해서 앞으로 쭉 오는 거를 백프로파게이션이라고 그래요. 지금은 약간 감이 안 오시죠? 그래서 이그 샘플을 통해서 한번 설명드리도록 할게요. 자 먼저 포워드 패스 만약에 우리가 임의의 식을 가정해 볼게요. fxy z는 스 더하기 와 곱하기 세트라고 이렇게 우리가 식을 한번 이그 샘플을 들어볼게요. 제가 임의의 식으로 이렇게 만들어 봤습니다. 그리고 우리가 인풋으로 XY z를 마이너스 2 5 마이너스 y 이렇게 한번 보내볼게요. 그러면은 이 컴퓨테이션 그래프는 이런 식으로 생길 거예요. x랑 y랑 먼저 더하고 그다음에 거기에다가 z를 곱해 가지고 아웃풋을 낸다. 근데 포워드 패스는 기본적으로 우리가 주어진 인풋 파라미터에다 값을 대입해 가지고 아웃풋까지 예측하는 계산의 과정이에요. 근데 여기에서 이 풋이 마이너스 2 5 마이너스 4라고 이렇게 주어졌으니까 x y z는 마이너스 2 5 4죠. 그리고 먼저 이 더하기를 해야 되니까 x 더하기 와 마이너스 2 더하기 5 해서 아웃풋이 3이 나올 거예요. 여기를 이제 q라고 한번 불러볼게요. x 더하기 y 이퀄 q 요 텀을 q라고 보내는 거예요. 그리고 그다음에 제를 곱해야 되겠죠 그래서 제를 곱해서 곱하기 오퍼레이션을 하면은 여기에서 나온 아웃풋이 3이고 제가 마이너스 4니까 3 곱하기 마이너스 4 해서 마이너스 12 이렇게 아웃풋까지 예측이 됐습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 0,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 1108,
      "char_count": 2069
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c001_e0bba6",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 네 쉽죠 요 그냥 컴퓨테이셔널 그래프를 따라서 계산만 하면 돼요. 이게 포드 프로세스고 그다음에 어려운 게 이 백프로파게이션입니다. 백프로파게이션은 기본적으로 이 뒤에서 로스를 계산을 해가지고 각각의 그 로스가 그러니까 예측 값이랑 정답 값의 차이가 이 단계에 있는 그 파라미터에 의해 미분을 했을 때 얼마만큼 변하는지를 예측을 하는 거 계산을 하는 겁니다. 그러니까 어 그냥 직관적으로 생각할 때 우리가 그 뉴럴넷을 트레이닝 할 때 로스를 구하잖아요. 그 로스가 이 단계 이 단계 그 각각의 컴퓨테이셔널 그래프의 단계에서 들어오는 인풋 값이 변할 때 얼마만큼 로스가 변하는지 즉 로스를 각각의 그 컴퓨테이션 그래프에서 인풋으로 받아오는 파라미터에 대해 미분한 값을 구한다고 생각을 하시면 돼요. 이렇게 말로 하면은 어려울 수 있으니까 한번 해볼게요. 자 우리가 아까 앞에서 x y z를 마이너스 25 4라고 생각을 하고 쭉 이렇게 계산을 했어요. 자 이제 백프로파게이션을 할게요. 마지막으로 이제 백프로파게이션은 이제 기본적으로 로스가 나오는 제일 마지막 단의 그래프에서부터 시작을 해요. 그래서 맨 마지막 단에서 이거 같은 경우에 아웃풋이 프라고 그러면은 여기 들어오는 그 인풋은 또 f죠. 그래서 여기에 아웃풋을 인풋에 대해서 미분을 하니까 이 마지막 단에서는 f를 f에 대해서 미분을 한 거예요. 그래서 백프로파게이션의 맨 마지막 단의 그 값은 항상 1이에요. 왜냐면은 아웃풋이랑 인풋 즉 우리가 미분하려는 대상이 되는 이 프랑 변하는 파라미터 프랑 같기 때문에 이 값은 항상 1이 됩니다. 그래서 한번 시작을 해볼게요. 먼저 여기 써 있죠. 마지막 백 프로파게이션 TM은 항상 1이에요. 그래서 이 뒤에서 애플을 애플에 대해서 미분했더니 1이",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 1,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 492,
      "char_count": 931
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c002_a5c043",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 그래서 한번 시작을 해볼게요. 먼저 여기 써 있죠. 마지막 백 프로파게이션 TM은 항상 1이에요. 그래서 이 뒤에서 애플을 애플에 대해서 미분했더니 1이다. 자 그럼 이제 한 칸 앞으로 가볼게요. 이 단계에서 에프를 요 인풋인 제트에 대해서 미분을 하려고 그래요. 프가 제가 바뀜으로써 얼마나 많이 바뀌는지 이거를 계산을 할 거예요. 자 우리가 여기 프식이 있죠. 그리고 x 더하기 y는 우리가 q라고 부르려고 그래요. 그러니까 우리 프식은 기본적으로 q 제가 되겠죠. 그래서 프는 q 제 그리고 이 식을 갖다가 제에 대해서 미분했으니까 q 제를 제에 대해서 미분해서 큐가 나오겠죠 그래가지고 이 단계에서의 그 편미분 값은 큐가 돼요. 큐는 뭐예요? 3이죠 그래서 여기다 이렇게 진한 파란색으로 3이라고 썼어요. 여기까지 잘 따라오고 계시죠 자 이제 요 단에서 해볼게요. 요 단에서는 애플을 뭐에 대해서 미분을 해야 돼요 여기에 있는 파라미터는 그렇죠 큐죠 그래서 애플을 큐에 대해서 미분을 하는 거예요. 근데 f는 뭐예요? q 제죠 그래서 q 제를 q에 대해서 미분했으니까 제가 나와요. 그런데 제는 마이너스 4니까 여기다가 마이너스 4를 써주는 거예요. 쉽죠 자 그러면은 이 앞으로 여기 다섯 그 더하기가 나와 있는 요 위아래 거를 한번 볼게요. 먼저 여기 여기는 애플을 여기에 있는 그 파라미터인 스에 대해서 미분을 하려고 그래요. 근데 딱 봤더니 이 f는 x에 대한 식이 아니에요. 여기 이게 q고 이게 제니까 큐제이기 때문에 바로 스에 대한 식이 아니에요. 그래서 여기서 체인룰 편미분 식을 써 가지고 먼저 이 프를 x에 대한 식인 q에 대해서 먼저 미분을 해요. 먼저 애플을 q에 대해서 미분하고 그다음에 큐를 갖다가 x에 대해서 미분을 하는 거예요. 그래서 여러분들 예전에 편미분 방정식 배워보셨다면은 이렇게 미분식이 쓰여져 있으면 여기가 이렇게 이렇게 상쇄돼서 f가 x에 대해서 미분한 거랑 같다라는 걸 배우셨을 거예요. 그래서 이렇게 미분식을 두 텀으로 쪼개 가지고 쓸 수 있어요. 그래서 먼저 첫 번째 애플을 큐에 대해서 미분하는 거 애플을 q에 대해서 미분하는 거 프는 뭐예요? q 지트니까 q에 대해서 미분하면 제가 나올 거예요. 그리고 뒤에 텀 큐를 엑스에 대해서 미분하는 거예요. q는 뭐예요? q는 x 더하기 와죠. 그래서 x 더하기 y를 x에 대해서 미분했으니까 1이 나와요. 그래 가지고 여기에 나오는 텀은 제가 될 거예요. 근데 제는 마이너스 4니까 여기 마이너스 4를 썼죠. 써서 이렇게 여기가 그 그레디언트가 마이너스 4가 되는 거죠. 한 번 더 해볼게요. 이 밑에 부분 이 밑에 부분도 똑같아요. 이 밑에 부분에 대한 파라미터는 이 애플을 갖다가 여기에 있는 파라미터인 y에 대해서 미분을 하는 게 되겠죠. 그래서 프를 y에 대해서 미분한",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 2,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 754,
      "char_count": 1449
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c003_53e9da",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 근데 프는 뭐예요? q 제죠? y에 대한 식이 아니에요. 그래서 이것도 똑같이 편미분 방정식을 써서 애플을 먼저 q에 대해서 미분을 하고 그다음에 q를 갖다가 y에 대해서 미분을 하면은 이렇게 써질 수 있죠. 먼저 애플을 q에 대해서 미분하고 q를 y에 대해서 그럼 이게 이렇게 상쇄되면서 프를 y에 대해서 미분하는 거랑 똑같이 되겠죠. 그래서 아까 앞에서랑 똑같이 먼저 프는 q 제니까 q에 대해서 미분하면 그렇죠 제가 나오죠. 그리고 q는 x 더하기 y고 스 더하기 와를 갖다가 와에 대해서 미분했더니 그렇죠 1이 됐죠. 그래서 여기의 값은 제 곱하기 1 그리고 제는 마이너스니까 이렇게 마이너스 4가 되는 거예요. 그렇게 어렵지 않죠 그러니까 기본적으로 우리가 이렇게 한 거를 쭉 보면요. 뒤에서부터 어떤 로스에 대해서 그 단계에 있는 변수에 대해 미분을 한 거예요. 근데 뒤에서는 쉬었는데 뒤에서는 그냥 1이 나왔죠. 근데 앞으로 갈 때마다 계속 편미분 체인 눈을 써 가지고 미분을 이렇게 여러 개로 쪼개서 해야 되는 거를 이렇게 볼 수가 있어요. 굉장히 복잡하죠 그렇죠 그래 가지고 이거를 복잡하지 않게 간단하게 도식화를 한번 해볼게요. 그래서 요 체인룰이라는 걸 써가지고 이렇게 도식화를 해봤어요. 여기 이 동그라미 동그라미가 어떤 수확식 같은 걸 계산을 할 때 오퍼레이션을 넣는 요 노드 단위라고 생각을 하시면 돼요. 우리 아까 앞에서 이글 샘플 봤을 때 더하기랑 곱하기 이런 오퍼레이션들이 있잖아요. 그런 것처럼 이렇게 우리가 어떤 그 연산의 오퍼레이션을 노드 동그란 노드로 이렇게 표현한다고 생각을 할 때 이 연산은 프라고 쓸 수가 있겠죠. 연산이니까 그리고 이 연산에는 인풋이 들어가고 아웃풋이 나와요. 그렇죠 여기에서 어 이 그림에서 인풋은 뭐겠어요? 스랑 와가 인풋이죠. 그리고 스랑 와가 프라는 펑션을 통해서 아웃풋인 제로 나갈 거예요. 이게 포워드 패스예요. 여기에 하늘색으로 표시되어 있는 이 화살표 이게 포워드 패스라고 생각하시면 됩니다. 자 그러면은 여기까지는 쉬워요. 포워드 패스는 그냥 계산만 하면 되죠. 근데 백프로파게이션이 조금 어려워요. 백프로파게이션을 하기 위해서 우리가 업스트림 그레디언트랑 로컬 그래디언트랑 요 다운스트림 그레디언트라는 이 세 가지 컨셉을 배우게 될 거예요. 자 업스트림 로컬 다운스트림 그러면은 먼저 업스트림부터 볼게요. 업스트림 그레디언트는 뭐냐면요. 이렇게 포워드 패스를 통해서 제가 아웃풋으로 나왔잖아요. 그러면 이 제트를 통해서 우리가 계산할 수 있는 건 로스죠. 이 제 예측 값이랑 그라운드 트루스 제 값의 차이를 계산을 해서 우리가 로스 엘이라는 거를 구한다고 해요. 그리고 업스트림은 뭐냐면요. 이 로스가 이 아웃풋 베리어블에 대해서 얼마나 바뀌는지 요 엘을 로스 엘을 이 아웃풋 베리어블 제에 대해서 미분한 게 업스트림 그레디언트예요. 다시 업스트림 그레디언트는 로스를 이 아웃풋 베리어블에 대해서 미분한 요 그레디언트입니다. 그렇죠 그다음에 이 로컬 그래디언트가 중요한데요. 이 로컬 그래디언트는 기본적으로 뭐냐면 여기 이 오퍼레이션 안에서는 하는 일이 뭐예요? 이 에프가 하는 일이 인풋을 받아 가지고 아웃풋을 내는 거죠. 그래서 이 로컬 그래디언트는요. 이 안에서 아웃풋에 대해서 인풋을 갖다가 미분한 값이에요. 그래서 예를 들어서 인풋이 스다 그러면은 우리 아웃풋은 제죠 이 z를 스에 대해서 미분한 게 이쪽 방향의 로컬 그래디언트고 여기 이쪽 방향은 당연히 뭐겠어요? 이쪽은 그 y에 대해서 받는 인풋을 가지고 있죠. 그러니까 이쪽 방향으로 나는 로컬 그래디언트는 아웃풋인 z를 요 인풋 파라미터 y에 대해서 미분하는 겁니다. 그래서 다시 로컬 그래디언트는 이 오퍼레이션 안에서 생기는 그레디언트로 아웃풋을 각각의 인풋 베리어블에 대해서 이렇게 미분을 해 가지고 그레디언트가 나갑니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 3,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 1049,
      "char_count": 1948
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c004_c2f7ad",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 그래서 다시 로컬 그래디언트는 이 오퍼레이션 안에서 생기는 그레디언트로 아웃풋을 각각의 인풋 베리어블에 대해서 이렇게 미분을 해 가지고 그레디언트가 나갑니다. 여기까지 이해되셨죠? 그러면 끝이에요. 다운 스트림 그레디언트는요 기본적으로 이 업스트림 그레디언트랑 로컬 그래디언트의 곱으로 표현이 돼 가지고 이 방향으로 그레디언트가 나가게 돼요. 한번 볼게요. 먼저 이 스 방향으로 보면은 이 스 방향의 로컬 그래디언트는 뭐예요? 이 제트를 엑스에 대해 미분한 거죠. 여기 이 부분 그래서 z를 x에 대해 미분했어요. 그리고 업스트림 그레디언트는 로스를 z에 대해서 미분한 요 값이죠. 그래서 이 업스트림이랑 로컬을 곱했더니 이 z가 지워지고 이 로스를 엑스에 대해서 미분한 값이 돼요. 그래서 로스를 엑스에 대해서 미분한 값이니까 이 x라는 베리어블에 대한 그레디언트죠. 그래서 이 x에 대한 어 그레디언트인 로스에 대한 x의 미분 값이 이렇게 나가게 돼요. 이쪽도 한번 해볼게요. 이쪽은 업스트림은 요거 로컬은 이거죠. 그래서 업스트림 곱하기 로컬 그래디언트를 했더니 아까랑 똑같이 제가 지워지고 로스를 와에 대해서만 미분한 값이 남게 돼요. 그래서 로스가 여기에 파라미터인 와에 대해서 얼마나 바뀌는지에 대한 그레디언 값을 우리가 계산을 할 수가 있습니다. 그래서 기본적으로 다시 요약을 하자면 업스트림 그레디언트를 로스를 갖다가 아웃풋에 대해서 미분한 거예요. 그리고 로컬 그래디언트는 아웃풋을 인풋에 대해서 어 그레디언트를 취해 가지고 계산을 한 거고 다운스트림은 로스를 각각의 인풋 파라미터에 대해서 미분한 값인데 이 다운스트림 그레디언트는 이 로컬이랑 업스트림의 곱으로 표현된다고 그랬어요. 그래 가지고 이렇게 쓰여져 있습니다. 그래서 이게 기본적으로 이 백프로파게이션의 전부인데 사실 이렇게만 보시면 좀 이해가 안 되실 수 있어서 이그잼플을 한번 넣어봤습니다. 자 이게 이 식 어디서 많이 본 거죠? 우리 여태까지 쭉 배운 로지스틱 리그레션의 시기에요. 그래서 엑스가 그러니까는 인풋이 두 개의 파라미터로 되어 있대요. 엑스 제로랑 엑스원으로 이루어진 두 개의 원소로 두 개의 엘레먼트로 이루어진 엑스라는 인풋을 받았을 때 각각의 인풋의 그 값에다가 웨이 파라미터인 더블 제로 더블 1을 곱해서 바이어스를 더한 거 이게 리니어 리그렉션이죠. 그렇죠 이 리니어 리그렉션에다가 소프트맥스를 씌운 게 로지스틱 리그렉션이에요. 그래서 기본적으로 이거는 이제 로지스틱 리그레션 식이고 우리가 이제 이그젬플 백프로파게이션 이그젬플을 보기 위해서 이 쉬운 로지스틱 리그레션 식을 갖고 온 거예요. 자 이제 이 식을 갖다가 컴퓨테이셔널 그래프로 만들어 볼게요. 여기에서의 인풋은 뭐예요? 요 그 익스포넨셜 위에 있는 이 값들이 다 인풋이 되겠죠 스 제로 스1 더블 제로 블1 비 이게 다 인풋이에요. 그래서 한번 써볼게요. 먼저 더블 제로랑 스제로랑 곱해야 돼요. 그래서 더블 제로랑 스제로랑 이렇게 곱했습니다. 그리고 블1이랑 스원이랑 이렇게 곱했어요. 이런 식으로 그리고 걔를 이렇게 더했어 이렇게 더한 더하기 이렇게 오퍼레이션이 있죠. 그래서 여기까지 한 거예요. 그다음에 바이어스텀 비를 요 텀에다가 이렇게 더했죠 그래서 여기부터 여기까지가 우리의 인풋 요 계산 요 리니어 리그레션 계산을 한 거죠. 자 그다음에 여기다가 마이너스 1을 곱했어요. 요 텀을 계산하기 위해서 그래서 마이너스 1을 곱했고 걔에다가 익스포넨셜을 취했죠. 그래서 익스포넨셜을 취한 거예요. 그다음에 1을 더하고 1 더하고 그거에 역수를 취했어요. 1 마이너스 x 역수를 취한 거죠. 그래서 기본적으로 이 컴퓨테이셔널 그래프는 이 식을 갖다가 우리가 그 각각의 그 오퍼레이터는 그 이렇게 노드로 이렇게 표현을 하고 그 인풋이 되는 파라미터는 이렇게 인풋 그래프 안에 인풋으로 들어가게 이렇게 그래프처럼 표현을 한 거예요. 그렇죠 여기까지는 문제가 없습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 4,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 1078,
      "char_count": 1983
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c005_9a643a",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 자 먼저 우리가 해야 되는 건 뭐죠? 우리가 배웠듯이 그렇죠 포워드 프로세스를 해야 돼요. 포워드 프로세스를 하기 위해서 제가 임의로 블랑 스랑 b 값을 줄게요. 더블제로는 이 어스 제로는 마이너스 1 블1은 마이너스 3 스1은 마이너스 2 b는 마이너스 3으로 한번 줘볼게요. 그러니까 이 값은 어떤 것이든 될 수 있어요. 여러분들이 이제 인풋에 따라서 이런 값들을 이렇게 이니셜 라이즈 해 가지고 한번 계산을 해볼 수 있어요. 그러니까 저는 임의로 이렇게 준 거예요. 자 포워드 프로세스는 그냥 계산이에요. 쭉 계산을 한번 해볼게요. w 제로랑 x 제로 이 값이랑 곱해서 이렇게 되고 이 두 개의 값 마이너스 3 마이너스 2 곱해서 6 되고 걔네를 더해서 4가 됐어요. 6이랑 마이너스 2니까 그래서 4가 되고 그다음에 바이어스 텀 더해서 1이 됐죠. 그리고 거기다가 마이너스 1을 곱하면은 마이너스 1 되고 마이너스 1에 익스포넨셜 씌우면은 익스포넨셜의 1이 되죠. 그래서 0.37이 되고 익스포넨셜에 1을 더하면 1.37이 되고 그거에 역수를 취해서 0.73이 됐다. 그래서 기본적으로 이 수확식에다가 각각의 파라미터에다가 인풋 요 이 숫자들을 넣고 계산을 쭉 하는 게 이 포워드 프로세스예요. 네 그럼 이제 본격적으로 백코드 프로세스를 시작하겠습니다. 자 마지막 단에서의 그래디언트는 항상 1이라고 그랬죠. 왜냐하면은 아웃풋을 여기에 아웃풋에 대한 미분을 취하니까 그러니까 아웃풋이 f면은 이 단계에서의 파라미터는 f니까 f로 f에 대해서 미분해서 맨 마지막 단은 1이 됐어요. 그래서 여기가 1 자 이제 여기에서 이제 요 동그라미 1 나누기 엑스라는 이 동그라미 이 오퍼레이션에서 한번 백프로파게이션 식을 해볼게요. 자 우리가 먼저 1을 구했죠. 이 1은 뭐예요? 뒤에 있으니까 업스트림 그레디언트예요. 그렇죠 아웃풋을 아웃풋에 대해서 미분한 게 1이다. 그러면 로컬 그래디언트는 뭐예요? 로컬 그레이디언트는 이 x분의 1이라는 식에 대해서 인풋인 x에 대해서 미분을 해야 되죠. 그렇죠 그래서 여기 이 동그라미 오퍼레이션에서의 식은 x분의 1이에요. 그 x분의 1이라는 식에서 인풋 x에 대해서 미분을 한 거니까 이 x분의 1을 엑셀에서 미분한 거죠. 그래서 이거를 엑셀에서 미분해서 마이너스 x 제곱 분의 1이 됐어요. 그렇죠 그러면은 요걸 미분을 했고 그다음에 우리 인풋은 뭐였어요? 1.37이었죠 인풋이 x가 1.37이었죠. 그래서 미분한 식에다가 1.37을 넣어준 거예요. 그래 가지고 로컬 그래디언트를 이렇게 구할 수 있죠. 여기까지 잘 따라오고 계시죠 자 이제 다운 스트림 그레디언트 즉 요 앞에다가 써야 될 그레디언트 로스를 인풋의 베리어블에 대한 그레디언트로 구해야 되는데 요 그 다운 스트림 그레디언트는 어떻게 구한다고 그랬어요? 업스트림이랑 로컬 그래디언트의 곱으로 구해진다고 그랬어요. 그래서 업스트림 그레디언트는 1이고 로컬 그래디언트는 이 값이니까는 얘를 갖다 곱해서 마이너스 0.53이 돼서 이 다운스트림은 이렇게 마이너스 0.53으로 나오게 됩니다. 이해되시죠? 자 이거는 이제 x분의 1이라는 오퍼레이션에 대해서 했어요. 한 단계 앞으로 가서 이 오퍼레이션에 대해서 할게요. 이 오퍼레이션은 인풋이라는 스가 들어왔을 때 그냥 상수 1을 더하는 오퍼레이션이에요. 그래서 프스이퀄 x분의 1이에요. 그리고 마이너스 0.53은 우리가 이미 구한 거죠. 이게 업스트림 그레디언트가 돼요. 그러면은 로컬 그래디언트는 뭘까요? 로컬 그래디언트는 이 프라는 스플러스 1이라는 식에 대해서 인풋 스에 대해서 미분을 해야 되죠. 요 프를 스에 대해서 미분한 거 그래서 우리가 이거를 엑스에 대해서 미분하면 1이 돼요. 그래 가지고 로컬 그래디언트는 1이 됩니다. 그리고 다운스트림 그래디언트 여기 노일 다운 스트림 그래디언트는 업스트림 곱하기 로컬 그래디언트니까 업스트림은 요거 마이너스 0.53 로컬은 우리가 구해준 거 1 그래가지고 또 마이너스 0.53이 돼요. 그래서 다운스트림 그레디언트는 마이너스 0.53이",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 5,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 1075,
      "char_count": 2045
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c006_9ef6a5",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 그리고 다운스트림 그래디언트 여기 노일 다운 스트림 그래디언트는 업스트림 곱하기 로컬 그래디언트니까 업스트림은 요거 마이너스 0.53 로컬은 우리가 구해준 거 1 그래가지고 또 마이너스 0.53이 돼요. 그래서 다운스트림 그레디언트는 마이너스 0.53이다. 그래서 여기다 써줬어요. 한 단계 앞으로 갈게요. 여기에서 업스트림은 뭐예요? 마이너스 0.53이죠. 이 뒤에 있는 그레디언트 뒤에서 오는 그레디언트 로컬 그래디언트 한번 구해볼게요. 여기에서의 함수는 뭐예요? 익스포넨셜의 스죠 여기 익스포넨셜 취했으니까 함수는 이거예요. 그리고 얘를 갖다가 인풋 x에 대해서 미분했더니 또 익스포넨셜의 스다. 그다음에 들어오는 인풋이 마이너스 1이니까 스가 마이너스 1이죠. 그래서 익스포넨셜의 마이너스 1이 로컬 그래디언트가 돼요. 그리고 다운스트림 그레디언트는 업스트림 그레디언트랑 로컬 그래디언트의 곡으로 표현되니까 이렇게 쓰여질 수 있죠. 그래서 이거 계산하니까 마이너스 0.2가 됐어요. 그렇죠 앞으로 가볼게요. 쭉쭉 잘 따라오고 계시기를 바랍니다. 여기서 업스트림은 뭐예요? 뒤에서 아까 우리가 구한 마이너스 0.2 그리고 여기에서의 f라는 식은 마이너스 1 곱하기 x예요. 그러니까 x 곱하기 마이너스 1이니까 마이너스 x라는 FX의 식이 요 안에서 이루어지는 겁니다. 그리고 엑스에 대해서 미분을 했더니 이거를 갖다가 엑셀에 대해서 미분을 했으니까 마이너스 1이 되겠죠. 그래서 로컬 그래디언트는 마이너스 1 그래서 업스트림이랑 로컬이랑 곱했더니 0.2가 돼서 다운 스트림 그레디언트에다가 써줬어요. 그래서 여기에서부터 여기까지는 그렇게 어려운 게 없어요. 하나의 오퍼레이션이 이런 식으로 되어 있을 때 쭉쭉쭉쭉 앞으로 이제 어 다운스트림 그레디언트를 구해 갔습니다. 자 이제 인풋이 두 개 있는 조금 어려운 케이스로 갈게요. 그렇지만 어렵지 않아요. 그래서 이거부터 볼게요. 자 이 더하기에서의 그 업스트림은 뭐예요? 0.2죠 그래서 0.2가 업스트림일 때 그 여기에서 들어오는 인풋이 x라고 그러고 여기에서 들어오는 인풋이 y라고 그랬을 때 이 오퍼레이션에서 이루어지는 FX는 뭐라고 표현이 돼야 되면은 2개의 인풋을 더한 거예요. 그래서 FX 콤마 y는 스 더하기 y라고 표현될 수가 있어요. 여기서 x는 제가 임의로 이 위에서 들어오는 인풋은 x라고 부르고 아래에서 들어오는 인풋은 와라고 제가 이렇게 부른 거예요. 자 그래서 먼저 엑스에 대한 것부터 해볼게요. 업스트림 그레디언트는 0.2예요. 그리고 먼저 엑스에 대한 그것부터 보면은 로컬 그래디언트는 이 프 스 코마 와를 여기서 들어오는 인풋인 스에 대해서 미분한 게 되겠죠. 그래서 이 식을 x에 대해서 미분하면 1이 되겠죠. 그리고 이 식을 갖다가 밑에서 들어오는 인풋인 y에 대해서 미분해도 여전히 1이 되죠. 왜냐하면 x 더하기 y를 갖다가 y에 대해서 미분하면 1이니까 그래서 이것들이 이 f를 갖다가 x에 대해서 미분한 거는 이 위에서 오는 인풋에 대한 로컬 그래디언트 이 f를 갖다가 y에 대해서 미분한 거는 이 아래에서 오는 값에 대한 로컬 그래디언트가 돼요. 똑같네요. 그렇죠 그래서 업스트림을 갖다가 로컬 그래디언트 1이랑 곱해주면은 다운 스트림 그레디언트 0.2가 되고 그게 위아래랑 똑같",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 6,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 886,
      "char_count": 1674
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c007_fea572",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 왜냐하면 로컬 그래디언트가 위랑 아래랑 똑같으니까 그래서 여기서 다운스트림 그레디언트를 0.2 여기도 0.2 이렇게 써줬어요. 자 여기도 똑같아요. 똑같은 단계로 이 위를 아래를 와라고 하면은 요 프스는 x 더하기 와니까 엑스에 대해서 미분해도 1 와에 대해서 미분해도 1이에요. 그래서 로컬 그래디언트는 둘 다 똑같이 1이니까 업스트림 그레디언트가 다운 스트림으로 그대로 이렇게 가는 거를 알 수가 있어요. 여러분 패턴을 발견하셨는지 모르겠지만 이렇게 더하기라는 오퍼레이션이 있을 때는요. 업스트림 그레디언트가 똑같이 다운 스팀으로 똑같은 값이 이렇게 나가게 돼요. 만약에 이 업스트림 그레디언트가 0.2면은 같은 그레디언트 값이 인풋 스랑 인풋 와에 똑같은 값으로 이렇게 나누어져서 가는 어 형성을 이렇게 볼 수가 있어요. 자 그래서 우리가 더하기 했고 그다음에 이제 곱하기를 볼게요. 이거 같은 경우에 업스트림은 뭐예요? 0.2 그리고 임의적으로 이 인풋을 더블스라고 부를게요. 왜냐하면 제가 여기에서 더블 제로랑 제로랑 곱했으니까 그래서 이 프스는 어떻게 표현되겠어요? w랑 x랑 곱한 거 이렇게 표현이 되겠죠. 그러면은 이 w에 대한 로컬 그래디언트는 w에 대해서 미분한 거니까 x만 남죠. wx를 w에 대해서 미분 그리고 이 밑으로 내려가는 로컬 그래디언트는 엑셀에 대해서 미분한 거니까 w만 남죠. 그래서 서로의 그 반대의 오퍼 엘레먼트 그러니까 파라미터들을 테이했어요. 그래서 이렇게 미분을 해서 로컬 그래디언트를 구했고 그러면 다운스트림 구할 수 있죠. 우리가 업스트림은 0.2였고 그다음에 여기로 가는 로컬 그래디언트는 x니까 스가 마이너스 1이죠. 그래서 마이너스 1을 곱해서 마이너스 0.2가 요 위로 가고 그다음에 이 밑에서 오는 로컬 그래디언트는 더블죠. 그러니까 더블유는 2니까 2랑 업스트림인 0.2랑 곱해서 0.4가 나와서 이런 식으로 갈라져서 그레디언트가 가게 됩니다. 이 밑에도 똑같죠. 이 오퍼레이션이 더블엑스니까 로컬 그래디언트를 구해주면은 이 위에 w에서 오는 거는 x만 남아가지고 마이너스 2가 되죠. 그래서 마이너스 2랑 업스트림인 0.2랑 곱해가지고 마이너스 0.4가 이 위로 가고 이 밑에는 이걸 갖다가 x에 대해서 미분했으니까 w만 남아서 마이너스 3이 되고 마이너스 3이랑 요 업스트림인 0.2랑 곱해서 마이너스 0.6이 되죠. 그래서 여러분 이렇게 두 개의 곱하기 오퍼레이션을 보시면 아셨다시피 이 곱하기가 있는 경우에는 이 상대방의 인풋이랑 업스트림이랑 곱해서 나의 다운스트림으로 가는 거를 볼 수가 있어요. 예를 들어서 이 위에 w에 대해서는 그 상대방의 인풋이 x니까 마이너스 2 그리고 마이너스 2랑 업스트림이 0.2니까 마이너스 2 곱하기 0.2 해서 마이너스 0.4가 됐고 이 밑에는 x에 대해서 구하는 거니까 상대방 인풋인 w 그러니까는 그게 마이너스 3 그래서 마이너스 3이랑 0.2랑 곱해서 또 마이너스 0.6으로 해가지고 그레디언트가 수합돼 가지고 그러니까 각각의 변수가 수확돼 가지고 흘러나가는 거를 볼 수가 있어요. 자 이게 다예요. 그래서 보시면은 어 우리가 지금 한 게 뭐예요? 뒤에서 로스를 계산해서 뒤에서부터 인풋 단까지 쭉 그레디언트를 흘러가면서 계산을 한 거예요. 그리고 우리가 앞에서 배웠던 업스트림 그레디언트 배우는 거랑 로컬 그래디언트 계산하는 거랑 업스트림이랑 로컬 곱해가지고 다운스트림 구해가지고 하는 거를 반복적으로 실행을 했어요. 그래서 이거를 실행하면서 여러분들이 눈치 빠른 사람은 눈치 챘을 수도 있겠는데 이렇게 어떤 그 백프로파게이션을 하는 형태의 패턴이 있습니다. 그래서 예를 들어서 더하기가 있다 하면은 x 더하기 y라는 펑션에 대해서 로컬 그레디언트를 구하면은 각각의 변수에서 구한 게 1이죠. 그래서 업스트림 그레디언트가 똑같이 이렇게 어 다운스트림으로 흘러가는 거를 볼 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 7,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 1044,
      "char_count": 1962
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c008_0fed66",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 그래서 예를 들어서 더하기가 있다 하면은 x 더하기 y라는 펑션에 대해서 로컬 그레디언트를 구하면은 각각의 변수에서 구한 게 1이죠. 그래서 업스트림 그레디언트가 똑같이 이렇게 어 다운스트림으로 흘러가는 거를 볼 수가 있습니다. 그래서 이 에드라는 그 더하기라는 그 게이트는요 그레디언트 디스트리뷰터라고 불러요. 왜냐면은 어 이 업스트림이 똑같이 그 다음 스트림으로 디스트리뷰터 돼 가지고 흘러가기 때문에 그래디언트 디스트리뷰터라고 부릅니다. 네 그리고 그 멀티플리케이션 게이트 같은 경우 이 곱하기 게이트 같은 경우는요 아까 앞에 예시에서 보셨던 것 같이 이렇게 두 개의 인풋 스랑 와가 들어오면은요 스 곱하기 y를 갖다가 x에 대해서 미분하면 y가 나오고 스 곱하기 y를 x에 대해서 미분하면 y가 나오기 때문에 상대방의 변수 곱하기 업스트림 그레디언트가 다운스트림이 됩니다. 그래서 무슨 말이냐면요. 예를 들어서 이 위쪽을 보면은요 상대방의 변수는 밑에 있는 거죠 3이죠. 그리고 3이랑 업스트림이랑 곱하면 53 15 이렇게 15가 나오는 거고 이 밑에 있는 거는 위에 있는 거랑 업시림이랑 곱해서 2 5 0 이렇게 10이 나옵니다. 그래서 이 멀티플리케이션 게이트는요 우리가 스와 멀티플라이어라고 해요. 왜냐하면은 변수를 이렇게 수확해 가면서 그레디언트가 바뀌어 가지고 흘러 들어가기 때문에 스와 멀티플라이어라고 합니다. 그리고 같은 개념으로 카피라는 오퍼레이션은요 어 이렇게 그레디언트가 더해져 가지고 다운스트림으로 갑니다. 그래서 업스트림이 이렇게 4랑 2가 있으면은 그 그레디언트가 똑같이 합해져서 4 더하기 2 해서 6이 이렇게 다운스트림으로 흘러가게 되고 맥시멈이라는 오퍼레이션은요 어 오퍼레이션이 일어나는 방향으로만 그레디언트가 흘러갑니다. 예를 들어서 이 초록색이 인풋이라고 하면은 4랑 5 중에 맥시멈을 택하면 5가 맥시멈이잖아요. 그래 가지고 사실 이쪽으로는 이 위 택하지 않았던 그 인풋에 대해서는 그레디언트가 흘러가지 않아요. 택해진 인풋에 대해서만 그레디언트가 흘러갑니다. 그래서 이거는 그레디언트 라우터라고 그럽니다. 그래 가지고 이거는 이제 여러분께서 기억을 하시면 되고 실제로 현업에서는 이렇게 우리가 손으로 백 프로파게이션을 해야 되는 경우는 거의 없을 겁니다. 왜냐하면은 요새 뭐 텐서플로우라든지 파이토치라든지 그런 라이브러리가 너무 잘 돼 있기 때문에 그런 것들이 알아서 이제 코딩 안하고 그러니까 우리가 스스로 제 손으로 미분하지 않아도 코드 안에서 백프로파게이션 오퍼레이션을 다 해줍니다. 그렇지만 이거를 이해는 해야 돼요. 이걸 이해를 못하고 딥러닝을 한다고 그러는 거는 사실은 어떻게 보면 말이 안 되는 겁니다. 네 그래서 여러분들 잘 따라오셨기를 바랍니다. 그래서 이거를 이제 그 백프로파게이션 인플리멘테이션을 이렇게 파이썬 코드로 한번 이렇게 써봤습니다. 그래서 사실 이거는 그렇게 막 엄청 어려운 거는 아니고요. 포드 패스는요 우리가 어떤 식이 주어졌을 때 그 식을 그냥 이 코드 안에서 쭉 계산한다고 생각을 하시면 돼요. 그리고 그레디언트 컴퓨테이션은 실제로 파이토치 안에서 이루어지는 거긴 하지만 우리가 어떤 리그레션 식을 줬을 때 하나하나 미분을 했을 때 이렇게 코딩으로 써 줄 수 있다. 그래서 우리가 아까 이그 샘플에서 봤던 그 케이스들을 그냥 이렇게 코딩으로 이렇게 파이썬 코드로 써줬다고 생각을 하시면 됩니다. 네 그래서 오늘 되게 중요한 내용인 백프로파게이션을 배웠어요. 사실 뉴럴넷에 대해서 배운다고 그랬을 때 가장 어렵고 또 이해하기 힘든 부분이 이 로스에 대한 그레디언트를 백프로파게이션을 이용해 가지고 어 구하는 거예요. 그래서 이 백프로파게이션이라는 이론 자체가 생긴 게 사실 딥러닝이 생긴 거랑 같다고 생각하시면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 8,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 1038,
      "char_count": 1907
    },
    {
      "id": "transcript_deep_learning_basic_5강_기초_신경망_이론_2_back_propagatio_c009_07ddc8",
      "content": "[Deep Learning Basic] (5강) 기초 신경망 이론 -2 - Back Propagation\n\n다. 그래서 우리가 아까 이그 샘플에서 봤던 그 케이스들을 그냥 이렇게 코딩으로 이렇게 파이썬 코드로 써줬다고 생각을 하시면 됩니다. 네 그래서 오늘 되게 중요한 내용인 백프로파게이션을 배웠어요. 사실 뉴럴넷에 대해서 배운다고 그랬을 때 가장 어렵고 또 이해하기 힘든 부분이 이 로스에 대한 그레디언트를 백프로파게이션을 이용해 가지고 어 구하는 거예요. 그래서 이 백프로파게이션이라는 이론 자체가 생긴 게 사실 딥러닝이 생긴 거랑 같다고 생각하시면 됩니다. 예전에 딥러닝이라는 개념은 있었는데 이거를 이렇게 컴퓨팅 방법을 이용해 가지고 자동으로 그레디언트를 구하는 어떤 테크니컬한 방법이 잘 정립되어 있지 않았는데 이 백포파게이션 방법이 나오고 이거를 어 굉장히 많은 지표를 이용해 가지고 계산할 수 있게 되면서 우리가 지금 쓰는 그런 뉴럴넷을 사용할 수 있게 되었습니다. 그래서 다음에 그 연속적인 두 개의 시간 동안에는 뉴럴넷을 트레이닝 하기 위한 여러 가지 또 부수적인 테크닉들이 있어요. 그것들을 하나씩 살펴보면서 우리가 뉴럴넷을 트레이닝 하기 전에 익혀 둬야 될 이론적인 백그라운드를 배우는 시간을 갖도록 하겠습니다. 네 긴 시간 집중해 주셔서 감사합니다. 다음 시간에 뵙겠습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(5강) 기초 신경망 이론 -2 - Back Propagation.json",
        "lecture_name": "(5강) 기초 신경망 이론 -2 - Back Propagation",
        "course": "Deep Learning Basic",
        "lecture_num": "5강",
        "lecture_title": "기초 신경망 이론 -2 - Back Propagation",
        "chunk_idx": 9,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:3a11a3f0efc61c3e7ea922a29049fc652882a5a687c4356c90a9d7455597e3c5"
      },
      "token_estimate": 358,
      "char_count": 670
    }
  ]
}