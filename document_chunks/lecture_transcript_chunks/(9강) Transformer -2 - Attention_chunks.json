{
  "source_file": "(9강) Transformer -2 - Attention.json",
  "lecture_name": "(9강) Transformer -2 - Attention",
  "course": "NLP",
  "total_chunks": 7,
  "chunks": [
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c000_1beb29",
      "content": "[강의 녹취록] 과목: NLP | 강의: 9강 | 제목: Transformer -2 - Attention\n\n네 안녕하세요 여러분 머신 러닝 라이프 사이트를 배우고 있고요. 저는 알리아드 대학교 인공지능 문학관의 김수현이 맞습니다. 네 저번 시간에 저희가 아레바 피퍼스트 시퀀스 모델에 대해서 배웠는데요. 오늘은 이 아레과 티퍼스트 시퀀스 모델의 단점을 보완하는 어텐션 모델에 대해서 배워보겠습니다. 그래서 오늘 내용이나 기본 정보 어플로그의 구조랑 비 어프레션을 쓰면서 너 주관하는 정보에 대해서 이기도록 하겠습니다. 아 그래서 이번 가에서 전시관에서 전시관에서 다룬 아래에는요 사실 아직 완벽하지 않습니다. 여기 세 번째 이슈에 대해서 있는 말씀을 볼 텐데요. 저희가 아내로 제한된 상태를 LSTM이랑 비알리에 대해서 배웠잖아요. 그런데 이 LSTM과 비알리 마저도 매일 기후 피커스를 결정할 때 문제가 발생했습니다. 그래서 만든 선물이 됩니다. 그래서 이 건물들에서 한번 심도 있게 보도록 하겠습니다. 어떤 점이 몇 평이지 이게 저번 시간에 배운 피터 티피컬트 모델을 비교해요. 먼저 이 피터스티키터스 모델은요 인크드 디큐브 비교를 이용했다 고요. 그래서 인큐디에서는 그래서 이게 스타트 센스 모델인데 이게 한계가 있습니다. 사실 이렇게 인테티드하게 생각했을 때 이 모델 자체는 우리가 인테트 2단 후반 굉장히 긴 선장을 다 처리를 할 수 있다고 하지만 여기에서 단점은 뭐냐면 예를 들면서 우리가 인풋 어투가 굉장히 긴 피설트라고 생각을 해 보세요. 그 예를 들어 3번 그림을 갖다가 인풋을 해가지고 마지막 장을 하나의 벡터는 신고 인도 등으로 인쇄하게 돼요. 기본적으로 그래서 증상이 비 시퀀스 같은 경우에는 하나의 대표를 인베딩을 해야 되는데 여러분들이 극단적으로 생각할 때 하나의 개체를 인쇄하기에 좀 역부족이지 않을까 이렇게 생각해 볼 수가 있는데요. 사실 그게 맞아요. 그래서 어 여기 써 있는 것처럼 레시턴트 같은 경우는요 인플레이션 노트가 불가피합니다. 즉 아파트 중에 있는 단어는 유전적 특성을 가지고 있습니다. 그래서 이전에 입력된 장비는 더 많이 잊혀지는 이런 전향성을 갖고 있습니다. 그래서 만든 어깨는 아이디어인데요. 기본적으로 시퀀스 피팅이 제한된 상태라고 보시면 됩니다. 이 디터진 장에서 모든 입시 시퀀스에 12지 고려해서 관련 시장이 높은 인식 패킹에 더 집중하는 구조를 어선적로 만들었습니다. 그래서 기본적으로 우리가 이렇게 아이스 패킹만 선팅을 하고 그 스스 유튜브 어가 이 앞에 있는 모든 문장들이 다 볼 수 있도록 모델을 바꾸는 겁니다. 예전에 시사 던 글들은 인풋에서 그런데 인도 중심 배터리 중심으로 그거를 인핏으로 받아서 마을에서 예측했던 점만 이 어떤 선에 대한 나오거든요. 기본거리를 인수하는 모든 개체에 대한 1베스테이크를 잡 인체크로 그리고 그 중에서 가장 자연성이 높은 시스케이크에 더 집중을 해 가지고 이렇게 알 것을 예측하는 건가요? 그렇게 되면 기본적으로 이렇게 온천 직접 콘텐츠에 대한 문제점이 어느 정도는 완화될 수 있을까요? 그래서 이제 본격적인 어떤 테크니컬 트랙터를 시작 될 텐데 특히 복잡할 수 있지만 잘 따라오시면 굉장히 쉬운 아이디어입니다. 그래서 우리가 에펜션 함수라는 거를 보는 거예요. 사실 에펜션 모델이 이 함수 하나를 잡는 데 있는데 이 어토니선 텐트는요 페리의 인식을 가지고 있어요. 커니티 밸리 스페이라고 써있는데 퍼니트 젤리에요. 그래서 이 커리티 젤리를 인쇄기를 잡아서 어체선 선트를 생겼어요. 이게 어떤 사람이 잡고 이렇게 해야 될 거예요 자 그러면 이 서비스 개리는 뭐냐면 파는 기본적으로 생각했을 때 이상은 기분이라고 생각하면 돼요. 부분 그리고 실험 결리는 여러분들이 기대하는 계단이라고 생각을 하면 됩니다. 그래서 여기 실험 결리는 따로 이렇게 썼는데 실제로는 실험 결리는 거의 같은 의미로 쓰입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 0,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 1033,
      "char_count": 1901
    },
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c001_775d98",
      "content": "[NLP] (9강) Transformer -2 - Attention\n\n다. 그래서 여기 실험 결리는 따로 이렇게 썼는데 실제로는 실험 결리는 거의 같은 의미로 쓰입니다. 피가 기여하는 대상이 되는 어떤 기준이고 그 기여하는 대상의 실질적인 값이 이제 저리라고 생각하면 되는데 얘가 다 이야기할 거예요. 자 시는 기교와 가는 게 뭐예요? 서류가 부부라는 부부이고 신은 기부하는 대상이라는 것이에요. 그런데 부부 가능해야 된다는 게 같은 표현 하나 절차라는 거예요. 예를 들어서 서류가 대표 자리에 걸쳐 있는 신혼 1표에 적혀 있고 젤리와 어센션 밸리는 동일한 기능들을 가진다고 써 있는데 밸류는 썸스에는 다른 기능 범위로 있어도 돼요. 예를 들어서 선스가 대창이나 대체하는 밸리는 예를 들어 이벤트에 이렇게 다른 기능 범위를 가져도 됩니다. 그래서 피터 피터스 2번의 정맥을 이 어떤 장 통해서 우리가 정리를 할 수 있는데 예를 들어서 기준이 되는 패링이 뭐가 되냐면은 기체에 따는 수종 스케이트 즉 기체에 다는 피드 스케이트를 비교가 되는 대상 비교가 되는 대상이니까 여기 수퍼 스케이트 모델에는 비교가 되는 대상이 뭐예요? 그러니까 공포됐다는 수비 스테이크죠. 피라 결론은 10대 점에 150% 가 있어요. 그래서 어떤 게 1이 어떻게 만들어졌냐면 먼저 파이랑 키 사이에 다 프로젝트를 합니다. 이 음식 아미 중에 어떤 더라스 아이스트 수동 스테이트가 가장 유사한지를 우리가 바스 파스를 통해서 배우는 겁니다. 그래서 여기 했는데 에이크 열이랑 여기 대출이 있다면 처리가 되는 스테이지랑 같이 배치를 해서 여기가 되고 그다음에는 에이스 프랑 다시 배출해서 미사를 개정하고 한다면 어떤 권리가 있을 수는데 우리가 생이 이렇게 하는 게 이해가 되죠. 근데 왜 이런 말을 하느냐 도대체 지금 우리가 하는 게 어떤 의미가 있나를 한번 보도록 할게요. 그러면은 이 어텐션 프레스센트 즉 어텐션 펑션에 의해서 보여지는 애플이션 프레스선트는 어떤 의미를 가지고 있느냐면요. 이 인터더 시드 스테이크가 디코더 스케이트 아프로젝트가 유사도를 예측을 한다면 유사도를 우선 계산을 한다면 그중에 가장 유사도가 높은 인코더의 시전 스테이츠는 팀 레이스를 줘가지고 유사도가 가장 높은 피드 스케치를 가지면 밤에 더 집중을 해 가지고 전혀 같은 작업을 하는 겁니다. 그래서 정리를 하지 않으면 처음에는 이체를 을 수도 있고 할 수 있은 경우는 그리고 피라 밸류는 인코더스 스테이크가 있고요. 그리고 에센션 밸류는 인테리의 시그니 스테이크에 웨이트 어베이트 즉 인테리어 모든 회전 스테이크에 어디에 더 붙은 사람들이 배워가지고 가중치를 곱해서 평균을 낸 거죠. 그러면 이 결과 자체를 아웃풋은 뭐가 되냐면은 인플레이 스리 스페이스를 갖다가 우리가 웨이트 에버리즘과 예측 값을 낸 결과가 됩니다. 이렇게 되면은 우리가 피파스티티타스 모델 같은 경우는 인풋 전비 중에 악장을 다 잊어버리는 특성을 가지고 있었는데 이렇게 어센션을 이용을 하면은 우리가 모든 인풋의 히든 스테이킹을 인핏으로 잡아 가지고 자금치 평균을 내기 때문에 앞의 정보를 잘 이용을 해서 효과 자금을 예측 값을 낼 수 있게 됩니다. 그래서 우리가 시뮬레이트 스퀘어에 대해서 이야기를 했는데요. 통상 메커니즘에서는 시뮬레이트 스케일이 다 프로젝트를 썼어요. 그래서 이렇게 이 시리 스테이크 인테리어 시리 스테이크를 그런데 여기서 이렇게 매각을 쓴 이유는 뭐냐 하면 매각 자체가 매개 변수가 적고 간단하기 때문에 이렇게 쓴 거예요. 그런데 실은 다른 여러 가지 요소들이 추천하는 방법들이 있어요. 왜 이렇게 플라이 벨트는 넣었는데요. 커피 가능한 가중치도 있는데 잡풀 백터 비트에 이 바풀 백트는 액트랑 얼크를 갖다가 이렇게 배출되기 했는데 그 사이에 어떤 워낙 한 메타미터인 더브리를 넣을 수 있을까 그리고 또 다른 점도 이유는 이렇게 3이란 키 이체베이스 베이스테이지랑 2015 페이스를 이렇게 컨디티메이션 해가지고 웨이 파이미터를 곱하기 액티베이션 원료를 띄우는 이 물을 것을 통해서 그리고 프레이트를 예측하면 나타날 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 1,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 1089,
      "char_count": 1998
    },
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c002_8b8433",
      "content": "[NLP] (9강) Transformer -2 - Attention\n\n다. 근데 실은 실제로 프로스트의 모델에서는 어플라이 많이 써요. 그래서 우리가 어떤 사람은 프리 점프를 대하는 법을 배웠고 있었는데 그거를 이제 적용을 해 가지고 실제로 매스 트랜스레이션 번역 테스트를 하는 이벤트를 한번 볼게요. 먼저 우리가 이렇게 인피지를 아스플이라는 여러 문장을 번역한다고 생각을 해보세요. 그러면은 먼저 뭐 예를 들어서 에어스를 마시는 걸로 이 각각의 단어들을 스테이크를 인테링을 하겠죠. 그다음에 이 캐릭더에서 이 스타트 테크는 인쇄 한다고 생각해 봐요. 그러면은 이 스마트 테크는 히브케이스인 에스를 인크드해 되겠죠. 그러면 여기서부터 시작입니다. 펀드가 뭐예요? 그렇죠 7의 히드케이스 스입니다. 그래서 예를 들어서 스 1이랑 이 개를 하면 유사 되는 밀 박스 이게 이렇게 되는 거죠. 그러면은 밀박이 무슨 의미를 갖냐면 이 각각의 단어들 아수 없음에 각각에 해당하는 단어들이 이 프로스토킹이랑 얼마나 더 유사하게 되는지 어떤 단어가 정확하게 가장 높은 플러그이티를 유용하게 작용되는지를 예측하게 되는 거죠. 그런데 이 수이 수이 가장 높은 확률이 맞아요. 그러면은 그 벡터를 그러니까 이 많은 예시를 어베이지를 해가지고 많은 이 대표를 이렇게 영어를 다른 언어를 테마로 이렇게 번역하는 테스트를 할 수가 있기 때문에 어렵지 않죠. 그리고 이 테스트를 연기 패킹이 나올 때까지 보고합니다. 그러니까 기본적으로 이제 매니스 메뉴가 이어지게 즉 무슨 말이냐면 그 여러 문장은 예를 들어서 길이가 100개고 그 월드의 배수가 100개고 이들이 단합했을 때 총 2장의 개수가 단어 수가 70g으로 하면은 인식과 아프고 길이가 다를 건데 그래도 처리할 수 있다는 거예요. 왜냐면은 이 단어를 하나하나하나 뽑아가지고 엔드 체크를 완성까지 하고 있었네요. 네 이해되시죠? 그래서 이 성적 메커니즘에서 굉장히 큰 관점이라고 할 수 있는데 이 앞에서 우리가 모델을 이렇게 구성을 했잖아요. 이거를 다 연한 다음에 우리가 애편적 레드라는 걸 만들 수 있습니다. 즉 예를 들면 아라는 단어를 우리가 키라고 번역할 때 여기가 이를 인지고 이라는 단어를 변형을 할 때 각각의 단어들이 어떤 확률 분포로 이루어져 가지고 예측을 했는지를 알 수 있다는 거죠. 즉 각각의 그 일축 단어란 아이스 단어들을 상관 관계를 이렇게 어필한 내부를 그 수가 있습니다. 그래서 보시면은 예를 들어서 뭐 유라는 단어를 이렇게 큰 언어를 번역할 때 요 단어가 가장 중요하게 썼던 것이요. 이 부분 근데 실제로 일하는 의미가 1년 5개 이렇게 전역이 된다는 걸 알 수가 있어요. 그래가지고 이게 뭐가 되냐면 이 어센션 가드를 예측을 하면은 예측 자체에서 끝나는 게 아니고 어떻게 미래의 예측 결과를 예측을 했는지 미래를 우리가 살짝 다른 형태로 디g얼라이즈 할 수 있다는 게 어렵지 않다. 그리고 다른 응용 서버는 우리가 피파스트티센스 모델을 적용하면 어떤 모델도 다 어센션을 이용해서 할 수가 있는데 테스트 세미라이제이션 같은 거를 볼 수 있어요. 테스트 세미라이제이션이 뭐냐 하면 어 말 그대로 어떤 길을 갖다가 서명하게 하는 거예요. 그래서 예를 들어서 이렇게 긴 문건이 들어왔을 때 이들을 갖다가 짧게 요약하게 하는 것을 볼 수가 있죠. 여기까지는 약간 힘이 되었습니다. 그다음에 있는 게 많아요. 트는 이 틀에는 지금 자리는 첫 GPP의 공간이 되는 모델 스타터입니다. 정보 이 프린스나 아이디어는 실제로 기술 리포트 컨테니스 19 미디어라는 논문을 통해서 소개가 됐는데 오늘 역사는 이 어떤 선에서 오리 미드라는 모임을 바탕으로 해가지고 구성되어 있어요. 이제 리뷰를 해 볼게요. 사실 우리가 앞에 그 정서에서 딥러닝 mat 그리고 신인에 대해서 간략하게 들었고요. 신명은 컨베이션 이력이 없습니다. 그렇게 기존에 우리가 개인 어떤 미래의 기준을 한 기본 적으로 실험 데이터를 입력 에서 레이드 와이를 가장 잘 매칭되는 아벤트의 기사를 통해서 제작이 되었습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 2,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 1065,
      "char_count": 1972
    },
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c003_2ef071",
      "content": "[NLP] (9강) Transformer -2 - Attention\n\n다. 정보 이 프린스나 아이디어는 실제로 기술 리포트 컨테니스 19 미디어라는 논문을 통해서 소개가 됐는데 오늘 역사는 이 어떤 선에서 오리 미드라는 모임을 바탕으로 해가지고 구성되어 있어요. 이제 리뷰를 해 볼게요. 사실 우리가 앞에 그 정서에서 딥러닝 mat 그리고 신인에 대해서 간략하게 들었고요. 신명은 컨베이션 이력이 없습니다. 그렇게 기존에 우리가 개인 어떤 미래의 기준을 한 기본 적으로 실험 데이터를 입력 에서 레이드 와이를 가장 잘 매칭되는 아벤트의 기사를 통해서 제작이 되었습니다. 즉 에너지와 수전은 이식을 통해서 아이를 예측하는 어떤 모델인데 이렇게 어떤 뉴런을 이용해 가지고 모델의 기본을 몇 개 만든 거니까 웨이퍼 미터인 버블리이 베이퍼 니트에서 베이는 거였어요. 근데 얘를 어떻게 계산을 하나 봤더니 이 리얼 라이트 끊어봤더니 얘는 우리가 개인 모형이에요. 인픽에다가 에이티미터 곱하기 액티베이션 키우고 또 레이스타미터 곱하기 액티베이션 키우고 이거를 영상 장부가 하는 니까 y를 갖다가 예측을 하는 거죠. 그래서 이게 뭐 하는 거냐 보면은 인픽에다가 에이티미터를 곱해 가지고 결국은 라이브 예측을 하는 거예요. 그리고 아르만도 똑같아요. RNN은 무엇인가요? 이 아르만도 사실 이렇게 피파스는 데이터를 처리하는 거는 맞지만 피어과 다르게 얘는 자본적인 데이터를 처리하는 거는 다르지만 이 본질적인 성질은 똑같아요. 인풋 엑스가 들어오면은 저를 갖다가 웨이트를 곱해 가지고 여러 가지 처리를 해 가지고 아웃풋은 와이를 예측을 하는 거예요. 자 클럽 와인은 인풋 스의 어떤 웨이트리 전 세밀이 표현될 어요. 근데 프랑스는 좀 달라요. 우리가 여태까지는 눈빛이 들어와서 아이핏을 예측할 때 인풋 채널에다가 어떤 웨이트를 곱해 가지고 라 를 했어요. 인풋 라이트 에스트라 이랬어요. 그런데 페이 채널은 좀 달라요. 뭐가 다르냐면 인식을 통해서 아웃풋을 예측하고 예를 들어서 한 단어를 다른 언어의 단어를 트렌드 레이드 했고요. 그 단어만 보는 게 아니고 그 문장을 트는 단어의 뒤에 있는 어떤 문 닫은 자녀들과의 편협성이 됩니다. 즉 여기 써 있어 기본 가정 이 시가 그 뒤에 있는 다른 여러 2세들과 이시세들을 오전에서의 선형을 수 있다고 가정을 합니다. 이게 무슨 말이냐 하시면 예를 들어서 이 그림을 보시면 이 여자가 있어요. 이 여자 뒤에는 수많은 사람들이 있습니다. 이 여자의 가족이 있을 수도 있고 직장 동료가 있을 수도 있고 수위가 있을 수도 있고 이 여자는 어떤 사회의 일원입니다. 그런데 이 여자를 갖다가 국자라는 어떤 개체를 만들 때 이 여자 자체에 대한 경쟁심을 놓지 않고 이 질문 듣는 사람들에 대한 어떤 네이티브 사이로 이 여자에 대한 설명을 한다고 생각을 하시면 됩니다. 즉 이 여자는 독립된 존재가 아니고 이 사회가 다른 사람보다 유기적으로 관련된 존재라고 생각을 하는 거죠. 네 이렇게 하니까 좀 더 이해가 될지도 모르지만 그래서 지금도 중에 골치 애펜션이라는 말이 많이 많은데요. 이 논문에서 세트 에펜션이라는 내년에는 각각의 엘리먼트를 어떤 콘택트의 생활과 유티젠테이션에 대응 형태를 학습을 하는 겁니다. 즉 이 사람에 대한 관계하고 똑같아요. 예를 들어서 이게 사람이 아니고 인간의 단어라고 생각을 하면 각각의 얼리먼트는 문장의 책에 있는 어떤 단어일 거요? 그 각각의 단어들은 그 전체의 문장에 있는 어떤 콘택트의 유지 사실이 소명이 돼 가지고 이틀전에서는 자리가 있는 사실 에체스 테너는 아까 앞에서 배운 애천선이랑 똑같아요. 그래서 그 어선이 다시 한번 얘기해 볼게요. 우리가 에천선 3수를 배웠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 3,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 979,
      "char_count": 1808
    },
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c004_469d61",
      "content": "[NLP] (9강) Transformer -2 - Attention\n\n다. 즉 이 사람에 대한 관계하고 똑같아요. 예를 들어서 이게 사람이 아니고 인간의 단어라고 생각을 하면 각각의 얼리먼트는 문장의 책에 있는 어떤 단어일 거요? 그 각각의 단어들은 그 전체의 문장에 있는 어떤 콘택트의 유지 사실이 소명이 돼 가지고 이틀전에서는 자리가 있는 사실 에체스 테너는 아까 앞에서 배운 애천선이랑 똑같아요. 그래서 그 어선이 다시 한번 얘기해 볼게요. 우리가 에천선 3수를 배웠습니다. 애천선 PT 코너를 위해서 탐지를 주었는데 에천선 탐지는 기본적으로 기준이 되는 기계의 기준이 되는 처리가 기계의 대상이 되는 시간을 같은 각지에서 10ml를 그린 다음에 그거에 대위를 갖다가 붙여가지고 그 값에 웨이트를 점을 해 가지고 어떤 건 프레스 상태를 줄 거예요. 그래서 그거의 어떤 의미는 뭐냐 비교가 되는 대상은 크 기조의 기준은 하라에 이사들 만큼의 이 밸리의 웨이트드 점을 해 가지고 그 밸리라는 여러 가지 단어를 머신 프로스라이선스에 있는 경우에는 그 밸리에 있는 여러 단어들을 어떤 시멘트 웨이트드 점을 갖다가 개정을 함으로써 우리가 번역하려고 하는 문장으로 가는 중에서 어디에 더 집중을 해야 되는지 우리가 저희는 옆에서 한 거 있을 거예요. 그래서 이 이름을 그대로 가지고 있는 겁니다. 결국은 우리가 쓰는 프로톤에서 우리가 쓰는 어텐션 선전은 어센션에서 나오는 어텐션 편션은 똑같아요. 그러면은 여러분들이 이렇게 질문을 할 수도 있어요. 그러면은 파스 밸류는 뭐가 돼야 할까요라고 생각할 수 있죠. 사실 이 트랜스포머가 에피스 모델과 다른 거는 이 사이트 밸류를 갖다가 만들 때 메리 파라미터를 쓴다는 거예요. 밸류는 그렇게 복잡하지 않을까요? 그래서 한번 보도록 할게요. 만약에 이렇게 예를 들어서 단어 테스트에서 인풋 팩트를 넣고 배우겠다고 생각을 하면 좋아요. 까지 그러면은 그 여러 에서 그냥 그 액체는 지금 단어를 의미하잖아요. 어떤 단어를 이렇게 지어준단 말이에요. 그러면은 이 단어를 갖다가 설치 개이라는 대체를 표현하기 위해서 그 대표의 변화에 서리를 보내는 백혈 시를 보내는 백혈 개울을 보내는 백터를 갖다가 붙여주는 거예요. 그래서 여기 그림에서 보여주는 것 같이 어떤 아이가 되어 봤을 때 여기에 웨이트 웨이트 웨이 밸류를 갖다가 붙여서 포인트 밸류를 붙여줍니다. 이렇게 각각의 토핑을 미의 트스템이 해진이 있어요. 리네 페스템이라는 거는 리뉴얼 레슨 더블를 곱한 멤버죠. 얘를 곱해서 서스 밸류를 보내줘요. 그런 데는 이제 애플레이션 밸류를 이렇게 보이는 겁니다. 어텐션 밸리를 원래 공간을 다시 예측하는 또 다른 사치가 가능한 레저 스인 박 시장입니다라고 써 있는데 내용에는 이제 우리가 나중에 리뷰를 통해 가지고 어텐션 플스 세트를 다 한다고 예측한다면 다시 이제 우리가 어떤 문장 형태를 적게 줘야 되잖아요. 그때 we를 쓴다는 겁니다. 그래서 좀 더 자세하게 이제 모델 비교하면서 설명을 하면요. 예를 들어서 이렇게 몇 개 몇 3가 우리의 인식이에요. 그러면은 기준이 되는 대상은 자기 자신이 될 수가 있겠죠. 그러니까 연구에서는 어떻게 하는 편이에요? 그런 다음에 피 대리는 뭐냐 하면은 나랑 비교가 되는 그 한계의 어떤 패턴들이 되겠죠. 그래 가지고 예를 들어서 스1이 터지면 키 스1 스가 되겠죠. 자기 자신과도 비교를 해야 돼요. 그다음에 밸류는 또 똑같이 엑스셀 없셀 이해가 되겠죠. 그래서 그 커트 밸류를 실제 배하게 만들어 주기 위해서 먼저 스터에다가 키를 곱해서 서류를 만들어요. 그런 다음에 또 키라는 대체를 만들기 위해서 체를 붙여서 소리 원인을 만듭니다. 그리고 그다음에 이제 편이라 키가 있으니까 그거를 뭐예요? 자체 같이 해서 요소들을 조사를 해야겠",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 4,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 1000,
      "char_count": 1851
    },
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c005_cffb06",
      "content": "[NLP] (9강) Transformer -2 - Attention\n\n다. 그리고 그다음에 이제 편이라 키가 있으니까 그거를 뭐예요? 자체 같이 해서 요소들을 조사를 해야겠다. 그럴 때는 의자비가 0.93으로 하고 이제 지방은 8일 0이랑 17을 갖다가 다시 했으면 이자비가 0.01로 낮고 그다음에 17년 같은 대체지는 0.01이 나왔어요. 그러면은 이 자들은 그 철이란 그 대상이 되는 치아의 어떤 위협이 되겠죠. 그다음에 여기에다가 젤리를 각각 붙어 밸리안 밸리트 밸리스 그러면은 각각의 젤리들의 웨이 선들이 이렇게 보여주게 되고 그게 제트원이 되는 겁니다. 배트 원리를 만들기 위해서 사고 오라는 미니 커스템을 한 번 더 해줍니다. 이 결과 그래서 우리가 이 과정을 통해서 대체 0이라는 것을 예측을 했는데 이 인도인 대체 0은 어떤 의미를 가지고 있냐면 이 에스원을 갖다가 위에 있는 얼라이먼트들과 유사들을 생각을 해 가지고 다시 20센트에서 연임을 한 거죠. 그러니까 소위 예를 들어서 우리가 아까 어떤 여자를 갖다가 태어날 때 그 여자 자신뿐만 아니고 그 여자의 친구라든지 뭐 가족이라든지 동료를 어떤 이기적인 관계라고 생각을 한다고 했잖아요. 그거랑 똑같아요. 예를 들어서 이 어깨 안이 그 여자라고 생각을 한다면 그 프리는 그 여자의 지위에 있는 사람들입니다. 이 어테란은 뭘 의미하냐면요. 이 여자를 설명하고 그 여자 자신이 아니고 그 여자 뒤에 있는 사람들의 유기적인 관계까지 고려해서 그 여자를 설명한 어떤 절차라고 생각을 하시면 됩니다. 그래서 이들이 뭘 했냐면요. 컨포스트라이즈 한다고 다 이 xm을 갖다가 어떤 그 다른 그 예를 들어 이제 머신 프로듀레이션이 가면은 하나의 원들을 그 어떤 문장 안에 있는 콘텐츠를 잘 이용해 가지고 다시 이틀간 연을 한 거예요. 그게 내 아이디어예요. 그래서 음 이제 우리가 컴퓨터 주변에서의 프로스트의 이용을 조금 보도록 할게요. 예를 들어서 우리가 뭐 비디오 클래스피케이션 같은 테스트를 생각해 볼게요. 비디오를 보고서 그 비디오가 어떤 비디오인지를 클래스다 하는 그런 테스트를 생각할 때 사실 이 프린스는 애플레이션도 똑같아요. 그러면 우리가 아까 배웠던 거 똑같이 하면 돼요. 먼저 이렇게 2개니까 내가 시한 어떤 이 2개를 각각의 사운드마다 이렇게 샘플링 해 가지고 연극적인 어떤 이미지들의 스파이를 표현을 한 거예요. 그러면은 한번 해볼게요. 먼저 뉴 x3 xx xx 파이브를 그리고 어 키를 갖다가 만들기 위해서 각각의 액수에다가 테를 갖다 붙여서 실로 테이프를 다 전환을 합니다. 그리고 그 엑스를 갖다가 비를 곱해서 제 페이스를 이렇게 보내다 그런 다음에 이 71이랑 21이랑 같이 다 해서 이사 배제 상태 이거를 모든 소에 대해서 하게 돼 있죠. 그런 다음에 각각의 의자들만큼 베이를 갖다가 급등하게 하도록 하기 위해서 이자이랑 베이랑 붙어요. 그다음에 에 에베이드 해 가지고 나 밖에다가 빨리 오이 붙어서 대체하는 예측을 했는데 아까 앞에 했던 거랑 똑같아요. 그래서 첫 번째 이미지에 대한 애플 전트를 정명을 주었어요. 이게 2기를 갖다가 돌이 된 이미지에다가 제출을 하게 됩니다. 그러면는 인풋은 각각의 그 타임 스텝에 대한 이미지였지만 it 10부터 19 사이즈는 각각의 이미지 뿐만 아니고 그 이미지가 그 옆에 있는 이미지가 어떻게 유지되고 프리베이션 됐는지를 가능한 어떤 좀 더 컴포트 라이즈 대한 역티젠테이션이라고 우리가 생각을 하면 되겠죠. 그래서 실제로 프렌스트는요 이렇게 하나의 레이어만 쌓지 않고 이 과정을 여러 번 반복하게 하기 위해서 여러 개의 레이어를 쏴요. 그래서 이 그림에서는 2개의 프랜스팅을 중하게 쌓 그다음에 어포션 에틸레이션을 성공하고 또 한 번 더 하면 되고 데이터 베이 훨씬 더 컨포티얼라이제이션을 알게 됐죠. 기대한 점 그래서 여기까지가 기본적인 프로그램에 대한 아이디어입니다. 그렇게 어렵지 않아요. 저희가 저번 시간에 아르나나 에틸레이션에서 배우부터 쭉 잡아주는 시트 시서스 모델에 대해서 배웠습니다. 그래서 그 슈퍼스트 시퀀스 모델의 장점을 지득하기 위한 게 어텐션 모델입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 5,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 1082,
      "char_count": 2015
    },
    {
      "id": "transcript_nlp_9강_transformer_2_attention_c006_9cec76",
      "content": "[NLP] (9강) Transformer -2 - Attention\n\n다. 그렇게 어렵지 않아요. 저희가 저번 시간에 아르나나 에틸레이션에서 배우부터 쭉 잡아주는 시트 시서스 모델에 대해서 배웠습니다. 그래서 그 슈퍼스트 시퀀스 모델의 장점을 지득하기 위한 게 어텐션 모델입니다. 그래서 오늘은 같이 해서 되는 게 그 어텐션 모델 중에서 어텐션 키트를 계산을 해 가지고 슈탄스트 시퀀스 모델을 개선하는 것이었어요. 그리고 프리스크램은 어떤 색 모델을 좀 더 저널 구한 셈이라고 좀 더 일반화된 셈이라고 생각하시면 돼요. 그래서 기본적으로 피스 피스 모델에서 이 프리스트의 모델까지 읽는 데까지의 그 어떤 협력의 힘이라든지 그 세트에서 발전 프로젝트들 같은 게 그렇게까지 어렵지 않습니다. 근데 프로젝트는 사이 있었던 부분을 이해하기가 어렵다. 그래서 여기까지가 이제 프레스 패널에 대한 이론이고요. 다음 시간에는 프레스 패널 모델에 대해서 좀 더 참고해 보도록 할 거예요. 그래서 어 프레스 채널 모델을 좀 더 집어서 하나하나 보도록 할 거고 그다음에 애플리케이션 점에서 프렌스키마 모델이 에너지 그 컴퓨터 기본에서 어디에 쓰이고 있는지 한번 알아보도록 하겠습니다. 네 오늘 배운 내용 되게 중요하니까 한 번 더 리뷰해 보고요. 다음 시간에 조금 더 레지던스 된 내용으로 찾아뵙겠습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(9강) Transformer -2 - Attention.json",
        "lecture_name": "(9강) Transformer -2 - Attention",
        "course": "NLP",
        "lecture_num": "9강",
        "lecture_title": "Transformer -2 - Attention",
        "chunk_idx": 6,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:afe1746b8b562343d1bcfd846b9f7504bac2aaa80cd0fb8e7a3f78bca6c65249"
      },
      "token_estimate": 359,
      "char_count": 671
    }
  ]
}