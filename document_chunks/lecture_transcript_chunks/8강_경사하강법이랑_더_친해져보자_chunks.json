{
  "source_file": "8강_경사하강법이랑_더_친해져보자.json",
  "lecture_name": "8강_경사하강법이랑_더_친해져보자",
  "course": "AI Math",
  "total_chunks": 6,
  "chunks": [
    {
      "id": "transcript_ai_math_8강_경사하강법이랑_더_친해져보자_c000_58596e",
      "content": "[강의 녹취록] 과목: AI Math | 강의: 8강 | 제목: 경사하강법이랑_더_친해져보자\n\n안녕하세요. 메스메릭스포 알티피셜 인텔레전스 8강 경사강법이랑 더 친해져 보자 수업을 진행하게 된 고려대학교 통계학과 임성빈 교수입니다. 네 저희는 지난 시간과 지지난 시간에 미분의 개념과 그리고 벡터 공간에서의 미분 개념 즉 다차원 공간에서의 그레디언트 벡터를 계산하는 방법을 배웠고 이를 통해서 경사하강법 알고리즘을 배워보았습니다. 그리고 경사하강법 알고리즘의 문제점도 한번 배워보았는데요. 이번 시간에는 이런 경사하강법 알고리즘을 좀 더 우리가 업그레이드해서 사용하는 방법을 배우도록 하겠습니다. 지난 시간에 배웠던 경사하강법의 문제점에 한번 복습을 해 볼 것인데요. 경사하강법 알고리즘은 그 값에 도달하면 미분 값이 0 근처로 떨어지기 때문에 더 이상 업데이트를 할 수가 없게 됩니다. 그러므로 최적화 과정이 매우 느려지게 되는데 딥러닝에서처럼 모델 파라미터에 대한 손실 함수가 넌 컴백스트 즉 볼록하지 않은 경우에는 문제가 심각해질 수 있다고 제가 말씀을 드렸던 적이 있습니다. 그래서 딥러닝 최적화 과정에서는 경사 하강법을 사실 그대로 쓰시면 안 됩니다. 그렇다면 이 문제를 우리가 어떻게 극복할 수 있을까요? 네 경사 강법의 업그레이드 버전인 확률적 경사 강법 즉 스토캐스틱 그레디언 디센트 줄여서 SGD 알고리즘을 배워보도록 하겠습니다. SGD는요 모든 데이터를 사용해서 그레디언트를 계산하는 경사 강법 대신 데이터를 1개 또는 일부를 활용해서 미분해 계산한 후 그걸로 페라미터를 업데이트 하는 방식입니다. 즉 경사강법의 경우는 모든 데이터를 이용을 해서 그 데이터를 기반으로 어떤 패러미터에 대한 그레이디언트를 계산하는데요. 확률적 경사감법은 데이터 전체가 아니라 데이터의 일부를 가지고 즉 이 전체 데이터 디가 아니라 그거의 서셋 부분 집합에 해당하는 데이터의 일부로 가지고 이렇게 목적식을 계산하는 거에 그레디언트를 계산해서 이걸로 파라미터를 업데이트하는 방식입니다. 얼핏 생각하면 데이터 전체가 아니라 데이터의 일부만 가지고 그레디언트 벡터를 계산하니까 정확하지 않은 그레디언트라서 왠지 경사 하강법 알고리즘이 제대로 동작하지 않을 것처럼 보이는데요. 네 맞습니다. 사실은 확률적 경사하강법은 데이터의 일부만 가지고 그레디언트를 계산하기 때문에 정확한 그레디언트를 계산해 주지는 않습니다. 다만 확률적 경사하강법은 우리가 기댓값의 측면에서 경사하강법 즉 그레디언트 벡터를 우리가 그 기댓값의 측면에서는 일치하도록 계산하는 것이기 때문에 우리가 많은 업데이트를 하는 과정에서는 비록 정확한 그레이디언트는 아니더라도 우리가 확률적으로 그 그래디언트의 원래 방향을 우리가 향하도록 설계를 할 수가 있게 되는 것이고 이 과정에서 바로 앞에서 살펴본 문제를 극복하도록 유도하게 됩니다. 즉 넌 컴백스 즉 볼록이 아닌 목적식은 우리가 에지지를 통해서 최적화할 수 있게 되는 것이죠. 물론 에지디라고 해서 항상 만능은 아니지만 딥러닝의 경우에는 그냥 경사 하강법을 쓰는 것보다 실증적으로 더 낫다라는 것이 굉장히 많이 검증되었고요. 현대 딥러닝에서 에스지디는 절대로 빼놓을 수 없는 기술이 되었습니다. 네 앞서 살펴 본 것처럼 확률적 경사감법 SGD는 데이터의 일부를 가지고 페라미터를 업데이트하기 때문에 연산 자원을 좀 더 효율적으로 활용하는 데도 도움이 됩니다. 한 가지 잊으시면 안 되는 점은요. 우리가 데이터의 일부를 사용한다고 해서 다른 데이터들을 사용하지 않는 것이 아닙니다. 네 다른 데이터들도 우리가 업데이트하는 과정에서 사용하게 됩니다. 즉 데이터 전체를 가지고 한꺼번에 경사광법을 사용하지 않는 것뿐이지 다른 데이터들은 이 베타 티를 업데이트하는 다른 스텝에서 똑같이 활용하게 됩니다. 그러므로 업데이트하는 과정에서 사실상 모든 데이터를 이용해 그레디언트를 계산하는 것인데 어 이 과정에서 한 번에 업데이트하는 과정에서 모든 데이터를 쓰지 않는다라는 그런 차이점이 있다라는 거를 기억하시면 되고요. 절대로 일부 데이터만 가지고 그것만 가지고 업데이트를 하는 것은 아니다라는 걸 기억하시면 좋겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "8강_경사하강법이랑_더_친해져보자.json",
        "lecture_name": "8강_경사하강법이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "8강",
        "lecture_title": "경사하강법이랑_더_친해져보자",
        "chunk_idx": 0,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:23a3a9cb7d3c919a62c84ddad7c07d646ae4036d41aa3f2777f39ba2e06fd9cf"
      },
      "token_estimate": 1130,
      "char_count": 2025
    },
    {
      "id": "transcript_ai_math_8강_경사하강법이랑_더_친해져보자_c001_529b35",
      "content": "[AI Math] 8강_경사하강법이랑_더_친해져보자\n\n다. 네 다른 데이터들도 우리가 업데이트하는 과정에서 사용하게 됩니다. 즉 데이터 전체를 가지고 한꺼번에 경사광법을 사용하지 않는 것뿐이지 다른 데이터들은 이 베타 티를 업데이트하는 다른 스텝에서 똑같이 활용하게 됩니다. 그러므로 업데이트하는 과정에서 사실상 모든 데이터를 이용해 그레디언트를 계산하는 것인데 어 이 과정에서 한 번에 업데이트하는 과정에서 모든 데이터를 쓰지 않는다라는 그런 차이점이 있다라는 거를 기억하시면 되고요. 절대로 일부 데이터만 가지고 그것만 가지고 업데이트를 하는 것은 아니다라는 걸 기억하시면 좋겠습니다. 정리하면 전체 데이터를 한 번에 쓰지 않고 미니 배치라는 거를 만들어서 부분적으로 나눠 가지고 업데이트를 하게 되는데 이 과정에 의해서 연산량이 어 이렇게 감소를 하게 됩니다. 즉 기존에는 모든 데이터 엔 개만큼 업데이트해야 됐다면은 그거를 배치 개수만큼 우리가 바꿔서 업데이트를 해주기 때문에 연산량이 n 분의 비로 감소하게 됩니다. 딥러닝의 경우에는 이 n의 개수가 굉장히 크고 미니 배치의 개수는 상대적으로 작기 때문에 연산량 측면에서 굉장히 이득을 볼 수가 있고요. 그리고 이론적으로도 이 비 볼록인 함수의 목적지 최적화 과정에서 이득을 또한 볼 수가 있게 됩니다. 그러면 SGD 확률적 경사 하강법의 원리를 한번 살펴보도록 하겠습니다. 경사 하강법은 전체 데이터를 가지고 목적식의 그레디언트 벡터를 계산한다고 말씀드렸었는데요. 즉 어떤 한 지점에서의 페라미터를 세타라고 표시를 한번 해보겠습니다. 이때 이 페라미터 지점에서의 그레디언트를 계산한 거를 우리가 이 손실 함수 엘이 주어졌을 때 전체 데이터를 가지고 이 페라미터 세타에 대한 그레디언트를 계산하는 걸 나블라 세타 엘이라고 표시할 수가 있겠죠. 그러면 우리는 앞서 배워본 미분의 개념처럼 이 손실 함수 엘이 주어졌을 때 주어진 파라미터 세타에 대해서 이 세터를 최소화하는 지점으로 그레디언트 벡터가 향하게 된다라는 걸 아실 수가 있죠. 다시 정리하자면은 손실 함수가 주어졌을 때 전체 데이터 디와 그리고 파라미터 세터로 측정한 목적식이 나오게 되는데 경사강법은 이때 모든 데이터를 가지고 계산한 그레디언트 방향을 향하게 됩니다. 이 과정에서 에스지드는 경사강법과 달리 미니 배치를 사용하게 되는데요. 위니 배치는 전체 데이터의 일부를 얘기하는 것이고 우리가 특별히 비라는 표기를 좀 하도록 하겠습니다. 이 경우에는 손실 함수에서 전체 데이터를 쓰지 않고 일부 데이터만 가지고 손실 함수 값을 계산하기 때문에 원래의 손실 함수가 아니라 이렇게 변형된 손실 함수 그래프가 나오게 됩니다. 즉 SGD의 원리란 전체 데이터를 쓰지 않고 일부 데이터에 대해서 손실 함수를 계산한 후에 그 손실 함수상에서의 그레디언트 벡터를 사용하게 되는 것입니다. 그러므로 원래 전체 모든 데이터를 사용했을 때의 그레디언트랑 조금 다른 모양이 나올 수가 있겠죠. 그리고 그 다음번 업데이트하는 과정에서는 우리가 또 다른 미니 배치 데이터를 가지고 그래프를 그려내기 때문에요. 이 과정에서는 이렇게 파란색 선처럼 바뀌게 됩니다. 그래서 새로운 미니 배치에 대해서는 새로운 손실 함수 그래프가 나오게 되고 그리고 그 지점에서 다시 새로운 그래디언트 벡터가 나오게 되는데 이 과정에서 우리가 확률적으로 미니 배치를 선택하게 되는데 이렇게 랜덤하게 바뀌는 손실 함수의 모양과 그 모양에서의 그레디언트를 가지고 업데이트를 하기 때문에 앞서 경사강법에서 부딪히는 그 값에 도달했을 때 알고리즘이 멈추는 현상을 우리가 확률적으로 피할 수가 있게 됩니다. 즉 SGD의 가장 큰 원리는 바로 매번 다른 미니 배치를 확률적으로 사용하기 때문에 곡선 모양이 바뀌게 되고 바뀐 곡선 모양에서 그레디언트를 계산해서 업데이트하는 것이 바로 SGD의 가장 중요한 원리다라고 이해하시면 되겠습니다. 그래서 SGD는 볼록이 아닌 넌 컨백스인 목적식에서 굉장히 효율적으로 동작할 수가 있고요. 경사 광법보다 기계 학습 모델의 최적화 방식에서 효율적으로 사용할 수가 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "8강_경사하강법이랑_더_친해져보자.json",
        "lecture_name": "8강_경사하강법이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "8강",
        "lecture_title": "경사하강법이랑_더_친해져보자",
        "chunk_idx": 1,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:23a3a9cb7d3c919a62c84ddad7c07d646ae4036d41aa3f2777f39ba2e06fd9cf"
      },
      "token_estimate": 1108,
      "char_count": 2001
    },
    {
      "id": "transcript_ai_math_8강_경사하강법이랑_더_친해져보자_c002_1d7085",
      "content": "[AI Math] 8강_경사하강법이랑_더_친해져보자\n\n다. 즉 SGD의 가장 큰 원리는 바로 매번 다른 미니 배치를 확률적으로 사용하기 때문에 곡선 모양이 바뀌게 되고 바뀐 곡선 모양에서 그레디언트를 계산해서 업데이트하는 것이 바로 SGD의 가장 중요한 원리다라고 이해하시면 되겠습니다. 그래서 SGD는 볼록이 아닌 넌 컨백스인 목적식에서 굉장히 효율적으로 동작할 수가 있고요. 경사 광법보다 기계 학습 모델의 최적화 방식에서 효율적으로 사용할 수가 있게 됩니다. 한 가지 유의하실 점은 만약에 볼록인 함수인 경우에는 에스지디 알고리즘보다 이렇게 경사하강법 알고리즘이 좀 더 효과적으로 페라미터 업데이트하는 거를 볼 수가 있습니다. 그러나 여러분들이 보통 업데이트를 하게 되는 목적식은 딥러닝의 경우 대부분은 비볼록 함수이기 때문에 어 항상 이렇게 동작하는 것보다는 SGD 알고리즘처럼 확률적으로 그레디언트 방향을 움직이면서 또한 전체 데이터를 우리가 랜덤하게 일부 사용하면서 그레디언트를 계산해 주면 기댓값 측면에서는 원래 그레디언트 방향으로 향하게 되어 있기 때문에 우리가 원하고자 하는 최적의 페라미터를 찾는 건 가능합니다. 그래서 어떤 급속값에 빠지는 즉 미분값이 0이 되는 지점에서 탈출하면서 동시에 최저값을 향하는 지점을 우리가 알 수가 있기 때문에 SGD의 원리가 이렇게 비볼록인 함수 최적화에서는 오히려 유용하다라고 정리해서 말씀드릴 수가 있고요. 그리고 하드웨어의 어떤 이 한계 때문에 우리가 경사광법에서 모든 데이터를 쓰기 어려울 때가 있는데 우리가 미니 배치 사이즈를 효율적으로 잘 조절함에 따라서 보시면 그레디언트 디센트 알고리즘을 쓰는 것보다 미니배치 사이즈가 100인 SGG를 썼을 때 훨씬 더 빠른 어 시간 안에 우리가 최적화를 할 수 있다라는 것도 우리가 볼 수가 있게 됩니다. 만약에 미니 배치 사이즈가 너무 작게 되면요. 사실 하드웨어를 효율적으로 사용하지 못하게 되고 이렇게 수량 속도도 느려질 수가 있는데 적절한 미니 배치 사이즈를 결정해서 업데이트를 해주게 되면은 우리가 수렴성이나 그리고 하드웨어의 효율적 사용도 동시에 달성할 수가 있게 됩니다. 이거를 좀 더 하드웨어 측면에서 설명을 좀 드려보도록 하겠습니다. 우리가 실제로 딥러닝 학습에서 사용하게 되는 데이터의 경우에는 고화질인 경우가 많고요. 그리고 데이터 자체의 개수도 적으면 수만 개 많으면 수백만 개 훨씬 더 많은 수억 개의 데이터를 사용하게 되는 경우도 있습니다. 이렇게 많은 데이터를 가지고 한꺼번에 경사 강법을 통해서 우리가 페라미터 업데이트를 하려고 하면은 사실 지피유 하드웨어의 메모리 측면에서 굉장히 버틀랙이 생길 수가 있고 또한 CPU 측면에서도 모든 데이터를 업로드해야 되는 과부하가 생길 수가 있습니다. 그래서 경사 하강법을 딥러닝 알고리즘에서 모든 데이터를 업로드해서 사용하려고 하면은 메모리 부족 문제에 우리가 빠질 수가 있게 되는데요. 에지드를 통해서 우리가 데이터를 쪼개서 경사 하강법을 쓰면은 즉 확률적 경사하강법을 쓰면은 어 시피유의 워크로드도 우리가 줄일 수가 있고 지피유도 효율적으로 메모리 바운드 내에서 어 그레디언트 업데이트를 통해 페라미터를 최적의 페라미터를 찾을 수가 있기 때문에 이렇게 하드웨어 측면에서도 에지디 알고리즘이 좀 더 우월한 측면을 가지게 됩니다. 네 이번에는 SGD 기반 선형 회귀 알고리즘을 한번 짜보도록 하겠습니다. 경사강법이랑 달리 SGD의 경우에는 우리가 미니 배치를 뽑아서 그레디언트를 업데이트한다라는 게 필요하기 때문에 이거는 두 가지 인풋이 필요하게 됩니다. 바로 뭐냐 하면 배치 사이즈 즉 몇 개의 일부 데이터를 가지고 그레디언트를 계산할지 정해주는 배치 사이즈와 그리고 그 해당하는 미니 배치를 전체 데이터에서 어떻게 골라낼 줄지를 선택해 주는 샤플러 즉 데이터를 랜덤하게 섞고 미니 배치 사이즈만큼 추출하는 제너레이터를 구현해 줘야 됩니다. 네 SGD 알고리즘을 구현하실 때는 이 셔플러 즉 제너레이터를 구현하는 게 굉장히 중요하다라는 거를 기억하시면 좋겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "8강_경사하강법이랑_더_친해져보자.json",
        "lecture_name": "8강_경사하강법이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "8강",
        "lecture_title": "경사하강법이랑_더_친해져보자",
        "chunk_idx": 2,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:23a3a9cb7d3c919a62c84ddad7c07d646ae4036d41aa3f2777f39ba2e06fd9cf"
      },
      "token_estimate": 1093,
      "char_count": 1978
    },
    {
      "id": "transcript_ai_math_8강_경사하강법이랑_더_친해져보자_c003_40b19d",
      "content": "[AI Math] 8강_경사하강법이랑_더_친해져보자\n\n다. 바로 뭐냐 하면 배치 사이즈 즉 몇 개의 일부 데이터를 가지고 그레디언트를 계산할지 정해주는 배치 사이즈와 그리고 그 해당하는 미니 배치를 전체 데이터에서 어떻게 골라낼 줄지를 선택해 주는 샤플러 즉 데이터를 랜덤하게 섞고 미니 배치 사이즈만큼 추출하는 제너레이터를 구현해 줘야 됩니다. 네 SGD 알고리즘을 구현하실 때는 이 셔플러 즉 제너레이터를 구현하는 게 굉장히 중요하다라는 거를 기억하시면 좋겠습니다. 그래서 앞에서 배운 경사 강법 알고리즘과 달리 추가적으로 이 2개를 인풋으로 받는다라는 거를 일단 기억을 해 주시고요. 원리적으로는 경사 하강법과 비슷합니다마는 바로 이 두 부분을 쓴다는 점에서 차이가 있습니다. 네 그래서 먼저 주어진 학습 횟수에 따라서 우리가 스텝을 티라고 했을 때 먼저 데이터 사이즈를 이렇게 와의 랭스로 받아오게 됩니다. y가 아니라 스의 랭스를 받아와도 됩니다마는 어 보통은 와가 좀 더 작기 때문에 이렇게 와의 랭스를 가지고 데이터 사이즈를 측정을 한번 해보는 것이고요. 그리고 주어진 스와 그리고 와와 그리고 전체 데이터 사이즈와 그리고 배치 사이즈를 인풋으로 받아서 우리가 샘플러를 다음과 같이 구현해 줍니다. 샘플러는 전체 데이터 중에서 미니 매치만큼 반복적으로 랜덤하게 추출해 주는 제너레이터 역할입니다. 네 그래서 이 제너레이터 코드를 예시를 좀 보여드릴 건데요. 앞서 설명드렸던 것처럼 전체 데이터 스랑 그리고 와를 인풋으로 받는데 유의하실 점은 이 전체 데이터 엑스를 만약에 메모리에 올리게 되면은 우리가 비효율적으로 코드가 동작하게 되겠죠. 데이터 크기도 워낙 크니까요. 그렇기 때문에 우리가 제너레이터를 사용하게 되는 것입니다. 우선 먼저 데이터 사이즈만큼 인덱스 집합을 만들어 주게 되고 그다음에 인덱스 집합을 랜덤하게 순서로 섞어주는 코드를 짜게 됩니다. 네 이렇게 해주게 되면은 우리가 이제 스와에서 랜덤하게 인덱스 순서로 데이터를 뽑는 이 코드를 짤 수가 있게 되는데 이게 이제 제너레이터를 리턴해 주는 코드입니다. 이렇게 제너레이터를 리턴해 줘서 샘플러를 우리가 만들게 되면은 그 샘플러에서 우리가 스랑 와의 일부 즉 미니 배치에 해당하는 데이터를 우리가 가져올 수가 있게 됩니다. 즉 제너레이터에서 우리가 매번 x 배치랑 y 배치를 뽑아낼 수가 있는데요. 이 뽑아낸 x 배치와 y 배치를 가지고 우리가 그레디언트 즉 일부 데이터를 가지고 그레디언트를 계산하는 수식을 쓰게 됩니다. 앞에서 보시다시피 일부 데이터를 가지고 그레디언트를 계산하는 거는 전체 데이터를 가지고 그레디언트를 계산하는 수식과 똑같은데 한 가지 차이점이 있다면 와랑 스 대신에 미니 배치로 뽑아낸 x와 y를 사용한다라는 차이가 있고요. 그리고 전체 데이터 개수인 n이 아니라 미니 배치 사이즈인 b로 나눠줘서 업데이트를 한다는 사실을 여러분이 기억하시면 되겠습니다. 그래서 이렇게 계산한 그레디언트 벡터를 그레디언트로 가져와서 우리가 미니 배치로 미니 배치 사이즈로 나눈 값 만큼에다가 학습률을 곱해 줘서 그레디언트 디센트를 수행할 수가 있게 되는 것입니다. 이와 같이 에지디 기반 선형 회귀 알고리즘을 짜서 우리가 한번 최적화를 돌려보면요. 경사강법 기반 회귀 모델이랑 SGD 기반 회귀 모델이랑 한번 비교를 해볼 수가 있게 됩니다. 이와 같이 선형 회귀 문제를 한번 설계를 해보았고요. 그라운트루스에 해당하는 계수 베타를 이와 같이 마이너스 0.5 3.5 마이너스 2.8 5.2로 한번 설정해 보았습니다. 선형 회의 문제이기 때문에 사실 경사 강법으로도 충분히 문제를 풀 수 있지만 에지디가 얼마큼 잘 찾는지를 한번 살펴보시면 네 경사광법 기반 알고리즘으로 찾은 개수랑 비슷한 정확도로 개수들을 찾을 수 있다는 거를 여러분들이 살펴보실 수 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "8강_경사하강법이랑_더_친해져보자.json",
        "lecture_name": "8강_경사하강법이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "8강",
        "lecture_title": "경사하강법이랑_더_친해져보자",
        "chunk_idx": 3,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:23a3a9cb7d3c919a62c84ddad7c07d646ae4036d41aa3f2777f39ba2e06fd9cf"
      },
      "token_estimate": 1028,
      "char_count": 1873
    },
    {
      "id": "transcript_ai_math_8강_경사하강법이랑_더_친해져보자_c004_a99d53",
      "content": "[AI Math] 8강_경사하강법이랑_더_친해져보자\n\n다. 이와 같이 선형 회귀 문제를 한번 설계를 해보았고요. 그라운트루스에 해당하는 계수 베타를 이와 같이 마이너스 0.5 3.5 마이너스 2.8 5.2로 한번 설정해 보았습니다. 선형 회의 문제이기 때문에 사실 경사 강법으로도 충분히 문제를 풀 수 있지만 에지디가 얼마큼 잘 찾는지를 한번 살펴보시면 네 경사광법 기반 알고리즘으로 찾은 개수랑 비슷한 정확도로 개수들을 찾을 수 있다는 거를 여러분들이 살펴보실 수 있게 됩니다. 경사하강법 알고리즘과 차이가 있는 걸 보실 수가 있는데요. 경사 하강법 알고리즘에서는 스랑 와라는 데이터 전체를 가지고 에러를 계산해서 그레디언트를 계산했지만 앞서 살펴본 에지디 알고리즘처럼 우리가 제너레이터를 구현하고 그 제너레이터에서 추출한 미니 배치로 그레디언트를 계산해서 업데이트하는 부분이 차이가 있습니다. 이와 같이 미니 배치를 가지고 그레디언트를 계산해서 최적화를 수행을 해도 우리가 원하고자 했던 적절한 최적 페라미터를 찾을 수 있다라는 거를 살펴보실 수 있었는데요. 네 두 기법 모두 적절하게 해를 찾았죠. 근데 살펴보시면 두 최적화 기법의 양상이 좀 다릅니다. 그래서 SGD의 경우에는 손실 함수를 전체적으로 어 최적화하는 미분 값을 계산하지 않기 때문에 보시면 이렇게 업데이트 하게 되는 이 에러의 크기가 랜덤하게 움직인다는 것을 보실 수가 있습니다. 네 당연히 경사강법이랑 달리 gd는 일부 데이터만 가지고 어 그레디언트를 계산하기 때문에 이 에러 부분이 랜덤하게 항상 움직일 수가 있게 되는 것이므로 에스지디는 사실 경사강법보다는 조금 부정확한 그레이디언트를 계산해서 업데이트하기 때문에 결과가 랜덤해질 수는 있습니다. 그래서 에지디 학습 시 여러분들께서 좀 유의하실 부분이 몇 가지가 있습니다. 적절한 에지디 알고리즘을 사용하게 되면은 어려운 최적화 문제도 잘 풀 수 있기는 하지만 에지디 학습 시 유의사항을 지키지 않으면은 기대와는 달리 수렴하지 않는 알고리즘들을 보실 수가 있는데요. SGD 알고리즘은 학습률을 고정시키면 사실 안 됩니다. 여러분들께서 꼭 반드시 기억하셔야 될 점은 학습률은 고정시키는 것이 아니라 어떤 스케줄러를 써서 매번 티 스텝마다 변하는 스케줄러를 가져오셔서 SGD의 학습률을 사용하셔야 된다는 점을 꼭 기억하시면 좋겠습니다. 이때 학습률이 만족해야 되는 조건은 뭐냐면 모든 스텝에 대해서 다 더했을 때 무한대가 되고 제곱해서 더했을 때는 수렴하는 네 이 조건을 로빈스 멀로 조건이라고 하는데요. 어 살짝 어렵게 느껴질 수가 있겠습니다만 이 조건에 만족한 스케줄러는 스텝의 역수에 해당하는 만큼 비례해서 우리가 학습률을 설정해 주면 이 두 가지 조건을 만족시킨다라는 거를 확인해 볼 수가 있습니다. 그래서 학습률을 세팅하실 때 반드시 스텝 수의 반비례해서 우리가 조정해 주는 알고리즘을 반드시 사용하셔야 되겠습니다. 네 이거를 구현하는 거는 앞서 알고리즘을 한번 수정하는 방법으로 보시도록 하겠습니다. 사실 그렇게 어렵지는 않습니다. 먼저 초반에 받는 학습률을 상수로 받지 않고요. 이번에는 함수로 우리가 구현해서 집어넣어 주면 됩니다. 이때 함수를 어떻게 바꿔주면 되냐면은 이니셜 러닝 웨이트를 1.0으로 디폴트로 받은 상태에서 매번 스텝이 바뀔 때마다 티로 즉 여기서는 티 플러스 1로 나눠주는 코드를 짜가지고 경사강법 알고리즘에서 상수 러닝레이트가 아니라 이렇게 함수에다가 티를 입력으로 넣어주는 부분으로 바꿔주시면은 이렇게 스케줄러를 반영한 스지디 알고리즘을 구현하실 수가 있게 됩니다. 그래서 학습률이 학습 스펙 티에 따라서 변하는 값을 가지도록 우리가 설정해 줬기 때문에 로빈스 멀로 컨디션에 만족하는 에지디 알고리즘을 사용할 수가 있게 되는 것이고요. 이렇게 구현하게 되는 경우에는 우리가 앞서 살펴본 경사 하강법 알고리즘보다 더 빨리 수렴하게도 만들 수가 있고 만약에 적절하지 않은 세팅을 하시게 되면은 수렴이 또한 안 되는 문제가 발생할 수도 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "8강_경사하강법이랑_더_친해져보자.json",
        "lecture_name": "8강_경사하강법이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "8강",
        "lecture_title": "경사하강법이랑_더_친해져보자",
        "chunk_idx": 4,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:23a3a9cb7d3c919a62c84ddad7c07d646ae4036d41aa3f2777f39ba2e06fd9cf"
      },
      "token_estimate": 1082,
      "char_count": 1958
    },
    {
      "id": "transcript_ai_math_8강_경사하강법이랑_더_친해져보자_c005_99ba98",
      "content": "[AI Math] 8강_경사하강법이랑_더_친해져보자\n\n다. 그래서 학습률이 학습 스펙 티에 따라서 변하는 값을 가지도록 우리가 설정해 줬기 때문에 로빈스 멀로 컨디션에 만족하는 에지디 알고리즘을 사용할 수가 있게 되는 것이고요. 이렇게 구현하게 되는 경우에는 우리가 앞서 살펴본 경사 하강법 알고리즘보다 더 빨리 수렴하게도 만들 수가 있고 만약에 적절하지 않은 세팅을 하시게 되면은 수렴이 또한 안 되는 문제가 발생할 수도 있습니다. 여러분들이 기억하실 점은 적절한 초기 값을 설정해 주시면은 SGD 알고리즘이 굉장히 효과적으로 동작할 수 있다는 부분을 보시면 좋고요. 여러분이 만약에 설정한 부분을 조금 어 너무 낮은 값을 쓴다든지 하시게 되면은 학습률이 너무 지나치게 많이 감소하기 때문에 이 경우에는 수렴이 좀 늦어질 수가 있습니다. 네 그래서 각각의 경우에는 초기 값을 바꿔 주시거나 아니면 학습 횟수를 늘리는 방향으로 SGD의 알고리즘을 수행하시면은 해결책이 되겠습니다. 네 이번 시간에는 지난 시간에 배웠던 경사강법의 문제점을 극복하기 위해서 확률적 경사 강법 SGD를 배워보았습니다. SGD 알고리즘은 딥러닝에서 굉장히 기본적으로 사용되는 최적화 알고리즘이기 때문에 여러분들께서 꼭 기억하셔야 되고요. 그리고 확률적 경사하강법의 원리를 여러분들께서 꼭 기억하시면 좋겠습니다. 수고하셨습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "8강_경사하강법이랑_더_친해져보자.json",
        "lecture_name": "8강_경사하강법이랑_더_친해져보자",
        "course": "AI Math",
        "lecture_num": "8강",
        "lecture_title": "경사하강법이랑_더_친해져보자",
        "chunk_idx": 5,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:23a3a9cb7d3c919a62c84ddad7c07d646ae4036d41aa3f2777f39ba2e06fd9cf"
      },
      "token_estimate": 369,
      "char_count": 673
    }
  ]
}