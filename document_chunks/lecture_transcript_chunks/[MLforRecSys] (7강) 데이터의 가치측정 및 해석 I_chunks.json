{
  "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
  "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
  "course": "MLforRecSys",
  "total_chunks": 11,
  "chunks": [
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c000_182e53",
      "content": "[강의 녹취록] 과목: MLforRecSys | 강의: 7강 | 제목: 데이터의 가치측정 및 해석 I\n\n네 안녕하세요 여러분 이번 시간에는 저희 데이터의 가치 측정 및 해석에 대해서 살펴보도록 하겠습니다. 이번 강의에서는 피처 어트랙션과 데이터 어트랙션이 어떻게 다른지 그다음에 데이터 가치 평가가 또 어떻게 활용될 수 있는지 그리고 그러한 데이터 가치 평가를 우리가 어떻게 진행할 수 있는지 살펴보도록 하겠습니다. 자 피처 어트리뷰션과 데이터 어트리뷰션을 일단은 저희가 각각의 차이를 보기 전에 데이터 어트리뷰션이 어떻게 좀 어느 곳에 활용될 수 있는지 보도록 할게요. 자 추천 시스템뿐만 아니라 사실 굉장히 다양한 테스크에 활용이 되고 있습니다. 그중에 한 가지는 익스플레인 어빌리티입니다. 즉 우리가 테스트 데이터 x에 대해서 우리 모델이 와를 도출했다고 하겠습니다. 자 그때 우리 모델이 이 와를 도출한 이유가 무엇인지 맞추는 것을 목표로 합니다. 그 과정에서 우리가 데이터 어트리뷰션을 활용하게 되면 테스트 데이터와 엑스에 대하여 아웃풋 와를 도출하는 데 가장 크게 기여한 트레이닝 데이터를 우리는 제시할 수 있게 되는 것입니다. 예를 들어서 현재 추천 시스템이 유저 첫 번째 유저에게 다섯 번째 영화를 추천했다고 하겠습니다. 그러면 해당 추천이 이루어질 수 있도록 가장 크게 기여한 트레이닝 데이터를 우리는 제시하고 싶은 거예요. 그때 첫 번째 유저가 과거에 이제 다섯 번째 영화와 장르가 유사한 다른 영화를 만족스럽게 시청한 기록이 있다 이런 거를 우리가 제시할 수 있게 되면 보다 더 그 유저 입장에서는 신뢰가 가게 되겠죠. 또 유사하게 의원과 좀 유사한 지역에 거주하는 비슷한 나이대에 다른 시청자들이 다섯 번째 영화를 재밌게 봤다 그래서 너도 재밌게 볼 거다라는 그런 설명을 우리가 해주면 유저가 보다 더 신뢰하고 우리의 추천을 잘 받아들이게 될 겁니다. 자 그럼 이제부터는 좀 더 본격적으로 피처 어트리뷰션과 데이터 어트리뷰션의 차이를 살펴보도록 하겠습니다. 자 피처 어트리뷰션은 각 피처에 대하여 가치를 걸고 평가를 내립니다. 트레이닝 데이터를 우리가 학습시킬 때 중요한 피처를 우리가 셀렉션 하기 위해서 많이 활용이 되고 있죠. 즉 여기 나와 있는 모든 피처를 다 써서 트레이닝 하는 게 아니라 특정한 피처만 우리가 쓰게 되는 겁니다. 즉 예를 들어서 뭐 이게 테이블러 데이터라고 할 때 키 몸무게 혈액형 등등등 있다고 하겠습니다. 그러면 그때 특정한 키나 뭐 혈액형과 같은 특정한 칼럼의 데이터는 우리가 별로 도움이 안 될 것 같다라고 하면은 그러한 피처는 우리가 제거하고 나머지 피처 기반으로 우리가 학습을 하는 형태입니다. 또는 테스트 데이터에서 우리가 우리 모델이 어떠한 피처에 집중하고 있는지 파악하는 용도로도 활용이 가능합니다. 예를 들어서 우리가 가장 많이 활용되는 피처 어트리뷰션으로는 다음과 같은 그래드 캠이 존재합니다. 즉 텍스트 데이터 이미지가 이렇게 있다고 할 때 이런 이미지에 우리 모델이 어떠한 영역에 집중해서 분류를 하고 있는지 그것을 우리가 파악할 수 있는 용도로 많이 쓰게 되죠. 그때 이 이미지에 여러 피처들이 존재한다고 볼 수 있죠. 각 픽셀들이 피처라고 볼 수 있습니다. 그때 어떤 피처에 집중하는지를 우리가 모델을 좀 해석하는 차원에서 활용할 수가 있습니다. 자 반면 데이터 어트리뷰션은 데이터 인스턴스마다의 가치를 측정하는 방식입니다. 즉 우리가 예를 들어서 테이블로 데이터라고 가정을 하면은 테이블 데이터라고 가정하면 이렇게 칼럼이 여러 개가 있고 그렇죠 그러면 첫 번째 사람에 대한 뭐 키 몸무게 혈액형, 두 번째 사람에 대한 키 몸무게 혈액형 그때 하나하나의 행을 우리가 데이터 인스턴스라고 볼 수가 있겠죠 그다음에 이 각각의 컬럼을 우리가 피처라고 볼 수가 있겠습니다. 여기서 데이터 어트리뷰션은 데이터 인스턴스마다 즉 행마다의 가치를 측정하는 방법입니다. 물론 이게 테이블 데이터가 아니라 이미지가 될 수도 있겠죠 즉 해당 데이터를 우리가 학습했을 때 트레이닝 데이터를 학습했을 때 과연 테스트 로스가 얼마나 낮아지는지 그거를 해당 데이터의 가치라고 볼 수도 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 0,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1119,
      "char_count": 2036
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c001_5ae509",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 여기서 데이터 어트리뷰션은 데이터 인스턴스마다 즉 행마다의 가치를 측정하는 방법입니다. 물론 이게 테이블 데이터가 아니라 이미지가 될 수도 있겠죠 즉 해당 데이터를 우리가 학습했을 때 트레이닝 데이터를 학습했을 때 과연 테스트 로스가 얼마나 낮아지는지 그거를 해당 데이터의 가치라고 볼 수도 있습니다. 즉 가치가 높다라는 말은 이 데이터를 우리가 트레이닝 데이터로 학습을 진행을 하면 테스트 데이터의 로스가 많이 내려갈 수 있다라는 것을 의미하게 되겠죠. 결국 이런 데이터 어트리뷰션이 중요한 이유는 데이터 어트리뷰션은 결국 트레이닝 데이터에 대한 가치 평가입니다. 이게 중요한 이유는 자 머신 러닝 모델은 기본적으로 트레이닝 데이터를 학습을 하고 테스트 데이터에 대해서 평가를 하는 겁니다. 그렇죠 그 까닭에 결국 우리 모델이 테스트 데이터에 대해서 왜 이렇게 작동되는지 알려면 트레이닝 데이터에서 얻은 널리지를 기반으로 테스트를 예측을 해야 되는 겁니다. 그렇죠 즉 테스트 데이터에 대한 우리 모델의 아웃풋을 해석하기 위해서는 트레이닝 데이터에 대한 분석이 필요하다는 겁니다. 자 그래서 우리가 앞서 봤던 피치 어트리뷰션은 이런 그라드 킴과 같이 좀 어떤 피처에 집중을 했는지 보는 거예요. 자 근데 이거는 우리 모델이 왜 여기에 집중했는지는 설명을 못하는 겁니다. 즉 우리 모델이 여기에 집중해서 고양이를 뭐 강아지라고 판단했다 그러면 고양이를 고양이라고 판단했다 그런 건 가능하지만 그래서 우리 모델이 왜 여기에 집중했는데 라는 거는 설명을 못해요. 데이터 어트리뷰션은 우리가 여러 가지 알고리즘이 있습니다. 인플루언서 펑션 쉐플리 밸류 여러 가지들이 있는데 그중에서 이번에는 저희가 인플루언서 펑션에 대해서 살펴볼 거고요. 여기서 우리 모델이 테스트 데이터에 대해서 여기서 왜 그러한 아웃풋을 내게 됐는지에 대한 설명을 제공해 주는 겁니다. 왜 애초에 우리 모델이 이러한 데이터로 학습이 됐기 때문에 이러한 테스트 데이터에 대해서는 이렇게 예측을 합니다 라고 말을 할 수가 있는 겁니다. 자 그러면 이제부터는 데이터 어트리뷰션을 어떻게 활용할 수 있는지 보겠습니다. 지금까지는 우리가 데이터 어트리뷰션이 무엇인지 봤고요. 이제부터는 데이터 어트리뷰션이 무엇인지 어떻게 활용할 수 있는지 저희가 보겠습니다. 데이터 어트리베이션은 아까 말씀드린 것처럼 굉장히 다양한 곳에 활용이 되고 있고 앞서 말씀드린 것처럼 이런 설명 가능성 측면에서 많이 활용이 됩니다. 그리고 모델의 진단에도 많이 활용이 됩니다. 자 우리 모델을 다음과 같은 트레이닝 데이터로 학습을 시켰는데 테스트에서 만약에 성능이 너무 저조해요. 만약에 자 그러면은 이 트레이닝 데이터가 별로 도움이 안 된다라는 뜻이 되겠죠 그때 이 트레이닝 데이터가 모든 트레이닝 데이터가 도움이 안 된다기보다는 도움이 안 되는 일부의 트레이닝 데이터가 존재할 겁니다. 그러한 가치가 없는 잘못된 데이터를 우리가 제거하고 재학습해야지 테스트 데이터의 성능이 개선될 수 있는 겁니다. 그러면 어떠한 데이터를 우리가 제거해야 될까요? 어떤 데이터를 제거하고 재학습해야지 좋은지 자동으로 판단하고 싶다는 겁니다. 즉 우리가 여기서 데이터 어트리뷰션을 이용하면 테스트 데이터에 대해서 로스가 작아지도록 기여하는 정도를 각 트레이닝 데이터마다 계산할 수 있는 거예요. 그러면 우리가 테스트 데이터에 대해서 성능 검증할 때 이렇게 테스트 데이터에 대한 성능을 올리기 위해서 또는 로스를 낮추기 위해서 크게 기여한 데이터들을 이렇게 순서대로 솔팅을 할 수 있어요. 그러면 오히려 방해가 되는 데이터는 우리가 제거하고 다시 학습을 진행할 수 있다는 겁니다. 뿐만 아니라 최근에는 이제 RNG에서도 많이 활용이 될 수 있겠죠 최근에는 그 대형 언어 모델을 활용하기 위해서 여러분들께서 알레지 방법을 또 많이 또 들어보신 분들이 계실 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 1,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1071,
      "char_count": 1942
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c002_8cf350",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 뿐만 아니라 최근에는 이제 RNG에서도 많이 활용이 될 수 있겠죠 최근에는 그 대형 언어 모델을 활용하기 위해서 여러분들께서 알레지 방법을 또 많이 또 들어보신 분들이 계실 겁니다. 즉 이런 RNG는 우리가 뭐 간단하게는 오픈북 시험이라고도 많이 비유를 하죠. GPT와 같은 라지 스케일 언어 모델에 데이터를 우리가 묻고 싶은 거를 그냥 물어봤을 때 그때는 우리가 이 GPT가 참조할 게 없이 이미 알고 있는 지식으로 우리가 아웃풋을 내보내야 됩니다. 그런데 오른쪽에서는 결국 rag를 쓴다라는 말은 우리가 물어보고 싶은 질문이 있을 때 그 질문과 가장 관련이 있는 지식을 데이터베이스에서 찾아서 그 지식을 같이 넣어준다는 겁니다. 언어 모델에 예를 들어서 우리가 언어 모델한테 뭐 조선시대에 대해서 한번 서술해 봐라라고 하면은 어느 정도 답은 하겠지만 그러한 조선시대에 어떠한 특징들이 있고 그러한 것들을 데이터베이스에서 찾아서 그러한 정보도 우리가 같이 넣어주게 되면은 그때는 더 잘 답할 수 있다는 겁니다. 자 그러면 여기서 여러분 이 알에쥐가 뭐 굉장히 유용한 알고리즘 중에 하나겠죠 자 근데 이 알에쥐가 오히려 방해가 되는 경우도 있습니다. 어느 문제 상황에서 이러한 알레지 방식이 오히려 데이터베이스에서 비슷한 아니면 도움이 될 법한 지식을 참조해서 같이 넣어주는 이 방식보다 그냥 물어보는 방식이 더 좋을 수가 있을까요? 이 데이터베이스에 정말 양질의 데이터만 다 있다면 RNG가 도움이 되겠죠 하지만 이런 데이터베이스에 잘못된 정보도 있다든지 조선시대에 만약에 뭐 BTS가 살았다 그런 만약에 잘못된 데이터베이스가 있다면 우리가 잘못된 정보를 꺼내서 언어 모델한테 주는 거니까 그때는 언어 모델의 성능이 오히려 떨어지게 됩니다. 그 까닭에 우리는 이러한 데이터베이스를 굉장히 양질의 데이터로 구축을 완료해 놔야 되는 거예요. 이거를 우리가 어떻게 할 수 있을까 데이터의 가치 평가를 진행을 하자는 겁니다. 그 까닭에 여러분 지금부터는 데이터 가치 측정이라는 것이 무엇이고 데이터 가치의 측정이 어떻게 활용될 수 있는지까지 살펴봤습니다. 이제부터는 이러한 데이터 가치 측정을 우리가 어떻게 할 수 있는지 한번 살펴보도록 하겠습니다. 트레이닝 데이터에 대한 가치 측정 방법론을 한번 생각해 봅시다. 특정한 데이터가 중요한지 여부를 판단할 수 있는 가장 직관적인 방법은 무엇일까요? 자 그중에 한 가지는 다음과 같은 두 가지를 비교하는 거예요. 첫 번째 해당 데이터를 우리가 트레이닝 데이터에 포함시키고 우리가 모델을 학습하는 겁니다. 그러고 나서 우리가 밸리데이션과 테스트를 진행했을 때 로스가 어떻게 진행되는지 보는 거예요. 자 두 번째는 해당 데이터를 우리가 트레이닝에서 제거하고 학습시켰을 때 보는 겁니다. 즉 우리가 딱 특정한 a라는 데이터의 가치가 궁금하다 그러면 a라는 데이터를 우리가 포함시키고 우리가 트레이닝 한 다음에 a라는 데이터를 트레이닝 데이터에 포함시키고 즉 학습에 포함시킨 거죠. 그다음에 우리가 다른 데이터로 밸리데이션이나 테스트를 했을 때의 로스랑 해당 데이터를 트레인 데이터에서 제거하고 즉 없는 상태에서 우리가 학습했을 때 과연 어떻게 달라지는지 우리가 보자는 거예요. 그렇죠 즉 이 첫 번째가 지금 트레닝 데이터가 우리가 한 개 더 많은 상황으로 생각할 수가 있는 겁니다. 그렇죠 즉 왜 첫 번째는 에라는 데이터가 포함된 거고 두 번째는 이라는 데이터가 포함되지 않은 상황이니까 자 근데 만약에 이 두 개 간의 차이가 만약에 안 커요. 즉 a라는 데이터를 트레이닝 데이터에 포함시켜서 a라는 데이터를 학습을 했어 그러고 나서 테스트를 진행했는데 그때 성능하고 a라는 데이터를 제거하고 학습하지 않고 그다음에 테스트를 했을 때 성능이랑 비교했는데 성능이 비슷해요. 테스트 성능이 그러면 a라는 데이터는 테스트 성능 향상에 크게 기여를 안 한다는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 2,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1064,
      "char_count": 1953
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c003_177c67",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 그렇죠 즉 왜 첫 번째는 에라는 데이터가 포함된 거고 두 번째는 이라는 데이터가 포함되지 않은 상황이니까 자 근데 만약에 이 두 개 간의 차이가 만약에 안 커요. 즉 a라는 데이터를 트레이닝 데이터에 포함시켜서 a라는 데이터를 학습을 했어 그러고 나서 테스트를 진행했는데 그때 성능하고 a라는 데이터를 제거하고 학습하지 않고 그다음에 테스트를 했을 때 성능이랑 비교했는데 성능이 비슷해요. 테스트 성능이 그러면 a라는 데이터는 테스트 성능 향상에 크게 기여를 안 한다는 겁니다. 그러면 그 관점에서는 a라는 데이터가 덜 중요하다는 거죠. 근데 만약에 두 개 간의 차이가 크다면 즉 그 데이터를 우리가 포함시켰을 때 그리고 그렇게 학습을 했을 때 테스트 성능이 확 올라간다고 하면 굉장히 중요한 데이터가 되는 것이죠. 그렇죠 물론 a라는 데이터가 만약에 잘못된 데이터였다. 즉 레이블링이 잘못된 데이터였다 그러면 반대의 의미로 a라는 데이터는 테스트 데이터 성능에 크게 영향을 준 겁니다. 왜 a라는 데이터가 만약에 잘못된 레이블이었다 아니면 a라는 데이터가 굉장히 노이즈가 심한 적대적인 샘플이었다 그러면 a라는 데이터를 학습하면 오히려 테스트 데이터의 성능이 확 내려갈 수도 있는 거예요. 그러면 그때는 a라는 데이터를 우리가 제거하고 학습하는 게 좋겠다 이러한 판단을 우리가 할 수 있게 되는 겁니다. 즉 이러한 두 가지를 우리가 비교하게 되면 그 데이터가 얼마나 중요한지를 우리가 판단할 수가 있다는 거예요. 자 여기까지는 굉장히 그렇죠 여러분 직관적이죠. 그러면 이 방식을 실제로 구현한다면 여러분 어떻게 구현할 수 있을까요? 자 실제로 만약에 구현한다면 이렇게 구현할 수 있겠죠. 데이터가 만약에 우리가 n 개 존재한다고 합시다. 그러면 우리는 모델을 총 n번 학습해야 되는 거예요. 왜 첫 번째 데이터를 제거하고 엔 마너스 1개의 데이터로 우리가 학습을 처음부터 다시 진행한 모델 딱 만들어서 밸리데이션과 테스트 로스를 측정합니다. 두 번째 두 번째 데이터를 이번에 제거하고 똑같이 하는 거예요. 세 번째는 세 번째 데이터만 제거하고 나머지 엔 마이너스 1개의 데이터로 학습한 모델의 성능을 보는 겁니다. 그러면 이거의 문제점은 무엇일까요? 구현하는 거는 생각보다 되게 간단할 거예요. 그냥 데이터 빼고 학습 데이터 다시 빼고 처음부터 다시 학습 또 다른 데이터 빼고 학습 그냥 그렇게 하는 거니까 여기 적혀 있는 것처럼 굉장히 비효율적입니다. 왜 모델은 결국 우린 엠번 학습해야 되는 거예요. 근데 모델 우리가 뉴럴 네트워크로 가게 되면 이 모델이 굉장히 무겁습니다. 그리고 데이터 개수가 많아지면 학습하는 데 굉장히 오래 걸려요. 그러면 실제로 얘는 현실 가능하지 않습니다. 그러면 우리가 어떻게 하면은 효율적으로 계산할 수 있을까요? 그게 바로 인플루언서 펑션입니다. 무슨 말이냐면 우리가 알고 싶은 거는 이 데이터가 제거됐다고 가정했을 때 그때 모델의 성능을 우리는 알고 싶은 거예요. 그렇죠 이 데이터가 트레이닝 데이터에서 제거됐을 때 밸류 데이션과 테스트의 성능을 알고 싶은 겁니다. 그때 그거를 우리가 모델을 다 재학습시키지 않더라도 근사해서 알 수 있는 방법이 있다는 겁니다. 그게 바로 인플루언서 펑션입니다. 굉장히 신기하죠. 다시 우리가 모델을 학습하지 않아도 그 데이터가 마치 빠지면 얼마나 성능이 나올까를 알 수 있다는 겁니다. 자 이거를 우리가 보기 위해서 로테이션을 정리하고 넘어가겠습니다. 즉 트레인 데이터 포인트를 우리가 이렇게 제 아이라고 할게요. 그래서 x아 콤마 와아 이미지가 있고 레이블이 있는 겁니다. 얘가 이미지라고 생각하셔도 되고 뭐 텍스트라고 생각하셔도 되고 아니면 테이블 데이터라고 생각하셔도 됩니다. 자 이렇게 트레이닝 데이터와 엔 개가 존재하는 것이죠. 모델 파라미터는 세타라고 하겠습니다. 그때 LG 콤마 세타는 특정한 트레이닝 데이터 지에 대한 이 지라는 게 뭐 이런 아이번지 데이터일 수도 있고 모르죠 그때 특정한 데이터 딱 하나에 대한 로스라고 가정합시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 3,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1100,
      "char_count": 2024
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c004_6e714b",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 즉 트레인 데이터 포인트를 우리가 이렇게 제 아이라고 할게요. 그래서 x아 콤마 와아 이미지가 있고 레이블이 있는 겁니다. 얘가 이미지라고 생각하셔도 되고 뭐 텍스트라고 생각하셔도 되고 아니면 테이블 데이터라고 생각하셔도 됩니다. 자 이렇게 트레이닝 데이터와 엔 개가 존재하는 것이죠. 모델 파라미터는 세타라고 하겠습니다. 그때 LG 콤마 세타는 특정한 트레이닝 데이터 지에 대한 이 지라는 게 뭐 이런 아이번지 데이터일 수도 있고 모르죠 그때 특정한 데이터 딱 하나에 대한 로스라고 가정합시다. 아시겠죠? 즉 우리는 일단 우리 모델의 트레이닝 로스 함수는 이렇게 정의할 수 있겠죠 즉 i번째 데이터에 대한 로스인 건데 데이터가 총 n개가 있는 거니까 n분의 1 서메이션 한 게 되겠습니다. 이런 거를 우리는 인페리컬 리스크라고 많이 부릅니다. 즉 전체 모집단에 대한 데이터에 대한 리스크가 아니라 우리한테 주어진 샘플 데이터에 대한 리스크다 보니까 우리가 인플레컬 리스크라고 다음과 같이 부릅니다. 그리고 우리는 보통 일반적으로 이러한 인플레커 리스크를 가장 최소화시키는 파라미터를 찾는 것이 목표입니다. 그렇죠 즉 여기까지는 아직 데이터 가치 평가랑은 전혀 무관한 인플루언서 펑션과는 전혀 무관한 겁니다. 일반적으로 우리가 모델을 학습시키는 방법에 대해서 본 것뿐이에요. 자 그때 인게이 데이터가 있고 엔게이 데이터 로스가 있을 때 이 로스를 우리가 낮추는 파라미터를 찾는 게 목표입니다. 그러면 우리가 결국 하고 싶은 거는 일단 중간 과정으로는 특정한 만약에 데이터 지가 데이터 하나라고 볼 수 있겠죠. 데이터 하나가 제거되었을 때의 인플레 리스크는 이렇게 정의할 수가 있을 겁니다. 한번 보시면 우리가 이 제아 전체에 대해서 서메이션을 하는데 지가 생략된 형태로 볼 수가 있습니다. 그렇죠 그리고 그때 구해진 파라미터를 우리가 아래와 같이 정의를 할 수가 있겠습니다. 그러면 여기서 세타 h의 마이너스 이라는 거는 지라는 데이터가 제거된 데이터에 대해서 지라는 데이터와 트레이닝 데이터에서 빠진 환경에서 우리가 로스를 구하고 가장 최적인 파라미터를 찾았을 때의 값이라고 볼 수가 있습니다. 우리는 그러면 그때 파라미터의 차이는 이렇게 정의할 수 있겠죠 즉 제라는 데이터를 제거하고 난 파라미터 그리고 나서 학습한 파라미터 그다음에 우리가 제거하지 않고 학습한 파라미터 이렇게 두 개를 차이를 비교할 수가 있습니다. 자 물론 이제 우리는 궁극적으로 로스의 변화량을 살펴볼 예정입니다. 결국 이런 과정을 보려면 앞서 말씀드린 것처럼 굉장히 오랜 시간이 필요한 비효율적인 과정입니다. 그래서 얘를 우리는 좀 효율적으로 계산하고 싶은 거예요. 그래서 아이디어는 바로 아래와 같은 퍼처베이션을 이용하는 겁니다. 퍼처베이션은 특별한 게 아니라 뭔가 약간의 노이즈를 추가한다라는 뜻입니다. 약간의 변화를 가한다 약간의 노이즈를 가한다라는 뜻입니다. 우리가 여기서 이러한 식을 한번 볼게요. 이러한 식 자 이러한 식에서 엡실론이라는 게 있죠 이 엡실론 값에다가 우리가 만약에 마이너스 n분의 1을 넣겠습니다. 그러면 어떻게 될까요? 여기에 마이너스 n분의 1이 있다라는 말은 무슨 말이에요? 결국 우리는 z라는 트레이닝 데이터가 제거된 상황이라는 거예요. 왜 데이터가 총 n개가 있는데 NG 전체에 대한 평균적인 로스죠 근데 여기에 우리가 마이너스 n분의 1을 넣었다 그러면 여기도 제가 있었을 텐데 그쵸? 제라는 데이터가 n분의 1 제트에 대한 로스 또 마이너스 n분의 1 제트에 대한 로스 두 개가 더해지면서 상쇄가 되는 겁니다. 그러면 0이 나오게 되는 것이죠. 즉 우리는 제라는 트레이닝 데이터가 제거된 상황을 이렇게 앱실로의 마이너스 n분의 1을 넣으면서 구할 수가 있게 됩니다. 자 그러면은 그때 우리가 계산하고 싶은 값은 뭐냐면 이겁니다. 우리는 이 엡실론이 변함에 따라서 이 로스가 결국 어떻게 변하는지가 알고 싶은 거예요. 즉 이걸 한번 보시면은 테스트 데이터 로스입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 4,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1087,
      "char_count": 2001
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c005_fb5a1c",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 그러면 0이 나오게 되는 것이죠. 즉 우리는 제라는 트레이닝 데이터가 제거된 상황을 이렇게 앱실로의 마이너스 n분의 1을 넣으면서 구할 수가 있게 됩니다. 자 그러면은 그때 우리가 계산하고 싶은 값은 뭐냐면 이겁니다. 우리는 이 엡실론이 변함에 따라서 이 로스가 결국 어떻게 변하는지가 알고 싶은 거예요. 즉 이걸 한번 보시면은 테스트 데이터 로스입니다. 그렇죠 즉 우리는 여기서 엡실론의 변화량에 대해서 엡실론이 변화할 때 이 테스트 로스가 얼마나 변하는지를 보고 싶은 거예요. 왜 그러면은 이 에실론이 변화한다라는 말은 데이터가 우리가 제거되는 상황을 모델링 할 수가 있겠죠. 그러면은 그때 이 테스트가 테스트 로스가 얼마큼 바뀌는지를 우리가 알게 되니까 그렇죠 그러면 데이터의 변화량에 따라서 로스의 테스트 로스에 대한 변화량을 알 수 있게 됩니다. 자 그리고 이거 같은 경우는 우리가 체인 룰을 쓰게 되면은 이렇게 우리가 체인 룰로 또 모델링을 할 수가 있게 되겠죠. 결국 엡실론이라는 거는 여기 세타에 여기에 영향을 미치는 애니까 여기에 대해서 우리가 미분하는 것으로 생각할 수가 있게 됩니다. 자 그러고 나면은 우리는 사실 이것만 좀 쉽게 계산만 할 수 있으면 이제 문제는 해결되는 겁니다. 그러면 얘를 우리가 좀 어떻게 계산할 수 있을까를 한번 살펴보도록 합시다. 즉 여기는 앞에는 우리가 로스트가 나오면 결국 그라덴트 구하면 돼요. 미분하면 되는 거예요. 근데 이 뒤에는 뭐 애초에 이 세타와 h의 엡실론 지가 뭔지도 모르겠고 얘를 어떻게 계산하는지 모르겠다 그래서 한번 얘를 보도록 하겠습니다. 자 얘를 우리가 어떻게 계산할 거냐인 건데 일단은 참고 사항으로 세타 햇의 앱실런지는 얘입니다. 지금 무슨 말이에요? 아까 우리가 세타 헤스 엡실론지는 이런 뒤에 엡실론이 있을 때 이에 대한 로스를 가장 낮춰주는 파라미터라고 우리가 정의를 했습니다. 그렇죠 그래서 이런 보라색 식에 대한 정답지가 세타 헤에 에틸로 콤마 지가 되는 것이죠. 그리고 여기 나와 있는 얘를 우리가 알세타라고 하겠습니다. 얘는 뭐 특정한 데이터가 추가되거나 삭제되거나 그런 건 아닙니다. 그쵸 얘를 우리가 알세타라고 하겠습니다. 자 그리고 우리가 파라미터의 변화량을 다음과 같은 델타 엡실론이라고 한번 해볼게요. 즉 무슨 말이에요? 여기서는 파라미터 변화량은 우리가 애초에 변화를 안 준 데이터의 변화를 안 준 상황에서 전체 데이터로 우리가 학습한 모델 파라미터 그다음에 여기 나와 있는 첫 번째는 우리가 모델 파라미터인데 앱슬로만큼 변화량을 준 그쵸 지라는 데이터에 대해서 앱실로만큼 우리가 웨이트를 가중치를 둔 그 상황에서의 모델 파라미터입니다. 그러면 두 개의 모델 파라미터 간의 변화량을 이렇게 정의할 수가 있겠죠. 자 그러면은 한번 보시면 여기 나와 있는 세타 햇이라는 거는 애초에 이 전체의 리스크 즉 우리가 어떠한 뭐 앱실론을 주거나 그러지 않은 거기 상황에서의 최적의 모델 파라미터니까 얘는 엡실론에 대해서 무관한 친구가 됩니다. 그래서 우리가 결국 얘는 이렇게 표현이 되게 되겠죠. 얘나 얘나 같은 말이 되겠죠 왜 세타 햇은 어차피 실론이 돼서 무관한 거니까 그렇죠 자 그다음에 한 가지 더 세타 햇에 엡스는 지는 이 보라색을 가장 미니마이즈 시키는 친구입니다. 그렇죠 가장 얘를 미니마이즈 시키는 친구입니다. 지금 무슨 말이에요? 그러면은 이 식에 대한 결국 익스트림 포인트라는 겁니다. 즉 로컬 미니멈 또는 로컬 맥시멈이라고 우리가 볼 수가 있겠죠. 즉 얘를 가장 낮추는 값이 얘니까 얘에 대한 얘를 우리가 미분취했을 때 이 지점에서 0이 된다라고 볼 수 있다는 겁니다. 그쵸 그래서 미분취했을 때 여기 나와 있는 보라색 식 아그민 제외하고 이거에 대해서 우리가 미분 체했을 때 그때 이 포인트에서 0이 된다라는 뜻을 나타냅니다. 그렇죠 여러분 우리가 만약에 뭐 y는 x 제곱이 있다 그러면은 이거 미분치하고 그렇죠 여기 나와 있는 익스트럼 포인트 가장 미니멈이 되는 포인트 거기를 넣으면은 이 미분 값이 0이 되잖아요. 그렇죠 그거를 지금 나타내고 있는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 5,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1113,
      "char_count": 2051
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c006_45b549",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 그쵸 그래서 미분취했을 때 여기 나와 있는 보라색 식 아그민 제외하고 이거에 대해서 우리가 미분 체했을 때 그때 이 포인트에서 0이 된다라는 뜻을 나타냅니다. 그렇죠 여러분 우리가 만약에 뭐 y는 x 제곱이 있다 그러면은 이거 미분치하고 그렇죠 여기 나와 있는 익스트럼 포인트 가장 미니멈이 되는 포인트 거기를 넣으면은 이 미분 값이 0이 되잖아요. 그렇죠 그거를 지금 나타내고 있는 겁니다. 자 여기서 결국 앞에 있는 거를 우리가 쓰게 되면 로컬 미니멈이니까 미분 치했을 때 그렇죠 그 기울기에 대해서 우리가 그 포인트를 넣게 되면 익스트림 포인트니까 미분했을 때의 값이 0이 됩니다. 그렇죠 여기까지는 앞에서 우리가 봤던 내용 그다음에 여기서 우리가 테일러 전개를 활용할 겁니다. 자 테일러 전개는 여러분들 배운 적이 있으시겠지만 기억이 안 날 수도 있기 때문에 제가 여기 아래에 참고 자료를 넣어 놨어요. 즉 에프엑스라는 함수를 미분 가능한 함수여야겠죠 미분을 해야 되니까 자 에프엑스라는 함수를 우리가 시를 중심으로 해서 한번 얘를 당식으로 근사를 하겠다라는 게 테일러 전개 또는 테일러 근사입니다. 그러면 이러한 프스는 어떻게 쓰이게 되냐면 프시 더하기 프프라임 시의 스 마이너스 더하기 프2 프라임 2번 미분했다는 거죠. 이런 식으로 써지게 됩니다. 자 얘를 우리가 그대로 쓸 거예요. 아시겠죠? 그대로 쓸 건데 어떻게 쓸 거냐면 여기서는 이 FX가 결국에 이걸로 생각하시면 되고요. 이 전체 얘가 FX 그다음에 중심이 씨니까 여기 프씨 같은 게 나오죠. 여기서는 중심을 우리가 세타 헷이라고 잡겠습니다. 세타 헷 자 그러면은 이 FC에 대응되는 게 이거가 되겠죠 에프 그대로에서 우리가 세타 햇을 넣은 것뿐입니다. 자 그다음에 뒤에는 여기 나와 있는 뒤에는 요항에 대한 겁니다. 요항에 대한 자 요항에 대한 거 한번 보시면 x 마이너스 c라고 되어 있잖아요 그러면 우리는 뭐예요? 여기서의 x는 세타 h의 앱슬론 콤마지 그다음에 c는 세타 햇입니다. 그럼 두 개 뺀 거를 우리는 파라미터 변화량을 델타 엡실론이라고 하기로 했잖아요. 그래서 x 마이너스 c가 델타 엡실론이 되는 겁니다. 그다음에 여기는 여기는 한 번 미분해서 씨를 넣은 거죠. 씨가 여기서는 세타라고 세타 햇이라고 볼 수 있다고 했습니다. 그래서 한 번 더 미분하고 여기에서 한 번 더 미분하고 그다음에 셋타잇을 넣은 모습을 볼 수가 있습니다. 그다음에 얘를 여기 나와 있는 델타 엡실론으로 우리가 한번 정리하면은 이렇게 쓸 수가 있겠죠. 델타 입실론을 정리하면 그렇죠 여러분 여기까지 우리가 한번 볼 수가 있고요. 자 그다음에 얘를 우리가 좀만 더 간소화를 한번 해보겠습니다. 좀만 더 간소화를 하면은 이 파란색은 이렇게 간소화가 돼요. 왜 얘는 이런 바이노미얼 인버스 세어럼이라는 게 있습니다. a 더하기 앱실론 b를 인버스 하면 이렇게 된다 그러면 얘가 a라고 가정하고 앱실론 그대로 있고 그다음에 여기 얘가 비라고 생각하면 되겠죠. 그러면은 a의 인버스에다가 뭔가 엡실론이 쫙 붙어 있는 형태예요. 걔를 우리가 뒤에 걸 만약에 생략해서 쓰면 이렇게 쓸 수가 있겠죠. 얘가 a의 인버스니까 에이니까 그쵸 그다음에 빨간색 같은 경우는 우리가 한번 생각을 한번 하게 되면은 이 뒤에만 남게 됩니다. 왜 셋타 햇이라는 거는 원래 알에 대한 익스트림 포인트였으니까 즉 이 알이라는 게 뭐였어요? 이 알이라는 거를 가장 미니마이즈 시켜주는 포인트가 세타 햇이라고 우리가 앞에서 정의를 했었습니다. 얘가 알이죠 알을 미니마이션 시키는 게 세타인 그러니까 2분 했을 때 세타 센소는 기울기가 0이 돼야만 하는 상황이 되겠죠. 그래서 이게 나오게 된 겁니다. 그러면 결국 남는 것만 우리가 고려하게 되면은 이거에다가 얘만 이렇게 쫙 남게 되는 거예요. 나머지 우리가 일부의 텀은 좀 생략을 했다고 치면 여기 마이너스가 이렇게 붙어 있는 형태가 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 6,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1056,
      "char_count": 1967
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c007_31cb92",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 얘가 알이죠 알을 미니마이션 시키는 게 세타인 그러니까 2분 했을 때 세타 센소는 기울기가 0이 돼야만 하는 상황이 되겠죠. 그래서 이게 나오게 된 겁니다. 그러면 결국 남는 것만 우리가 고려하게 되면은 이거에다가 얘만 이렇게 쫙 남게 되는 거예요. 나머지 우리가 일부의 텀은 좀 생략을 했다고 치면 여기 마이너스가 이렇게 붙어 있는 형태가 됩니다. 그럼 우리가 이렇게 파라미터 변화량을 우리가 추론을 할 수 있게 된 것이고요. 자 그러면은 앞서서 이게 우리가 구하고 싶었던 건 이 빨간 박스예요. 이 빨간 박스인데 이 빨간 박스가 이 파라미터 변화량이랑 동일하다고 했습니다. 왜 파라미터 변화량은 우리가 이렇게 생겼는데 어차피 이 세타 잇은 엡실론에 대해서 무관하니까 사실 얘만 미분한 거랑 똑같은 거니까요. 그래서 이렇게 우리가 쓸 수 있고 그다음에 얘는 우리가 방금 얘는 우리가 방금 봤죠 얘는 이렇게 나온다라고 봤습니다. 이렇게 나온다 그렇죠 그러면은 이 델타 엡실론이 이렇게 나오니까 얘를 엡실론에 대해서 미분하면은 여기 엡실론이 쫙 하고 없어지게 되겠죠. 그럼 요것만 남게 됩니다. 이것만 이것만 남게 되는데 여기서 우리가 그라이언트를 이렇게 두 번째 한 거를 해시안이라고 말합니다. 그럼 해시안에 대한 인버스가 되는 거죠. 그렇죠 그다음에 요게 남아 있는 과정입니다. 즉 우리는 이제 얘도 구한 상황이에요. 그러면 이제 우리가 구할 수 있는 건 다 구했고 결국 최종적인 식은 이겁니다. 만약에 앞에 있던 과정들이 좀 많이 어려웠다 하시는 분들은 조교분들을 통해서 아니면은 뭐 저에게 메일을 통해서 질문을 해 주셔도 되고요. 여기서 하지만 뭐 이게 너무 어렵다라고 하시면 이 결론만 사실 기억해야 됩니다. 이 결론이 의미하는 것은 뭐냐면 자 우리가 제 테스트라는 우리가 테스트 데이터에 대해서 로스를 구할 거예요. 그렇죠 그때 그 로스에 미치는 영향 즉 이 데이터를 잘함에 있어서 이 지라는 트레이닝 데이터가 미치는 영향을 이렇게 나타낸 겁니다. 이렇게 우리가 예로 볼 수가 이렇게 구할 수 있다는 겁니다. 자 한번 여기서 한번 보면은 자 우리가 만약에 데이터를 제거한다고 하면은 앱실로는 마이너스 n분의 1이 됩니다. 엡실론은 마이너스 n 분의 1이 돼요. 그러면은 엡실론이 마이너스 n분의 1이 됐다라는 말은 마이너스가 됐다는 말이고 그럼 데이터가 제거됐어요 그쵸 데이터가 제거가 됐고 그때 만약에 로스가 커진다라고 하면 얘는 점점 양수 이 변화량은 이제 우리가 점점 커진다라고 볼 수 있겠죠. 그렇죠 그러면 엡실론은 만약에 마이너스가 엡실론는 작아졌는데 엡실론은 마이너스가 됐는데 로스가 커진다라고 하면 양수로 되고 얘는 음수가 되니까 음수 쪽에서 굉장히 큰 아주 왼쪽에 있는 값이 되는 겁니다. 마이너스 100 마이너스 200과 같은 만약에 값이 나오면 그 데이터는 굉장히 중요하다라는 뜻입니다. 아니면 마이너스를 붙여서 애초에 마이너스 얘를 붙인 게 크다고 하면은 그 데이터는 중요하다고 볼 수 있다는 것이죠. 그거를 우리가 어떻게 계산한다 이렇게 계산한다. 즉 우리가 모델 학습이 끝나면 이거는 그래서 실제로는 모델 학습이 다 끝나면 그냥 일반적인 모델 학습입니다. 우리한테 주어진 엔게이 데이터로 모델 학습을 다 하고 나면 그때 추가적인 학습 없이 이 데이터에 대해서 이 트레이닝 데이터가 얼마나 중요한지를 우리가 판단할 수 있는 도구로 쓸 수 있다는 얘기가 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 7,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 934,
      "char_count": 1726
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c008_5ea0a9",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 아니면 마이너스를 붙여서 애초에 마이너스 얘를 붙인 게 크다고 하면은 그 데이터는 중요하다고 볼 수 있다는 것이죠. 그거를 우리가 어떻게 계산한다 이렇게 계산한다. 즉 우리가 모델 학습이 끝나면 이거는 그래서 실제로는 모델 학습이 다 끝나면 그냥 일반적인 모델 학습입니다. 우리한테 주어진 엔게이 데이터로 모델 학습을 다 하고 나면 그때 추가적인 학습 없이 이 데이터에 대해서 이 트레이닝 데이터가 얼마나 중요한지를 우리가 판단할 수 있는 도구로 쓸 수 있다는 얘기가 됩니다. 그리고 얘를 한번 생각해 보시면 리트리벌 즉 우리가 검색하는 것과도 유사하긴 해요. 왜냐하면 테스트 데이터에 대한 그라디언트와 그라던트랑 트레이닝 데이터에 대한 그라던트랑 두 그라디언트가 얼마나 유사한지 보는 거잖아요. 얘가 만약에 아이덴티티라고 한다면 그렇죠. 얘를 만약에 무시한다면 테스트 데이터에 대한 그라디언트랑 트레이닝 데이터에 대한 그라디언트가 얼마나 유사한지를 기반으로 해서 이 데이터가 얼마나 중요하고 안 중요한지를 판단하게 되니까 우리는 그렇게 트리을 검색하는 과정으로도 즉 이 테스트 데이터랑 유사한 트레이닝 데이터가 뭔지 검색하는 거를 그라디언트를 기반으로 한다라고도 생각할 수가 있게 됩니다. 자 여기까지 해서 우리가 인플루언서 펑션이 뭔지 한번 살펴봤고요. 그래서 이거를 우리가 어떻게 활용할 수 있는지 이제부터 한번 보도록 하겠습니다. 즉 우리는 지금같이 인플루언서 펑션을 통해서 특정 데이터 포인트가 삭제되거나 추가될 때의 로스 변화량을 추정할 수 있는 걸 봤습니다. 즉 더 모델을 학습하지 않더라도 로스가 뭐 작아질지 커질지 예측을 할 수 있다는 거예요. 이거를 활용하면 각 데이터 포인트마다의 트레인 데이터마다 얼마나 중요한지를 우리는 이제 측정할 수 있게 됩니다. 그러면은 실제로 얘가 지금 얼마나 정확하게 로스를 예측하는지 우리가 한번 테스트를 해보자는 거예요. 어떻게 그러면은 우리가 인플루언서 펑션으로 예측되는 로스의 변화량과 실제로 우리가 오래 걸리지만 검증을 위해서 오래 걸리지만 특정한 트레이닝 데이터 빼고 전체 학습하고 또 몇 개 또 다른 트레이닝 데이터 빼고 전체 학습하고 그 과정을 여러 번 해서 실제로도 한번 계산해 보자는 겁니다. 데이터가 트레이닝 데이터에서 이게 빠졌을 때 테스트 로스가 어떻게 되는지 실제로 계산한 값 그다음에 인플루언서 펑션으로 계산한 값 그때 사실 우리가 뉴럴 네트워크로 만약에 가게 되면 약간의 오차가 있긴 하지만 경향성은 굉장히 잘 나오는 것을 확인할 수가 있습니다. 그다음에 이제 이런 게 가능해진다는 거예요. 여기에서 결국 테스트 데이터에 대해서 이러한 이미지가 들어왔을 때 이러한 이미지 즉 강한 시인지 물고기인지 맞추는 이런 문제를 우리가 분류함에 있어서 가장 중요하게 작용한 트레이닝 데이터가 무엇인지 우리가 이젠 알 수가 있다는 겁니다. 자 한번 살펴보면은 우리가 SVM으로 우리 모델을 학습시키고 테스트했을 때는 가장 도움이 되는 이미지가 이런 식으로 나왔다라는 뜻이고요. 인플루언서 펑션으로 우리가 계산했을 때 그다음에 우리가 인셉션 네트워크라는 뉴럴 네트워크를 통해서 모델을 트레이닝하고 테스트했을 때는 이거를 우리가 테스트해서 예측함에 있어서 가장 중요하게 작동한 트레이닝 데이터는 이거라는 뜻입니다. 자 얘를 고를 때는 결국 얘가 가장 큰 데이터를 우리가 고르게 되는 겁니다. 얘가 가장 큰 마이너스를 취한 값이 가장 큰 이미지 왜 그렇다고 말씀드렸죠 결국 우리는 이런 형태이지 않았습니까? 이 형태는 어떤 거였어요? 우리가 앱실론이 만약에 뭐 음수로 가게 되면 마이너스 n분의 1과 같이 그때 로스가 만약에 커진다. 즉 우리가 엡실론이 마이너스 n분의 1과 같이 됐을 때 음수로 갔을 때 로스는 만약에 커진다라고 하면은 그 데이터는 굉장히 중요한 겁니다. 그렇죠 얘는 이제 마이너스로 큰 값을 가지게 되는데 거기에 마이너스를 치워주면 그냥 이제 양수로 큰 값이 되는 거죠. 그래서 마이너스로 취했을 때 그때 가장 큰 값을 우리가 고르면 그 데이터가 중요한 데이터다라는 말이 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 8,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1124,
      "char_count": 2044
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c009_8f39d8",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 즉 우리가 엡실론이 마이너스 n분의 1과 같이 됐을 때 음수로 갔을 때 로스는 만약에 커진다라고 하면은 그 데이터는 굉장히 중요한 겁니다. 그렇죠 얘는 이제 마이너스로 큰 값을 가지게 되는데 거기에 마이너스를 치워주면 그냥 이제 양수로 큰 값이 되는 거죠. 그래서 마이너스로 취했을 때 그때 가장 큰 값을 우리가 고르면 그 데이터가 중요한 데이터다라는 말이 됩니다. 즉 지금 뭐가 되는 거예요? 우리 모델이 여기서 얘를 우리가 물고기라고 했습니다. 그 이유가 뭔가요? 트레이닝 할 때 이러한 데이터를 우리가 물고기라고 학습시켰기 때문입니다라고 설명할 수가 있게 됩니다. 그러면 우리가 앞서서 말씀드렸던 추천 시스템에서도 우리 알고리즘이 왜 이런 유저에게 이런 아이템을 추천했나요? 아 그러면 그때 이런 아이템을 유저가 봤기 때문입니다 라고 말을 할 수가 있다는 겁니다. 그렇죠 그럴 때 우리가 추천 시스템을 활용할 수가 있고요. 그다음에 또는 우리가 언러닝 관점에서도 인플루언스 펑션을 또 활용할 수가 있게 되겠습니다. 즉 무슨 말이에요? 언러닝이라는 거는 이미 잘 학습된 레코멘데이션 시스템 또는 뉴럴 네트워크에서 특정 영역을 삭제하는 겁니다. 뭐 여러 가지 이유가 있겠죠 이 데이터가 굉장히 잘못된 데이터였거나 아니면은 뭐 우리가 저작권을 갖고 있지 않은 데이터일 수도 있으니까 학습시켰다가 나중에 지우고 싶을 때도 있겠죠 그러면 그럴 때 인플루언서 펑션을 우리가 활용하면은 이러한 데이터가 만약에 삭제되었을 때 모델의 성능이 테스트에서 어떻게 바뀔지를 우리가 예측할 수 있게 됩니다. 그리고 여기서 인플루언서 펑션 심화를 한번 볼게요. 이거는 심화 버전이라서 제가 큰 컨셉만 한번 짚고 넘어가겠습니다. IFRS 펑션을 통해서 특정 데이터 포인트가 삭제되거나 추가될 때의 로스 변화량을 추정할 수 있는 방법에 대해 살펴봤습니다. 그렇죠 이를 활용하면 각 데이터 포인트마다 중요도를 측정할 수 있게 돼 근데 우리가 마주하게 되는 상황은 이러한 데이터 포인트가 추가되거나 삭제되는 상황만 있을까요? 아니면 또 어떠한 상황이 있을까요? 실제로 우리는 데이터 증강을 통해서 변형된 데이터도 많이 있어요. 숫자 데이터에서 특정 숫자를 우리가 뭐 20도 로테이션 돌리기도 하고 그렇죠 그다음에 우리가 뭐 여러 가지 데이터의 가우샷 노이션을 통해서 새로운 데이터로 증강을 하고자 합니다. 자 그러면 이러한 데이터 퍼티베이션 데이터 변화를 데이터 가치 부여에서 어떻게 고려할 수 있을까요? 한번 고민을 한번 해보시겠어요 즉 무슨 말이냐면 자 우리가 특정한 데이터가 제거되거나 추가되었을 때 그때 로스의 뭔가 변화량이나 파라미터 변화량을 모델링 하기 위해서 목적 식을 이렇게 설계했습니다. 그렇죠 왜 엡실론이 마이너스 n분의 1이면 그 데이터가 제거되는 거고 앱슬론이 n분의 1이면 제라는 데이터가 새로 들어오는 환경을 모델링 할 수 있는 겁니다. 근데 데이터가 제거되거나 추가되는 게 아니라 원래 있던 데이터가 약간 변화되는 거는 어떻게 고려할 수 있을까요? 이렇게 표현할 수 있겠죠 자 데이터가 원래 스코마 와였는데 얘가 스 더하기 델타 콤마 와로 바뀌었다고 가정하겠습니다. 그러면 어떻게 됩니까? 그러면 스 콤마 와이라는 데이터가 없어진 거라고 생각할 수가 있겠죠. 그리고 스 더하기 델타 콤마 와라는 데이터가 새로 생겼다고 볼 수가 있습니다. 그래서 앞서 배웠던 내용을 응용하는 겁니다. 즉 이 데이터가 새로 생긴 거니까 우리가 더하기 엡실론 엡실론은 이제 n분의 1이 되겠죠 그다음에 이 데이터가 삭제된 거니까 마이너스 엡실론 엡실론은 마찬가지로 n분의 1이 될 겁니다. 그러면 이거 삭제되고 이거는 추가된 형태를 우리는 모델링 할 수 있게 되는 거예요. 그리고 이거는 저희가 유도를 하지 않겠습니다. 이거는 이제 심화 과정이고 이거를 우리가 앞에서 배웠던 이 유도 과정을 이 과정을 그대로 따라가면 앞과 유사한데 이게 하나 그냥 붙는 것뿐이에요. 왜 데이터가 이제 퍼티베이션 되는 거니까 데이터의 변화량이 나타나는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 9,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 1103,
      "char_count": 2023
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_7강_데이터의_가치측정_및_해석__c010_992cc1",
      "content": "[MLforRecSys] [MLforRecSys] (7강) 데이터의 가치측정 및 해석 I\n\n다. 그러면 이거 삭제되고 이거는 추가된 형태를 우리는 모델링 할 수 있게 되는 거예요. 그리고 이거는 저희가 유도를 하지 않겠습니다. 이거는 이제 심화 과정이고 이거를 우리가 앞에서 배웠던 이 유도 과정을 이 과정을 그대로 따라가면 앞과 유사한데 이게 하나 그냥 붙는 것뿐이에요. 왜 데이터가 이제 퍼티베이션 되는 거니까 데이터의 변화량이 나타나는 겁니다. 그래서 데이터의 변화량을 설명하는 그라디언트 x가 하나 더 추가되는 것뿐이에요. 이것만 없으면 우리가 앞에서 봤던 거랑 완전 동일합니다. 자 그리고 이런 걸 우리가 응용하게 되면은 우리가 어드버서리얼 데이터에 대해서도 효과적으로 고려할 수 있게 됩니다. 즉 로스트를 가장 키우기 위해서는 어떠한 데이터가 가장 이제 퍼티베이션을 줘야 될지 그런 것들도 우리가 효과적으로 고려할 수가 있게 되는 것이죠. 네 여러분 여기까지 해서 이번에는 데이터의 가치 평가가 무엇이고 그게 어느 분야에 활용되는지 그렇죠 그다음에 그러한 인플루언서 펑션을 기반으로 해서 우리가 데이터 가치 평가를 어떻게 할 수 있는지까지 한번 살펴봤습니다. 실제로 여러분 추천 시스템 또는 여러 가지 머신 러닝과 관련된 테스크에서 이제는 성능이 굉장히 잘 나오기 시작하다 보니까 이 알고리즘이 왜 이렇게 작동했는지 보다 더 유저에게 신뢰를 주기 위해서는 그러한 설명 가능성이 굉장히 또 대두되고 있는 상황입니다. 그때 설명 가능성이 여러 가지가 존재하지만 그중에서 가장 강력한 방법 중의 하나인 데이터 밸류에이션 즉 우리 모델이 이렇게 테스트에서 작동한 이유는 트레인 게이트에서 어떤 널리지를 가지고 와서 이렇게 작동하는 것인지 판단할 수 있는 방법이 굉장히 중요하게 됩니다. 그러면 우리 모델의 설명 가능성도 높아지게 되고, 그다음에 우리 모델의 성능이 좋지 않을 때 트레 데이터에서 어떤 부분이 문제였는지 저희가 판단해서 모델을 재학습시킬 수 있는 자동적인 툴을 만들 때에도 큰 도움이 되게 됩니다. 네 고생 많으셨습니다. 여러분.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I.json",
        "lecture_name": "[MLforRecSys] (7강) 데이터의 가치측정 및 해석 I",
        "course": "MLforRecSys",
        "lecture_num": "7강",
        "lecture_title": "데이터의 가치측정 및 해석 I",
        "chunk_idx": 10,
        "total_chunks": 11,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:4604f64df38bbda5f0932acfc941defb5d0be62ba2192993e957d58dd55f3aba"
      },
      "token_estimate": 563,
      "char_count": 1034
    }
  ]
}