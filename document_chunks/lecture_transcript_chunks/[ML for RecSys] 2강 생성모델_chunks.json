{
  "source_file": "[ML for RecSys] 2강 생성모델.json",
  "lecture_name": "[ML for RecSys] 2강 생성모델",
  "course": "ML for RecSys",
  "total_chunks": 13,
  "chunks": [
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c000_9a47ce",
      "content": "[강의 녹취록] 과목: ML for RecSys | 강의: 2강 | 제목: 생성모델\n\n네 안녕하세요 여러분 이번에는 저희 생성 모델 쪽으로 한번 같이 공개해 보도록 하겠습니다. 저희가 생성 모델 기반의 추천 시스템이 지금 굉장히 많이 활발하게 연구가 되고 있습니다. 그래서 그러한 생성 모델의 원리가 무엇이고 그다음에 우리가 어떻게 또 구현을 할 수 있을지 한번 살펴보도록 하겠습니다. 네 그래서 오늘 저희가 같이 살펴볼 내용은 생성 모델이라는 게 무엇인지 저희가 한번 살펴보고 그다음에 그런 생성 모델을 이해하기 위한 그래피컬 모델이에요. 그다음에 생성 모델에 굉장히 다양한 종류가 있지만 그중에서 추천 시스템에 많이 활용되고 있는 VA를 저희가 한번 살펴보도록 하겠습니다. 지금 이미지 쪽에서는 저희가 뭐 디퓨션 모델 기반의 방법론들이 많이 활발하게 연구가 되고 있지만요. 결국에 이 VA를 여러분들께서 이해를 하시면 이 VA에서 여러 스텝을 확장한 게 디퓨처 모델로 우리가 해석을 할 수 있기 때문에 이 VA를 여러분들께서 이 강의를 통해서 충분히 이해만 잘 하신다면 디퓨처 모델도 문제없이 이해하실 수 있을 거라 생각합니다. 네 그럼 첫 번째 파트로 생성 모델의 개요에 대해서 한번 살펴보도록 하겠습니다. 자 생성 모델은 다음과 같이 x랑 y에 대한 조인트 또는 x에 대한 결국 마지널 라이클루드를 우리가 학습하는 겁니다. 이것을 우리가 조금만 더 한번 살펴보면 엑스는 이미지 와는 레이블이라고 생각하시면 됩니다. 스가 텍스트일 수도 있고 이미지일 수도 있고 또는 웨이팅 매트릭스라고 생각하셔도 됩니다. 자 그랬을 때 우리는 이러한 데이터 분포를 알고 싶은 거예요. 다만 이러한 모집단의 데이터 분포를 우리가 모르기 때문에 그러한 모집단의 데이터 분포와 유사한 우리만의 모델을 만들어 가지고 얘를 추론을 한번 해보겠다가 결국 승소 모델의 개요입니다. 자 그래서 이 슬라이드는 보시다시피 우리가 검은색 점만 있을 때 끝에 이 검은색 점이 생성된 회색의 분포를 우리가 추론하고 싶은 게 목표입니다. 즉 우리가 이러한 회색의 분포를 추론만 할 수 있다면 그때는 뭐가 될까요? 만약에 우리한테 샘플 데이터가 또 이런 게 주어졌어요. 그러면 이러한 샘플 데이터는 이쪽에 있는 샘플 데이터는 우리가 아웃라이어라고 생각해서 아웃라이어처럼 이렇게 재고도 할 수 있고 그 까닭에 분포로 추론하는 것은 그런 뭐 아웃라인을 제거한다든지 그런 것도 가능할 뿐만 아니라 우리가 이러한 분포를 알게 되면 거기서 새로운 데이터도 막 무지막지 샘플링을 많이 할 수가 있게 됩니다. 그래서 분포 추정과 그다음에 샘플 제너레이션 이 두 가지가 모두 다 가능해지기 때문에 다음과 같이 실제 이미지 샘플 데이터를 가지고 우리가 학습해서 거기에서 새로운 이미지들도 막 무수히 많이 만들어 낼 수 있다입니다. 자 그리고 그러한 제런티 모델의 또 한 가지 장점은 어떤 것이냐면 그러한 데이터들이 관측된 데이터들이 나오게 된 내재적인 패턴을 또 우리가 알 수 있게 됩니다. 즉 어떤 과정을 거쳐서 데이터가 만들어졌는지를 우리가 알게 되면 그 데이터에 대한 이해도도 더 커지게 되고 그러한 데이터를 우리가 변형함에 있어서도 우리가 좀 더 손쉽게 데이터의 스타일을 바꾼다든지 콘텐츠를 바꾼다든지 하는 것들도 가능해집니다. 자 뭐 여러분들께서 이미 한번 들어봤을 법한 이 리처드 파인만이 이런 얘기를 했습니다. 뭐 내가 만약에 만들지 못한다면 나는 뭐 이해를 하지 못한 것이다 라는 말이 있습니다. 그렇죠 우리가 이거를 또 다르게 해석하면 내가 우리가 또 만들 수 있으면 나는 이걸 이해하는 거다라고 또 생각을 할 수 있겠죠. 여러분들께서 마치 뭐 학창 시절에 공부하실 때 남에게 내가 잘 가르쳐 줄 수 있다면 나는 그 지식에 대해서 이미 충분히 잘 알고 있는 거라는 내용도 있잖아요. 그런 것과 유사하겠죠. 물론 이제 여기서 최근에는 생성 모델이 뭐 이런 패러독스도 있기는 합니다. 만들 수는 있는데 또 이해는 하지 못하는 이런 문제들도 있기는 해요. 예를 들어서 우리가 이러한 언어 모델 그다음에 비전 모델 한번 생각해 보겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 0,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1097,
      "char_count": 2010
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c001_88a9ac",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 만들 수는 있는데 또 이해는 하지 못하는 이런 문제들도 있기는 해요. 예를 들어서 우리가 이러한 언어 모델 그다음에 비전 모델 한번 생각해 보겠습니다. 자 여기서 이제 오스팅이라는 사람이 너무 힘든 하루를 보내 가지고 그 밤에 술을 마시러 바에 갔다고 합니다. 그렇죠 그러고 나서 이게 그러면 왜 오스틴이 그랬을까라고 우리가 오너 모델한테 물어본 거예요. 생성 모델한테 물어봤다는 겁니다. 자 그랬더니 생성 모델이 오스틴은 그날 너무 힘들어서 좀 위안을 받고 싶어서 같다라고 말을 했어요. 언어 모델이 굉장히 잘 생성을 한 걸 볼 수가 있습니다. 그렇죠 그러고 나서 그 언어 모델한테 다시 한 번 물어봐요. 그 우리가 원래 물었던 질문하고 언어 모델이 생성한 답변하고 두 개를 동시에 묶어서 언어 모델한테 물어봅니다. 자 그러면은 결국에 왜 간 건지 객관식으로 한번 맞춰봐 라고 한 건데 정답은 그렇죠 좀 안식을 취하기 위해서였죠. 언어 모델이 그렇게 답했으니까 근데 언어 모델이 이상한 답을 하는 경우도 있습니다. 즉 실제로는 생성을 하고 있는데 뭐 이해를 하지 못하는 경우도 언어 모델에서 발생하고 있죠. 물론 이런 리차드 파이만는 생성 모델에 대해서 한 말은 아니고 사람의 경우에 대해서 한 말이긴 하겠죠. 사람 같은 경우는 오히려 좀 더 생성이 어려운 이해하는 건 가능하지만 생성하는 게 좀 더 어려운 거를 말하고 있는데 예를 들어서 우리가 뭐 영어를 뭐 보고 이해하는 거는 쉬워도 새로운 영어를 우리가 뭐 스피팅 하는 건 좀 더 난이도가 있다라고 느끼는 것처럼 사람은 그런 반면 인공지능 모델은 또 생성은 잘하는데 이해는 하지 못하는 뭐 그런 문제점이 또 있긴 합니다. 그래서 생성 모델은 계속해서 지금 발전을 하고 있고요. 이것과 관련된 보다 더 구체적인 내용은 나중에 생성 모델과 관련된 또 콘텐츠에서 여러분들께서 다룰 기회가 있다고 생각합니다. 그러면 저희가 다시 한 번 생성 모델의 원리 쪽으로 넘어오겠습니다. 앞서 말씀드린 것처럼 생성 모델이라 함은 자 이러한 내재적인 패턴을 파악하는 것이기 때문에 우리가 이러한 데이터가 어떻게 만들어진 데이터인지 이해도가 더 커지기도 하고 그다음에 우리가 특정한 뭐 성별이나 아니면은 뭐 스타일이나 그런 것도 쉽게 바꿀 수가 있게 됩니다. 자 여기까지 보면은 우리가 생성 모델이 과연 어떤 것인지 그다음에 어떠한 장점을 갖고 있는 것인지 한번 살펴봤습니다. 자 이제부터는 이러한 생성 모델을 이해하기 위한 그래피컬 모델을 한번 보도록 할게요. 자 여러분 이러한 로테이션들을 혹시 보신 적이 있으신가요? 자 왼쪽과 오른쪽은 먼저 같은 표현입니다. 자 여기서 색칠이 되어 있는 거는 관측이 된 걸로 생각하시면 되고 색칠이 안 된 거는 관측이 안 된 걸로 생각하시면 됩니다. 자 무슨 말이냐면 이 와라는 건 레이턴트라고 우리가 많이 부르기도 합니다. 예를 들어서 다음과 같은 로테이션은 자 와라는 거는 클래스입니다. 고양이일 수도 있고 강아지일 수도 있는 것이고요. 그때 여기 나와 있는 x 색칠이 되어 있는 건 이미지라고 생각하시면 됩니다. 즉 특정한 클래스가 정해지면 그 클래스에서 우리가 이미지들을 막 이렇게 디게 많이 만들어 낼 수 있다라는 거를 나타냅니다. 자 그리고 왼쪽처럼 나타내면은 우리가 좀 복잡하잖아요. 왜 데이터가 디에 있는 거를 우리가 이렇게 여러 개 표현해 줘야 되니까 그래서 그거를 다음과 같이 사각형으로 우리가 나타냅니다. 그러면은 이렇게 사각형으로 하고 d라고 나타내요. 그러면 무슨 말이다 그러면은 이러한 x가 디에 있다라는 뜻입니다. 반복해서 그래서 왼쪽과 오른쪽이 같은 건데 보다 더 우리가 좀 더 쉽게 표현을 한 걸로 생각하시면 되고 이런 걸 우리가 플레이트 노테이션이라고 부릅니다. 자 여러분들께서 또 머신 러닝 수업에서 아마 이미 한번 접했을 가능성이 큰 가오샤 믹스처 모델에 대해서 한번 보도록 하겠습니다. 자 가오샤 믹스처 모델은 우리가 이렇게 생긴 모델인 거죠. 즉 우리가 가우시안 분포가 총 k개가 있다라고 가정하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 1,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1092,
      "char_count": 2012
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c002_1333d8",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 그러면은 이렇게 사각형으로 하고 d라고 나타내요. 그러면 무슨 말이다 그러면은 이러한 x가 디에 있다라는 뜻입니다. 반복해서 그래서 왼쪽과 오른쪽이 같은 건데 보다 더 우리가 좀 더 쉽게 표현을 한 걸로 생각하시면 되고 이런 걸 우리가 플레이트 노테이션이라고 부릅니다. 자 여러분들께서 또 머신 러닝 수업에서 아마 이미 한번 접했을 가능성이 큰 가오샤 믹스처 모델에 대해서 한번 보도록 하겠습니다. 자 가오샤 믹스처 모델은 우리가 이렇게 생긴 모델인 거죠. 즉 우리가 가우시안 분포가 총 k개가 있다라고 가정하겠습니다. 여기서는 가우시안 분포가 3개가 있는 그림을 나타내겠죠. 그랬을 때 이 가우시안 분포의 각 비중은 어떻게 되는지를 모델링 하는 것입니다. 그래서 보시면 가오션 분포가 하나 있고 그다음에 그 분포에 대한 비율이 나와 있고 그게 케이에 대해서 서메이션이 되어 있습니다. 즉 이 파이아이라는 건 예를 들어서 20% 30% 50% 그러한 비율을 나타내는 것입니다. 그리고 그러한 가우시 믹스처 모델을 우리가 앞서 배웠던 플레이트 노트선으로 나타내면 이렇게 나타낼 수가 있게 됩니다. 자 여기서 사각형은 우리가 반복되는 거를 의미한다고 했습니다. 즉 xn이라는 데이터 포인트가 총 라지 n개 만큼 있다. 그다음에 우리가 가우시안 분포가 총 라지 k개만큼 있다고 했습니다. 그 말은 가우지 분포 하나당 민과 코베런스가 하나씩 있을 거니까 그게 케이크에 있다라는 뜻이 되겠습니다. 그리고 파이는 결국 각 가우지 분포의 비율을 의미하니까 하나 이렇게 존재하는 게 되겠죠. 그렇죠 그래서 그러한 가우시안 문기처 모델을 그림으로 나타내면 이런 왼쪽 그림으로 나타낼 수가 있는 것이고요. 그렇죠 그다음에 수식으로 나타내면 이렇게 우리가 나타낸다는 것도 봤습니다. 자 그리고 걔를 우리가 이렇게 제너티브 프로세스 즉 실제로 데이터가 어떻게 생성되는지를 우리가 이렇게 텍스트로 나타내는 제너러티브 프로세스는 이렇게 정의할 수가 있습니다. 즉 우리가 총 라이 엔개의 데이터가 있는데 그때 이 데이터 하나가 어떻게 만들어지는 거냐 즉 이러한 데이터가 어떻게 만들어지는 거냐라고 하는 겁니다. 자 그러면 우리는 어떤 과정을 거치게 되냐면 지금 파이가 이렇게 이 빨간색 초록색 파란색 가우시안 모델들의 결국 비중을 나타내는 거였습니다. 그러면은 이 빨간색이 만약에 뭐 한 30% 초록색이 50% 빨간색이 20%라고 하면은 파이는 0.3 콤마 0.5 콤마 0.2가 되는 겁니다. 거기서 우리가 하나를 샘플링해요. 자 그랬을 때 만약에 빨간색 클러스터가 딱 선택이 됐다고 하겠습니다. 그렇죠 그러면은 클러스터 1이라는 게 여기 올 겁니다. 1 그러고 나서 그에 대응되는 가우시안 분포가 총 3개가 있고 여기로 치면 3개가 있는데 그 클러스터에 대응되는 민과 코베리언스에서 우리가 가우시안 분포에서 값을 이렇게 샘플링해서 지금의 데이터가 만들어진다 와 같이 데이터가 만들어지는 과정을 이렇게 프로세스로 나타내는 것을 제너티브 프로세스라고 부릅니다. 즉 우리가 생성 모델을 표현할 때 이렇게 제널티브 프로세스로 나타내기도 하고 플레이트 노테이션으로 나타내기도 하고 그다음에 이러한 식으로 나타내기도 합니다. 여기서는 이 세 가지가 다 같은 표현이 되겠죠 네 지금까지 우리가 생성 모델이 무엇이고 그러한 생성 모델을 우리가 이해하기 위한 플레이트 노테이션에 대해서 한번 학습을 하였습니다. 자 이제부터는 생성 모델 중에 대표적인 모델 중의 하나인 VA에 대해서 한번 저희가 살펴보도록 하겠습니다. 저희가 VA로 넘어가기 전에 오토 인코더라는 것을 한번 먼저 보고 넘어가도록 하겠습니다. 자 한번 보시면은 오토인코더라 함은 왼쪽 그림과 같은 것인데요. 즉 우리가 인풋 데이터가 들어오면은 그러한 인풋 데이터를 아웃풋에서 다시 복원하는 게 목적입니다. 여기서 엑스라는 게 인풋이라고 생각하시면 되겠고 스 프라임이라는 게 우리 모델의 아웃풋 값으로 생각해 주시면 됩니다. 그러면은 그 두 개의 차이를 줄이는 게 목표입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 2,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1084,
      "char_count": 2000
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c003_bc0b07",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 자 이제부터는 생성 모델 중에 대표적인 모델 중의 하나인 VA에 대해서 한번 저희가 살펴보도록 하겠습니다. 저희가 VA로 넘어가기 전에 오토 인코더라는 것을 한번 먼저 보고 넘어가도록 하겠습니다. 자 한번 보시면은 오토인코더라 함은 왼쪽 그림과 같은 것인데요. 즉 우리가 인풋 데이터가 들어오면은 그러한 인풋 데이터를 아웃풋에서 다시 복원하는 게 목적입니다. 여기서 엑스라는 게 인풋이라고 생각하시면 되겠고 스 프라임이라는 게 우리 모델의 아웃풋 값으로 생각해 주시면 됩니다. 그러면은 그 두 개의 차이를 줄이는 게 목표입니다. 그 까닭에 원래 있던 데이터를 우리 모델이 복원하기 위해서 우리 모델이 학습이 되는 형태가 되겠습니다. 자 이거는 그러면은 선생님 여기서 인풋 데이터를 아웃풋 데이터로 그대로 다시 복원할 거면 이거를 굳이 뭐 하러 하냐 인풋으로 들어왔던 거를 아웃풋으로 다시 내보내는 것뿐이지 않냐 이거의 목적이 뭐냐라고 하면은 앞서 말씀드렸던 오토 인코더 기반의 추천 시스템이 될 수도 있고요. 아니면은 보다 법령적으로는 다음과 같이 차원 축소 알고리즘으로 얘를 많이 활용을 합니다. 즉 무슨 말이냐면 우리가 여기서 인풋 데이터의 차원이 만약에 784차원이라고 할게요. 자 여러분 m 리스트 데이터를 생각하면 28 바이 28 그렇죠 가로 28 세로 28이니까 걔를 우리가 일자로 쭉 나열하면 784차원이 되지 않습니까 그러면은 그러한 784차원을 지금 이 인플레이어에 쫙 넣었다고 가정하겠습니다. 그렇죠 자 그리고 아웃풋으로 다시 쫙 784차원이 나오길 원해요. 근데 그걸 우리가 바로 784차원으로 보내는 게 아니라 10차원으로 한 번 줄였다가 10차원에서 다시 784차원으로 늘릴 겁니다. 그렇게 되면은 그러한 784차원의 고차원의 데이터가 이러한 10차원의 데이터로 우리가 압축을 시킬 수 있었던 겁니다. 네 자 그러면 여러분 여기서 784차원을 10차원으로 줄이려면 어떻게 해야 될까요? 불필요한 데이터는 제거하고 중요한 정보만 남겨놔야지 다시 나중에 784차원으로 우리가 늘릴 수가 있겠죠 그렇죠 그러면 중요한 데이터만 우리가 압축을 하는 형태로 생각하시면 되겠습니다. 자 그래서 이러한 오토 인코더 즉 가장 단순한 형태의 오토 인코더죠 지금 x가 있으면 여기서 w 곱해서 그냥 g를 만들고 여기서 다시 우리가 w 곱해서 x 프라임 만들고 어떠한 롤리니어 액티베이션도 없습니다. 그리고 하나의 레이어로 이루어져 있고요. 이러한 리니어 옥토인 코더는 차원 축소 알고리즘 중 하나인 PCA랑 굉장히 깊게 연관된다라고 알려져 있습니다. 하지만 우리는 보통 뉴럴 네트워크를 쓴다고 하면은 더 깊게 쌓게 되고 놀리니어 액티베이션도 쓰지 않습니까 그 까닭에 우리는 PCA보다 더 좋은 차원 축소 알고리즘이 오토 인코더로 가능하게 되는 겁니다. 자 그러면은 차원 축소를 어떻게 하는지는 뭐 대략 알겠다 그래 뭐 중요하지 않은 부분은 버리고 진짜 중요한 부분만 압축해서 뭐 이렇게 10차원으로 남기겠지 그래서 뭐 그걸 어디에 쓸 건데라고 여러분들께서 생각하실 수가 있겠습니다. 자 그때 이런 방식으로 쓰이게 됩니다. 즉 여기서는 지금 어떤 걸 비주얼라이즈 한 거냐면 우리가 500개의 그 m리스트 데이터 0에서 9까지의 수기로 표현된 숫자 있지 않습니까 그러한 엠리스트 데이터를 우리가 이러한 오토인 코더로 학습을 시키고 나서 여기 나와 있는 지 부분을 시각화한 겁니다. 즉 우리가 실제로는 784차원이지만 얘를 우리가 예를 들어서 2차원으로 줄이고 다시 784차원으로 늘리게 되면은 28이 28이라는 이미지 하나가 즉 2차원의 데이터로 바뀌게 되니까 우리가 이러한 시각화도 마음껏 할 수 있는 거예요. 그렇죠 2차원이니까 그러면은 그때 다음과 같이 클래스별로 굉장히 잘 분류되는 것을 볼 수가 있습니다. 즉 이 데이터에 내재된 중요한 피처를 우리가 잘 학습한 것으로 볼 수가 있겠죠. 자 얘를 우리가 다시 한번 정리하면은 만약에 다음과 같이 세 가지의 변수가 있다고 하겠습니다. 그리고 각각의 변수는 0 또는 1의 값을 가질 수가 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 3,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1086,
      "char_count": 2032
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c004_4f36ba",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 즉 이 데이터에 내재된 중요한 피처를 우리가 잘 학습한 것으로 볼 수가 있겠죠. 자 얘를 우리가 다시 한번 정리하면은 만약에 다음과 같이 세 가지의 변수가 있다고 하겠습니다. 그리고 각각의 변수는 0 또는 1의 값을 가질 수가 있게 됩니다. 자 그때 총 가능한 가지 수는 2의 3승 8가지입니다. 근데 앞서 말씀드렸던 것처럼 엠리스트 데이터는 28바이 28이에요. 그렇죠 그러면 얘를 쫙 늘리면 일자로 늘리면 784차원의 벡터라고 우리가 볼 수도 있습니다. 자 이런 784차원의 벡터가 각각의 변수들이 바이너리 0 또는 1의 값을 가지게 됩니다. 그러면 우리가 고려해야 되는 가지수는 몇 가지일까요? 이에 784개만큼 고려를 해야 됩니다. 모든 가시수를 고려한다면 그렇죠 왜 여기서는 2의 3승이었고 여기서는 784개니까 2에 784승만큼 되게 됩니다. 그렇죠 자 즉 우리가 고려해야 되는 가지수가 무수히 많게 돼요. 우리가 여기서 새로운 이미지를 하나 만든다고 치면은 이러한 2에 784차원 2에 784개만큼의 가지수 중에서 우리가 고려해서 그럴듯한 이미지를 만들어야 된다는 겁니다. 0과 2를 적절히 넣어서 이거는 굉장히 쉽지 않은 문제겠죠 하지만 실제로 이미지를 한번 보자는 겁니다. 실제로 이미지를 보면 이렇게 생겼겠죠 자 그럼 여기서 0이 어디 0이 이렇게 있습니다. 자 한번 보시면 0이라는 이미지는 대부분 비슷하게 생겼어요. 그렇죠 일단은 거의 대부분 이 한가운데에는 검은색 점인 거고 그렇죠 흰색 없이 그다음에 양 거의 모서리에도 거의 다 마찬가지로 검은색입니다. 근데 이런 대각선 형태로 1이라는 흰색이 이렇게 들어가 있는 걸 볼 수가 있습니다. 그렇죠 1도 마찬가지죠 1도 한번 보시면 대부분은 검은색이에요. 근데 이 가운데가 이렇게 흰색으로 되어 있는 것뿐입니다. 즉 거의 비슷한 거죠. 0을 나타내는 거나 1을 나타내는 거나 2를 나타내는 거나 그것들끼리만 다른 거지 0끼리는 되게 비슷하고 1끼리는 또 비슷하고 2끼리도 비슷하다는 겁니다. 그 말은 우리가 실제로 이에 784개를 다 고려할 필요가 없이 한 10개 정도만 잘 고려해도 된다는 거예요. 그렇죠 실제로는 얘네가 가질 수 있는 게 여기도 흰색이 가질 수 여기도 흰색이 올 수 있고 여기도 흰색이 올 수 있고 실제로는 이 모든 픽셀이 다 흰색인 값도 가질 수 있지만 그런 거는 거의 일어나지 않을 거잖아요. 그쵸 우리가 일어나는 것만 고려하게 되면은 거의 10개의 패턴으로 우리가 줄일 수 있게 된다 입니다. 그 까닭에 우리가 베리에이션 오토인코더는 차원 축소를 통해서 이미지를 충분히 잘 생성할 수 있다는 겁니다. 무슨 말이냐면 이러한 이미지가 있다고 할 때 아니 선생님 여기서 우리가 차원을 줄였는데 어떻게 이렇게 유의미하게 잘 구분이 되나요? 라고 하면은 이러한 784차원을 사실은 다 유의미하게 쓰지 않고 실제 데이터는 얘가 실제로는 굉장히 작은 차원의 이러한 데이터가 매핑될 수 있기 때문입니다. 그래서 베네션 오토 인코더는 이러한 가정에서 결국 만들어지는 거예요. 우리가 저차원으로 보내서 다시 데이터를 샘플링하자라는 전략입니다. 자 예를 들어서 왼쪽은 사실 일반적인 오토인코더랑 똑같아요. 그렇죠 근데 우리가 결국 생성 모델이라 하면 일단 데이터 생성은 할 줄 알아야 되는 거잖아요. 그러면 데이터 생성을 우리가 어떻게 할 수 있을까 한번 생각해 보면 여기서 우리가 이미지가 만들어지는 곳이 어딥니까? 인풋 쪽은 이미지가 들어가는 곳이에요. 그렇죠 우리가 갖고 있는 이미지를 넣어주는 곳 그러면 새로운 이미지가 만들어지는 곳은 여기 뒷단입니다. 그래야지 여기 아웃풋 이미지가 짜잔짜잔 하고 만들어지지 않습니까? 자 그럼 그 뒷단만 우리가 한번 생각하면 이렇게 되는 거예요. 뒷단만 생각하면 그렇죠 여기서 앞단 제거한 겁니다. 자 그러면은 여기서 한번 생각해 봅시다. 우리가 왼쪽과 같이 오토인코더를 학습을 시켰어요. 데이터를 다시 복원하는 형태로 오토인코더를 학습시켰습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 4,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1060,
      "char_count": 1983
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c005_8b295d",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 그래야지 여기 아웃풋 이미지가 짜잔짜잔 하고 만들어지지 않습니까? 자 그럼 그 뒷단만 우리가 한번 생각하면 이렇게 되는 거예요. 뒷단만 생각하면 그렇죠 여기서 앞단 제거한 겁니다. 자 그러면은 여기서 한번 생각해 봅시다. 우리가 왼쪽과 같이 오토인코더를 학습을 시켰어요. 데이터를 다시 복원하는 형태로 오토인코더를 학습시켰습니다. 자 그러고 나서 이 뒤에만 떼온 거예요. 뒤에만 뒤에만 떼와서 이 지 자리에 그냥 가오샤인 용 콤마 1에서 값을 그냥 랜덤하게 샘플링을 해요. 그러면 z라는 값이 이렇게 쫙 나오겠죠 그리고 우리가 이미 학습했던 여기 디코더 를 그대로 씁니다. 그러면 어떻게 될까요? 뭔가 나올 거예요 뭔가 나오는데 그게 우리가 생각했던 그럴듯한 이미지가 나올까요? 안 나올까요? 안 나오게 됩니다. 왜냐하면 어떻게 생각하면은 왼쪽을 트레이닝으로 생각하고 오른쪽을 테스트로 생각할 수가 있습니다. 자 트레이닝 할 때는 우리가 주어진 x라는 데이터에서 z로 매핑을 하고 그다음에 z에서 다시 x 프레임으로 매핑을 합니다. 그러면 그때 여기 나와 있는 z는 가우션 분포를 따를지 아닐지 그다음에 가우시안 분포를 따른다고 하더라도 얘가 가우시안 0 콤마 1을 따를지 가우시안 뭐 5콤마 10을 따를지 우리는 전혀 알 수가 없는 거예요. 왜 이 데이터에 따라서 이 지가 달라지겠죠 그러면 우리가 알 수 없는 겁니다. 즉 여기 나와 있는 지는 어떤 지인지 모르는데 즉 임의의 지에 대해서 학습이 됐는데 그리고 어쩌면 얘는 뭐 가우시안 5콤마 10에 대해서 학습된 걸 수도 있는데 갑자기 테스트에서는 가우시안 0 콤마 1에 대해서 제를 샘플링해서 다시 디코더를 시킨다라고 하면은 트레이닝에서 봤던 지랑 테스트에서 봤던 지가 굉장히 상이하게 되겠죠 그렇게 되면은 우리가 테스트에서 잘 못하게 됩니다. 그러면 이걸 어떻게 할 수 있을까 이렇게 생각을 또 할 수 있겠죠. 그러면 여기 나와 있는 지를 우리가 이러한 pg에 가깝게 만들어 주자 여기서 케엘이라는 거는 KL 다이버전스를 의미하는 것이고 여기 나와 있는 g랑 이 pg 사이가 얼마나 가까운지 먼지를 나타냅니다. 그래서 이게 작을수록 가깝다라는 뜻으로 생각하시면 됩니다. 그러면은 우리가 애초에 여기 나와 있는 지를 가우시안 0 콤마 1에 가깝게 만들어주면 되겠네라고 생각을 할 수가 있겠죠. 그렇죠 그래서 얘를 뭔가 낮추면 되겠다라고 생각할 수가 있을 겁니다. 하지만 뒤에서 우리가 보겠지만 이러한 케이엠 다이버전스라는 것은 확률 분포 사이의 다이버전스로 정의가 되는 것입니다. 여기 지는 확률 분포가 아니에요. 그래서 우리는 케일 다비런스의 성질을 이용하기 위해서는 이 지를 바꿔줘야 됩니다. 어떻게 바꿀 거냐면 이렇게 q의 지 바 x로 바꿀 겁니다. 즉 x가 주어졌을 때 지에 대한 확률 분포라고 우리가 생각할 수가 있겠죠. 즉 스가 주어졌을 때 이 지라는 확률 분포를 우리가 예를 들어서 가오션으로 모델링을 할 겁니다. 그러면 얘도 가오산 분포고 피제도 가오션 분포니까 동일한 분포에 대해서는 우리가 케일 라비던스를 계산을 할 수가 있게 됩니다. 그 까닭에 우리는 이러한 큐 지바스랑 피지를 낮추기를 원합니다. 자 여기서 q GVX는 우리가 x를 인풋으로 넣어서 나오는 가우시안 분포 포스테리어라고 생각을 할 수도 있겠죠. pg는 프라이어 우리가 설정한 프라이어입니다. 0 콤마 1로 우리가 여기서 설정을 했었습니다. 자 두 개 간에 다이버전스를 우리가 미니마이즈 시킨다라는 말은 두 개가 가까워진다라는 것을 의미합니다. 결국에는 얘네들을 우리가 낮추려면은 이 q 지바스를 우리가 가우지 분포로 모델링을 해야 됩니다. 그 까닭에 브에 같은 경우는 다음과 같이 구현이 됩니다. 결국 q g바 x가 가우시안 분포를 따르기 때문에 우리는 평균 그다음에 분산을 모델링 해야 됩니다. 그래서 이러한 x가 주어지면 우리가 웨이트 매트릭스 w를 곱해서 민을 구하고 또 웨이트 매트릭스 w를 곱해서 나오는 아웃풋을 코베리언스라고 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 5,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1072,
      "char_count": 1995
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c006_0afc96",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 자 두 개 간에 다이버전스를 우리가 미니마이즈 시킨다라는 말은 두 개가 가까워진다라는 것을 의미합니다. 결국에는 얘네들을 우리가 낮추려면은 이 q 지바스를 우리가 가우지 분포로 모델링을 해야 됩니다. 그 까닭에 브에 같은 경우는 다음과 같이 구현이 됩니다. 결국 q g바 x가 가우시안 분포를 따르기 때문에 우리는 평균 그다음에 분산을 모델링 해야 됩니다. 그래서 이러한 x가 주어지면 우리가 웨이트 매트릭스 w를 곱해서 민을 구하고 또 웨이트 매트릭스 w를 곱해서 나오는 아웃풋을 코베리언스라고 하겠습니다. 즉 뉴럴 네트워크의 아웃풋이 윙이고 뉴럴 네트워크의 아웃풋이 코베런스가 되는 겁니다. 그러면 2개가 나왔으니까 가우지 분포가 정의가 되는 거예요. 그렇죠 자 그러면 여기에서 우리가 q 지바스에서 여기에서 우리가 얘를 평균 얘를 공분산으로 하는 가오션에서 지를 샘플링 하는 겁니다. 왼쪽에 그림과 연관 지으면 지는 그때 4차원의 벡터가 되게 되는 것이죠. 그러고 나서 다시 우리가 뉴런 네트워크를 통과시켜서 디코더 즉 디코딩 과정을 거쳐서 원래의 아웃풋을 만들어 내게 됩니다. 자 여기에서 약간 이제 수식이 나오는데요. 우리가 이 케엘의 큐 지바스랑 피제를 낮추는 게 지금 한 가지 목적이에요. 왜 낮추는 게 목적이었나요? 그래야지 트레이닝 할 때 우리가 봤던 지랑 그다음에 테스트할 때 우리가 봤던 지가 비슷할 거니까 그래야지 테스트할 때 우리가 지를 여기서 샘플링 가오샤 용 콤마 1에서 샘플링해도 그럴듯한 이미지가 나오겠죠 왜 애초에 트레이닝 할 때부터 가오샤 용 콤마 1과 유사한 지에서 우리가 학습을 시켰으니까 자 그래서 이러한 정규 분포와 이런 정규 분포 사이에 케이엘 다이버전스를 미니마이즈 하는 게 목표입니다. 자 그러면 아직 근데 우리가 케일 다변성의 정의를 사실 본 적이 없어요. 자 이제부터 시작입니다. 자 이런 두 개의 가우시안 분포가 있다고 하겠습니다. 두 개의 가우시안 분포 자 여기에서 두 가우시안 분포 사이에 케이엘 다이버전스를 우리가 정의를 어떻게 할 거냐라고 하면은 이렇게 진행합니다. 자 얘를 어떻게 이해하시면 되냐면 q랑 p 사이에 레이쇼를 본다 비율을 보는 겁니다. 비율을 봐서 실제로 q랑 p가 만약에 비슷하다고 하면은 큐에서 만약에 비율이 큰 포인트는 피에서도 비율이 크게 될 겁니다. 그렇죠 즉 피랑 q가 만약에 비슷하다고 하면은 우리가 큐에서 크면 피에서도 크게 되겠죠 그렇죠 자 그러한 거를 가지고 우리가 모델링을 하게 되는 것인데요. 즉 이러한 우리가 피가 있다고 할 때 자 만약에 데이터가 이렇게 생겼습니다. 데이터가 여기 있어요. 그러면 지금 피스가 굉장히 큰 포인트입니다. 그럼 p랑 q가 만약에 비슷하다라고 하면은 q도 이런 식으로 먹는 게 생겼을 거예요. 그렇죠 그러면은 PX랑 qx의 여기 이 점에 대한 비율은 거의 비슷하겠죠 둘 다 큰 걸로 그렇죠 그러면 얘는 거의 1에 가까운 값이 되니까 로그를 씌우면 0이 되게 되는 구조가 됩니다. 그렇죠 여러분 그런 식으로 모델링이 되고 있는 것이고요. 그래서 이거는 결국 두 개가 얼마나 멀고 가까운지를 나타낸다. 그리고 얘가 작을수록 얘가 작을수록 PX랑 qx의 비율은 1에 가까워지고 로그를 씌우면 0이 되니까 점점 얘가 작아진다. 즉 KL 다변수가 작다라는 말은 약간 다른 개념이긴 하지만 거리가 작다라고 생각해도 사실상 괜찮습니다. 그래서 여기서는 얘를 낮추는 게 목표인데 이거 수식이 어떻게 생겼냐라고 하면은 이런 식으로 우리가 쓰겠습니다. 자 결국 얘가 큐에 대한 적분인 거예요. 큐에 대한 즉 로그 피분의 큐를 큐에 대해서 적분을 하는 겁니다. 그래서 우리는 얘를 이렇게 쓸 거예요. 로그 피분의 큐를 그냥 로그 빼기 로그 피로 분해해서 쓴 것뿐입니다. 왜 왼쪽을 보면은 우리가 뭐에 대한 기댓값 즉 로그 피분의 큐에 대한 기댓값을 구하는데 이 q라는 분포에 대해서 우리가 이거에 대한 기대값을 구한다고 볼 수가 있습니다. 그거에 대한 로테이션을 익스펙테이션 q라고 우리가 나타낼 겁니다. 즉 두개는 같은 말로 우리가 정의하고 가겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 6,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1095,
      "char_count": 2036
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c007_ee5afd",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 자 결국 얘가 큐에 대한 적분인 거예요. 큐에 대한 즉 로그 피분의 큐를 큐에 대해서 적분을 하는 겁니다. 그래서 우리는 얘를 이렇게 쓸 거예요. 로그 피분의 큐를 그냥 로그 빼기 로그 피로 분해해서 쓴 것뿐입니다. 왜 왼쪽을 보면은 우리가 뭐에 대한 기댓값 즉 로그 피분의 큐에 대한 기댓값을 구하는데 이 q라는 분포에 대해서 우리가 이거에 대한 기대값을 구한다고 볼 수가 있습니다. 그거에 대한 로테이션을 익스펙테이션 q라고 우리가 나타낼 겁니다. 즉 두개는 같은 말로 우리가 정의하고 가겠습니다. 머신 러닝에서 굉장히 많이 쓰이는 로테이션으로 기억해 주시면 됩니다. 자 그러고 나서 그러면은 우리가 큐가 뭔지 알고 피가 뭔지 사실 다 알잖아요 여기에 로그를 씌우면 되는 겁니다. 그러면 이렇게 쫙 하고 우리가 나올 수가 있는 거예요. 그렇죠 왜 여기 두 개의 텀 때문에 이 앞에가 나오게 되고 그다음에 여기 나와 있는 q 요거 우리가 로그 씌우게 되니까 얘가 나오게 되겠죠 그다음에 마이너스 로그 p인데 여기에 로그 피하면은 익스포넨셜 부분에 이 안쪽만 남게 되고 마이너스의 마이너스니까 더하기로 얘가 되는 겁니다. 이렇게 우리가 쓸 수 있겠죠. 자 그다음에 q에 대한 우리 기댓값을 구하는데 자 기댓값이라는 건 우리가 랜덤 베리어블에 대한 기댓값입니다. 즉 우리가 만약에 그냥 주사위를 던졌는데 항상 무조건 3이 나온다. 항상 무조건 그러면 기댓값은 그냥 3인 거예요. 계산할 것도 없는 거예요. 그렇죠 근데 뭐 3이 나올 수도 있고 6이 나올 수도 있고 뭐가 나오느냐에 따라서 우리가 그때는 기댓값을 구해야 되는 거고요. 여기서는 시그마나 뮤나 사실은 얘네들은 랜덤 베리어블이 아니고 여기 나와 있는 스가 랜덤 베리어블이다 스가 랜덤 베리어블 자 그래서 익스펙테이션을 취할 때 이 엑스가 들어있는 부분에 대해서만 익스펙테이션이 취해지는 거예요. 얘는 스가 없으니까 익스펙테이션 없이 그냥 상수로 나오는 겁니다. 왜 우리가 앞서 말씀드린 것처럼 3에 대한 익스펙테이션은 그냥 3이니까 이제부터는 이 각각 이 앞에 거랑 이 뒤에 거를 계산하는 거를 한번 살펴볼게요. 약간 수식이 복잡해요. 사실 그리고 이거에 대한 디테일까지는 사실 몰라도 vl을 구현하는 데에는 문제가 없습니다. 하지만 이러한 종류의 트릭은 사실 굉장히 다양하게 많이 쓰이기 때문에 여러분들께서 이참에 이러한 트릭을 한번 정리하고 가시면 좋겠습니다. 자 한번 보면 이 빨간색 식을 우리가 좀 간소화하고 싶은 거예요. 이 빨간색 식이 여기 그대로 써져 있습니다. 여기서 첫 번째에서 두 번째로 갈 때는 트랜스트릭이라는 게 있습니다. 즉 트랜스트릭은 이 x가 만약에 컬럼 벡터라고 할게요. 이렇게 예를 들어서 n 바이 1차원 같은 거 이제 거기에 우리가 트랜스포즈를 하니까 1 바이 n이 되는 거예요. 1 바이 n a는 n 바이n이에요. x는 다시 n 바이 1 그렇죠 x가 n 바이 1 x 트렌지 포즈는 1 바이 n 그러면 이거 다 곱하면 뭐가 돼요? 1 바이 1이에요 1 바이 1 스칼라가 나옵니다. 그렇죠 숫자 하나 그런 상황에서는 이러한 트레스트를 쓸 수 있습니다. 즉 우리가 애초에 숫자가 하나이기 때문에 숫자 하나에 대해서 트레이스트를 하면은 자기 자신 하나죠. 트레이스라는 건 대각선의 합이니까 자 그다음에 얘는 우리가 이렇게 1차원이 만들어질 때는 고의 이 순서를 또 바꿔서 이렇게 쓸 수 있다라는 트랙이 있습니다. 그렇게 하면은 x가 n 바이 1이고 xt가 1 바이 n이에요. 그럼 두 개 곱하면 n 바이n입니다. a도 n 바이n이에요. 그럼 곱하고 나면 n 바이 n이 됩니다. 근데 트레이스는 대강 원소의 합이니까 다시 숫자가 하나만 나오게 되겠죠. 즉 이 3개가 같게 만들어진다가 트랜스트릭입니다. 자 이거를 우리가 그대로 쓰게 되면 곱의 순서로 막 바꿀 수 있는 거예요. 그러면 이렇게 되겠죠 얘가 앞으로 쫙 나오게 되면서 그렇죠 이렇게 되고 그다음에 익스펙테이션하고 트레이스는 우리가 교환할 수 있다는 것이 알려져 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 7,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1067,
      "char_count": 2010
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c008_b78dc0",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 그렇게 하면은 x가 n 바이 1이고 xt가 1 바이 n이에요. 그럼 두 개 곱하면 n 바이n입니다. a도 n 바이n이에요. 그럼 곱하고 나면 n 바이 n이 됩니다. 근데 트레이스는 대강 원소의 합이니까 다시 숫자가 하나만 나오게 되겠죠. 즉 이 3개가 같게 만들어진다가 트랜스트릭입니다. 자 이거를 우리가 그대로 쓰게 되면 곱의 순서로 막 바꿀 수 있는 거예요. 그러면 이렇게 되겠죠 얘가 앞으로 쫙 나오게 되면서 그렇죠 이렇게 되고 그다음에 익스펙테이션하고 트레이스는 우리가 교환할 수 있다는 것이 알려져 있습니다. 그러면은 우리가 여기서 마찬가지로 스가 없는 부분은 사실 익스펙테이션 안 해도 되니까 시그마가 이제 나오는 거예요. 시그마가 나왔습니다. 짜잔 하고 나왔어요. 그러면 이 안에 걸 봅시다. 안에 거 안에 거는 여러분 뭐랑 똑같이 생겼네요 안에 거 이거는 여러분 여기 랜덤 베리어블에서 평균 뺀 거죠. 그죠 랜덤 베어러블에서 또 평균 뺀 거 마치 스 마이너스 뮤의 제곱이라고 생각하시면 되겠죠. 얘는 벡터니까 트랜스포즈로 되어 있는 것뿐이지 벡터가 아니었으면 그냥 제곱이랑 똑같은 겁니다. 즉 스 마이너스 뮤의 제곱에 대한 익스펙테이션은 우리가 베리언스 또는 코비언스라고 여러분들께서 이미 배우신 적이 있습니다. 그러면은 어떻게 됩니까? 두 개 곱하면 아이덴티티 매트릭스고 디바이디 매트릭스니까 d1을 디게 더한 거죠. 그래서 2분의 1 d가 되게 됩니다. 뒤에 것도 우리가 비슷하게 유도할 수 있어요. 뒤에 것도 트랜스트링을 쓰면은 이렇게 엑스 마이너스 뮤끼리 딱 모을 수가 있는 겁니다. 그러면 뭐가 좋다 스 마이너스 뮤끼리만 모을 수 있으면 이 코베이런스를 밖으로 짜잔하고 나올 수가 있게 되는 것이죠. 그쵸? 그게 큰 장점이고 여기서 한번 보면은 여기서는 여기서 한번 보시면은 x 마이너스 뮤p의 제곱인데 q에 대한 익스펙테이션이에요. 앞에서는 x 마이너스 뮤q에 대해서 우리가 제곱을 하고 q에 대한 익스펙테이션이니까 코베런스가 바로 나왔는데 얘는 q고 얘는 p입니다. 그래서 바로는 안 돼요. 그래서 얘를 우리가 다시 쪼갭니다. x 제곱 그다음에 x 마이너스 x MP 그렇죠 p x 그래서 2가 있는 거고 MP 제곱 이렇게 자 그렇게 되면 어떻게 됩니까? 얘는 여러분 이거는 어떻게 되나요? 제곱의 평균은 제곱의 평균 빼기 평균의 제곱은 코베런스다라는 시기 여러분 알고 있을 겁니다. 그래서 제곱의 평균은 코베런스 더하기 평균의 제곱으로 얘가 이렇게 되는 거죠. 나머지는 우리가 그대로 쓰면 됩니다. 그리고 얘를 쭉 나열하면은 결국 이런 형태의 비교적 단순한 형태로 바뀌게 되고 정리하면은 그래서 이 두 개를 정리하면 최종적으로 이식이 나옵니다. 그러면 이제 우리는 케일 다변수를 쉽게 계산할 수 있는 거예요. 왜 이 PX에 대한 뮤p 시그마 p 만약에 알고 있고 qx에 대한 시그마 q 뮤q를 알고 있으면 그냥 두 개에 대한 다이버전스를 각각에 넣어서 계산만 하면 끝인 거니까요. 이렇게 하면 케일 변수를 쉽게 계산할 수가 있게 됩니다. 다시 그러면 VA로 넘어오겠습니다. 다시 VA로 넘어오면 VA는 목적식이 2개가 있어요. 하나는 앞서 말씀드린 것처럼 q 지바스가 피지랑 가까워지도록 하는 거 자 여기서 큐지바스랑 피지 사이의 거리를 우리가 줄이면 되는데 이거는 어떻게 계산하는지는 우리가 앞에서 배웠습니다. 두 개가 가우시안 분포를 따를 때 그때 두 가우시안 분포 사이의 케 다변수는 이렇게 우리가 계산할 수 있음을 살펴봤습니다. 얘가 우리가 다뤄야 되는 또 목적식 중에 하나고요. 두 번째 목적식은 뭐가 있을까요? 리컨스트럭션 로스입니다. 얘는 오토 인코더랑 동일하죠. 우리한테 주어진 인풋이 들어오면 그에 맞는 아웃풋이 나오도록 하는 게 우리의 목적입니다. 그렇죠 그래서 이러한 리컨스트렉션 로스까지 두 개가 붙게 되는 겁니다. 그래서 한번 보시면 뒤에 있는 목적식이 이렇게 생겼습니다. 즉 뒤에 있는 파트는 우리가 q 지바스에서 피지 사이의 거리를 낮추는 거로 우리가 생각할 수 있고요. 얘는 로스 함수니까 얘를 낮추는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 8,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1085,
      "char_count": 2038
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c009_a02603",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 얘가 우리가 다뤄야 되는 또 목적식 중에 하나고요. 두 번째 목적식은 뭐가 있을까요? 리컨스트럭션 로스입니다. 얘는 오토 인코더랑 동일하죠. 우리한테 주어진 인풋이 들어오면 그에 맞는 아웃풋이 나오도록 하는 게 우리의 목적입니다. 그렇죠 그래서 이러한 리컨스트렉션 로스까지 두 개가 붙게 되는 겁니다. 그래서 한번 보시면 뒤에 있는 목적식이 이렇게 생겼습니다. 즉 뒤에 있는 파트는 우리가 q 지바스에서 피지 사이의 거리를 낮추는 거로 우리가 생각할 수 있고요. 얘는 로스 함수니까 얘를 낮추는 겁니다. 그러면 q 지바스가 pg랑 가까워지게 된다는 겁니다. 그래서 파이라는 말이 붙은 이유는 여기서 이 큐 지바스를 만들 때 여기 파라미터들이 있을 거잖아요. 뉴럴 네트워크 파라미터들 블랑 비 그런 것들을 그냥 우리가 파이라고 나타내는 것뿐입니다. 파이가 학습해야 되는 파라미터가 되는 거겠죠. 자 그다음에 이 앞단은 리컨트럭션이 됩니다. 즉 복원하는 게 되는 거예요. 무슨 말이냐면 지를 우리가 샘플링을 합니다. 지를 우리가 샘플링해요. 여기서 지를 샘플링하고 그때 다시 디코더에 넣었을 때 지를 우리가 샘플링하고 지를 다시 이 디코더에다가 넣었을 때 디코더를 여기서 피세타라고 한 겁니다. 세타는 이 디코더에 대한 파라미터들 뉴럴 네트워크 파라미터라고 생각하시면 되겠죠 그때 이 뉴럴 네트워크를 통과시켰을 때 원래 있던 x가 잘 나오도록 하는 겁니다. 즉 얘를 미니마이즈 한다라는 말은 이 마이너스가 있으니까 얘를 맥시마이즈 한다는 말이잖아요. 즉 지를 샘플링하고 지를 통해서 원래 있던 스가 나올 확률이 높도록 원래 있던 스를 잘 복원하도록 학습이 된다는 겁니다. 자 여기서 우리가 실제로 구현물을 보게 되면은 파이썬 코드의 구현물은 또 굉장히 간단하게 구현되어 있는 걸 여러분들께서 보실 수가 있습니다. 자 실제로는 이 지가 이 엑스를 통해서 이런 피세터처럼 뭔가 모델링 되는 것처럼 보이는데 실제로 구현물을 보면은 결국 엠세 로스로 많이 구현이 돼요. 자 왜 그런지 한번 살펴보면은 우리가 컨티너스 케이스 즉 x와 같은 이런 데이터가 만약에 연속적인 값이다 0에서 1 사이에 뭐 실수 값이다라고 하면은 이 p 세타의 x바 g를 우리가 가우s으로 많이 모델링 합니다. 이런 형태로 자 이런 형태로 모델링해요. 자 그러면 어떻게 돼요? 여기에 우리가 로그를 씌우면 이 앞에 거는 우리가 만약에 무시한다고 치면 로그랑 익스포넨셜이니까 두 개가 없어지겠죠 그렇죠 그다음에 만약에 여기서 코비런스를 아이덴티티로 가정하면 어떻게 돼요? 스 마이너스 뮤의 그냥 제곱이 되는 겁니다. 왜 마이너스 마이너스 앞에 있으니까 그러면 스 마이너스 뮤의 제곱이니까 그때는 엠스의 로스랑 같아지게 되는 것이죠. 그래서 VA가 MSA 로스로 구현이 많이 되고 있는 겁니다. 근데 여기 있는 x가 만약에 파이널이다. 즉 흑백 이미지라서 x가 0 또는 1의 값만 가진다. 그때 우리가 베르누 분포로 다음과 같이 모델링을 하게 되겠죠. 그래서 여기서 근데 또 우리가 실제로 구현하는 걸 보게 되면은 뭔가 제를 우리가 샘플링하고 그거에 대한 기댓값을 구하는 것처럼 구현식이 적혀 있죠. 그래서 정확하게는 이걸 우리가 다 구현해야 되지만 이 또 기댓값을 우리가 구하려면 또 인티그럴이 들어가게 되고 복잡해집니다. 그래서 여기서 우리가 실제로 구현할 때는 제를 우리가 쓰게 샘플링을 해서 그러면 z를 만약에 뭐 100개 샘플링 해요. 그러면 첫 번째 제를 넣어서 나오는 x 값 리컨스트럭션 로스 또 두 번째 제트 넣어서 나오는 스에다가 리컨트럭션 로스 그렇게 리컨스트럭션 로스가 총 3개가 나오게 되겠죠 그렇죠 그거를 우리가 평균 차서 계산을 합니다. 얘를 우리가 몬테카를로 에스티메이션이라고 부릅니다. 즉 이렇게 익스펙테이션 값을 우리가 정확하게 인티그랄로 구하기 어려울 때 샘플링을 우리가 측정한 횟수만큼 해서 대신 대체하겠다라는 뜻입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 9,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1056,
      "char_count": 1953
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c010_e86353",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 그래서 여기서 우리가 실제로 구현할 때는 제를 우리가 쓰게 샘플링을 해서 그러면 z를 만약에 뭐 100개 샘플링 해요. 그러면 첫 번째 제를 넣어서 나오는 x 값 리컨스트럭션 로스 또 두 번째 제트 넣어서 나오는 스에다가 리컨트럭션 로스 그렇게 리컨스트럭션 로스가 총 3개가 나오게 되겠죠 그렇죠 그거를 우리가 평균 차서 계산을 합니다. 얘를 우리가 몬테카를로 에스티메이션이라고 부릅니다. 즉 이렇게 익스펙테이션 값을 우리가 정확하게 인티그랄로 구하기 어려울 때 샘플링을 우리가 측정한 횟수만큼 해서 대신 대체하겠다라는 뜻입니다. 그래서 결국 우리가 한번 생각하게 되면은 인코더 파트에서 뭐가 나와요? 인코더 파트에서 우리가 엑스를 통해서 뮤가 나오게 되고 시그마가 나오게 돼요. 그렇죠 자 그러면은 이 뮤랑 시그마가 나오게 되면은 다음과 같이 우리가 이러한 과정을 거쳐서 지라는 거를 하나 딱 샘플링을 하게 됩니다. 왜 여기서 우리가 지라는 거를 바로 만약에 샘플링을 바로 다이렉트하게 여기서 샘플링을 하게 되면은 6 콤마 시그마에서 샘플링을 하게 되면은 그땐 어떻게 된다 여기서 그라디언트를 타고 이렇게 막 흘러야지 여기 있는 파라미터들이 업데이트가 되겠죠. 근데 여기 샘플링 과정이 있으면 여기에서 그라디언트가 더 이상 안 흘러요. 끊깁니다. 그러면 이걸 우리가 방지하기 위해서 그래서 이렇게 가는 거예요. 여기서 엡실론만 가오샤 용 콤마 이래서 샘플링이 들어갑니다. 앱실론만 근데 여기서 엡실론은 어차피 우리가 학습해야 되는 게 아니에요. 여기 학습해야 되는 게 없습니다. 그렇죠 우리는 이때는 이 안에 거를 학습해야 되니까 제를 가우시 뉴 콤마 코베렌스에서 샘플링을 하게 되면은 이 가오 안에 있는 것들을 우리가 학습을 못해서 문제가 됐습니다. 근데 지금 한번 보시면 이때는 샘플링 하는 이 가우시안 분포 안에 러너블 파라미터가 없어요. 그래서 우리가 문제없이 샘플링을 할 수가 있게 됩니다. 그래서 우리가 실제로는 샘플링을 이런 과정을 거쳐서 샘플링을 하게 되고 지를 샘플링하고 그 지을 디코더에 딱 넣어줘 가지고 우리가 다시 원래 이미지가 복원되는지를 보게 됩니다. 네 그래서 이런 식으로 구현이 되어 있고요. 여기서 한 가지 문제를 하나 내겠습니다. 여기서 가우시안 분포라고 했습니다. 분명히 신은 가우시안 분포를 따른다. 그러한 가우시안 분포의 코베리언스를 요거라고 했어요. 그렇죠 자 얘를 우리가 어떻게 모델링 할까라는 거예요 자 일단은 우리가 코베리언스로 바로 가면 어려우니까 얘를 우리가 베리언스라고 그냥 가정합시다. 베리언스 시그마 시고 자 그러면은 이 코베렌스든 베리언스든 얘는 뭐예요? 뉴럴 네트워크의 아웃풋으로 우리가 얘를 모델링하기로 했잖아요. 그렇죠 그러면 시그마 제곱은 항상 무슨 값이어야 돼요 베런스는 항상 양수여야 됩니다. 근데 뉴럴 네트워크의 아웃풋이 항상 양수가 보장이 되나요 안 되나요? 항상 양수가 된다는 보장은 없습니다. 그쵸 뉴럴 네트워크의 아웃풋은 음수일 수도 있고 양수 일일 수도 있는 거예요. 그러면 그거를 우리가 베리언스로 쓰기에는 무리가 있을 수 있다는 겁니다. 그래서 이걸 어떻게 해결할까요? 해결하는 방법은 랠루를 쓰는 걸로 생각할 수도 있지만 렐루는 실제로 잘 안 씁니다. 왜 랠루를 쓰게 되면은 렐루 쓰면 0도 나오잖아요 그쵸 근데 배런스가 0인 경우는 잘 없으니까 그렇죠 대신에 이런 소프트 플러스와 같은 항상 양수가 보장되는 액티베이션 함수를 쓰게 됩니다. 그러면은 우리가 여기서 한번 생각해 보시면은 그 스라는 게 만약에 뭐 100차원이에요. 그러면 여기 100 곱하기 10 매트릭스를 곱해서 6가 10차원으로 나왔습니다. 자 코베레스도 그러면 우리가 100 곱하기 10 매트릭스를 곱해서 10차원이 나왔어요. 그리고 소프트 플러스와 같은 액티베이션 함수를 하게 되면 얘는 항상 양수인 10차원이 나오는 겁니다. 근데 코베레스는 뭐예요? 여러분 매트릭스가 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 10,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1066,
      "char_count": 1964
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c011_caaadd",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 그러면은 우리가 여기서 한번 생각해 보시면은 그 스라는 게 만약에 뭐 100차원이에요. 그러면 여기 100 곱하기 10 매트릭스를 곱해서 6가 10차원으로 나왔습니다. 자 코베레스도 그러면 우리가 100 곱하기 10 매트릭스를 곱해서 10차원이 나왔어요. 그리고 소프트 플러스와 같은 액티베이션 함수를 하게 되면 얘는 항상 양수인 10차원이 나오는 겁니다. 근데 코베레스는 뭐예요? 여러분 매트릭스가 됩니다. 코비런스 매트릭스예요. 즉 10차원의 값을 모델링 하려면 미는 10차원이고 코베리언스는 10바이 10이에요. 그래서 얘를 다이고놀로 하는 얘를 다이거놀로 하는 나머지는 다 0인 우리가 코비런스 매트릭스를 만들어서 보통 지를 많이 모델링을 하게 됩니다. 이런 식으로 아시겠죠? 그래서 이렇게 소프트 플러스를 많이 쓰게 되고요. 아니면 애초에 여기서 나온 값을 시그마가 아니라 그냥 로그 시그마라고 가정합니다. 로그 시그마는 음수일 수도 있고 양수일 수도 있는 거잖아요. 그래서 스에다가 w를 곱한 거를 그냥 로그 시그마처럼 생각하면 시그마는 또 항상 양수가 나오게 보장이 되는 것이죠. 그렇게 되는 것이고 만약에 여기서 이거는 심화라서 저희가 그냥 말씀만 드리고 넘어가겠습니다. 바로 깊게 안 하고 만약에 선생님 저는 여기서 다이고널 코베런스 말고 풀 코베런스 즉 이런 값들이 0이 아니라 다 풀로 차 있는 것 변수들 사이의 관계성을 다 고려할 수 있는 코베런스를 만들려면 어떻게 해야 되나요 라고 할 때는 촐레스키 분열을 해가지고 이 촐레스키 el이라는 로 트라잉글러 매트릭스를 모델링 하게 됩니다. 이게 뉴라이언 토크의 아웃풋으로 l이 나오도록 자 이거는 굉장히 심화된 내용이기 때문에 저희가 넘어가도록 하겠습니다. 자 그러면은 이제 우리가 준비는 끝났습니다. 한번 보시면 브에는 목적식이 다음과 같이 나온다고 봤죠. 즉 첫 번째는 우리가 인코더를 통해서 지를 샘플링하고 지를 디코더에 넣어서 원래 데이터가 얼마나 잘 복원되는지를 나타내는 로스 하나 그다음에 우리의 그 포스테리어 q 지바스가 프라이어 피지랑 여기서 피지는 우리가 보통 0 콤마 1로 많이 모델링 한다고 했습니다. 두 개 사이에 다이버전스가 얼마나 낮은지를 나타내는 겁니다. 자 여기서 그 뒤에 있는 케일 라비던스는 앞에서 우리가 유도를 했어요. 이렇게 생긴 애다라고 유도했습니다. 자 얘는 실제로 여러분들께서 이거를 구현을 해도 되고요 아니면은 사실 파이 터치에는 이미 이러한 케일 자브넌스를 모델링 하는 게 이미 구현이 되어 있습니다. 그래서 이거를 그대로 여러분들께서 쓰셔도 괜찮습니다. 여기서 한 가지 주의할 점은 이 앞에서는 우리가 이 익스펙테이션이 여기에 대해서 들어가면서 그렇죠 딱 여기까지 익스펙테이션이 들어가면서 결국 인티그럴을 해야 되는 게 존재하고 있고 그래서 이 지를 우리가 샘플링해서 몬테카롤로 에스티메이션을 통해서 우리가 이렇게 구현했다는 것을 봤습니다. 그렇죠 그래서 g를 샘플링해서 우리가 그냥 여기 나와 있는 디코더에 넣어서 계산을 했습니다. 근데 케일 다비스는 g를 샘플링 하지 않아요 왜 k자 변수는 이렇게 정확하게 계산할 수 있다고 우리가 봤으니까 애초에 샘플링 할 필요가 없는 거예요. 정확한 분포들 간의 다이전스를 우리가 계산하게 됩니다. 자 그래서 목적식은 이렇게 생겼고 결국 테스트할 때는 즉 이 학습이 끝나고 나서 이미지를 생성하거나 우리가 뭔가를 하고 싶을 때는 이제 가우시안 용 콤마일에서 값을 샘플링해서 디코더에다가 넣는 겁니다. 그러면 원래 있을 법한 그럴듯한 이미지가 나오게 된다는 겁니다. 즉 우리가 지금 브이를 이런 식으로 나타내면은 스가 들어오면은 q파일 지바 스를 만들기 위해서 뮤랑 코베런스 만들고 거기서 우리가 q파이지바스를 만듭니다. 그럼 q파이 지바스에서 우리가 그거랑 pz 사이의 거리를 낮추게 되고 큐파이 지바스에서 지를 샘플링해서 그 z를 디코더에 넣어 가지고 우리가 원래 이미지를 복원합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 11,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 1067,
      "char_count": 1970
    },
    {
      "id": "transcript_ml_for_recsys_ml_for_recsys_2강_생성모델_c012_2bae4c",
      "content": "[ML for RecSys] [ML for RecSys] 2강 생성모델\n\n다. 그러면 원래 있을 법한 그럴듯한 이미지가 나오게 된다는 겁니다. 즉 우리가 지금 브이를 이런 식으로 나타내면은 스가 들어오면은 q파일 지바 스를 만들기 위해서 뮤랑 코베런스 만들고 거기서 우리가 q파이지바스를 만듭니다. 그럼 q파이 지바스에서 우리가 그거랑 pz 사이의 거리를 낮추게 되고 큐파이 지바스에서 지를 샘플링해서 그 z를 디코더에 넣어 가지고 우리가 원래 이미지를 복원합니다. 근데 우리가 테스트할 때는 새로운 이미지를 만들어야겠죠 원래 있던 게 아니라 그래서 0 콤마일에서 가우샷에서 랜덤하게 샘플링해서 디코더에 넣어주면은 원래 있을 법한 굉장히 좋은 이미지가 만들어진다 까지 왔습니다. 자 그거를 또 다르게 표현하면 이렇게도 표현할 수가 있겠죠. 자 우리가 이런 x가 있고 인코더가 있으면 인코더에서 뮤랑 코비런스가 존재하고 뮤랑 코비런스 나오면 그걸로 우리가 케일 다변스 계산하기로 했습니다. 그리고 그러한 유랑 코비렌스에서 지를 샘플링해서 지를 디코더에 넣고 우리가 복원을 잘하도록 목적식을 만들었습니다. 여기서 엠스세가 나오기 전 인스퀘어 에러가 나오게 된 이유는 가우시안으로 우리가 아웃풋을 모델링 하게 되면은 로그 가우시안 하면은 가우시안 피디프에서 익스포넨셜 안에 있는 텀만 딱 남게 되니까 그게 이러한 엠스의 형태라서 이렇습니다. 자 근데 실제로는 우리가 구현할 때는 또 여기서 샘플링 하면은 딱 그라젠드가 끊긴다고 했잖아요. 즉 이 가우시안 안에 있는 거를 우리는 학습시켜야 되는 건데 얘가 학습이 안 되는 겁니다. 왜 UX랑 코밸런스 x가 지금 인코더를 나타내는 여기 이거에 대한 함수잖아요. 인코더에 대한 파라미터를 기반으로 뮤랑 코비런스가 만들어진 겁니다. 그러면 우리는 얘를 토대로 그라이던트를 타고 내려가서 인코더를 학습시켜야 되는 건데 샘플링이 되는 순간 끊긴다라고 말씀드렸습니다. 자 그래서 우리가 이거를 어떻게 했습니까? 뮤랑 코베런스는 놔두고 앱실론 0 콤마 1 0 콤마 1에서 앱실론을 우리가 샘플링한 다음에 그다음에 위에다가 이 코베런스의 2분의 1승 곱하기 앱실론을 해서 우리가 지를 모델링했습니다. 그러면 샘플링 하니까 여기로는 끊겨요. 여기로는 이제 그라인드가 못 갑니다. 근데 상관없어요. 어차피 여기에는 우리가 학습해야 되는 게 없습니다. 그리고 여기는 지금 샘플링이 안 일어났죠 그쵸? 샘플링은 필론에서만 일어났으니까 그라이던트가 쭉 타고 잘 흐르게 됩니다. 얘가 리퍼라멘트레젠션 트릭이라고 우리가 부릅니다. 자 이러한 VA 기반으로 추천 시스템 연구가 활발하게 또 이루어지고 있습니다. 오토 인코더 기반의 추천 시스템 우리가 한번 봤었죠. 레이팅 매트릭스를 인풋으로 넣어 가지고 우리가 레이팅 매트릭스를 다시 복원하는 형태로 우리가 추천 시스템을 활용할 수가 있었습니다. VA도 마찬가지입니다. VA도 우리가 나타낸다고 하면은 이러한 레이팅 매트릭스가 있다고 할 때 그러한 레이팅 매트릭스를 통해서 우리가 인코더를 거치고 다시 디코더를 거쳐서 원래 있었을 법한 좋은 레이팅 매트릭스를 우리가 다시 복원을 할 수 있게 됩니다. 그러면은 관측됐던 레이팅은 그대로 잘 복원하고 관측이 안 됐던 우리가 모르던 레이팅은 굉장히 전체적인 맥락 상에서 자연스러운 값으로 모델링이 완료돼서 우리가 추천 시스템을 활용을 할 수가 있게 됩니다. 네 여기까지 해서 이번 주는 저희가 생성 모델이 무엇이고 그다음에 그러한 생성 모델을 표현하기 위한 그래피컬 모델 그다음에 제너티 프로세스가 무엇인지 그리고 생성 모델의 대표적인 방법론 중 하나인 베레이션 오토인 코더에 대해서 한번 살펴봤습니다. 네 여러분 고생 많았습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[ML for RecSys] 2강 생성모델.json",
        "lecture_name": "[ML for RecSys] 2강 생성모델",
        "course": "ML for RecSys",
        "lecture_num": "2강",
        "lecture_title": "생성모델",
        "chunk_idx": 12,
        "total_chunks": 13,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:1ba9eadbd1df6588d6253f8330fc334d962347a6eed07eb42665feb300b395f5"
      },
      "token_estimate": 993,
      "char_count": 1814
    }
  ]
}