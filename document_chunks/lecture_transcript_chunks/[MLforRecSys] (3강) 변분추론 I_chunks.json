{
  "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
  "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
  "course": "MLforRecSys",
  "total_chunks": 12,
  "chunks": [
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c000_a570a6",
      "content": "[강의 녹취록] 과목: MLforRecSys | 강의: 3강 | 제목: 변분추론 I\n\n네 안녕하세요 여러분 이번에는 저희 변분 추론 베레션 인퍼런스라는 것을 한번 배워보도록 하겠습니다. 저희가 생성 모델을 기반으로 추천 시스템 연구가 또 많이 활발하게 진행이 되고 있는데요. 즉 생성 모델을 이용해서 이미지나 텍스트뿐만 아니라 우리가 추천 시스템 또는 테이블 데이터 분석 또한 굉장히 많이 활용되고 있습니다. 그러한 추천 시스템 또는 테이블 데이터에 활용되는 생성 모델을 이해하기 위해서는 그러한 생성 모델을 학습시킬 수 있는 방법론에 대해서 저희가 학습을 해야 됩니다. 그 까닭에 그 생성 모델 학습을 시키는 가장 또 대표적인 방법론 중의 하나인 변분 추론 또는 베레이션 인퍼런스에 대해서 저희가 살펴보도록 하겠습니다. 그리고 그것에 보다 더 간략한 심플화된 버전인 윈필드 베리션 인퍼런스에 대해서 보도록 하겠습니다. 네 그러면 변분 추론의 개념 및 유도 과정에 대해서 저희가 보겠습니다. 자 변분 추론이 일단 필요한 이유를 다시 한번 보겠습니다. 저희가 생성 모델 쪽에서 한번 살펴보면은 스에서 와이로 가는 이 관계성을 학습하는 게 슈퍼바이스 러닝 그다음에 이 언슈퍼바이스 러닝은 이제 우리가 클러스터링과 같은 이 스에 내재된 패턴을 우리가 보는 것 지노트 모델은 우리가 결국 이런 x 콤마 y 이미지랑 레이블이 같이 있거나 이런 이미지가 있을 때 그러한 것에 대한 조인트 또는 이러한 로그 마지널 라이클루드를 우리가 구하는 게 목표입니다. 즉 쉽게 말해서 이 스가 어떤 분포에서 샘플링이 있을까 이 스코마 y가 어떤 분포에 샘플링이 있을까 그러한 분포를 우리가 추정할 수 있을까에 대한 얘기를 지금 하고 있는 겁니다. 결국 우리는 모델이라는 것을 잘 만들어서 얘랑 근사할 수 있도록 만들고 싶다입니다. 자 그러면 우리가 변분 추론을 한번 한다고 하면은 어떤 과정으로 생성 모델이 학습이 되냐 같이 보도록 하겠습니다. 여기서 p세타라고 되어 있는 거는 모델이라고 생각하시면 됩니다. 피세타는 뉴런의 토크 기반의 모델일 수도 있고 어떤 거든 될 수 있어요. 아니면은 가우시안 믹스처 모델이 될 수도 있습니다. 여기서 세타라고 하는 것이 모델의 파라미터입니다. 그다음에 피가 여기서는 이제 모델을 의미하는 것이고요. 그럼 피세타가 이제 세타라는 파라미터를 가진 모델이 됩니다. 그리고 우리한테 주어진 데이터는 이렇게 n개가 있는 겁니다. 엔게 거기서 우리가 지금 하고 싶은 게 뭐라고 했습니까? 여기서 검은색 점들만 관측된 건데 이 검은색 점들이 어떤 분포에서 나왔는지 이 회색 분포를 추론하고 싶은 게 우리의 목표예요. 그러한 회색 분포를 우리가 어떻게 추론할 것인가 그러면은 이러한 데이터를 이런 검은색 데이터를 가장 잘 만들 법한 가장 그럴듯하게 만들 법한 분포를 찾고자 하는 겁니다. 그래서 로그 라이클루드를 맥시마이즈 시킬 수 있는 세터를 찾고자 합니다. 즉 이러한 데이터를 가장 잘 생성했을 법한 데이터의 분포를 찾는다는 것이죠. 만약에 쉽게 말해서 여러분 여기서 우리가 가우시안 분포를 이렇게 가우시안 분포로 가져간다고 할게요. 가오시 그러면 그때 세타는 뭐가 되나요? 세타는 세타는 미하고 코베런스가 됩니다. 아시겠죠? 얘가 세타입니다. 얘가 세타 그다음에 얘가 우리가 피세타라고 생각하시면 됩니다. 얘가 우리가 모델이에요. 자 그러면은 이러한 데이터들을 가장 잘 생성할 법한 세타가 뭔가요? 얘를 가장 잘 생성할 법한 세터는 이런 걸로 생각할 수 있겠죠 얘도 포함하고 얘도 포함해야 되니까 그러면은 그때는 평균이 한 이 정도 되겠네요. 이 그렇죠 얘는 한 이 정도 되고 얘는 뭐 예를 들어서 또 한 이 정도 된다고 하겠습니다. 이런 식의 우리가 세타를 추정하는 게 목표입니다. 지금 있는 데이터를 가장 잘 설명하는 세터를 찾는 게 우리의 목표다라고 생각하시면 되겠죠. 그러면은 다시 돌아와서 우리가 그러면은 이러한 로그 라이필드는 이렇게 정할 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 0,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1061,
      "char_count": 1943
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c001_53583a",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 이런 식의 우리가 세타를 추정하는 게 목표입니다. 지금 있는 데이터를 가장 잘 설명하는 세터를 찾는 게 우리의 목표다라고 생각하시면 되겠죠. 그러면은 다시 돌아와서 우리가 그러면은 이러한 로그 라이필드는 이렇게 정할 수가 있습니다. 즉 p세타의 XI라는 거는 아이 번째 데이터를 가장 잘 설명하는 거 그리고 그걸 모든 데이터에 대해서 근데 얘는 어떻게 생각하면은 제를 우리가 마시라이즈 아웃 즉 인티그럴 한 걸로 생각할 수도 있습니다. 그렇죠 자 이런 게 들어가는 이유는 뭐예요? 우리가 당장 가오샤 믹스처 모델만 쓰더라도 가오샤 분포가 이렇게 2개 있다고 하겠습니다. 이렇게 그러면은 이러한 데이터가 제가 여기서 뭐 예를 들어 0 번째 클래스에서 왔는지 첫 번째 클래스에서 왔는지 이 데이터가 생성된 게 그런 거를 이제 우리가 모델링을 하기 시작해야 된다는 거예요. 그렇죠 그러면은 제라는 거는 어떤 클래스에서 왔는지를 일단 결정하는 요소 예시입니다. 예시 제는 우리가 x를 제외한 나머지 모든 걸 여기서 제라고 할 거예요. 관측된 스를 제외한 나머지 모든 걸 제라고 할 건데 그중에 한 가지가 여기서는 이러한 어떤 클러스터에서의 데이터가 만들어졌는지 그러한 클러스터 어사인먼트를 제라고 하겠습니다. 그러면은 이 z가 0일 수도 있고 1일 수도 있고 그렇죠 그다음에 0이면 그에 맞는 또 클러스터에서 가오션에서 이 데이터가 샘플링 되겠죠. 그러한 과정을 나타내는 그걸 하기 위해서 우리가 제라는 것을 도입하게 됩니다. 자 그런데 얘가 도입되는 순간 보시다시피 적분이 들어가요. 복잡해진다는 겁니다. 적분이 들어가니까 그쵸 그 까닭에 우리가 이러한 적분을 정확하게 계산할 수 없는 상황을 인트랙터블이라고 합니다. 트랙터블이라는 건 우리가 정확하게 계산할 수 있는 거 인트랙터블 우리가 정확하게 계산할 수 없는 것을 의미합니다. 이런 적분 형태는 여기서 얘가 어떤 분포에 따르냐에 따라 다르지만 대부분의 경우에는 인트랙터블 해요. 그 과정에 우리는 다른 접근 방법이 필요합니다. 자 여기서 우리가 샘플링을 해서 인티그라를 정확하게 할 게 아니라 우리가 z를 어딘가에서 뽑아오는 거예요. z에 대해서 우리가 적분을 하는 거니까 그렇죠 그러면 이 제트를 특정 범위 안에서 모든 제트에 대해서 우리가 이 적분을 취해야 되는데 그게 우리가 쉽지 않으니까 정확하게 계산하기 어려우니까 또는 인트랙터블 하니까 제를 어딘가에서 그냥 랜덤하게 하나 샘플링 해 오자는 거예요. 그다음에 그 제트를 넣어서 계산하자 그러면 제를 한 번 샘플링해서 계산하고 제도 샘플링해서 한 번 계산하고 그걸 여러 번 해서 기댓값을 취하면 평균 취하면 우리는 된다라는 얘기를 지금 하고 있는 겁니다. 어프록시메이션 인 거죠. 이런 형태를 우리가 몬테카를로 어포시메이션이라고 뒤에서 부를 겁니다. 얘를 우리가 실제로 샘플링해서 진행하는 거예요. 아시겠죠? 즉 지금 여기 나와 있는 거는 그러면 한번 보시면 이 제트를 어디서 지금 샘플링을 합니까? 포스테리어에서 샘플링을 해요. 포스테리어에서 아시겠죠? 결국 포스테리어에서 우리가 샘플링을 하는데 이 포스테리에서 우리가 샘플링을 해야 되는 이유가 뭘까 제를 그냥 임의의 가오샹 같은 데서 샘플링 하는 게 아니라 왜 포스테리어에서 샘플링을 해야 될까 그다음에 포스테리어는 우리가 어떻게 구할 수 있을까 그거를 이제부터 한번 살펴보도록 하겠습니다. 그러한 개념을 저희가 이해하기 위해서 먼저 좀 선행학습 또는 이제 이미 많은 분들이 배웠을 수도 있는 컴백스 펑션과 젠센 이니퀄리티 등을 한번 보고 넘어가도록 하겠습니다. 컴백스 펑션이라는 것은 이러한 조건을 만족하는 함수를 의미합니다. 즉 우리가 쉽게 말해서 스랑 이렇게 생긴 함수를 우리가 컴백스 함수라고 부르는데요. 컴백스 함수의 한 가지 예시입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 1,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1035,
      "char_count": 1895
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c002_fcc30c",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 그러한 개념을 저희가 이해하기 위해서 먼저 좀 선행학습 또는 이제 이미 많은 분들이 배웠을 수도 있는 컴백스 펑션과 젠센 이니퀄리티 등을 한번 보고 넘어가도록 하겠습니다. 컴백스 펑션이라는 것은 이러한 조건을 만족하는 함수를 의미합니다. 즉 우리가 쉽게 말해서 스랑 이렇게 생긴 함수를 우리가 컴백스 함수라고 부르는데요. 컴백스 함수의 한 가지 예시입니다. 자 얘는 여기 엑스라는 점하고 이 와라는 점의 가운데 점은 여기죠. 여기서 우리가 y를 구한 것과 그렇죠 이렇게 위로 올린 것과 그게 뭡니까? 이거라고 볼 수 있겠죠. 이 두 개의 포인트를 우리가 뭔가 인터폴레이션 해서 그거에 내분점을 하나 고르고 그 내분점에 대응되는 f 값을 찾는다인 거죠. 그게 왼쪽에 있는 거고 오른쪽은 프 값끼리 더하는 겁니다. 평균 내는 거예요. 여기 나오는 프 값끼리 얘랑 얘를 평균 내니까 뭐 이쯤 오겠죠 그쵸 그러면은 굉장히 우리가 얘랑 얘랑 평균 내면 여기가 올 겁니다. 얘가 굉장히 크다고 볼 수 있죠. 그쵸? 그러면 얘랑 얘랑 이제 평균 내서 그때 에프를 올리면 여긴데 f로 각각 올리고 나서 우리가 내분을 하면은 더욱 큰 값에 존재한다. 크거나 같다 그걸 우리가 컴백스 펑션이라고 부르고 예를 들어 이런 게 있습니다. cnca는 반대예요. 마이너스를 붙였을 때 컴백스면 얘는 컨케이블입니다. 아시겠죠? 자 쉽게 말해서 마이너스 엑스 제곱은 컨케이블입니다. 왜 마이너스 엑스 제곱에 마이너스를 붙이면 그냥 엑스 제곱이죠. 걘 컴백스니까 이렇게 생긴 거 컨케이브라고 하고 중요한 거 금지하게 적어놨죠. 로그 엑스가 컨케이브의 대표적인 함수입니다. 그럼 뭐다 마이너스 로그는 뭘까요? 마이너스 로그는 컴백스가 된다 아시겠죠? 그러고 나서 뭐 여기서 우리가 뭐 여러분들 이미 아시겠지만은 뭐 두 번 미분했을 때 만약에 그게 0이거나 음수면 두 번 미분했을 때 음수라는 건 이렇게 생겼다는 거잖아요. 얘가 아니라 그렇죠 이렇게 생겼으면 두번 미분이었을 때 음수면 이렇게 생긴 거고 금연은 컨케이브하다 그런 것들이 이제 세어럼으로 존재하긴 합니다. 이런 것들은 여러분 뭐 한 번 미분 두번 미분 같은 거는 이미 배우신 거니까 그렇죠 자 그다음에 컨백스랑 컨케이브가 정의되고 나면 이러한 이니퀄리티를 우리가 정의할 수가 있게 됩니다. 여기서 이는 익스펙테이션 기댓값을 의미하는 것으로 생각해 주시면 되고 여기 나와 있는 거는 이제 뭐 정확하게 우리가 뭐 벌 파이라고 읽기도 하는데 여기서 그냥 파이라고 하겠습니다. 우리가 이런 파이가 어떤 함수냐에 따라서 얘가 만족할 수도 있고 1번이 만족할 수도 있고 2번이 만족할 수도 있다는 겁니다. 아시겠죠? 얘가 어떤 함수냐에 따라서 컨벡스 함수면 익스펙테이션 값이 더 크거나 같다 라는 거고 컨케이 보면 익스펙테이션 값이 더 크거나 작다라고 생각해 주시면 되겠습니다. 앞에랑 사실은 유사해요. 여기서 보시면 x에 대해서 우리가 뭔가 기댓값 뭔가 인터폴레이션을 하고 프로 올린 거 그렇죠 얘는 f로 올리고 기댓값 한 거 컨백스일 때는 컴백스일 때는 프로 올리고 뭔가 기댓값을 취하든 인터폴레이션을 하든 그게 평균을 취하고 올리는 것보다 더 컸습니다. 그래서 평균을 취하고 올리는 것보다 애플을 취하고 평균을 치는 게 더 크거나 같다. 컴백스는 앞에 있던 성질을 이용하면 굉장히 어쩌면 또 자명한 얘기가 나오게 됩니다. 예를 들어서 베리언스를 한번 생각해 봅시다. 베리언스 베리언스 x는 공식에 의해서 스 스퀘어의 스펙트 대선 빼기 2x의 스퀘어입니다. 그렇죠 제곱의 평균 빼기 평균의 제곱으로 여러분들이 많이 외웠을 겁니다. 베리언스는 일단 0보다 크거나 같다는 건 모두가 알고 있어요. 그쵸 얘는 그러면 다르게 성은 뭡니까? 얘를 쫙 넘겨요. 그러면은 뭐예요? 평균의 제곱은 제곱의 평균보다 같거나 작다라는 얘기를 하고 있는 겁니다. 자 그러면 여기서는 이 파이가 뭐예요? 엑스 제곱인 거예요. 이게 제곱인 겁니다. 제곱 컴백스 함수죠. 그래서 얘가 만족하는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 2,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1079,
      "char_count": 1999
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c003_fd5451",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 베리언스 베리언스 x는 공식에 의해서 스 스퀘어의 스펙트 대선 빼기 2x의 스퀘어입니다. 그렇죠 제곱의 평균 빼기 평균의 제곱으로 여러분들이 많이 외웠을 겁니다. 베리언스는 일단 0보다 크거나 같다는 건 모두가 알고 있어요. 그쵸 얘는 그러면 다르게 성은 뭡니까? 얘를 쫙 넘겨요. 그러면은 뭐예요? 평균의 제곱은 제곱의 평균보다 같거나 작다라는 얘기를 하고 있는 겁니다. 자 그러면 여기서는 이 파이가 뭐예요? 엑스 제곱인 거예요. 이게 제곱인 겁니다. 제곱 컴백스 함수죠. 그래서 얘가 만족하는 겁니다. 이런 식으로 우리가 예시를 볼 수도 있다는 점 그쵸 굉장히 재밌죠. 베리언스 x가 0보다 크다는 걸 우리가 이렇게 젠센 퀄리티로도 증명을 할 수가 있습니다. 자 그러면 이제 우리가 좀 기본적인 학습은 끝이 났습니다. 이제부터는 우리가 본격적으로 변분 추론 또는 베리에이션 인퍼런스로 넘어가도록 하겠습니다. 자 결국 우리가 앞서서 말씀드린 결론은 아 포스테리어에서 우리가 뭔가 샘플링해서 값을 계산하자였습니다. 그렇죠 자 이거를 우리가 지금 계속하고 있는 이유는 뭐다 p 세타 x를 우리가 맥시마이즈 하기 위해서 그렇죠 앞에서 우리가 봤던 것처럼 결국 얘 얘가 맥시마이즈가 되는 세터를 찾고 싶으니까 그걸 하기 위해서 우리가 그에 대한 대안으로 이거를 하고 있는 겁니다. 얘가 맥시마이즈 되는 걸 찾자는 겁니다. 왜 얘가 포스테리어인지 조금만 기다려 주세요. 이제 볼 겁니다. 자 그전에 그러면 다음과 같은 그 로그 픽스를 한번 생각해 보겠습니다. 자 데이터 포인트에 대한 로그라이크 로드인 거죠. 자 얘는 우리가 제를 붙여서 우리가 이렇게 모델링 할 수도 있겠죠 그렇죠 이게 pxi 콤마 제니까 제로 우리가 미분화 적분하면 피아만 남습니다. 즉 아이번즈 데이터에 대한 로그라이킬로드 일단 편의상 데이터를 한 개만 저희가 생각하도록 하겠습니다. 자 그러고 나서 여기에 위아래에다가 qi z를 곱해줘요. 그럼 위아래도 1을 곱한 거니까 똑같겠죠 그렇죠 그러고 나서 여기서 우리가 이렇게 한번 생각해 볼게요. 이렇게 분자는 잠깐 띄워놓고 이렇게 qi 시트 분모랑 PXG 그다음에 피제 이렇게 3개만 한번 묶어 보겠습니다. 이렇게 묶어집니다. 그렇죠 이렇게 묶어집니다. 자 그 상황에서 우리가 얘를 다시 쓰면 이렇게도 쓸 수 있다는 거예요. 여기서 익스펙테이션에다가 뭐 qi지 이런 게 붙었죠 얘는 뭐냐면 익스펙테이션을 취하는데 우리가 qi 지에 대해서 익스펙테이션을 취하는 거다라는 뜻입니다. 왜 qi지만 우리가 떼놓고 나머지를 여기다가 넣었잖아요 큐아지만 우리가 딱 빼온 겁니다. 그래서 얘는 큐알지에 대한 익스펙테이션이라고 우리가 적을 수가 있는 거예요. 자 머신 러닝 쪽에서는 이런 노트에서는 굉장히 많이 씁니다. 여러분들께서 익숙해지시면 좋을 것 같고요. 결국 얘가 얘랑 그냥 같은 거다 마치 그냥 정의처럼 외워놓으시면 되겠습니다. 이 두 개는 그다음에 여기서 우리가 젠센 인퀄리티를 쓸 거예요. 젠센 인퀄리티를 쓰게 되면 어떻게 됩니까? 로그가 안으로 들어갈 수 있다 그러면 너무 행복한 거예요. 왜 곱하기로 되어 있는 익스펙테이션 밖에 로그에 있어 봤자 이 로그 있나 없나 도움이 하든 안 돼요. 근데 로그가 안에 들어가면 아 이제 이 곱하기들이 더하기로 분해가 되면서 매우 행복해집니다. 그렇죠 젠스 이닝 퀄리티를 써서 우리가 로그가 컨케이브니까 로그를 안으로 넣을 수 있다 였습니다. 그렇죠 그러면서 이렇게 부등호를 이런 관계로 가게 되고 그다음에 얘를 그냥 풀어 쓴 겁니다. 이렇게 쫙 그다음에 얘는 엔트로피라는 것의 정의라서 엔트로피 qi라고 적은 겁니다. 얼마나 얘가 불확실한지 얼마나 랜덤성이 있는지를 측정하는 매트릭이라고 생각하시면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 3,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1008,
      "char_count": 1870
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c004_7cba09",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 그렇죠 젠스 이닝 퀄리티를 써서 우리가 로그가 컨케이브니까 로그를 안으로 넣을 수 있다 였습니다. 그렇죠 그러면서 이렇게 부등호를 이런 관계로 가게 되고 그다음에 얘를 그냥 풀어 쓴 겁니다. 이렇게 쫙 그다음에 얘는 엔트로피라는 것의 정의라서 엔트로피 qi라고 적은 겁니다. 얼마나 얘가 불확실한지 얼마나 랜덤성이 있는지를 측정하는 매트릭이라고 생각하시면 됩니다. 즉 우리는 다음과 같은 관계를 발견한 거예요. 다음과 같은 관계는 어떤 관계입니까? 얘에 대한 로우 바운드가 얘라는 거예요. 자 얘는 얼마나 불확실한지 얼마나 이 랜덤 베리블이 얼마나 랜덤한지를 나타내는 거고 이 앞에 있는 거는 뭐냐면 결국 우리가 지을 샘플링을 할 건데 지을 샘플링해서 다시 이 XI를 얼마나 잘 설명할 수 있는지 우리한테 주어진 데이터를 그것을 의미하고 있습니다. 그래서 얼마나 이러한 로그 퍼블리티를 우리가 잘 설명할 수 있는지 시트리 샘플링 그에 대한 식으로 우리가 볼 수 있는 것이고요. 그럼 얘가 포스테리어랑 무슨 관련이 있냐라는 것은 앞에서 대체 왜 여기 포스테리어가 있냐 뭐 이거 말했는데 여기에 이거 포스테리오랑 무슨 관련이 있냐 이 식도 이제 아마 고민을 하고 계실 겁니다. 이제부터 한번 볼게요. 대신 그전에 저희가 케일 다비스를 한 번만 더 보고 넘어가겠습니다. 자 분포 간의 다이버전스 얼마나 두 분포가 유사하고 상이한지를 측정하는 다이버전스라는 게 존재합니다. 그러한 다이버전스는 f 다이버전스 h 다이버전스 그다음에 아피엠 크게 이렇게 세 가지로 나뉩니다. 그리고 그런 f 다이버전스 중에서 특정한 형태를 띤 케이스가 KL 다이버전스입니다. KL 다이버전스는 이렇게 두 분포 사이를 레이시오로 로그 레이시를 통해서 비교를 합니다. 두 개가 얼마나 가깝고 두 개가 얼마나 먼지를 측정하는 것입니다. 자 여기 나와 있는 스가 디스크릿일 때는 서메이션인 거고 스가 연속일 때는 인테그랄로 표현이 된 것뿐입니다. 아시겠죠? 자 그래서 이거는 정의라고 생각하시면 돼요. 자 어떻게 기억하시면 되냐면 결국에는 이 피와 큐 사이에 레이시오를 보는 겁니다. 레시 레시를 보는 건데 특히 로그 레이시를 보는 건데 앞에 비중이 있어요. PX라는 게 붙어 있습니다. 여기가 p면 여기도 p고 얘가 만약에 QBP였다 그러면 QQ p로 주기가 바뀌게 됩니다. 아시겠죠? 그래서 여기서는 p에 대해서 뭔가 웨이트를 주고 p와 q 사이에 케일 다변수를 측정한 걸로 기억하시면 됩니다. 또는 로그의 분자 분모 바꿔서 마이너스가 앞에 오는 형태로 또 생각을 하시면 되겠죠. 자 일단 이런 케일 다변수는 0 이상이에요. 왜 이것도 우리가 젠센 퀄리티를 활용하면 바로 알 수가 있습니다. 자 여기서 케일 다변스의 정의를 그대로 쓰면 우리가 이렇게 쓸 수가 있습니다. 그렇죠 이렇게 쓰든 아니면 pp 이렇게 이렇게 분자 분모에 바꿔 쓰면서 마이너스를 붙이든 여기서는 마이너스를 한번 붙여볼게요. 자 이렇게 되고 그다음에 여기서 우리가 앞에서 정리했던 것처럼 피를 만약에 떼와서 가져온다 이것만 남게 되고 그러면 우리가 피에 대한 익스펙테이션이라고 할 수 있다고 했습니다. 그렇죠 자 그다음에 로그가 여기서 마이너스 로그는 뭐예요? 컨백스 펑션입니다. 로그가 콘케이브고 마이너스 로그는 컴백스 함수죠 이렇게 나오게 됩니다. 부등호 이렇게 되면서 그렇죠 그러고 이걸 한번 보시면 다시 얘를 쓰게 되면 정의 그대로 쓰면 피 곱하고 이거 곱하면 되겠죠. 그다음에 피에 대해서 적분해버리면은 이거 약분되고 인티그랄 qx DX니까 그러면은 q도 우리가 확률 분포예요. 그러면 1이 나오고 마이너스 1이니까 0이 나옵니다. 그래서 0 이상이다 KL 다이비전스는 결국 두 분포 사이에 얼마나 상이한지를 나타내는 거고 0 이상이라는 겁니다. 두 분포가 완전히 동일하면 0이에요. 두 분포가 이제 차이가 나면 점점 이 케 다변수가 커지게 되는 것으로 생각하시면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 4,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1053,
      "char_count": 1958
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c005_d7c5f4",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 부등호 이렇게 되면서 그렇죠 그러고 이걸 한번 보시면 다시 얘를 쓰게 되면 정의 그대로 쓰면 피 곱하고 이거 곱하면 되겠죠. 그다음에 피에 대해서 적분해버리면은 이거 약분되고 인티그랄 qx DX니까 그러면은 q도 우리가 확률 분포예요. 그러면 1이 나오고 마이너스 1이니까 0이 나옵니다. 그래서 0 이상이다 KL 다이비전스는 결국 두 분포 사이에 얼마나 상이한지를 나타내는 거고 0 이상이라는 겁니다. 두 분포가 완전히 동일하면 0이에요. 두 분포가 이제 차이가 나면 점점 이 케 다변수가 커지게 되는 것으로 생각하시면 됩니다. 자 쉽게 생각해서 거리를 측정하는 걸로 생각하셔도 돼요. 다만 일반적인 거리랑 다르게 여기 나와 있는 PB q랑 이거랑 q랑 피를 바꿔 쓰는 거 순서를 두 개는 다르긴 합니다. 그래서 우리가 거리라는 말을 굳이 쓰지 않고 우리가 왜 큐바 피랑 피바 큐가 다르니까 거리라는 말을 굳이 쓰지 않고 다이버전스라는 말을 우리가 써서 진행을 하게 됩니다. 여기서 우리가 뭐 두 개가 어떻게 다른지 좀만 더 자세히 한번 봐볼게요. 자 PBA q를 우리가 보면은 이렇게 생겼습니다. 이렇게 그렇죠 그러면 얘는 뭐예요? q랑 p의 레이시를 보는 건데 p의 가중치가 가해져 있습니다. 피에 그렇죠 무슨 말이냐면 만약에 우리가 피가 이렇게 있다고 할게요. 피가 파란색입니다. 피가 파란색 또는 하늘색 q가 초록색입니다. 근데 우리가 q를 만약에 바꿀 수 있다고 할게요. 그러면은 우리가 피바를 케일 다변스를 이렇게 디케일로 나타내기도 합니다. 다이버전 스케일이라고 해서 이 피바큐를 낮추려면 우리가 q를 어디로 옮겨야 할까라는 얘기를 지금 하고 있는 겁니다. 그때 p랑 q의 레이시를 비교하는데 p의 웨이드가 걸려 있어요. 무슨 말이냐면 p가 큰 곳에서는 피가 큰 곳에서 이 레이쇼가 큰 차이가 난다 그러면 어떻게 됩니까? 보면은 엄청 이 다이버전스가 엄청 커지게 되는 거예요. 그러면 두 개 간의 다이브수를 줄이려면 피가 큰 곳에서 큐가 어떻게든 뭔가를 하긴 해야 되는 거예요. 이렇게 그렇죠 자 근데 반대로 큐바 피입니다. q 피면 여기에 q가 붙어 있는 거예요. q q p 분야 q 이렇게 그냥 p 자리에 q가 들어가고 q 자리에 p가 들어가는 거니까 그렇죠 그러면 이때는 우리가 q에 대해서 웨이트가 걸려 있습니다. q가 큰 곳에서 우리가 비슷하게 만들고 싶은 거예요. 그러면 어떻게 합니까? 그러면은 q가 큰 곳에서 우리가 비슷하게 만들어야 되니까 그때는 이 피가 이런 바이모더리라고 하더라도 추가 이거 하나만 잘 덮어도 게임이 끝난다라고 생각할 수가 있겠습니다. 아시겠죠? 그러면 얘를 우리가 한번 또 다르게 이 케일 다변수를 가져와서 한번 해석을 한번 해보겠습니다. 자 이러한 케일 레비넌스를 우리가 한번 생각해 볼게요. 자 이거는 뭐예요? 포스테리오죠 포스테리오 그다음에 얘는 우리가 qg라는 얘를 보통 뭐 베리에이셔널 포스테리어라고 부른다. 베리에이셔널 포스테리오 얘가 우리가 좀 최적화가 될 대상이에요. 근데 왜 베리에이셔널 포스테리어냐 이 포스테리어랑 좀 유사하도록 우리가 얘를 학습할 거라서 즉 얘를 낮추는 방향으로 우리가 학습할 거라서 그렇습니다. 아시겠죠? 얘가 낮아져서 완전히 0이 돼버리면 뭐가 된다 얘는 포스트 테리어랑 같아진다인 거니까요. 낮춰볼 겁니다. 자 그러면 얘를 일단 쓰겠습니다. 얘를 우리가 써서 얘가 어떻게 표현되는지 한번 볼게요. 자 그러면 얘는 이런 식으로 써지게 됩니다. 정의에 따라서 그렇죠 그다음에 여기에서 이 조건부 확률을 우리가 풀어서 쓰면은 이렇게 쓸 수 있겠죠 그쵸 그다음에 로그 qig 로그 pxi 그쵸 그다음에 여기서 마이너스 로그 이렇게 그러니까 마이너스 앞에 떼면은 로그 gx pg 콤마 x인데 조건부로 쓰면 이렇게 되고 그렇죠 그다음에 마이너스 로그 큐 마이너스 로그 피 근데 우리가 여기에 마이너스가 있으니까 그렇죠 이렇게 우리가 쓸 수가 있게 됩니다. 여러분 그러면은 여기서 한번 또 우리가 생각을 해봅시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 5,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1063,
      "char_count": 1998
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c006_f3315c",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 정의에 따라서 그렇죠 그다음에 여기에서 이 조건부 확률을 우리가 풀어서 쓰면은 이렇게 쓸 수 있겠죠 그쵸 그다음에 로그 qig 로그 pxi 그쵸 그다음에 여기서 마이너스 로그 이렇게 그러니까 마이너스 앞에 떼면은 로그 gx pg 콤마 x인데 조건부로 쓰면 이렇게 되고 그렇죠 그다음에 마이너스 로그 큐 마이너스 로그 피 근데 우리가 여기에 마이너스가 있으니까 그렇죠 이렇게 우리가 쓸 수가 있게 됩니다. 여러분 그러면은 여기서 한번 또 우리가 생각을 해봅시다. 그러면은 이 앞에 있는 거를 어디서 보지 않았나요? 여러분 이 앞에 있는 거 어디서 봤을 텐데 즉 우리가 앞에서 이미 저식을 유도를 한번 해봤어요. 여기 얘입니다. 얘 얘가 지금 여기에 그대로 있는 건데 부호만 마이너스로 해서 있는 거예요. 그래서 마이너스 l이라고 적어놓겠습니다. 그리고 얘는 그대로 가져올게요. 아시겠죠? 그러면 얘를 정리하면 어떻게 돼요? 로그 pxi는 그렇죠 얘 넘기면 이렇게 두 개의 썸으로 됩니다. 그리고 부등호를 쓰면 왜 얘는 양수라고 했잖아요. 얘는 0 이상입니다. 0 이상이니까 얘를 안 더하면 더 작겠죠. 작거나 같다. 우리가 앞에서 봤던 식을 케일 다비넌스로 또 다른 방식으로 우리가 한번 유도를 해본 겁니다. 그렇죠 여러분 자 그리고 나서 한번 생각해 봅시다. 여기서 결국 이 왼쪽에 있는 거는 우리가 데이터에 대한 로그 라이프로드예요. 그러면은 우리가 만약에 이미 데이터가 다 샘플이 됐어 샘플링이 끝났어 그럼 우리한테 데이터가 100개가 주어진 거예요. 그럼 데이터 100개에 대한 로그 라이킬로드는 고정된 값인 겁니다. 이미 이 데이터가 어떻게 나왔는지 이 데이터가 나올 확률 같은 건 우리가 이미 고정되어 있는 거고 얘를 우리는 추정하고 싶은 거예요. 자 무슨 말이냐면 우리가 데이터가 만약에 자 이런 검은색 샘플 데이터가 관측이 됐다고 하겠습니다. 자 이 데이터가 어디서 나왔는지 우리 알 수 없지만 이러한 회색에서 나왔다고 가정합시다. 그러면은 이 각 데이터들에 대한 라이클루드는 이미 정해진 거예요. 고정입니다. 이제 안 바뀌어요. 그렇죠 자 그러면 얘는 우리가 고정이라고 볼 수 있습니다. 그러면 어떻게 되는 거예요? 얘가 고성이면 얘가 커지면 얘는 작아지는 거예요. 반대로 얘가 커지면 얘는 작아지는 거예요. 그렇죠 근데 우리의 목표는 뭐다 우리의 목표는 얘를 우리는 맥시마이징 하고 싶은 거예요. 왜 우리는 어떻게든 얘를 추정하고 싶은 건데 얘가 이거의 로어바운드잖아요. 그럼 어떻게든 얘를 올려야지 얘를 올려야지 이거랑 가까워질 거 아니에요 그렇죠 그때 어떻게 되냐 q가 피랑 같아져야죠. q가 피랑 같아져야지 얘가 쫙 올라가서 얘랑 같아질 수 있는 거예요. 그렇죠 만약에 얘가 100이다. k 변수가 만약에 100이에요. 굉장히 큰 값이죠. 100까지 보통 잘 안 나오지만 얘를 100이라고 할게요. 그러면은 얘는 우리가 아무리 맥시마이즈 해봤자 한계가 있는 거예요. 얘가 만약에 예를 들어서 뭐 1이다 얘를 0.1이라고 하겠습니다. 그러면 얘는 0.9까지밖에 안 올라가는 거야. 1까지 안 올라가 근데 q랑 피가 같아지면 얘는 0이 되고 얘는 그러면 계속 올리면 엘까지도 올라갈 수 있는 거예요. 이거랑 똑같이 1까지 올라갑니다. 그래서 우리는 뭐다? 이 여기 나와 있는 qi지가 피의 제바 XI랑 같아지길 원하는 거예요. 얘가 포스테리어랑 같아지길 원하는 겁니다. 얘가 아시겠죠? 그래서 어떻게 생각하면 우리는 포스테리어만 잘 추정해도 문제가 거의 다 해결되는 상황입니다. 사실은 이 포스테리어를 추정하기가 어려워서 그렇지 포스테리어 추정만 하면은 그러면은 그 포스트에 대해서 우리가 값을 딱 샘플링 해 가지고 나머지 계산하면 되는 거예요. 아시겠죠? 자 그러면 여기서 한번 우리가 보면은 우리가 결국 풀고 싶었던 거는 이건데 그렇죠 실제로는 우리가 이거의 로어바운드인 얘를 맥시마이즈 시킨다. 얘를 맥시마이즈 시키는 세터를 우리가 찾겠다라는 겁니다. 여기서 세타는 어떤 모델마다 이 세터는 달라질 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 6,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1078,
      "char_count": 2021
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c007_285157",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 사실은 이 포스테리어를 추정하기가 어려워서 그렇지 포스테리어 추정만 하면은 그러면은 그 포스트에 대해서 우리가 값을 딱 샘플링 해 가지고 나머지 계산하면 되는 거예요. 아시겠죠? 자 그러면 여기서 한번 우리가 보면은 우리가 결국 풀고 싶었던 거는 이건데 그렇죠 실제로는 우리가 이거의 로어바운드인 얘를 맥시마이즈 시킨다. 얘를 맥시마이즈 시키는 세터를 우리가 찾겠다라는 겁니다. 여기서 세타는 어떤 모델마다 이 세터는 달라질 수 있습니다. 만약에 우리가 그냥 가우시안으로 그냥 q를 가우시안으로 잡겠다라고 하면은 이때 우리가 최적화해야 되는 거는 뭐다 결국 그래서 이 가오샤의 민과 코비런스가 뭔데 최적이 되려면 즉 얘가 포스테리어랑 가까워지려면 최적이 된다라는 말은 포스테리어랑 같아진다는 말이니까 이러한 포스테리어랑 가까워지려면 우리가 QRZ를 어떻게 잡아야 되는데 가오젠으로 잡았다고 하더라도 민과 베리언스가 뭐가 돼야 되는데라는 얘기입니다. 아시겠죠? 얘가 세타라고 생각하시면 되겠죠. 여기서는 실제로 VA 같은 경우는 우리가 나중에 세터랑 뭐 파이랑 두 개 다 학습돼요. 여기도 학습하는 파라미터가 있고 여기도 학습하는 파라미터가 있어서 지금은 저희가 VA가 아니까 굉장히 단순한 모델로 하고 있기 때문에 여기에는 러너블 파라미터가 없고 여기에 러너블 파라미터만 있다라고 생각합시다. 자 그러면 우리가 가우시안 분포 민과 코베리언스를 우리가 최적화해야 되는 거예요 어떻게 뭐 결국 그라디언트 디센트 인데 여기서는 맥시마이즈를 해야 되니까 디센트가 아니라 그라디언트 어센트를 통해서 합니다. 이거에 문제점이 있을까요? 이런 방식의 문제점 즉 지금 보면은 이런 방식을 통해서 우리가 얘를 맥시마이즈 하는 q를 찾게 되면은 그 q가 이제 정확한 포스테리얼을 우리가 구한 거다라고 또 해석을 할 수가 있는 것이고 또 여기서 우리가 얘를 맥시마이즈를 잘만 시킬 수 있으면 실제 데이터의 분포에 가깝게 우리가 추정한 것이다라고도 해석을 할 수가 있습니다. 이러한 방식의 문제점은 뭐가 있을까요? 여기서 파라미터의 개수를 한번 카운트를 한번 해보시면 자 지금 데이터 아이마다 아이폰트 데이터마다 뮤아이랑 시그마가 하나씩 있는 거예요. 이 모델은 그러면 굉장히 파운트 개수가 많아지는 겁니다. 그쵸 데이터 개수만큼 파라미터가 있는 거예요. 데이터 개수가 n개 있다 그러면 이거는 데이터 하나당 지금 두 개 필요하죠 민간 코베리아스 그러면 2행 개만큼 파라미터가 필요한 겁니다. 데이터가 많아지면 많아질수록 우리가 학습해야 되는 파라미터도 많아지는 문제가 발생하게 되는 것이고요. 자 얘를 우리가 그러면 어떻게 해결할 것인가라는 겁니다. 얘를 해결하는 방법은 아모르타이스 베리지 인플런스입니다. 자 브에 생각하시면 돼요. 자 VA가 어떻게 되는 거였어요? 스가 있으면 스에서 각 스마다 각 아이 번트 데이터마다 민과 코비런스를 각각 만듭니다. 근데 어떻게 만들어요? 그 XI를 뉴로 네트워크에 넣어주고 그 뉴로 네트워크의 아웃풋으로 민간 코비런스가 나오는 거예요. 그러면은 러너블 파라미터는 뉴런 네트워크 하나입니다. 민과 코베리언스는 데이터마다 따로 나올지언정 우리가 학습해야 되는 모델 파라미터는 하나인 거고 데이터가 많아져도 모델 파라미터의 개수가 늘어나진 않습니다. 이러한 컨셉이 아모르타이스 베레션 앰퍼런스고 사실 VA는 여기에 이러한 모티베이션에서 나온 논문이긴 합니다. 여기까지 해서 저희가 베레이션 인퍼런스의 기본적인 내용과 원리를 살펴봤습니다. 네 여러분 이번에는 저희가 미 필드 베리션 인퍼런스로 넘어가도록 하겠습니다. 얘는 사실은 베레션 인플런스보다 더 간략화된 버전이에요. 근데 유도 과정이 좀 어렵긴 합니다. 즉 결론은 쉬운데 이거를 유도하기까지는 약간의 개념이 필요하다는 점 기억해 주시고요. 만약에 진짜 너무 외계어를 하는 것 같다라고 하면은 마지막 내용만 이해하셔도 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 7,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1070,
      "char_count": 1943
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c008_3d3fb9",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 이러한 컨셉이 아모르타이스 베레션 앰퍼런스고 사실 VA는 여기에 이러한 모티베이션에서 나온 논문이긴 합니다. 여기까지 해서 저희가 베레이션 인퍼런스의 기본적인 내용과 원리를 살펴봤습니다. 네 여러분 이번에는 저희가 미 필드 베리션 인퍼런스로 넘어가도록 하겠습니다. 얘는 사실은 베레션 인플런스보다 더 간략화된 버전이에요. 근데 유도 과정이 좀 어렵긴 합니다. 즉 결론은 쉬운데 이거를 유도하기까지는 약간의 개념이 필요하다는 점 기억해 주시고요. 만약에 진짜 너무 외계어를 하는 것 같다라고 하면은 마지막 내용만 이해하셔도 됩니다. 대신에 우리가 이걸 유도할 때 필요한 수학적인 테크닉들을 저희가 지금 한 땀 한 땀 다 따라가고 디테일하게 라인 바이 라인으로 다 설명드릴 거거든요. 그 까닭에 여러분들께서 이 베리샤 인프런스뿐만 아니라 이런 스킬은 다른 곳에서도 쓰입니다. 그래서 여러분들께서 가급적이면 그러한 스킬들까지 다 이해해서 가시면 좋겠습니다. 민필 베리션 인퍼런스의 필요성을 일단 보겠습니다. 이게 뭔지를 일단 보기 전에 이거의 필요성 자 저희가 결국 앞에서 봤던 것처럼 베리션 인퍼런스는 이러한 로그 라이클로드 또는 우리가 얘를 사실은 지를 넣었다가 지에 대해서 마지라이즈 아웃 인티그럴 아웃 시킨 걸로도 생각할 수 있잖아요. 그 까닭에 로그 마지널 라이클루드라고도 부릅니다. 그렇죠 가끔씩 뭐 트위터에서 연구 대가들끼리 막 싸워요. 로그 마지널 라이클루드냐 마지널 로그 라이클루진이야 그렇죠 되게 재밌습니다. 여기서 정식 용어는 이 pxi가 그렇죠 z에 대해서 인티그럴 아웃 된 거니까 마지널라잇이 된 거죠. 그럼 얘가 마지널라이트 라이크로니 그렇죠 거기에 로그를 붙인 거니까 로그 마시놀 라이트 라이클루드라고 보는 게 맞겠죠 또는 로그 마시놀 라이클루드 그렇죠 그래서 마지널 로그가 아니라 로그 마지널이다라고 생각하시면 되겠고요. 여기서 그러면 우리가 얘를 구하고 싶은데 얘를 정확하게 구하기가 어려우니까 얘 로어 바운드를 우리가 구해서 계산을 진행했습니다. 자 그리고 얘 로어바운드를 또 우리가 다르게 해석하면 뭐 이렇게 쓸 수가 있었죠 그렇죠 여기서 한번 보면은 우리가 여기서 결국 qg를 구해야 됐습니다. qg 근데 qg가 어떤 qg가 제일 좋았나요? qg라고 하는 것은 우리가 포스테리어 실제 포스테리어랑 가까운 게 제일 좋았습니다. 결국 이 포스테리어를 닮은 큐지를 우리는 찾고 싶은 게 목표예요. 그쵸 그러면 q지를 우리가 어떻게 모델링 할 거냐 q지를 뭐 가우시으로 할 거냐 뭘로 할 거냐 우리가 정해야죠. 그런데 여기서 한 가지 문제가 있습니다. 이런 거 한번 생각해 볼게요. 얘가 이제 레이턴트 디리클리 어플리케이션이라고 토픽 모델링에서 이제 많이 연구가 되었던 2003년도 논문입니다. 여기에 여러분들께서 그 아마 이미 이름 많이 들어봤을 법한 그 데이비드 블라이 그다음에 앤드류 옹 그다음 코세라 그쵸 그다음에 마이클 조던 마이클 조던이 이제 그 머신 러닝 학자로도 존재합니다. 그 3명이 이제 같이 쓴 연구인데 여기서 한번 보면은 지라는 거는 우리가 여기서 지라는 건 상징적인 거예요. 관측된 게 아닌 나머지 전부를 우리가 지라고 했습니다. 자 여기서도 우리가 이런 색칠된 거는 관측된 거라고 우리가 앞에서 했었죠. 그 외에 나머지는 다 관측이 안 된 거예요. 그럼 변수가 엄청 많은 겁니다. 이 지가 나타내는 게 알파 세타 지 에타 베타 이 모든 걸 다 조인트가 여기 나와 있는 피지랑 이큐지인 거예요. 아시겠죠? 자 그럼 포스테리어를 우리가 정확하게 구하기가 사실은 어렵다는 겁니다. 왜 당장 이게 알파 세타 지에타 베타 이 각각이 따르는 분포도 다 달라요. 얘는 디리클레 분포 얘는 뭐 멀티노미얼 분포 얘도 뭐 그렇죠 우리가 뭐 멀티노미얼 분포 뭐 여러 가지 진짜 다양합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 8,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1030,
      "char_count": 1899
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c009_f55d31",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 이 지가 나타내는 게 알파 세타 지 에타 베타 이 모든 걸 다 조인트가 여기 나와 있는 피지랑 이큐지인 거예요. 아시겠죠? 자 그럼 포스테리어를 우리가 정확하게 구하기가 사실은 어렵다는 겁니다. 왜 당장 이게 알파 세타 지에타 베타 이 각각이 따르는 분포도 다 달라요. 얘는 디리클레 분포 얘는 뭐 멀티노미얼 분포 얘도 뭐 그렇죠 우리가 뭐 멀티노미얼 분포 뭐 여러 가지 진짜 다양합니다. 자 그러면은 이것들의 조인트가 또 뭐 큐 알파 콤마 세타 콤마 지콤마 에타 콤마 베타가 어떤 분포를 따를지는 우리는 계산할 수가 없어요. 너무 복잡해요. 그거에 대한 조인트를 보는 거 그래서 그거에 대한 조인트를 독립적으로 우리가 고려하겠다라는 게 미인 필드 베니셔널 인플루언서입니다. 즉 이렇게 독립적으로 팩트라이즈가 된다라고 우리가 가정하는 겁니다. 아시겠죠? 즉 무슨 말이냐면 우리가 앞에서 봤던 식을 앞에서 봤던 식을 다시 쓰면 이렇게 써지지 않습니까? 이렇게 써질 거예요. 그렇죠 이렇게 써지는데 이 q지라는 게 우리가 만약에 m개가 있다고 하면은 그렇죠 m개의 레이턴트로 되어 있다고 하면 그 m기가 다 독립이라고 가정하는 겁니다. 즉 여기로 치면은 레이턴트가 5개 있는 거죠. 그 다섯 개가 우리는 다 독립이라고 가정하는 거예요. 그래서 q 1 콤마 제2 콤마 제3가 아니라 그쵸 q 제1 곱하기 q 제 2 곱하기 q 제3 제들끼리는 다 독립이다라고 우리가 가정하고 넘어가는 겁니다. 아시겠죠? 그런 상황이면은 우리가 좀 더 문제를 쉽게 풀 수 있지 않겠냐라는 겁니다. 그러고 나서 그러면 이제 우리가 할 거는 각각의 q만 우리가 구하면 되는 거예요. 얘를 맥시마이즈 시키는 각각의 q만 그전에는 얘를 맥시마이즈 시키는 q의 큐의 제원 콤마 제2 콤마 제 3 이거를 우리가 구했어야만 했습니다. 근데 그게 아니라 얘를 맥시마이즈 하는 q의 z1을 구하면 되고 제를 맥시마이즈 하는 q z2를 구하면 되고 z 3를 구하면 되고 각각을 구하면 되니까 문제가 훨씬 더 간단해진 거죠. 자 그런 상황에서 저희가 그다음으로 한번 또 넘어가 봅시다. 그러면 얘를 어떻게 구할 건데 그래서 뭐 알겠어 더 간단해진 건 알겠는데 맥시마이즈 시키는 뭔가 전체를 찾기보다 뭐 하나씩만 맥시마이즈 시키면 된다고 하면은 뭐 그러네 뭐 좀 더 쉽긴 하겠네 각각만 하면 되니까 근데 뭐 그걸 대체 어떻게 찾아 라고 생각할 수가 있겠죠 자 그러면 이거를 이제 유도를 할 건데 식이 좀 복잡하게 보이지만 라인 바이 라인으로 제가 다 적어놨어요. 그쵸 여기에서 하나하나 또 적어가면서 하기에는 저희에게 주어진 시간이 너무 짧기 때문에 미리 다 라인 바이 라인을 적어왔습니다. 상세하게 한번 여러분들께서 보시면 대부분 이해가 가실 거라고 이제 그 기대를 하고 있는데요. 즉 여기서 여기까지는 우리가 처음 봤던 그 엘보 그대로입니다. 여기서 엘보라는 거는 우리가 여기 빨간색 박스 로 바운드를 엘보라고 우리가 부릅니다. 얘를 우리가 큐지를 우리가 민필드 어섬션을 통해서 즉 이러한 지 레지턴트 베리블끼리는 다 독립이다라는 어썸션 가정 사항이죠. 그걸 통해서 이렇게 분해를 했습니다. 그러고 분해하고 나면 우리가 이렇게 쓸 수 있겠죠 그렇죠 이렇게 해서 그러면 익스펙텐션이니까 얘를 우리가 뭐 일단 쥐가 컨티셔스라고 가정할게요. 연속 변수라고 이렇게 인티그랄로 써지게 되고 그다음에 얘가 또 서메이션으로 써지게 됩니다. 그렇죠 자 그러고 나서 우리가 여기서 관심 있는 거는 하나만 관심 있는 거예요. 일단 얘를 맥시마이즈 시키는 얘를 맥시마 시키는 q 제 하나만 구하겠답니다. 왜 이제는 우리가 독립적으로 생각하면 됩니다. q 제 구하고 q 제이 플러스 1 구하고 q 제이 플러스 2 구하고 하면 되는 거예요. 독립적으로 하면 됩니다. 그 까닭에 우리가 하나만 빼고 하나만 쏙 빼우는 거예요. 하나만 쏙 빼우고 나머지를 이렇게 집어넣었습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 9,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1042,
      "char_count": 1949
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c010_fcb264",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 그렇죠 자 그러고 나서 우리가 여기서 관심 있는 거는 하나만 관심 있는 거예요. 일단 얘를 맥시마이즈 시키는 얘를 맥시마 시키는 q 제 하나만 구하겠답니다. 왜 이제는 우리가 독립적으로 생각하면 됩니다. q 제 구하고 q 제이 플러스 1 구하고 q 제이 플러스 2 구하고 하면 되는 거예요. 독립적으로 하면 됩니다. 그 까닭에 우리가 하나만 빼고 하나만 쏙 빼우는 거예요. 하나만 쏙 빼우고 나머지를 이렇게 집어넣었습니다. 아시겠죠? 자 그다음에 여기서도 우리가 이렇게 다 분해할 수 있는데 지금 우리의 관심사는 뭐다 결국 우리의 관심사는 q j가 들어 있는 항만 관심이 있다 그래서 q j가 들어 있는 것만 쏙 빼운 겁니다. 그러고 나서 얘를 우리가 정리하면은 이렇게 정리할 수 있겠죠. 여기서도 또 q 제가 포함된 것만 생각하면 이렇게 그럼 얘가 우리가 다뤄야 되는 식입니다. 실제로는 더 많지만 q j와 관련된 것만 남긴 거예요. 얘를 다시 쓰면 이렇게 됩니다. 이렇게 그렇죠 이렇게 됩니다. 그러고 나서 얘를 우리가 또 간단하게 얘를 쓰면은 좀 더 생략해서 쓰면 익스펙테이션으로 쓸 수 있겠죠. 요거 자 이거에 대한 우리 익스펙테이션 그쵸? 근데 모든 qi에 대해서 하는 건데 제는 아닌 아이에 대해서 우리가 익스펙테이션을 취하는 겁니다. 그래서 이렇게 되어 있는 거고 얘가 그대로 오게 됩니다. 자 그러고 나서 우리가 얘를 로그 피틀다 스콤마 제제라고 할게요. 얘가 뭔지는 저희가 나중에 좀 이따 말씀드리겠습니다. 얘를 이렇게 우리가 표현하겠습니다. 그러면 어떻게 돼요? 그러면은 자 얘랑 얘랑 이제 엮어서 이렇게 쓸 수 있겠고 그러면 케일 레벤스가 짜잔하고 나오게 됩니다. k를 잡으면서 정의에 따라서 그러면 뭐가 됩니까? 결국 qj는 얘랑 같아지면 돼요. 우리가 qj를 얘로 잡으면 얘가 가장 맥시마이즈가 된다는 겁니다. 왜 얘는 0 이상이니까 얘가 0일 때가 가장 좋은 거잖아요. 얘가 가장 커지는 거니까 그래서 q j를 얘랑 동일하게 잡으면 된다 입니다. 그럼 얘가 뭔데 얘가 뭔데 그러니라고 생각할 수가 있겠죠. 얘가 뭔지 한번 봅시다. 얘가 뭔지는 한번 보면은 자 얘를 우리가 정의 그대로 쓰면 이렇게 써지는 거예요. 그렇죠 여기 있는 거 그대로 썼습니다. 그렇죠 정의를 그대로 쓴 것뿐입니다. 자 그러고 나서 우리가 뭐 이렇게도 표현할 수 있겠죠 z 제만 또 여기서 쏙 빼우고 제제가 아닌 것들만 또 이렇게 조건부 확률로 우리가 쓰게 되면은 이거 두개 곱하면 PX 콤마 z랑 같다는 것을 알 수 있게 됩니다. 자 그러고 나서 얘를 또 우리가 표현하면은 이렇게 되겠죠 그렇죠 자 얘를 표현하면 우리가 이런 식으로 왜 로그에 이거 더하기 로그에 이거니까 그냥 그거 2개를 분리한 겁니다. 로그 이렇게 아시겠죠? 자 그러고 나서 우리의 관심사는 결국 여기 이쪽에 우리가 있는 거니까 그렇죠 그래서 얘를 우리가 모델링 하게 되면은 이렇게 바뀌게 되는 것을 볼 수가 있습니다. 여기까지 괜찮으신가요? 자 그러면은 얘가 결국 얘라는 뜻이에요. 그렇죠 c는 우리가 우리가 알 수 없는 상수 그러고 나면은 만약에 이게 잘 이해가 안 간다 이거보다 더 쉬운 유도 과정을 뒤에 나옵니다. 아직 포기하지 마시고 그러면 우리는 뭐다 얘가 얘랑 같아지면 된다 그러면 얘가 가장 맥시마이즈 된다입니다. 그러면 우리는 로 바운드가 가장 맥시마이즈가 되는 q j를 찾은 거예요. 여러분 그래서 우리가 q j를 이렇게 두는 게 가장 얘가 맥시마이즈 된다라고 생각을 할 수가 있고 그러면 로그를 취하면 양 옆에 로그 취하면 되죠. 그렇죠 그다음에 얘는 결국에 어떤 식이었나요? 이 피틀다라는 거는 우리가 여기 정확하게 얘는 아니고 뭔가 더 있는 거였어요. 그렇죠 그래서 우리가 정확하게 등으로는 표현이 안 되지만 이렇게 비례 관계로 나타낼 수가 있겠습니다. 자 그러고 나서 다시 익스프레션을 취해줘요. 그러고 나서 여기 나와 있는 큐도 우리가 확률 분포라고 생각하면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 10,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1052,
      "char_count": 1981
    },
    {
      "id": "transcript_mlforrecsys_mlforrecsys_3강_변분추론_i_c011_e55a17",
      "content": "[MLforRecSys] [MLforRecSys] (3강) 변분추론 I\n\n다. 자 그러고 나서 다시 익스프레션을 취해줘요. 그러고 나서 여기 나와 있는 큐도 우리가 확률 분포라고 생각하면 됩니다. 서메이션 했을 때 1이 되고 또는 인티그라 했을 때 1이 돼야 되는 거예요. 그래서 우리가 그게 가능하도록 노멀라이징 컨스턴트를 써줘야 되는 거죠. 그래야지 이걸 전체로 접근했을 때 1이 되게 만들 수가 있으니까 그래서 이거에 대한 접근값 노멀라이징 콘서트가 이렇게 추가된 형태로 규제를 이제 구할 수가 있게 되는 겁니다. 아시겠죠? 이게 mfva의 공식입니다. 자 여러분 저희가 앞에서 다음과 같은 엘보를 유도하는 과정을 자세하게 한번 살펴봤었는데요. 그 과정이 저희가 자세하게 한 줄 한 줄 살펴보긴 했지만 쉬운 과정은 아니었을 것 같습니다. 그거를 저희가 또 다른 방식으로 더 쉬운 방식으로 유도할 수 있는 것을 저희가 이 페이지에서 잠깐 한번 살펴보도록 하겠습니다. 자 여기 한번 보시면 엘보가 이렇게 생겼다라는 거는 앞에서 우리가 계속해서 정리했던 게 똑같습니다. 앞에서 썼던 내용을 우리가 계속해서 반복해서 쓰고 있는 것이고요. 자 여기서 우리가 로그랑 익스포넨셜을 같이 한번 여기서 붙여주겠습니다. 그러면 여러분 뭐 로그랑 익스포넨셜 우리가 같이 하니까 상세해진다고 생각하면 그렇죠 우리는 등호 관계라고 쓸 수가 있습니다. 자 그러고 나면은 여기 로그랑 익스포넨션 쓰고 나면 등호가 됐고 그러면 여기서 우리가 이 로그를 놔두고 여긴 이렇게 한번 생각해 봅시다. 여기 익스포넨셜이 있고 또 여기도 우리가 뭐가 있나요? 여기 큐가 여기 있으니까 큐로 우리가 묶어주면 여기 로그 익스포넨셜 그쵸 로그 큐 제이가 됩니다. 즉 우리가 이 q 제라는 거를 이렇게 앞에 묶어주면은 로그가 이렇게 오게 되고 그다음에 분자에 이렇게 익스포넨셜이 오고 분모에 이렇게 q 제가 오게 되는 형태로 우리가 바꿀 수가 있게 됩니다. 자 그러면 이거는 여러분 뭐랑 같나요? 앞에서 KL 다비넌스의 정의를 한번 그대로 활용을 한번 해보시면 얘는 KL 다버전스인데 그때 앞에 마이너스가 하나 붙어 있는 KL 다버전스로 우리가 해석을 할 수가 있습니다. 그렇죠 여러분 왜 우리가 여기 나와있는 q 그다음에 로그 p 분의 q는 KL q p고 q 로그 q 분의 피면 그때는 또 여기 마이너스가 붙는다고 하지 않았습니까? 왜 로그의 분자 분모 바꾸면 우리가 앞에 마이너스 달아주면 분자 분모 바꿀 수가 있잖아요 그 얘기를 하고 있는 겁니다. 즉 케이엘 다이버전스의 정의에 따라서 우리는 이렇게 케엘로 쓸 수 있게 된다라는 이야기입니다. 즉 우리가 앞에서 좀 비교적 복잡하게 좀 엄밀하게 우리가 유도를 했지만 또 이렇게 로그랑 익스포넨셜의 이러한 트랙을 이용해서도 우리가 비교적 간편하게 유도할 수 있는 점 참고하시면 되겠습니다. 자 그다음에 사실 이러한 과정을 또 유도하는 또 다른 방법이 있습니다. 이걸 유도하는 또 다른 방법은 다음과 같이 펑셔널 디리버티브를 우리가 이용하는 건데요. 이 펑셔널 디리버티브를 유도하는 과정은 저희가 조금 이따가 네 나중에 저희가 또 살펴볼 기회가 있으면 살펴볼 수 있도록 하겠습니다. 잠깐 말씀만 드리자면 결국 펑셔널 디베티브라는 것은 얘를 맥시마이즈 할 거면 얘를 맥시마이즈 하는 뭔가를 찾고 싶다. 그러면은 그냥 얘를 미분하면 되는 거 아니냐 왜 우리가 뭐에 대한 맥시멈을 찾을 때 항상 미분해서 0 되는 걸 찾지 않습니까? 이것도 우리가 미분하면 되는 거 아닌가라고 생각할 수가 있고 실제로 그러면 그런 미분을 어떻게 할 건지 왜 이거는 다른 거랑 약간 달라요. 이거는 한번 보시면 우리가 찾고자 하는 게 뭐예요? 큐예요 q 자체가 하나의 함수입니다. q가 가오션 분포라든지 그렇죠 이게 하나의 함수예요. 함수에 대해서 우리가 미분을 해야 되는 겁니다. 그렇죠 그래서 이런 걸 우리가 베리에이셔널 디리베트 또는 펑셔널 디리뷰트라고 부르는데요. 이거에 대한 개념을 저희가 한번 살펴봐야 됩니다. 그래서 이거는 저희가 다음번에 또 기회가 있으면 같이 한번 보도록 하겠습니다. 네 여러분 고생 많았습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MLforRecSys] (3강) 변분추론 I.json",
        "lecture_name": "[MLforRecSys] (3강) 변분추론 I",
        "course": "MLforRecSys",
        "lecture_num": "3강",
        "lecture_title": "변분추론 I",
        "chunk_idx": 11,
        "total_chunks": 12,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:8894e4d9c0b6cf202bcb5fcf969fe304ac2179dc8f07ff97c3e9d5937c8992a8"
      },
      "token_estimate": 1090,
      "char_count": 2022
    }
  ]
}