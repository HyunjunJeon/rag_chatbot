{
  "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
  "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
  "course": "Deep Learning Basic",
  "total_chunks": 9,
  "chunks": [
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c000_b86b83",
      "content": "[강의 녹취록] 과목: Deep Learning Basic | 강의: 4강 | 제목: 기초 신경망 이론 -1 - Neural Networks\n\n안녕하세요. 여러분 ML 라이프 사이클 강의를 맡고 있는 김수경입니다. 4강 시작하겠습니다. 어 저번 3강 시간에 조금 어려우신 내용이 많이 나왔을 수 있을 줄 압니다. 그래서 저번 시간에 저희가 기본적으로 ML 모델을 만들고 트레이닝하고 로스 펑션을 이용해서 최적화하는 아주 가장 개념적이고 되게 중요한 그런 내용들을 굉장히 빠른 속도로 다뤘는데요. 굉장히 중요한 내용이니까 꼭 숙지해서 이해하시기를 바랍니다. 오늘은 저번 시간에 배운 내용보다는 약간 조금 더 편하게 가실 수 있는 기초 신경망 이론 뉴럴 네트워크의 개요와 그 인트로덕션 개괄적인 소개를 이렇게 다루도록 하겠습니다. 그래서 어 이번 시간에 배울 중요한 두 가지는요 저희 리니어 모델 배웠잖아요 그 리니어 모델을 이용해서 이미지를 통해서 이미지 클래스를 분류하는 이미지 분류 예측 모델을 리뷰를 하고 저희가 어 지난 3강 동안 계속해서 배우고 있는 이글 샘플이죠. 그거를 리뷰를 하고 그 테스크에 대해서 어 뉴럴넷으로 확장시켜서 디자인을 만들어 보는 것을 한번 배워보도록 하겠습니다. 자 리니어 모델 저희 여태까지 배운 내용들을 한번 리뷰해 볼게요. 저희가 저번 시간이랑 저번 시간에 이렇게 두 개의 강에 대해서 이미지 클래시피케이션 테스크에 대해서 리니어 클래시피케이션 모델을 만드는 거 배웠죠. 그래서 리니어 모델은 제가 설명드렸듯이 이렇게 FXW로 이렇게 이루어져 가지고 인풋 x에 대해서 가중치 혹은 파라미터 w를 이렇게 곱해서 편향 바이어스 텀을 더해가지고 이렇게 wx 더하기 b 꼴로 나타나는 거를 리뉴얼 모델이라고 했습니다. 그리고 이 모델을 클래시피케이션 테스크에 적용하기 위해서 이 모델 뒤에다가 어 소프트맥스 액티베이션을 씌웁니다. 그래서 소프트맥스 로스를 이용해 가지고 이 아웃풋이 0이랑 1 이 2개만 나오게 확률 분포처럼 이렇게 나오게 우리가 하는 거를 배웠어요. 그래서 로스 펑션 중에 여러 가지 종류를 배웠고 그중에 클래시피케이션에 적용할 수 있는 소프트 맥스를 적용하면은 이렇게 되는 겁니다. 그리고 이거를 어떻게 최적화를 한다고 그랬어요? 요 블랑 b를 갖다가 최적화하기 위해서 이 매개 변수들이라고 그러죠. 파라미터들을 스토캐스틱 그레디언트 디센트에 의해서 최적화를 한다고 그랬어요. 스토캐스틱 그레디언트 디센트 리뷰 하면은 기억나세요? 그러니까 여러분들이 어떤 노스 펑션을 미니마이즈 하기 위해서 어떤 파라미터 스페이스에서 처음에 시작을 해서 마치 산 위에서 내려가는 것처럼 기울기를 각각 포인트에 대해서 계산을 해서 가장 가파르게 내려가는 방향으로 계속해서 해서 내려가다 보면은 결국은 산 밑으로 내려가게 되겠죠. 그렇게 산 밑으로 내려갔을 때에 파라미터를 갖다가 테이크 해가지고 그 파라미터로 이 블랑 b 값을 정하는 것이 그레디언트 디센트입니다. 스토캐스틱 그레디언트 디센트는 기억나세요? 요 앞에 스토캐스틱 자를 붙였죠 그건 뭐냐면은 우리가 예를 들어서 어떤 파라미터를 업데이트할 때 모든 데이터 샘플에 대해서 하지 않고 일부의 미니 배치 샘플에 대해서만 파라미터를 기울기를 그레디언트를 계산해 가지고 업데이트를 하는 거를 스토캐스틱 그레디언트 디센트라고 그랬어요. 이런 경우에는 미니 배치 사이즈를 갖다가 조절을 하면서 우리가 스토캐스틱 그레디언트 디센트를 업데이트를 할 수가 있고 기울기가 항상 음수가 이렇게 안정적으로 줄어들지 않기 때문에 러닝 커브를 보면 이렇게 지글지글지글지글하다고 그랬어요. 그래서 그 러닝 커브를 이렇게 옵티마이즈를 했을 때 멈추는 법을 갖다가 계산을 하기 위해서 이렇게 로스가 지글지글 떨어질 때 그 얼마나 떨어지는지 그 변화량을 갖다가 측정을 해 가지고 요 로스가 변화량 얼마 이하로 되면은 이제 트레이닝을 멈추는지 이런 거를 이렇게 러닝 커브를 보면서 여러분들이 어떤 그 실험에 대해서 자율적으로 조절을 해가지고 할 수 있다고 이렇게 배웠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 0,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1079,
      "char_count": 1974
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c001_22374a",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 그래서 이게 저번 시간에 배운 다예요. 모델을 갖다 트레이닝을 하기 위해서 우리가 모델 정의하고 로스 정의하고 요 로스를 갖다가 최적화하는 그레디언트 디센트 방법을 이용해서 최적화하는 방법을 갖다가 적용을 해서 어 최적화를 한다 이게 머신러닝 모델을 트레이닝하는 어떤 하나의 프로토타입적인 스텝입니다. 자 그래서 우리가 이렇게 리니어 클래스 파이어를 저번 시간부터 이렇게 쭉 다뤄왔는데요. 실은 리니어 클래스 파이어는 잘 쓰이지가 않아요 계속 이렇게 뭔가 제가 어떤 설명을 드리고 그거의 단점을 말씀드리고 그거에 보완되는 방법을 말씀드리고 계속 이렇게 하는데 저희가 리니어 클래식 파이어가 리얼리스트 네이버 클래시스 파이어보다 훨씬 좋은 장점을 가지지만 실은 실제 현업에서는 잘 쓰이지가 않습니다. 왜냐 왜냐하면은요 여기 여러 가지 이유가 있는데 첫 번째 이유는 시각적으로 봤을 때 각 클래스 당 하나의 템플릿만 학습할 수 있습니다 라고 쓰여 있죠. 그게 뭔 말이냐면요 저희가 아까 그 멀티 레이블 클래시피케이션 테스크를 봤는데 여기 이 그림에서도 그렇죠 이 멀티 레이블 클래시피케이션 테스크에서 봤을 때 어 여러 가지의 그 클라스를 갖다가 우리가 분류를 해야 되는데 각 클래스 당 하나의 웨이 파라미터만 배우는 문제가 있어요. 네 다시 한 번 말씀드릴게요. 요 리니어 클래식 파이어는요 기본적으로 우리가 그 모델을 프스이꼴 블스 꼴로 정리를 해 놓고 이 더블를 학습하는 방법이죠. 그리고 요 w는 실제적으로 어떤 시멘틱한 의미를 가지냐면요 각각의 클래스에 해당하는 어떤 이미지의 패턴을 갖다가 템플릿화해가지고 학습하는 그런 효과를 가지고 있어요. 그래서 보시면 예를 들어 이 말 같은 것도 저희가 저번 시간에 봤듯이 이 웨이 파라미터를 비주얼라이즈 해보면 이 말 같은 경우는 실제로 말이 이렇게 보이고 차 같은 경우는 차가 보이고 이런 템플렛을 배우게 된다고 그랬어요. 그런데 어 이렇게 템플릿 별로 다양한 리프레젠테이션을 배우는 게 아니고 딱 하나의 이미지밖에 배우지를 못해요. 그건 무슨 말이냐면요. 하나의 그 클래스당 하나의 분류 클라스 당 딱 하나의 웨이트 팩터들 즉 딱 한 가지의 플랫폼 그러니까 템플렛 더블를 배운다는 거예요. 이 차 같은 경우에 차를 앞에서 봤을 때 템플릿을 이렇게 데이터를 통해서 배운 것 같은데 실은 이 차는 이 템플릿 하나만으로는 디파인 되지 않죠 차를 옆에서 볼 수도 있고 차를 뒤에서 볼 수도 있고 차가 이렇게 갈 때 쌩 하고 갈 때의 이미지를 어 이미지를 딱 이렇게 테이크 해 가지고 그 패턴을 배울 수도 있고 사실 차라는 클라스가 굉장히 다양한 리프리젠테이션을 가지고 있는데 이렇게 딱 하나의 템플릿만으로 정의가 된다는 거예요. 그래서 이렇게 이미지를 분류하기 위해서 굉장히 다양한 패턴의 템플릿을 학습시켜야 할 필요가 있는데 리니어 클래식 파이어는 그렇게 할 수 없다는 치명적인 단점이 있습니다. 자 그리고 두 번째 단점이 있어요. 지오메티컬리 즉 기하학적으로 우리의 리니어 클래식 파이어는 가정이 뭐예요? 디시전 바운더리가 항상 리니어하다는 가정을 가지고 있죠. 저희가 첫 번째 강의랑 두 번째 강에서 배웠어요. 리니어한 모델은 이 디시전 바운더리가 리니어하다라는 가정을 가지고 있습니다. 기본적으로 우리의 그 모델 자체가 선형 모델이기 때문에 리니어 모델이기 때문에 항상 디시전 바운더리가 직선 형태로 그려집니다. 하지만 생각해 보면요 세상에 많은 문제들이 직선이 아닌 곡선 형태의 디시전 바운더리가 있어야만 풀릴 수 있는 문제들이 되게 많습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 1,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 979,
      "char_count": 1793
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c002_451ffe",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 기본적으로 우리의 그 모델 자체가 선형 모델이기 때문에 리니어 모델이기 때문에 항상 디시전 바운더리가 직선 형태로 그려집니다. 하지만 생각해 보면요 세상에 많은 문제들이 직선이 아닌 곡선 형태의 디시전 바운더리가 있어야만 풀릴 수 있는 문제들이 되게 많습니다. 그래서 요 이그 샘플이 2개가 있어요. 이 그래프에 보이는 바와 같이 지금 요 빨간색 점이랑 파란색 점들이 이렇게 혼재돼 가지고 곡선을 이루고 있죠. 이런 경우에 우리가 디시전 바운더리를 그려서 요 빨간색 점이랑 파란색 점을 클리어하게 클래시파이 하고 싶어요. 그러면은 어떻게 직선을 구워도 이 빨간색 점이랑 파란색 점이 구분되지 않는다는 것을 우리가 알 수가 있어요. 이것도 마찬가지죠. 빨간색이 동그라미고 파란색이 또 동그라미인데 빨간색 동그라미 안에 파란색 동그라미가 있죠. 이거를 어떤 방향으로든 어떤 방법으로든 이렇게 리니어하게 디시전 바운더리를 선형적으로 그려봐야 이 점들 두 개가 이 두 개의 클러스터가 깔끔하게 분리가 되지 않아요. 따라서 기하학적으로 봤을 때 리니어 모델을 이용해서 만들어진 어떤 클래스 파이어는요. 복잡한 관계로 섞여 있는 이런 비선형적인 클래스들을 완벽하게 분리해 낼 수 없는 굉장히 치명적인 단점이 있어요. 그래서 이를 해결하기 위한 방법이 피처라이제이션입니다. 여기서 피처라고 써 있죠 여러분들 머신 러닝 배우게 되면은 피처 피처라이제이션 요런 텀들이 많이 사용이 될 거예요. 기본적으로 그 개념에 대해서 배우도록 할게요. 저희가 아까 그 선형 모델은 비선형적으로 분포되어 있는 데이터를 분류할 수 없다라고 말하면서 나온 이그 샘플이 이 케이스였어요. 이렇게 하나의 클라스가 동그랗게 분포하고 다른 클라스가 그 가운데에 있는 그 안에 있는 동그란 데이터셋으로 분포를 한다고 생각을 해봅시다. 이 케이스가 스랑 y 좌표에서 이렇게 퍼져 있는 데이터 포인트들의 분포를 나타내고 있다고 볼게요. 이거는 스랑 와의 좌표계예요. 스와 코디네트예요. 자 이 점을 그대로 갖고 와 가지고 폴라 코디네이트로 보내볼게요. 폴라 코디네이트는 가로축이 어 알이고 즉 반지름이고 세로축이 세타 앵글로 표현된 어떤 좌표기입니다. 즉 이 x y 좌표를 r과 세타 좌표로 보내는 겁니다. 그러면 어떻게 될까요? 이 빨간색은 요 빨간색 하나의 좌표에 대해 r은 요거고 그러니까 그 중심 연콩마 0에서 요 빨간 점까지의 거리 그걸 알이라고 그러고 세트 안은 이렇게 선을 그었을 때 이 선이랑 스축과의 앵글이죠. 그렇게 바꿔 가지고 변환을 했더니 요 빨간 점은 이렇게 선이 그려지고 파란 점은 이렇게 선이 그려지죠 이해되세요? 요 x y 좌표를 갖다가 r세타 좌표로 변환을 했더니 이렇게 빨간 점이랑 파란 점이 리니어하게 세퍼러블하게 바뀝니다. 그렇죠 그래서 이게 기본적인 피처라이제이션의 아이디어예요. 어떤 인베딩 스페이스에서 어떤 좌표계에서 데이터가 우리가 분류하지 못하는 어려운 형태로 분포됐을 때 이 점들을 이 데이터 점들은 다른 스페이스로 보내서 다른 좌표계로 보내서 이 데이터 포인트들이 깔끔하게 우리의 모델로 분리될 수 있는 형태로 만드는 거를 피처라이제이션이라고 그래요. 그래서 어 직관적으로 생각을 해보면은 우리가 어떤 좌표계가 있어요? 그 좌표계에서 데이터들이 막 분포가 되겠죠. 그 좌표계를 막 꾸겨요. 다양한 방법으로 막 꾸겨서 다른 좌표계로 만들어서 그 점들이 우리가 가지고 있는 이 선형 모델로 완전하게 분리가 될 수 있도록 이 좌포계를 구겨 가지고 점들을 다른 스페이스로 보내는 겁니다. 즉 하나의 그러니까 우리의 데이터셋을 어 우리의 모델을 이용해서 처리하기 쉬운 형태로 바꾸는 이런 작업을 우리가 피처라이제이션이라고 합니다. 그리고 이렇게 변형된 데이터들을 우리는 피처라고 말합니다. 그래서 이 오른쪽 데이터들은 즉 폴라 코디네이트에 있는 이 데이터들은 이 XY 좌표기에 있는 왼쪽 데이터들의 어떤 피처라고 생각될 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 2,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1078,
      "char_count": 1978
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c003_53b61c",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 즉 하나의 그러니까 우리의 데이터셋을 어 우리의 모델을 이용해서 처리하기 쉬운 형태로 바꾸는 이런 작업을 우리가 피처라이제이션이라고 합니다. 그리고 이렇게 변형된 데이터들을 우리는 피처라고 말합니다. 그래서 이 오른쪽 데이터들은 즉 폴라 코디네이트에 있는 이 데이터들은 이 XY 좌표기에 있는 왼쪽 데이터들의 어떤 피처라고 생각될 수 있습니다. 그래서 기본적으로 피처는 원본 데이터의 성질은 그대로 유지하되 다른 좌표계 혹은 다른 인베딩 스페이스라고 그러죠. 인베딩 스페이스로 보내가지고 더 처리가 잘 되게 만드는 것이라고 볼 수가 있습니다. 그래서 읽어보시면 입 출력 관계들을 리니어 클래스 파이어로 직접 연결하는 대신에 입력을 표현하기 위해 몇 가지의 특징들을 추출하는 것 그런 과정을 일련의 과정을 피처라이제이션이라고 그래요. 즉 한국말로 하면 특징 추출 과정이라고 보시면 됩니다. 그래서 만약 이런 피처 스페이스에서 입력이 이렇게 입력이 선형 분류가 가능하다면 이 데이터 스페이스에서는 리니어 클래식 파이어도 잘 동작할 수가 있습니다. 그래서 기본적으로 이 슬라이드에서 여러분들이 기억해야 될 거는 피처라이제이션이란 뭐냐 우리가 원본 데이터 스페이스에서 우리가 가진 모델로 이 테스크를 수행할 수 없을 때 이 데이터를 다른 스페이스로 보내서 처리가 되게 만드는 과정을 피처라이제이션이라고 그러고 처리한 데이터들을 피처라고 말합니다. 아시겠죠? 자 그러면은 저희가 지금 이미지 데이터를 다루고 있기 때문에 피처라이제이션 중에서 이미지 데이터를 피처라이제이션 하는 어떤 어 예전의 그 고전적인 방법들을 한번 보도록 할게요. 그래서 고전적인 그런 피처라이제이션 테크닉들은 굉장히 룰 베이스대로 이루어졌습니다. 즉 우리가 어떤 방법을 딱 정해놓고 알고리즘처럼 정해놓고 그 방법을 이미지들에다가 이렇게 적용하는 식으로 이루어져 있었어요. 그래서 직접 이 픽셀 레벨의 입출력 관계를 매핑하는 대신 몇 가지 특징을 추출해서 입력을 표현을 한다고 그랬어요. 그래서 이미지가 주어지면은 이 이미지를 어떤 룰 베이스 된 피처라이제이션 메소드에 기반해 가지고 처리하는 과정을 거쳤습니다. 그래서 예시가 쭉 나와 있는데 한번 볼게요. 예시로 칼라 히스토그램 이렇게 나와 있어요. 어 그냥 예를 들어 이 개구리 사진이 있죠 이 개구리 사진은 실은 픽셀 밸류로 숫자로 이루어진 어떤 3차원 행렬이라고 그랬어요. 그렇게 3차원 행렬로 이 개구리 사진을 표현하는 대신 이 개구리 사진에 있는 어떤 그 색깔의 분포의 히스토그램을 그립니다. 이게 개구리 사진이니까 초록색이 많겠죠. 그래서 이 색깔별 히스토그램을 그려보면은 이 초록색이 이렇게 많다고 이렇게 분포도가 나올 겁니다. 이 컬러 히스토그램을 가지고 와서 이 이미지를 표현하는 어떤 벡터로 쓸 수도 있고 또 여기 히스토그램 오브 오리엔티드 그레디언트라고 호그라고 되어 있는 건데 이것도 되게 고전적인 방법이에요. 색깔이 빨간색에서 파란색이든지 아니면 파란색에서 빨간색이든지 이렇게 각각의 픽셀 당 색깔이 변하는 어떤 그 변화도를 이렇게 맵처럼 나타낸 겁니다. 그냥 어 이런 게 있다고만 알아두세요. 왜냐하면 요새는 잘 쓰이지가 않거든요. 그리고 백 오브 월드처럼 각각 이미지를 패치 패치별로 나눠 가지고 각 패치가 어떤 패턴을 가지고 있는지 그렇게 다 정리해 가지고 각 패치별로 이거는 개구리의 눈이야 이거는 개구리의 발이야 뭐 얘는 잎사귀야 하면서 이렇게 백 오브 월드처럼 백 오브 패치로 이미지를 나타내는 수도 있습니다. 그래서 여러 가지 그 고전적인 이미지의 피처라이제이션 방법들이 있는데요. 이런 것들의 단점은요 굉장히 큰 단점이라고 할 수 있는데 룰 베이스로 이루어진 어떤 고전적인 피처라이제이션 방법의 단점은 데이터마다 우리가 각각의 다른 피처라이제이션 방법을 디자인하고 적용해야 되는 소위 노가다적인 노력이 든다는 게 단점입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 3,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1074,
      "char_count": 1950
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c004_97e8b0",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 그래서 여러 가지 그 고전적인 이미지의 피처라이제이션 방법들이 있는데요. 이런 것들의 단점은요 굉장히 큰 단점이라고 할 수 있는데 룰 베이스로 이루어진 어떤 고전적인 피처라이제이션 방법의 단점은 데이터마다 우리가 각각의 다른 피처라이제이션 방법을 디자인하고 적용해야 되는 소위 노가다적인 노력이 든다는 게 단점입니다. 가령 뭐 예를 들어서 저희가 이미지가 아니고 어떤 대기 데이터를 처리한다고 생각해 볼게요. 이거는 이제 이미지가 아니고 전 지구적인 어떤 대기의 기후의 현상을 나타낸 h 맵 이라고 데이터를 정의를 해보면은 그거를 잘 처리하기 위해서 어떤 과학적인 널리지가 들어간 피처라이제이션 테크닉이 적용될 수 있겠죠. 그리고 뭐 예를 들어서 이런 개구리 이미지 같은 경우는 히스토그램을 갖다가 이용할 수도 있고 즉 각각의 이미지의 특성별로 우리가 그 특성을 잘 나타낼 수 있는 어떤 도메인 널리지를 적용을 해가지고 이렇게 룰 베이스로 피처라이제이션 테크닉을 한 땀 한 땀 사람이 디자인해야 되는 이런 노가다가 필요합니다. 그래서 질문이 아 그러면은 우리가 기본적으로 머신 러닝이라는 거는 데이터셋을 통해서 어떤 패턴을 캐치할 수 있는 모델을 학습하는 거라고 배웠는데 그럼 요 피처라이제이션 테크닉을 갖다가 손으로 한 땀 한 땀 만들지 말고 데이터를 통해서 피처라이제이션 테크닉을 배우는 건 어떨까라고 질문하셨다면은 되게 좋은 질문이시고요. 그게 사실 이 뉴럴넷을 이용해 가지고 이미지를 처리하는 이유입니다. 그래서 아까 앞 슬라이드에서 저희가 그 룰 베이스드로 디자인된 어떤 피처라이제이션 방법의 단점에 대해서 이야기했죠. 기본적으로 저희가 룰 베이스로 디자인된 어떤 특징 추출 피처라이제이션 알고리즘을 갖다가 만들면은 데이터가 바뀌었을 때 그 피처라이제이션 알고리즘이 적용이 안 되는 그런 단점이 있을 수 있어요. 그래서 어 이 룰 베이스 방법은요 기본적으로 그 인풋 데이터에서 특징을 추출한 다음에 추출한 피처에 다시 모델을 돌려서 아웃풋을 예측하는 식이죠. 이 이미지에서 보면은 이 슬라이드에서 보면 알 수 있듯이 이 이미지 셀에 대해서 우리가 피처라이제이션을 해서 피처를 뽑아요. 피처가 제죠 그리고 이 제가 이 룸풋이 돼가지고 우리의 모델 프를 이용해서 아웃풋인 y를 예측하는 시기에요. 그래서 이렇게 피처라이제이션 방법이 인풋 데이터와 맞지 않는 경우에는 당연히 모델의 성능이 좋지 않을 거예요. 그러면 이런 생각을 해볼 수 있어요. 여기 상자에도 써 있듯이 만약 앤드 투 엔드 방식의 모델이 가능해진다면 어떻게 될까요? 피처 익스트랙션 스텝 올소 테이스 그레디언트 프롬 클래시피케이션 로스 이렇게 써 있어요. 만약에 우리가 이 특징 추출이랑 f랑 구별을 안 하고 이 특징 추출을 f의 일부로 합치는 거예요. 그래서 그 뒷단에서 나오는 로스가 이렇게 이 모델뿐만 아니고 특징 추출 알고리즘을 업데이트하는 데도 쓰인다면 어떻게 될까요? 그러면은 좋은 성능을 가질까요? 네 좋은 성능을 가집니다. 그게 사실은 뉴럴넷입니다. 그래서 우리가 원하는 거는 기본적으로 뉴럴넷을 이용해서 이 이미지부터 아웃풋까지 인풋에서 아웃풋까지를 하나의 모델로 만들고 그 모델이 피처라이제이션 뿐만 아니고 클래시피케이션까지 모든 작업을 하나의 프로세스로 완결하게 할 수 있는 게 우리의 목표입니다. 그래서 뉴럴넷을 사용하는 목표는요. 우리가 모델을 만들고 그 모델이 인풋에서 아웃풋을 예측하는 일련의 모든 과정 피처라이제이션뿐만 아니고 클래시피케이션까지 그 과정을 다 담당하게 한 다음에 각각의 그 파라미터들을 엔드 투 엔드로부터 배운다고 생각을 하는 겁니다. 여기서 앤드 투 엔드라고 그랬는데요. 이 앤드 투 엔드는 무슨 말이냐면은 기본적으로 인풋에서 아웃풋으로 가는 모든 과정을 하나의 단일의 모델로 담당하게 한다는 그런 뜻이에요. 네 그래서 여태까지 이제 뉴럴넷을 사용하는 이유에 대해서 설명드린 거고요. 이제 본격적으로 뉴럴넷에 대해서 배워보도록 하겠습니다. 이 부분은 그렇게 어렵지 않으니까 긴장을 좀 낮추시고 편안하게 들으시면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 4,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1123,
      "char_count": 2040
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c005_d8ea96",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 여기서 앤드 투 엔드라고 그랬는데요. 이 앤드 투 엔드는 무슨 말이냐면은 기본적으로 인풋에서 아웃풋으로 가는 모든 과정을 하나의 단일의 모델로 담당하게 한다는 그런 뜻이에요. 네 그래서 여태까지 이제 뉴럴넷을 사용하는 이유에 대해서 설명드린 거고요. 이제 본격적으로 뉴럴넷에 대해서 배워보도록 하겠습니다. 이 부분은 그렇게 어렵지 않으니까 긴장을 좀 낮추시고 편안하게 들으시면 됩니다. 왜 뉴럴넷이 뉴럴넷일까요? 뉴럴 뉴럴 뭐예요? 신경이잖아요. 왜 신경이라는 말을 썼을까요? 그거는요. 기본적으로 뉴럴넷의 동작이 그 인간이나 동물이나 뭐 어떤 생명체의 뇌에 어떤 그 신경 퍼셉트론이라는 신경의 그 오퍼레이션을 흉내 내기 때문입니다. 그래서 이 퍼셉트론이라는 게 기본적으로 인간의 뉴런이에요. 신경망이에요. 여기가 이렇게 그 가지돌기라고 그러는데 영어로는 밴드 라이트라고 합니다. 이 가지 돌기가 외부에서 어떤 자극 시그널을 받아요? 어떤 자극 시그널을 외부에서 이렇게 받는 게 이 가지 돌기예요. 그리고 여기 축삭 돌기라는 게 있는데 이게 엑시온이라는 거예요. 이렇게 자극을 받아서 신호를 받아들이면은요. 요 축삭 돌기에서 자극을 모아 가지고 이 자극이 어떤 트레숄드 이상이면은 그다음 뉴런으로 신경을 전달시킵니다. 여기서 이렇게 막 시그널을 받아요. 외부 자극을 받아요. 그리고 그것들의 합이 어떤 트레숄드가 넘어가면은 이 축삭 돌기가 신호를 하나의 퍼셉트론에서 다른 퍼셉트론으로 이렇게 전달을 하게 되는 거죠. 그런데 이 동작을 갖다가 모사를 해가지고 만든 게 이 뉴럴 네트워크입니다. 뉴럴 네트워크 또는 퍼셉트론이라고 그래요. 그래서 이거를 갖다가 보시면은요. 아까 앞에서 보신 이 뉴런의 신호 전달 체계랑 똑같은 방식으로 이루어져 있어요. 우리가 어떤 그 시그널 인풋이 되겠죠 엑스 제로를 갖다가 받아요. 그리고 이 x 제로에다가 웨이 파라미터인 w0를 곱해가지고 우리의 덴드라이트로 보내는 겁니다. 그리고 이 시그널이 여러 개가 있을 수가 있겠죠. x 제로 x1 x2 이렇게 다양한 인풋이 들어왔을 때 그 인풋에 해당하는 특정 웨이 파라미터들을 곱해 가지고 다 더하는 거예요. 다 더해서 바이어스 텀을 곱해 가지고 그다음에 이게 덴드라이트의 신호 전달 체계가 되겠죠. 이렇게 신호를 전달한 다음에 이 신호가 어떤 트레숄드 이상이면 아웃풋으로 내보내는 거예요. 기본적으로 이 신경 전달 체계랑 똑같다는 거를 알 수가 있겠죠. 그래서 이거를 갖다가 그대로 컴퓨터로 구현을 한 게 바로 뉴럴 네트워크인 겁니다. 그래서 하나하나 뜯어서 보시면 이렇게 다양한 외부 신호를 갖다가 받아들일 때 이 외부 신호를 갖다가 x라고 우리가 표현을 할 수가 있죠. 그리고 각각의 외부 신호에다가 각각의 웨이 파라미터 여기 써있는 것처럼 w제 w1 w2 x에다가 이렇게 곱했죠 그렇게 곱해가지고 더하는 과정이 이 wx의 과정입니다. wx의 과정이고 그리고 그다음에 그 신호들을 다 합해서 그 신호들이 어떤 트레숄드 이상이면 아웃풋으로 내보내는 거죠. 이게 액티베이션 펑션 시그모이드 같은 액티베이션 펑션이 되겠죠 그래서 기본적으로 우리가 앞에서 배운 소프트 맥스 클래시파이어는요 기본적으로 퍼셉트론이랑 똑같은 방법으로 이루어진다고 생각을 하시면 돼요. 그래서 소프트 맥스 클레이스 파이어는 요 펄세트론 혹은 뉴럴 네트워크의 특수 케이스라고 보시면 됩니다. 자 여기서 한 가지 이제 짚고 넘어갈 게 만약에 이 액티베이션 펑션이 없다면 이렇게 액티베이션 펑션이 유닛 펑션처럼 이렇게 아로 나타낸다면은 뭐가 될까요? 리니어 리그렉션이랑 똑같아요. 기본적으로 인풋에다가 웨잇을 곱해 가지고 아웃풋 와를 예측하는 거니까 이 프가 없다면 즉 액티베이션 펑션이 없다면 리니어 리그레션 모델이랑 똑같아집니다. 즉 리니어 리그레션 모델에 액티베이션 펑션만 붙인 게 퍼셉트론이에요. 굉장히 간단하죠. 그래서 이거를 갖다가 이제 컴퓨테이션을 네트워크로 한번 표현해 볼게요. 우리가 x가 들어왔다 그 x를 갖다가 블랑 곱합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 5,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1096,
      "char_count": 2016
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c006_b835ee",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 즉 리니어 리그레션 모델에 액티베이션 펑션만 붙인 게 퍼셉트론이에요. 굉장히 간단하죠. 그래서 이거를 갖다가 이제 컴퓨테이션을 네트워크로 한번 표현해 볼게요. 우리가 x가 들어왔다 그 x를 갖다가 블랑 곱합니다. 그래서 아웃풋을 내보내죠. 그래서 이거를 어 네트워크처럼 이렇게 표현을 해보면은 이렇게 플리 커넥티드 된 어떤 네트워크 형태로 우리가 표현을 할 수가 있어요. 그리고 이 각각의 노드랑 노드를 연결하는 이 선이 그 웨이 파라미터가 되는 거죠. 즉 x의 각각의 엘라먼트에다가 아웃풋을 예측하기 위해서 각각 다른 웨이 파라미터를 곱해주는 거예요. 그렇죠 그래서 이거를 이제 여러 창을 한번 여러 층을 한번 쌓아 볼게요. 여러 층을 쌓으면은 이렇게 한 번 층을 쌓고 두 번 층을 쌓고 이렇게 될 때 웨이 파라미터를 각각 다른 거를 두 번을 곱해주면 돼요. 예를 들어서 첫 번째 인풋 디멘전이 디라고 하면은 웨이 파라미터 원은 디 디멘전을 갖다가 아웃풋 디멘저인 치로 바꿔 줘야 되니까 크기는 HVD가 되겠죠. w1은 그리고 아웃풋의 사이즈는 h가 되겠죠. 그리고 h를 갖다가 마지막 아웃풋 사이즈인 씨로 만들어 준다면은 w2는 뭐가 되겠어요? CVH가 되겠죠. 그래서 기본적으로 행렬 고비고 여전히 또 이거를 컴퓨테이션을 그래프로 그리면은 이렇게 표현될 수 있어요. 어려운 거 없죠. 근데 여기서 이제 여러분들이 눈치채셨을 수도 있겠지만 기본적으로 두 층을 쌓나 한 층을 쌓나 똑같다는 걸 알 수가 있어요. 만약에 이렇게 두 층을 쌓잖아요. 그러면은 x에다가 w1을 곱해서 h가 되고 거기다가 블2를 곱해서 스가 되는 구조예요. 그래서 얘를 갖다가 수학식으로 행렬 곱으로 표현을 하면은 먼저 스 곱하기 더블1 더블원에다 스를 곱해요. 그게 이 첫 번째 레이어예요. 그게 이 부분이 h가 되는 거죠. 그리고 요 h에다가 w2를 곱해가지고 아웃풋이 나오게 되죠. 즉 w1을 먼저 곱하고 그다음에 블2를 곱하니까는 수학식으로 이렇게 쓰여질 수가 있어요. 그런데 실은 각각의 행렬 곱은 하나의 행렬 곱으로 나타낼 수가 있어요. 즉 다시 말하면 블2 곱하기 1은 또 다른 더블죠. 즉 멀티 레이어 레이어는 여전히 리니어합니다. 왜냐 이 더블들을 곱하면은 여전히 또 다른 행렬로 나타내기 때문에 즉 리니어 모델을 여러 층을 많이 쌓아봤자 결국은 그 모델은 리니어 모델이라는 거죠. 만약에 우리가 리니어 모델 더블를 100개를 쌓았어요. 그러면은 w1 w2 w3 해서 100까지 곱한 거랑 똑같잖아요. 그거는 여전히 다른 행렬 더블로 표현이 될 수가 있다는 거죠. 그러면은 사실은 리니어 모델을 여러 개 층을 쌓을 필요가 없어요. 왜냐하면 결국은 여러 층을 쌓던 뭐 몇 층을 쌓건 간에 결국은 리니어 모델로 표현될 수 있기 때문이죠. 그러면 여기에 질문 어떻게 하면은 요 리니어 모델을 넌 리니어하게 만들 수 있을까요? 그게 바로 액티베이션 펑션입니다. 각각의 레이어가 끝날 때마다 우리가 어떤 액티베이션 펑션을 넣으면은 여러 층을 쌓는 게 의미가 있겠죠. 가령 우리가 이렇게 w1 w2 이렇게 곱해 가는 게 아니고 각각의 레이어가 끝날 때마다 이 액티베이션 펑션인 a1이랑 a2를 통과하게 만듭니다. 그러면은 이 가운데에 각각의 레이어가 끝날 때마다 넌 리니어리티가 이 모델의 인듀스가 되니까 이 모델 자체가 훨씬 더 복잡해지게 됩니다. 그래서 어 액티베이션 펑션은 사실 별게 아니고 우리가 앞에서 로스 펑션 할 때 배운 이런 시그모이드 펑션 같은 걸 써요. 그래서 시그모이드 펑션은 아웃풋을 0이랑 1이 되게 강제하는 이런 펑션이죠. 그래서 뉴럴넷 통과한 다음에 0과 1이 되게 강제해서 확률 분포로 놔두고 그다음에 또 뉴럴넷 통과한 다음에 0과 1로 강제하는 이런 식으로 중간중간에 이 시그모이드 펑션을 넣어줍니다. 그리고 비슷한 모양으로 생긴 어 탄젠트 치랑 랠로 펑션 같은 것들이 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 6,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1036,
      "char_count": 1959
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c007_90651e",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 그래서 어 액티베이션 펑션은 사실 별게 아니고 우리가 앞에서 로스 펑션 할 때 배운 이런 시그모이드 펑션 같은 걸 써요. 그래서 시그모이드 펑션은 아웃풋을 0이랑 1이 되게 강제하는 이런 펑션이죠. 그래서 뉴럴넷 통과한 다음에 0과 1이 되게 강제해서 확률 분포로 놔두고 그다음에 또 뉴럴넷 통과한 다음에 0과 1로 강제하는 이런 식으로 중간중간에 이 시그모이드 펑션을 넣어줍니다. 그리고 비슷한 모양으로 생긴 어 탄젠트 치랑 랠로 펑션 같은 것들이 있습니다. 그래서 이런 것들은 사실 이번 강의에서는 크게 다루지 않을 것이고 여러분들이 모델을 만들 때 필요하신 액티베이션 펑션을 문제에 맞게 디자인해서 어 모델 중간중간에 이렇게 인클루드 하시면 됩니다. 그래서 기본적으로 이 뉴럴 네트워크의 구조는 다음과 같습니다. 먼저 인풋 레이어가 있어요. 인풋 레이어에 우리가 원하는 인풋 파라미터들이 이렇게 들어가요. 그런 다음에 웨이 파라미터를 곱해줘요. 그다음에 액티베이션 레이어를 넣어요. 이렇게 시그모이드 같은 거를 이렇게 넣고 그다음에 그다음 단에 레이어가 들어갑니다. 웨이 파라미터 넣고 액티베이션 펑션 넣고 이렇게 웨잇이랑 액티베이션 펑션 웨잇이랑 액티베이 펑션 이거를 무수히 반복하면 굉장히 복잡한 비선형적인 패턴을 갖는 데이터 셋에 대해서도 리프레젠테이션을 그 데이터에 내재된 어떤 패턴을 잘 배울 수 있게 되게 됩니다. 그러면은 저희가 저번 시간에 최적화에 대해서 배웠죠. 어떻게 그레디언트를 계산을 하고 어 그 그레디언트를 이용해서 로스를 미니마이즈 하는 스토캐스틱 그레디언트 디센트 방법을 적용하는지 그걸 배웠어요. 요 식은 저번 슬라이드에서 나왔던 식입니다. 이 파라미터 세터를 이 코스트 펑션 혹은 로스 펑션을 그래디언트를 취함으로써 옵티마이제이션을 하는 이 어 그레디언트 디센트 식이에요. 이걸 보아하면은 이걸 이용을 하면은 우리는 사실 우리의 그 뉴럴넷에서 해당하는 이 섹터는 블죠 우리의 더블를 옵티마이제이션 하기 위해서 이 그래디언트 디센트 식을 갖다가 쓸 수가 있을 거예요. 그래서 이 제 대신에 로스를 넣고 이 세터 대신에 우리가 원하는 파라미터인 w1 w2를 넣으면은 요 식을 계산을 하기 위해서는 이 부분을 계산해야 된다는 걸 알 수가 있어요. 즉 이 부분에 해당되는 게 이거죠. 스토캐스틱 그레디언트 디센트에서는 클래시피케이션 로스에 대한 그레디언트를 필요로 한다. 즉 이 로스가 그 우리의 뉴럴 네트워크의 예측 값이랑 실제 그라운드 트루스 사이의 차이가 되겠죠. 이 로스가 주어졌을 때 우리가 이 더블유를 갖다가 옵티마이즈 하기 위해서는 이 로스를 각각의 w로 미분해 줘야 되겠죠. 이거를 구할 수 있어야지 우리가 그레디언트 디센트를 이용해 가지고 최적의 w를 찾을 수 있는 거예요. 그렇죠 즉 우리가 이 뉴럴넷을 트레이닝 하기 위해서는 로스를 각각의 파라미터에 대해서 미분할 수 있어야 되는 거죠. 그러면은 여기서 질문 이거를 갖다가 어떻게 구하느냐 이거를 갖다가 이제 볼 거예요. 그래서 어떻게 하면은 이 그래디언트를 배울 수 있느냐 저희가 이거는 편미분이라는 방법을 이용할 거예요. 그래서 보시면은 이건 굉장히 심플한 케이스니까 한번 쭉 볼게요. 이게 클래시피케이션 테스크라고 생각을 해봐요. 그럼 로스는 뭐가 되겠어요? 로스는 예를 들어서 이런 로스 펑션을 이용하기로 해요. 예측 값이랑 관측값의 차이를 제곱한 거예요. 그리고 이 y를 갖다가 수학식으로 나타내면은 스에다 더블원을 곱하고 시그모이드를 씌운 다음에 블투 곱한 게 와 프라임이",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 7,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 971,
      "char_count": 1787
    },
    {
      "id": "transcript_deep_learning_basic_4강_기초_신경망_이론_1_neural_networks_c008_aa53ce",
      "content": "[Deep Learning Basic] (4강) 기초 신경망 이론 -1 - Neural Networks\n\n다. 그렇죠 그러면은 우리가 원하는 거는 이 로스를 각각의 블1 블2에 대해서 미분을 해야 돼요. 그럼 먼저 뒤에서부터 할게요. 로스를 블2에 대해서 미분을 해야 돼요. 그런데 이 로스라는 펑션은요. 블2에 대한 식이 아니죠. 와 프라임이랑 와에 대한 식이죠. 그래서 편미분 방정식을 사용해 가지고 먼저 로스를 와이 프라임에 대해서 미분하고 와 프라임을 더블 2에 대해서 미분을 해요. 근데 와 프라임은 블2에 대한 식이니까 미분할 수 있죠. 로스는 와이 프라임에 대한 식이니까 이것도 미분할 수 있습니다. 그래서 이렇게 미분 값이 나왔어요. 자 그러면 좀 한 단계 앞으로 나갈게요. 로스를 블1에 대해서 미분하는 거 여전히 로스를 보면은 로스는 w1에 대한 식이 아니에요. 와 프라임이랑 와에 대한 식이죠. 그래서 이 와이 프라임에 대해서 먼저 미분을 하고 와 프라임을 딱 보시면은 이 뒷부분 여기가 치니까 치에 대한 식으로 나타날 수 있어요. 와 프라임은 w2 곱하기 치죠 이 뒷부분이 치니까 그래서 치에 대해서 미분할 수 있어요. 그리고 마지막에 비로소 이 치를 w1에 대해서 미분하게 됩니다. 그래서 각각의 미분 값을 구해서 이렇게 곱하는 겁니다. 즉 이렇게 뒤에서부터 미분 값을 계산을 해가지고 체인룰 편미분 방정식을 이용해 가지고 이렇게 그레디언트를 계산을 해야 돼요. 그래서 이 과정을 갖다가 백프로파게이션이라고 그래요. 그래서 굉장히 중요한 건데 저희가 그 최적화 방법을 배웠는데 그 최적화 방법 다음으로 중요한 게 요 백프로파게이션입니다. 그래서 다음 시간에 이 백프로파게이션에 대해서 배울 거고요. 아까 이 투 레이어드 MLP에 대해서 저희가 손으로 미분을 계산을 해 가지고 어 각각의 그 로스를 웨이트 원과 웨이 투에 대해서 미분한 식을 계산을 했는데 그 식을 갖다가 이렇게 파이썬 코드로 나타낸 게 이 코드입니다. 그래서 앞에서 한 계산을 파이tson 코드로 이렇게 적은 것이라고 보시면 됩니다. 그래서 한번 이렇게 따라가 보시고 실제로 이렇게 미분을 하는 과정이 굉장히 TDI어스한 테스크예요. 저희가 되게 심플한 이 투 레이어드 엠알피에 대해서만 계산을 했는데도 꽤 복잡한 미분 과정을 거쳤잖아요. 그런데 실제로 뉴럴넷이 투 레이어만 되어 있는 건 아니잖아요. 굉장히 여러 층을 쌓는단 말이에요. 그 여러 층을 쌓았을 때 미분하는 거는 굉장히 복잡해요. 이게 어 엘스티엠이라는 모델을 갖다가 백프로파게이션 하기 위해서 뭐 미분한 어떤 노트인데 와 보기만 해도 엄청 힘들죠 이렇게 많은 미분 과정이 필요한데 예를 들어서 뭐 이렇게 더 복잡한 뉴럴넷 같은 걸 봅시다. 이게 알렉스넷인데 알렉스넷 굉장히 딥하게 쌓은 뉴럴넷이죠. 레이어가 몇 개예요 셀 수도 없어요. 이거를 다 하나하나 손으로 백프로파게이션 하는 미분을 한다고 생각을 해보세요. 너무 끔찍하죠. 그래서 요 미분하는 과정을 알고리즘처럼 만들어 가지고 자동화 컴퓨테이셔널이 자동화하게 하는 게 요 역전파 백프로파게이션 프로세스입니다. 그래서 이 백프로파게이션에 대해서 설명을 하기 위해서는 기본적인 미분 같은 방법들에 대해서 잘 아시고 있어야 되고 수학적인 어떤 그런 계산들이 많이 필요합니다. 그래서 다음 시간에는 백프로파게이션에 대해서 배울 거고 기본적으로 로스를 각각의 뉴럴넷 파라미터에 대해서 미분하는 과정 에서 편미분 방정식을 자동으로 푸는 방법이라고 생각하시면 됩니다. 이번 시간에는 조금 더 원론적이고 개념적인 쉬운 내용을 다뤘다면은 다음 시간에는 백프로페케이션을 한 땀 한 땀 계산을 해보면서 여러분들이 실제로 잘 이해할 수 있게 가르쳐 드릴 것입니다. 그래서 다음 시간 좀 어려울 거고요. 이번 시간도 집중해 주셔서 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(4강) 기초 신경망 이론 -1 - Neural Networks.json",
        "lecture_name": "(4강) 기초 신경망 이론 -1 - Neural Networks",
        "course": "Deep Learning Basic",
        "lecture_num": "4강",
        "lecture_title": "기초 신경망 이론 -1 - Neural Networks",
        "chunk_idx": 8,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:79c85486d51a422bf7e42db0dccf0478f48c8b2568480fdee4ae62bd037f7f1a"
      },
      "token_estimate": 1005,
      "char_count": 1865
    }
  ]
}