{
  "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
  "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
  "course": "AI Math",
  "total_chunks": 9,
  "chunks": [
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c000_933b53",
      "content": "[강의 녹취록] 과목: AI Math | 강의: 10강 | 제목: 2_인공지능을 위한 통계학\n\n그렇다면 이런 최대 가능도 추정법을 기반으로 하는 원리를 포함해서 우리가 이런 통계학적 원리들이 딥러닝에서 왜 필요한지 여러분들이 궁금하실 수 있겠습니다. 딥러닝에서의 학습 이론은 사실 많은 부분 이 통계적 기계 학습 이론에 근거를 두고 있습니다. 그래서 방금 전에 설명드렸던 최대 가능도 추정법도 우리가 딥러닝에서 모델 학습에서 굉장히 많이 원리적으로 쓰고 있는 것이고요. 그뿐만 아니라 손실 함수들 여러분이 민스퀘어로스라거나 아니면 크로스 엔트로피 로스라거나 이런 로스 함수들이 사실은 통계학에서 연구된 근본적인 원리를 통해 가지고 딥러닝에서 학습 때 사용되게 되는 것입니다. 여러분들께서 이런 부분들을 이해하실 때 어떤 원리를 통해서 하게 되는지를 좀 더 설명을 드리면요. 이런 기계 학습 즉 통계적 기계 학습의 근본 원리는 우리가 설정한 손실 함수에서 이 예측이 틀릴 위험을 리스크라고 우리가 부르는데 이 위험을 최소화하도록 손실 함수를 설계하고 그리고 이 손실 함수의 기댓값 측면에서 어 이 리스크를 모델링하고 이 리스크를 최소화하는 게 바로 우리가 사용하게 되는 이 기계 학습의 근본적 원리인데요. 이 기계 학습의 근본적 원리는 딥러닝에서도 똑같이 사용되게 됩니다. 그래서 보시면은 우리가 주어진 데이터의 확률 분포 비 데이터가 있고 그리고 우리가 모델 함수 f가 있을 때 이 주어진 데이터 확률 분포와 모델 함수 f에서 계산되게 되는 리스크 즉 예측이 틀릴 위험이라는 거는 주어진 손실 함수와 그리고 관찰되는 이 데이터 샘플에 대해서 이 FX와 그리고 y의 손실 값을 어 기댓값을 계산했을 때 우리가 리스크라고 정의하게 되는 것이죠. 이때 이 손실 함수는 우리가 mse 로스를 쓸 수도 있고 크로스 엔트로피를 쓸 수도 있습니다. 근데 이런 원리적으로 어떤 로스 함수를 쓰시든지 간에 우리는 이 로스 함수로 정의된 리스크를 최소화하는 것이 기계 학습 그리고 딥러닝에서의 학습 이론의 궁극적 목적이 되는 것이죠. 그리고 이러한 트루 리스크의 경우에는 우리가 실제 데이터 분포를 관찰할 수 있는 것은 아니기 때문에 우리가 몬테칼러 샘플링에 근거한 즉 대수의 법칙에 근거한 본테 칼러 샘플링으로 우리가 추정을 하게 되는 것인데 이렇게 추정하게 되는 리스크를 우리는 엠피리컬 리스크라고 부릅니다. 그래서 기계 학습에서 사용되는 학습 원리를 우리는 엠피리컬 리스크 미니마이제이션 줄여서 erm이라고 부릅니다. 그리고 이 erm 기법은 데이터 분포로부터 학습 데이터를 독립적으로 추출했다라는 즉 대수의 법칙이 성립한다는 가정 하에서 우리는 트루 리스크를 엠피리컬 리스크로 추정을 하고 이렇게 손실 함수에 대한 관찰된 데이터 상에서의 산수 평균으로 우리가 엠피리컬 리스크를 추정해서 이렇게 추정된 손실 함수의 값을 기준으로 우리는 기계 학습 모델의 학습 성능을 평가하게 되는 것이죠. 이런 부분들이 이제 통계학적 원리를 두고 우리가 딥러닝 학습 결과를 평가할 때 사용되는 것이고요. 그리고 앞에서 설명드렸던 맥시멈 라이클리우드도 사실은 여기서 사용되게 되는 것입니다. 네 예를 들어서 회귀 문제에서 사용되는 윈스퀘어 에러 즉 평균 제곱 오차는 모델 예측 오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도하게 설계되어 있습니다. 그래서 민스퀘에러는 이렇게 함수 프에다가 입력 값을 넣었을 때 그라운트로스에 해당하는 와의 엘 투노름의 제곱을 계산하게 되어 있죠. 이 로스 값을 기준으로 트루 리스크를 설정해 주게 되면은 추정된 와 햅시랑 그리고 그라운드 로스 와 간의 엘투 노름의 제곱의 기댓값을 우리가 리스크로 정하게 되는 것이고요. 이 모델 예측의 틀릴 어떤 이 부분에 대해서 MSA 로스는 이 추정된 와랑 그리고 그라운트루스 와 간의 틀릴 모델 예측의 분산을 최소화하는 것으로 우리가 해석할 수가 있게 되는 것입니다. 정리하자면 회귀 분석에서의 목적식은 모델 예측 오차의 분산이 얼마큼 틀릴지를 모델링하고 개를 최소화하는 방향으로 학습한다라고 여러분이 그 학습 원리를 이해하실 수가 있겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 0,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1107,
      "char_count": 2010
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c001_56378d",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 정리하자면 회귀 분석에서의 목적식은 모델 예측 오차의 분산이 얼마큼 틀릴지를 모델링하고 개를 최소화하는 방향으로 학습한다라고 여러분이 그 학습 원리를 이해하실 수가 있겠습니다. 그리고 분류 문제에서 사용되는 크로스 엔트로피의 경우에는 모델 예측의 로그 가능도를 최대화하는 방향으로 학습하도록 유도합니다. 로그 가능도란 앞에서 다시 설명드렸듯이 우리가 관찰하고 있는 어 데이터를 가장 잘 설명하는 모수를 찾는 거다라고 설명을 드렸는데요. 즉 로스 함수를 우리가 설정할 때 이렇게 크로스 엔트로피 함수로 설정할 수가 있는데 이 부분을 잘 살펴보시면은 모델 예측 값에 시 번째 클래스에 대해서 모델 예측값의 로그 값을 씌우고 거기다가 와아 씨라는 그라운트루스에 해당하는 세 번째 클래스의 경우인지 아닌지를 판단하는 이 원화 벡터의 값을 이렇게 앞에다 곱해주게 됩니다. 참고로 이 값은 해당 클래스면 1이고 해당 클래스가 아니면 0인 값이죠. 네 이런 패턴은 어디서 아까 봤는지 기억나실까요? 맞습니다. 카테고리 분포에서의 로그 가능도의 공식이랑 정확히 일치하게 되는 것이죠. 따라서 분류 문제에서 사용되는 교차 엔트로피 크로스 엔트로피는 사실 카테고리 분포의 로그 가능도랑 같습니다. 그리고 우리가 미니마이즈 하고자 하는 즉 최소화하고자 하는 분류 예측 모델의 분류 리스크는 바로 이 로그 가능도에 대한 기댓값이 되는 것이죠. 네 그리고 이거를 최소화하도록 학습을 유도하는 것이 바로 분류 문제에서의 모델 예측의 방향 즉 통계적 기계 학습의 원리라고 우리가 설명할 수가 있겠습니다. 즉 현재 설정된 분류 예측 모델 하에서 어 우리가 현재 관찰되는 데이터를 얼마큼 가장 많이 잘 설명할 수 있는지로 이렇게 로그 가드를 설정하고 그걸 달성하도록 학습시켰을 때 분류 문제 풀이에 가장 최적화할 수 있다라는 원리를 가지고 학습을 시키는 것이죠. 네 이렇게 확률 분포에 대해서 우리가 여러 가지의 원리들을 좀 배워봤고 또한 통계학을 기반으로 해서 데이터에 근거해서 모수를 추정하는 방법도 우리가 살펴보았고 이게 딥러닝에서 어떻게 사용되는지도 학습 원리를 관점으로 또 배워봤습니다. 이번에는 확률 분포 사이의 거리라는 개념을 좀 배워보도록 하겠습니다. 우리가 앞선 시간에 벡터에 대한 거리를 좀 배워봤는데요. 다차원 공간에서의 어떤 데이터의 거리를 계산하는 건 익숙하실 텐데 확률 분포 사이의 거리라는 개념은 조금 익숙하지 않을 수도 있겠습니다. 네 근데 사실 이 확률 분포 사이의 거리라는 개념도 이 기계 학습에서 굉장히 많이 사용되는 개념입니다. 주로 언제 사용되냐면은 여러분이 생성형 인공지능 즉 생성 모델에서의 모델 평가를 할 때 이 확률 분포의 거리라는 개념을 많이 사용하게 됩니다. 네 그래서 기계 학습에서 사용되는 손실 함수들 특히 이제 모델이 학습하는 확률 분포랑 데이터에서 관찰되는 확률 분포의 거리를 통해서 유도하게 되는데 이 데이터에서 관찰되는 확률 분포, 그리고 모델이 학습하는 확률 분포의 사이의 거리를 측정할 때 우리는 이 확률 분포의 거리가 개념이 필요하게 되겠죠. 그래서 이 확률 분포 사이의 거리가 생성 모델에서 사용된다라는 얘기는 우리가 생성 모델은 주로 이 데이터를 만들어낼 때 이 데이터를 만들어내는 확률 분포를 모델링하는 것이잖아요. 그래서 우리가 모델이 학습하는 확률 분포랑 데이터 분포랑 최대한 가깝게 학습을 해야 좀 더 실제 데이터에 가까운 데이터를 생성할 수 있게 되는 것인데 이걸 위해서는 두 확률 분포 사이의 거리를 모델링하는 즉 거리를 계산하는 방법이 필요합니다. 조금 더 자세하게 설명하도록 하겠습니다. 데이터 공간에 두 개의 확률 분포 피랑 q가 있을 때 이 두 확률 분포 사이의 거리 디스턴스를 우리가 어떤 식으로 계산할지 고민해 볼 필요가 있는데요. 우리가 벡터의 노름을 계산할 때 에런 디스턴스도 있고 엘투 디스턴스가 있다고 배웠죠. 네 거리 함수는 사실 한 가지 종류만 있는 것은 아닙니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 1,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1082,
      "char_count": 1964
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c002_1e53eb",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 조금 더 자세하게 설명하도록 하겠습니다. 데이터 공간에 두 개의 확률 분포 피랑 q가 있을 때 이 두 확률 분포 사이의 거리 디스턴스를 우리가 어떤 식으로 계산할지 고민해 볼 필요가 있는데요. 우리가 벡터의 노름을 계산할 때 에런 디스턴스도 있고 엘투 디스턴스가 있다고 배웠죠. 네 거리 함수는 사실 한 가지 종류만 있는 것은 아닙니다. 마찬가지로 두 확률 분포 사이의 거리를 계산할 때도 한 가지 종류만 있는 것이 아니고 이와 같이 굉장히 다양한 거리 함수를 사용할 수가 있습니다. 어 토탈 베리에이션이라고 부르는 총 변동 거리 함수도 사용할 수도 있고요. 그리고 쿨백 라이블러 다이브전스라는 함수도 사용할 수가 있습니다. 어 참고로 쿨뱅 라이벌 다이버전스는 디스턴스라는 개념보다는 다이버전스라는 개념이 좀 많습니다만 어쨌든 이것도 두 개의 확률 분포가 얼마큼 멀어져 있는지를 계산하는 데 쓰이는 개념 중 하나이고요. 그리고 바슈타인 디스턴스라는 함수도 우리가 거리를 계산할 때 사용할 수가 있습니다. 그러면은 각각의 거리 함수들을 이용해 가지고 우리가 어떻게 어 두 확률 분포 사이의 거리를 계산할 수 있는지 배워보도록 할 텐데요. 그중에서 가장 많이 사용되는 쿨백 라이블러 다이버전스를 배워보도록 하겠습니다. 쿨백 라이블러 다이버전스는 케이엘 다이버전스라고 부르는데 이 케이와 엘 이니셜을 따서 케이엘이라는 기호를 많이 쓰고요. 그리고 두 확률 분포 pq를 각각 이렇게 기호로 넣어주게 되는데 보시면 이렇게 작대기 두개가 들어가 있습니다. 작대기 두 개가 들어가 있는 이유는 이게 조건부랑 같은 개념의 뜻은 아니고 이 p랑 q에 들어가 있는 확률 분포 위치에 따라서 값이 다르게 나오기 때문에 구별하기 위해서 쓰는 것입니다. 이 pq가 어떤 식으로 쓰이게 되냐면은 먼저 이 쿨백 라이블러 다이버전스는 우리가 이 앞서 먼저 들어가는 피에 대해서 기댓값을 계산하는 거다라고 이해하시면 되는데 이렇게 피x가 이산 확률 변수에는 확률 밀도 함수가 확률 질량 함수가 되겠죠. 이 확률 질량 함수가 이렇게 서메이션의 피스로 곱해지게 되는 것이고 연속 확률 변수일 때는 확률 밀도 함수로서 이렇게 적품 값에 곱해지게 되는 것입니다. 그렇다면 뭐에 대한 기댓값을 취하는 것일까요? 네 맞습니다. 이 p랑 q 사이에 비율 즉 PX 분의 qx 이 비율을 로그를 씌워서 이 로그 비율에 대한 기댓값을 계산하게 되어 있습니다. 즉 pq를 순서로 p가 분자 q가 분모에 들어가는 것이고 그다음에 이 p에 해당하는 확률 질량 함수나 밀도 함수를 가지고 기댓값을 취한다. 즉 피에 대해서 기댓값을 취한다라고 기억하시면 되겠습니다. 이때 쿨백 라이블러 다이버전스를요. 우리가 로그의 성질을 이용해서 다음과 같이 분해를 할 수가 있습니다. 네 보시면은 먼저 로그 피엑스에 대해서 우리가 이렇게 먼저 그 뒤쪽에 쓸 수가 있고요. 그다음에 로그 큐엑스에 대해서 우리가 이렇게 앞쪽에 쓸 수가 있는데 큐스가 분모에 있으니까 어 로그를 씌워 주게 되면은 우리가 음수가 나오게 되는데 그 음수 부분을 이렇게 앞으로 뺄 수가 있겠죠 이때 PX가 서로 같은 이 밀도 함수나 질량 함수에 대해서 로그 PX를 계산하는 이 부분은 우리가 엔트로피라고 부르는 부분이고요. 그다음에 이 기댓값을 계산하는 이 PX랑 그다음에 대상인 qx랑 서로 다른 확률 분포일 때는 교차 엔트로피라고 부릅니다. 크로스 엔트로피라고 부르는 부분이죠 네 많이 들어본 용어죠 맞습니다. 분류 모델에서 우리가 사용하게 되는 손실 함수가 바로 이 교차 엔트로피가 되는 것입니다. 네 좀 이따가 얘네들이 어떻게 걸렸는지 좀 살펴보게 될 텐데요. 먼저 여러분들이 이와 같은 사실을 좀 볼 수가 있습니다. 만약에 피랑 큐가 서로 같은 확률 분포이면 어떻게 될까요? 그러면 이 자리에 피가 들어가게 되고 그러면 이 교차 엔트로피는 원래 엔트로피랑 같은 텀이 되기 때문에 두 개가 서로 마이너스 플러스가 돼서 0이 됩니다. 즉 쿠백 라이블러 다이버전스는 피랑 큐가 서로 같은 확률 분포이면은 0이라는 값을 출력하게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 2,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1096,
      "char_count": 2024
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c003_fbb459",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 분류 모델에서 우리가 사용하게 되는 손실 함수가 바로 이 교차 엔트로피가 되는 것입니다. 네 좀 이따가 얘네들이 어떻게 걸렸는지 좀 살펴보게 될 텐데요. 먼저 여러분들이 이와 같은 사실을 좀 볼 수가 있습니다. 만약에 피랑 큐가 서로 같은 확률 분포이면 어떻게 될까요? 그러면 이 자리에 피가 들어가게 되고 그러면 이 교차 엔트로피는 원래 엔트로피랑 같은 텀이 되기 때문에 두 개가 서로 마이너스 플러스가 돼서 0이 됩니다. 즉 쿠백 라이블러 다이버전스는 피랑 큐가 서로 같은 확률 분포이면은 0이라는 값을 출력하게 됩니다. 그리고 이 쿠백 라이블러 다이버전스는 항상 0보다 크거나 같기 때문에 거리 함수처럼 우리가 쓸 수 있는 개념이 되는 것이죠. 앞에서 설명드렸듯이 분류 문제 클래시피케이션 문제에서 정답 레이블 정보를 p 그리고 모델 예측을 q라고 뒀을 때 어 우리가 앞서 살펴본 최대 가능도 추정법을 해석하면은 이 쿠백 라이블러 발산 측면에서는 이 최대 가능도 추정법은 사실 쿠백 라이블러 다이버전스를 최소화하는 문제로 여러분이 해석할 수가 있게 되는 것이죠. 자 무슨 말이냐면요 교차 엔트로피 손실 함수를 가지고 최소화하는 것은 우리가 사실 쿨백 라이블러 다이버전스 측면에서는 두 확률 분포의 거리를 최소화한다라고 이해하실 수가 있고요. 이 뜻은 정답 레이블에 해당하는 확률 분포와 모델 예측으로 얻게 되는 확률 분포 사이의 거리를 최소화한다는 개념이고 이걸 최소화한다는 것은 바로 최대 가능도 추정법에 근거해서 데이터에 가장 적합한 모델 예측을 찾는다라는 원리와 모두 같은 뜻으로 해석할 수가 있게 되는 것입니다. 네 이제 외 분류 문제에서 크로스 엔트로피 로스를 사용하는지 이해할 수 있을 거라고 생각합니다. 어 이번에는 통계학에서 좀 중요한 분야를 좀 소개를 하려고 하는데요. 이제 앞서 살펴본 조건부 확률의 개념을 좀 다시 한 번 복습을 좀 해보려고 합니다. 네 어 조건부 확률을 우리가 개념을 배울 때 우리가 어떤 사건이 발생했다는 전제하에서 어떤 b라는 여기서 이 같은 경우에는 b라는 사건이 발생했다는 전제하에서 a라는 사건이 발생할 확률을 조건부 확률로 우리가 이해할 수 있다고 했고 그다음에 베이지 정리는 이 조건부 확률을 이용해서 정보를 갱신하는 방법을 알려준다고 지난 시간에 확률론 시간 때 설명을 드렸었습니다. 그래서 에라는 세로가 정보가 줬을 때 어 피비로부터 a가 주어졌을 때 b가 일어날 이벤트의 확률을 계산하는 방법을 제공하는 게 우리가 이 조건부 확률에 대한 이해라고 말씀을 드렸었죠. 그래서 이 조건부 확률값을 계산하고 싶을 때 에라는 정보가 새로 주어졌을 때 비비로부터 계산하는 방법을 얻는 게 베이즈 정리이고 이 베이즈 정리를 이해하는 것은 이 정보 업데이트 관점에서 조건부 확률의 개념을 이용해서 이해할 수 있다고 말씀을 드렸었습니다. 어 근데 이 조건부 확률을 여러분들께서 해석하실 때요 한 가지 좀 주의하실 점이 있습니다. 사실 기계 학습 분야 모델링을 할 때 우리가 예측 모델을 이해할 때는 대체로 이 조건부 확률의 개념을 많이 사용하고 실제로 앞에서도 조건부 확률 모델링으로 분류 모델이나 이제 예측 모델들을 상정을 많이 했었는데요. 한 가지 주의하실 점은 어떤 사건이 발생한 걸 조건부로 해서 다른 사건이 발생할 확률을 모델링하는 게 조건부 확률이지만 이걸 해석할 때는 인과관계랑은 좀 별도로 해석을 하셔야 됩니다. 즉 인과관계 커s리티를 추론할 때는 우리가 조건부 확률 개념을 우리가 사용할 수는 있겠지만 절대로 함부로 사용해서는 안 된다. 즉 조건부 확률을 이용해서 인과관계를 해석할 때는 조금 주의하셔야 된다라는 말씀을 드리고 싶습니다. 네 제가 이런 말씀을 드리는 이유는 보통 우리가 기계 학습이나 딥러닝 모델에서 예측 모델을 많이 얻는데요. 이 예측 모델은 조건부 확률을 모델링을 많이 합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 3,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1050,
      "char_count": 1921
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c004_94f0ab",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 즉 인과관계 커s리티를 추론할 때는 우리가 조건부 확률 개념을 우리가 사용할 수는 있겠지만 절대로 함부로 사용해서는 안 된다. 즉 조건부 확률을 이용해서 인과관계를 해석할 때는 조금 주의하셔야 된다라는 말씀을 드리고 싶습니다. 네 제가 이런 말씀을 드리는 이유는 보통 우리가 기계 학습이나 딥러닝 모델에서 예측 모델을 많이 얻는데요. 이 예측 모델은 조건부 확률을 모델링을 많이 합니다. 근데 이 조건부 확률을 모델링 한다고 했을 때 인과 관계 모델링을 했느냐라고 말하는 건 어렵다라는 거를 말씀드리기 위해서 이 수업 시간에 좀 준비를 했는데요. 인과관계 추론이랑 조건부 확률 관계 추론이랑 좀 다르다라는 예시를 좀 보여드리려고 합니다. 어 참고로 인과관계 추론은 데이터가 아무리 많다고 해서 즉 조건부 확률과 조건부 확률을 가지고 데이터가 아무리 많아져도 인과관계를 추론하는 것은 불가능하다라는 예시를 좀 보여드릴 거긴 한데요. 인과관계라는 거는 데이터의 생성 어떤 모형 즉 다시 말해서 제 데이터의 어떤 제너레이팅 프로세스가 어떻게 되는지를 알 수 있을 때 그때 인과관계를 추론할 수 있는데 만약에 우리가 데이터 제너레이팅 프로세스를 고려하지 않고 데이터의 어떤 생성되는 어떤 원리나 이런 부분들을 모르고 어 관찰된 데이터의 결합 분포만 가지고 조건부 확률을 계산하게 되면은 어 우리가 어떤 인과관계에 대한 예측은 할 수는 없습니다. 인과관계를 알 때는 이런 데이터 분포가 어떤 변화가 발생했을 때 즉 데이터 분포 그 자체도 변할 수 있는 가능성이 있는데요. 이러한 데이터 분포에 좀 로버스트한 예측 모형을 만들 때는 이 인과관계를 고려해서 예측 모델링을 해야 됩니다. 만약에 데이터 분포의 변화가 생겼을 때는 조건부 확률을 가지고 예측 모델을 만들면 그 모델의 예측력이 떨어질 수가 있습니다. 그래서 우리가 두 가지 시나리오를 생각해 볼 수가 있는데 우리가 학습 시에는 예측 정확도가 굉장히 높았던 모델이 만약에 데이터의 분포와 변화가 생겨서 예측 정확도에 따라서 그 예측 정확도가 이렇게 어떤 경우에는 그대로 높게 나올 수도 있지만 어떤 경우에는 예측 정확도가 많이 떨어지는 경우가 발생할 수도 있는데요. 이제 데이터 분포에 강건하지 않는 모델들 같은 경우에는 각각의 시나리오에 따라서 예측 정확도가 여전히 높게 나올 수도 있지만 어떤 경우에는 예측 정확도가 굉장히 많이 떨어질 수도 있습니다. 네 그 이유는 조건부 확률 모델은 어 데이터 분포의 어떤 변화를 측정하는 것이 아니라 우리가 관찰된 데이터를 기반으로 해서 확률 분포를 추정하는 것이기 때문에 데이터 분포 그 자체의 변화에 대해서는 우리가 추론할 수는 없습니다. 그래서 우리가 인과관계 기반 예측 모형을 할 때는 그런 데이터 분포의 변화에 대해서 우리가 조금 더 강건하게 예측을 할 수는 있는데요 단 조건부 예측 모델은 일반적으로 굉장히 높은 예측 정확도를 담보할 수는 있습니다. 그렇기 때문에 인과관계 기반 예측 모형은 우리가 상대적으로 조금 이 데이터 변화에 대해서 로버스트하게 예측 정확도를 가져가고 싶을 때 우리가 설정하는 모델링 모델링 방법이고요. 우리가 만약에 데이터 분포의 변화에 대해서 변화할 가능성이 별로 없다. 여러분들께서 상상하실 수 있다면 그때는 예측 모델링을 쓰셔도 큰 문제는 없을 것입니다. 여기서 중요한 점은 바로 이겁니다. 우리가 데이터를 수집하는 과정과 실제로 학습한 모델을 실사용 애플리케이션에 여러분이 디플로이 즉 사용하실 때 사용자나 또는 이 데이터의 분포의 흐름에 변화가 생길 가능성이 매우 높으면 여러분들께서 예측 모델의 예측이 항상 떨어질 수 있다. 즉 예측 모델 정확도가 어 학습 데이터에서의 테스트 퍼포먼스 성능이랑 굉장히 다르게 나올 가능성이 높다라는 거를 여러분들이 인지하시면 좋겠습니다. 그래서 데이터 분포의 변화에 예측 모델링은 상당히 그 성능이 흔들릴 수 있다라는 거를 기억하시면 좋겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 4,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1076,
      "char_count": 1952
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c005_508503",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 여기서 중요한 점은 바로 이겁니다. 우리가 데이터를 수집하는 과정과 실제로 학습한 모델을 실사용 애플리케이션에 여러분이 디플로이 즉 사용하실 때 사용자나 또는 이 데이터의 분포의 흐름에 변화가 생길 가능성이 매우 높으면 여러분들께서 예측 모델의 예측이 항상 떨어질 수 있다. 즉 예측 모델 정확도가 어 학습 데이터에서의 테스트 퍼포먼스 성능이랑 굉장히 다르게 나올 가능성이 높다라는 거를 여러분들이 인지하시면 좋겠습니다. 그래서 데이터 분포의 변화에 예측 모델링은 상당히 그 성능이 흔들릴 수 있다라는 거를 기억하시면 좋겠습니다. 그렇다면 우리가 인과관계를 알아내기 위해서는 어떤 분석을 해야 될까요? 네 사실 기계 학습 모델링과는 조금 다른 접근 방법일 수도 있겠지만 사실 인과관계 모델링도 인공지능 모델링에서 사용되는 방법론 중 하나입니다. 그중 가장 좀 간단한 예시들을 좀 몇 개를 좀 소개해 드리려고 하는데요. 먼저 인과관계를 알아내기 위해서는 각각의 변수들이 이렇게 주어져 있을 때 그냥 x와 y 간의 관계가 아니라 각각의 변수들이 서로 어떤 식으로 이 데이터 분포를 형성할 때 사용되는지를 즉 각 변수들 간의 관계를 알아내야만 합니다. 이게 무슨 말이냐면요. 우리가 예측 모델링을 할 때는 주어진 입력 변수랑 그리고 예측하고자 하는 y 이 스에서 와로 가는 함수만 알아내면 됐었는데 인과관계 추론을 하기 위해서는 주어지는 입력 변수들끼리의 관계도 알아야 된다는 것입니다. 즉 스1 스2 3가 있을 때 x1이 x2에 어떻게 영향을 주고 그리고 x2가 x3에 어떻게 영향을 주고 이런 관계들을 모델링 할 수 있을 때 우리는 인과관계 추론을 할 수 있는 것인데요. 그중에서도 중첩으로 이렇게 영향력을 주는 변수들이 있는 경우에는 이 변수들이 실제로 인과관계 추론을 방해할 수 있는 요인이 되기 때문에 중첩 요인을 제거하고 오로지 커s 이펙트에서 인과의 원인에 해당하는 변수로 인과관계 계산을 해야 됩니다. 참고로 이런 중첩 요인을 넣고 우리가 조건부 모델링을 해주게 되면은 이 중첩 요인의 이팩트가 같이 반영이 되기 때문에 실제로 데이터 분포의 변화가 발생했을 때 이 중첩 요인에 의해서 예측 값이 흔들리게 될 수 있어요. 그 경우에 어 우리가 스퓨리얼 스포레이션이라고 부르는 즉 제트의 영향력으로 인해서 가짜 연관성이 우리가 관찰될 수가 있기 때문에 이 부분을 제거하지 않으면은 데이터 분포의 변화가 생겼을 때 로버스트하게 정착하지 않을 수 있게 되는 것이죠. 그래서 여러분들께서 예측 모델링의 성능을 좀 포기하더라도 좀 더 강건하게 돌아가는 모델링을 하고 싶으시면은 이 중첩 요인의 효과를 제거를 하고 이 어떤 변수의 순수한 인과적 관계를 모델링하는 기법을 고려하셔야만 합니다. 네 그래서 조건부 확률 기반 예측 모델링은 우리가 굉장히 많이 사용하고는 있지만 학습 데이터랑 그리고 실제 디플로이 했을 때의 확률 분포가 일치한다라는 가정이 충족돼야만 이 모델을 신뢰할 수가 있습니다. 최근에 신뢰 가능한 인공지능 즉 트러스트 월드 AI라는 게 굉장히 많이 중요해지고 있는데요. 신뢰 가능한 인공지능 모델링을 하시려면 여러분들께서 어 조건부 확률 기반으로만 모델링을 할 때 이렇게 트레인 환경과 그리고 테스트 환경에 분포가 동일한 경우에는 예측 모델링을 쓰는 게 적절하지만 만약에 이 분포가 바뀔 가능성이 높은 경우에는 네 여러분들께서 상당히 유의를 하셔야 됩니다. 참고로 이 뜻은 어 테스트 데이터나 학습 데이터가 같아야 된다는 의미는 아닙니다. 데이터가 같다는 의미랑 확률 분포가 같다는 의미는 매우 다릅니다. 데이터는 그 확률 분포에서 샘플링한 것이고요 그 확률 분포는 이 데이터가 실제로 어떻게 흐름을 가지고 있는지를 보는 것이기 때문에 우리가 실제로는 관찰할 수는 없습니다만 네 추출한 데이터들의 패턴을 보시면 우리가 추론을 할 수가 있겠죠. 그랬을 때 조건부 확률 기반 예측 모델링은 학습 시와 그리고 실제 디플로이 테스트했을 때 데이터 분포가 일치했을 때 쓸 수 있다라는 거를 여러분이 기억하시면 좋겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 5,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1105,
      "char_count": 2015
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c006_59cf71",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 데이터가 같다는 의미랑 확률 분포가 같다는 의미는 매우 다릅니다. 데이터는 그 확률 분포에서 샘플링한 것이고요 그 확률 분포는 이 데이터가 실제로 어떻게 흐름을 가지고 있는지를 보는 것이기 때문에 우리가 실제로는 관찰할 수는 없습니다만 네 추출한 데이터들의 패턴을 보시면 우리가 추론을 할 수가 있겠죠. 그랬을 때 조건부 확률 기반 예측 모델링은 학습 시와 그리고 실제 디플로이 테스트했을 때 데이터 분포가 일치했을 때 쓸 수 있다라는 거를 여러분이 기억하시면 좋겠습니다. 네 그러면 인과관계 추론의 예제를 한번 살펴보도록 하겠습니다. 어 이거는 사실 굉장히 유명한 예제 중 하나인데요. 바로 어떤 신장 결석이 발생했을 때 어떤 치료법을 쓰는 것이 유효한지를 판정하는 문제가 되겠습니다. 그래서 어떤 환자가 들어왔을 때 에랑 비 둘 중 어떤 치료법을 쓰는 것이 가장 좋은가를 판단하는 예측 모델을 하고 싶은 겁니다. 이때 변수는 크게 3개가 있습니다. 바로 뭐냐면 먼저 신장 결석의 크기를 우리가 지라고 할 텐데요. 이 신장 결석의 크기가 큰 경우는 우리가 1이라고 하고요. 그리고 신장 결석의 크기가 작은 경우는 우리가 0이라고 하겠습니다. 이때 출료법은 a랑 b 둘 중 하나로 우리가 나눠 보겠습니다. 참고로 여기서는 명시를 하지는 않았지만 어 비침습적인 치료법이 있고 그다음에 수술로 치료하는 치료법이 있는데 이 두 치료법 중에서 어떤 것이 실제로 완치 가능성이 높은지를 우리가 한번 모델링하고 싶으신 겁니다. 다시 말해 이 물음표에 해당하는 즉 어떤 치료법이 실제로 완치에 도움이 되는지를 우리가 추론하고 싶은 것이죠. 물론 이때 신장 결석의 크기도 같이 고려해서 통계적 분석을 해야 정확한 추론이 될 텐데요. 문제는 이겁니다. 이 신장 결석의 크기가 실제로 우리가 완치 확률을 예측할 때는 이 g에서 r로 가는 이 관계에서는 유효한 정보가 되지만 만약에 데이터에서 이 신장 결석 크기에 따라 치료법이 결정되는 게 만약에 이 패턴이 관찰되게 되면은 이 요인 때문에 우리가 이 실제 치료법의 효과를 추론하는 데 방해가 될 수가 있습니다. 이게 바로 중첩 효과입니다. 컴파운딩 임팩트라고 하는데요. 네 한번 이렇게 살펴보도록 하겠습니다. 심장 결석의 크기가 작은 경우에는 보시면은 치료법 비보다 치료법 에가 더 치료 그 완치율이 더 높은 걸 보실 수가 있죠. 93%와 80%니까요. 그리고 신장 결석 크기가 1일 때도 보시면 치료법 a가 치료법 비보다 더 완치율이 높다는 걸 볼 수가 있습니다. 네 이렇게 보게 되면은 각각의 신장 결석 크기에 따라서 치료법 a가 치료법 b보다 항상 우월하다는 거를 보실 수가 있는데 보시면 전체 환자 수에서 완치율을 보니까 치료법 비가 치료법 a보다 더 완치율이 높다라는 거를 보실 수가 있습니다. 네 왜 이런 일이 발생할까요? 네 그거는 이렇습니다. 네 이 데이터를 여러분이 한번 그림으로 그려보면 이렇게 나오게 되는데요. 스축은 이제 제가 0인 즉 신장 결석의 크기가 작은 환자들의 그룹이고 어 그다음에 이 부분은 제가 1인 즉 신장 결석의 크기가 큰 그룹이 되겠습니다. 이때 y축은 완치율을 나타내는 확률을 의미하는 것인데요. 네 잘 보시면 신장 결석의 크기가 작은 환자들 같은 경우에는 치료법 a보다 치료법 b를 쓰는 확률이 높죠. 그리고 심장 결석의 크기가 큰 경우에는 치료법 a보다 치료법 b를 쓰는 확률이 낮습니다. 즉 치료법 a를 더 많이 쓴다는 걸 볼 수가 있습니다. 이 말은 무슨 말일까요? 네 맞습니다. 신장 결석의 크기에 따라서 어떤 치료법을 쓰는지의 확률도 같이 결정된다는 것이죠. 이 말은 신장 결석의 크기에 따라서 치료법 에비를 선택할 확률도 결정이 되는 이 관계가 있다는 거를 보실 수가 있습니다. 그리고 당연히 신장 결석의 크기와 치료법 a b 각각에 의한 이 완치율을 결정하는 확률도 분명히 이 완치율을 결정하는 데 쓰이게 되는데 문제는 이 완치율을 결정할 때 요 정보도 쓰이게 된다는 것입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 6,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 1066,
      "char_count": 1981
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c007_2c32b1",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 즉 치료법 a를 더 많이 쓴다는 걸 볼 수가 있습니다. 이 말은 무슨 말일까요? 네 맞습니다. 신장 결석의 크기에 따라서 어떤 치료법을 쓰는지의 확률도 같이 결정된다는 것이죠. 이 말은 신장 결석의 크기에 따라서 치료법 에비를 선택할 확률도 결정이 되는 이 관계가 있다는 거를 보실 수가 있습니다. 그리고 당연히 신장 결석의 크기와 치료법 a b 각각에 의한 이 완치율을 결정하는 확률도 분명히 이 완치율을 결정하는 데 쓰이게 되는데 문제는 이 완치율을 결정할 때 요 정보도 쓰이게 된다는 것입니다. 문제는 이 정보가 같이 들어가게 되면은 실제 치료법에 의한 완치율의 요 인과관계를 추정할 때는 방해가 되는 것입니다. 지에 상관없이 결함 분포 측면에서만 보게 되면은 우리가 치료 효과가 비가 에보다 더 좋은 것처럼 우리가 볼 수가 있게 되는데 어 이제 그 이유는 이제 뭐가 되냐면은 그 보시게 되면은 이 치료법 b 같은 경우는 우리가 이제 빨간색 표시할 수가 있고 그다음에 치료법 a 같은 경우는 우리가 이제 파란색으로 이제 표시를 해 봤는데요. 네 잘 보시면 신장 결석의 크기가 작은 사람들의 경우에는 치료법 에보다 치료법 비를 선택하는 경우가 더 많다는 걸 보실 수가 있으시죠. 즉 신장 결석의 크기가 작은 경우에는 치료법 비를 선택하는 경우가 더 많습니다. 그리고 신장 결석의 크기가 1인 경우에는 앞서 즉 신장 결석의 크기가 큰 경우에는 앞서 살펴본 것처럼 치료법 a를 선택한 확률이 더 크죠. 자 문제는 이 각각의 데이터에 대해서 전체 빨간색만 고려했을 때 완치율 평균을 고려해 보시면은 이 빨간색은 여기에 찍히게 되고요. 그다음에 치료법 a에 대해서 이 신장 결석 크기의 군을 고려하지 않고 평균만 취해주게 되면은 이 파란색은 여기에 찍히게 됩니다. 즉 전체적으로 봤을 때는 치료법 a가 치료법 b보다 전체적으로 완치율이 더 높게 형성이 되어 있는데 이 치료법 b는 실제로 심장 결석 크기가 작은 환자들의 집단이다 보니까 전체적으로 완치율이 더 높은 군을 형성하게 되어 있고 이 군을 형성하겠다라면서 평균값이 위로 더 많이 올라가게 되니까 전체 평균으로 보게 되면은 마치 치료법 비의 완치율이 마치 더 높은 것처럼 모델링 되는 것입니다. 그래서 조건부 확률로 여러분들이 실제로 계산해 보게 되면은 치료법 a를 선택했을 때의 완치율보다 치료법 비를 선택했을 때의 완치율이 마치 더 높은 것처럼 계산이 되는 것입니다. 그렇지만 아까도 설명드렸듯이 지의 크기에 따라 치료법 티를 선택하는 패턴이 데이터 수집 과정에 만약에 존재한다면 네 이거는 치료 효과 추정에 악영향을 줄 수가 있습니다. 자 그래서 이렇게 신장 결석 크기에 따라서 치료법을 선택하게 되는 요 중첩 요인을 우리가 제거를 할 수가 있는데요. 이걸 제거하는 거를 인터벤션이라고 합니다. 그랬을 때 이 제거하는 방법을 통해서 우리가 실제로 인과 효과를 추정해 볼 수가 있는데 어 이걸 제거한다라는 거는 우리가 데이터를 새로 모은다는 뜻은 아니고요 물론 이제 그렇게 데이터를 새로 모을 수도 있지만 의료 데이터 같은 경우에는 환자의 데이터를 새로 모으는 건 거의 불가능하죠. 그래서 이런 경우에는 인과관계 추론을 위한 방법론을 쓸 수가 있는데요. 네 바로 인터벤션의 효과를 반영해서 우리가 이와 같이 그 인과관계 추정 방법의 추정법을 쓰게 되면은 우리가 좀 더 정확하게 조건부 확률이 아닌 치료법 에를 선택했을 때 이 중첩 효과를 제거한 실제 인과관계에 해당하는 치료법 a의 효과를 추정할 수가 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 7,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 954,
      "char_count": 1761
    },
    {
      "id": "transcript_ai_math_ai_math_10강_2_인공지능을_위한_통계학_c008_3c95b9",
      "content": "[AI Math] [AI Math]_10강_2_인공지능을 위한 통계학\n\n다. 네 그래서 이거 같은 경우에는 이 a를 선택했을 때 이 조건부 확률은 그대로 쓰게 되지만 실제 심장 결석의 크기에 따를 때 이 신장 결석 크기 지에 따라서 t를 선택하는 이 조건부 확률은 반영하지 않고 이 중첩 요인을 제거해서 평균값을 계산하는 공식을 쓰게 되면은 어 이 지에 의해서 티를 선택하게 되는 이 중첩 요인을 제거해서 우리가 확률 값을 추정할 수가 있게 되고요. 이걸 통해서 계산해 보게 되면 치료법 a의 치료 효과를 보시면 그 78%가 아니라 83.25%로 우리가 추정할 수가 있게 되죠. 또한 마찬가지로 치료법 비에 대해서도 신장 결석 크기에 대해서 우리가 중첩 효과를 제공하고 확률 값을 추정해 보게 되면은 원래 조건부 확률로 계산했던 83%가 아니라 이렇게 77.89%라는 값이 나오게 됩니다. 네 따라서 제가 0일 때 우리가 a가 더 나왔고 제가 1일 때 a가 더 나왔다라는 요 이 각각의 케이스에 대해서의 분석 결과와 그리고 인과관계 분석에서 얻은 분석 결과랑 일치하는 결과가 나오게 되는 거고요. 조건부 확률로 예측했을 때의 예측 모델과 정반대의 해석 결과를 얻을 수 있게 되는 것이죠. 네 사실 이 예제는 통계학에서 심슨 스페라독스 심슨의 역설이라고 부르는 굉장히 유명한 역사 중 하나인데요. 어 이렇게 인과관계 추론을 할 수 있는 추정 방법을 사용하게 되면은 조건부 확률 기법에서 놓칠 수 있는 컴파운딩 임팩트를 고려해서 정확하게 인과관계를 우리가 추정할 수가 있게 됩니다. 그래서 이런 인과관계를 기반으로 우리가 추정했을 때 좀 더 과학적으로 또는 우리가 통계학적으로 좀 더 정확한 치료법에 대한 우리가 판정을 내릴 수가 있게 되는 것이죠. 네 이와 같이 통계학이 어떻게 인공지능 모델링 그리고 딥러닝에서 어떻게 사용될 수 있는 부분에 대해서 우리가 배워보았습니다. 네 지금까지 10개의 강의를 통해서요 선형대수 그리고 미적분 확률 그리고 통계학이 어떤 식으로 기초가 쓰이게 되는지를 배워봤습니다. 상당히 어려운 내용들이 많았겠지만 여러분들께서 지금까지 학습한 내용들을 복습하시고 끊임없이 기초들을 채워 나가시면은 앞으로 더 어려운 딥러닝 내용들을 공부하실 때 큰 도움이 될 거라고 믿습니다. 네 지금까지 수고하셨습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[AI Math]_10강_2_인공지능을 위한 통계학.json",
        "lecture_name": "[AI Math]_10강_2_인공지능을 위한 통계학",
        "course": "AI Math",
        "lecture_num": "10강",
        "lecture_title": "2_인공지능을 위한 통계학",
        "chunk_idx": 8,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:0a8858ebf1d9d353f03de726aff57c0fdaf3312583bc043945f627ce376a7834"
      },
      "token_estimate": 608,
      "char_count": 1134
    }
  ]
}