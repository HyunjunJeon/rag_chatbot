{
  "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
  "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
  "course": "기타",
  "total_chunks": 7,
  "chunks": [
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c000_648285",
      "content": "[강의 녹취록] 과목: 기타 | 강의: 3강 | 제목: 학습, 검증, 테스트 데이터 이해하기(1)\n\n안녕하세요. 도메인 공통 프로젝트 3강 학습 검증 테스트 데이터 이야기 시작하겠습니다. 이번 강의에서는 데이터 이해하기 그리고 학습 데이터를 준비할 때 주의할 점에 대해서 이야기해 보겠습니다. 첫 번째 데이터 이야기입니다. 여러분들이 AI 모델을 학습할 때 훈련 검증 테스트 데이터라는 말을 들어보셨을 거예요. 이 3개의 데이터에 대해서 먼저 이야기해 보겠습니다. 훈련 데이터는 모델을 학습시키기 위해 사용하는 데이터입니다. 검증 데이터는 모델의 과적합 여부 판단 및 하이퍼 파라미터 튜닝에 사용되는 데이터이고요. 마지막 테스트 데이터는 모델의 최종 성능을 평가하기 위해 사용하는 데이터입니다. 밑에 예시를 보시면 전체 데이터에서 여러분들이 최종 성능을 평가하기 위해 사용하는 테스트 데이터를 쪼개는 것을 보실 수가 있고요. 남은 데이터 중에 트레인 밸리데이션 데이터를 분리해서 3개 데이터를 구성하는 것을 보실 수가 있습니다. 물론 제가 든 예시에서는 테스트 데이터를 먼저 분리를 하지만 실제로는 순서가 바뀔 수 있다는 점 염두에 두시면 감사하겠습니다. 훈련 데이터를 조금 더 자세히 보겠습니다. 모델을 학습시키기 위해서 사용하는 데이터입니다. 그렇기 때문에 우리가 훈련 데이터로는 반드시 모델 학습에만 사용을 해야 합니다. 검증 데이터는 모델 성능을 평가하고 모델이 훈련 데이터에 과적합되었는지 판단하는 용도로 사용됩니다. 일반적으로 과적합된 모델 같은 경우는 훈련 데이터에서는 낮은 로스 혹은 높은 매트릭 수치를 보여주고요. 검증 데이터에서는 높은 로스 낮은 매트릭 수치를 보여줍니다. 이를 통해 우리가 검증 데이터를 기반으로 모델이 훈련 데이터에 과적합되었는지 판단할 수가 있습니다. 또한 모델 선택 및 하이퍼 파라미터 튜닝 과정에 사용할 수 있습니다. 방금 설명드렸던 과적합을 판단하는 프로세스와 동일하게 모델 선택과 하이퍼 파라미터 튜닝 과정에서 이 프로세스를 사용한다 정도로 이해하시면 되겠습니다. 테스트 데이터는 모델이 실제로 얼마나 잘 작동하는지를 나타내는 일반화 성능을 예측합니다. 뭐 검증 데이터에서도 한 번 이 모델의 성능이 괜찮은지 검사할 수 있는데요. 한 번 더 한 번 더 모델 성능을 확인한다 정도로 여러분들이 이해하시면 되겠습니다. 만약 여러분들이 검증 데이터가 없다면 테스트 데이터만 잘 맞추고 실제 환경에서는 성능이 떨어지는 모델이 구현될 수 있습니다. 그렇기 때문에 한 번 더 제가 성능을 검사한다라고 말씀을 드렸던 이유가 여기에서 나옵니다. 그리고 검증 데이터가 없는 경우에는 데이터 스누핑 편향이라는 문제가 발생할 수 있습니다. 실제 시스템에 배포했을 때 기대한 성능이 나오지 않는 문제라고 설명드릴 수 있습니다. 좀 더 쉬운 예제로 말씀을 드려보겠습니다. 대학교에서 기말고사를 보는 상황을 가정해 보겠습니다. 교수님께서 기출 문제와 비슷하게 시험을 구성하겠다고 말씀하셨습니다. 이런 경우에는 우리가 기출 문제를 열심히 풀겠죠. 실제 시험을 치렀을 때는 좋은 성적을 받을 수가 있습니다. 하지만 실제 시험이 기출 문제와 비슷하긴 했는데 조금 다르다면 시험을 잘 못 볼 수가 있겠죠. 근데 여러분들이 이 기출 문제를 현재 실력을 가늠하는 용도로 사용한다면 어떻게 해야 될까요? 전공 문제로 여러분들이 먼저 공부를 하고 기출 문제로 실력을 가늠한다면 실제 시험을 치렀을 때 약간 문제가 다르더라도 좋은 시험 결과를 받을 수가 있게 됩니다. 따라서 별도의 검증 데이터를 통해 모델을 평가하고 튜닝하는 것이 중요합니다. 그리고 테스트 데이터는 가능한 한 실제 환경 그러니까 우리 모델이 실제로 서비스될 환경과 유사하도록 구성하는 것이 매우 중요합니다. 데이터를 분할하는 대표적인 방법 중의 하나는 사이클론에 있는 트레인 테스트 스플릿 함수를 이용하는 것입니다. 다음 예시는 불균형한 데이터셋을 생성하고 이 데이터를 분할하는 예시를 보여줍니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 0,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 1075,
      "char_count": 1942
    },
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c001_c71386",
      "content": "[기타] (3강) 학습, 검증, 테스트 데이터 이해하기(1)\n\n다. 따라서 별도의 검증 데이터를 통해 모델을 평가하고 튜닝하는 것이 중요합니다. 그리고 테스트 데이터는 가능한 한 실제 환경 그러니까 우리 모델이 실제로 서비스될 환경과 유사하도록 구성하는 것이 매우 중요합니다. 데이터를 분할하는 대표적인 방법 중의 하나는 사이클론에 있는 트레인 테스트 스플릿 함수를 이용하는 것입니다. 다음 예시는 불균형한 데이터셋을 생성하고 이 데이터를 분할하는 예시를 보여줍니다. 먼저 메이크 클래시피케이션이라고 하는 이 함수는 내가 전달한 샘플의 개수 그리고 피처의 수 그리고 클래스의 수를 기반으로 데이터셋 가짜 데이터셋을 만들어 주고요. 웨이츠 인자로 전달한 비율에 따라서 각 클래스의 비율이 설정이 됩니다. 이 생성한 데이터셋 라지 x와 스몰 y를 트레인 테스트 스플릿에다가 순차적으로 전달을 하고 트레인 사이즈 0.8 그리고 랜덤 스테이트 42를 전달을 해서 데이터를 분할할 수 있습니다. 여기에서 랜덤 스테이트는 여러분들이 사이킬런에서 랜덤한 숫자를 만들어내기 위한 시드 값으로 사용됩니다. 어떤 숫자를 사용해도 상관이 없으니 이 시드 값은 고정하시는 것을 강력하게 추천을 드립니다. 이제 분할된 데이터의 쉐입을 한번 보겠습니다. 오리지널 데이터 셋은 100만 개의 로우가 있고 20개의 컬럼이 있었습니다. 트레인 데이터 셋 같은 경우에는 80만 개 그리고 테스트 데이터 셋은 20만 개로 분할된 것을 확인할 수가 있습니다. 다음으로는 앞서 설명드렸던 훈련 검증 테스트 데이터로 분할하는 예시를 보겠습니다. 이전과 동일한 조건으로 데이터셋을 생성을 하고요. 이번에는 트레인 테스트 스플릿을 두 번 진행하겠습니다. 첫 번째로 트레인 데이터셋은 0.7 70% 정도를 할당하겠습니다. 그다음 생성된 스탬프 와 템프라고 하는 거를 한 번 더 스플릿을 해서 반반으로 분할하는 예시를 보여줍니다. 이런 식으로 엑스트레인 엑스펠리드 엑스 테스트 이 3개의 데이터셋을 구성을 할 수가 있습니다. 이런 식으로 구성하게 되면 70대 15대 15 비율로 데이터를 분할하게 됩니다. 출력된 결과를 보시면 아시겠지만 70만 개, 15만 개, 15만 개로 데이터가 분할된 것을 확인할 수가 있습니다. 다음은 훈련 검증 테스트로 분할된 데이터의 레이블을 확인을 한번 해 보겠습니다. 우리가 이 데이터셋을 생성을 할 때 7 대 3 대 90 비율로 클래스를 생성을 했습니다. 그래서 오리지널 레이블 레이시오를 확인을 해 보시면 7 대 3 대 90으로 분할되어 있는 것을 확인하실 수가 있고요. 트레인 레이블 같은 경우도 6.96 3.0 90.04 밸리데이션은 7.06 3.04 89.91 테스트는 7.12 2.98 89.9로 분할된 걸 보실 수가 있습니다. 오리지널 레이블 레이시오가 거의 유사하게 분할된 것을 확인할 수가 있습니다. 이런 경우는 데이터 샘플이 충분히 큰 경우에 가능한 상황입니다. 하지만 데이터를 적게 해 보겠습니다. 100만 개의 데이터를 생성했던 것과 달리 이번에는 천 개 정도 데이터만 생성을 해보겠습니다. 트레인 레이블 같은 경우는 8.0 3.0 89.9 밸리데이션은 5.33 5.33 89.33 테스트는 4.0 0.67 95.33 이런 식으로 레이블의 비율이 좀 틀어진 것을 볼 수가 있습니다. 이런 경우에는 우리가 스트레티 파이라고 하는 인자를 추가를 해서 기존 오리지널 레이블 레이시오를 유지하는 연산을 할 수가 있습니다. 이 스트레티파이라고 하는 인자에 어떤 것들을 전달하면 되냐면 기존 레이블을 전달하면 됩니다. 그러면 트레인 테스 스플릿이라고 하는 이 함수 내에서 스트레디파이로 전달된 저 와의 클래스 비율을 기반으로 해서 예시를 보시면 아시겠지만 약간 숫자가 틀어지긴 했지만 그래도 거의 비슷한 수치가 나오는 것을 확인하실 수가 있습니다. 앞서 말씀드렸던 예시와 같은 경우는 레이블을 기반으로 스트레이디 파일을 하기 때문에 사실상 분류 문제에 해당한다고 볼 수가 있는데요. 이번에는 연속적인 수치를 예측하는 회귀 문제에 적용할 수 있는 방법에 대해서 이야기해 보겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 1,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 1063,
      "char_count": 2000
    },
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c002_2028fb",
      "content": "[기타] (3강) 학습, 검증, 테스트 데이터 이해하기(1)\n\n다. 그러면 트레인 테스 스플릿이라고 하는 이 함수 내에서 스트레디파이로 전달된 저 와의 클래스 비율을 기반으로 해서 예시를 보시면 아시겠지만 약간 숫자가 틀어지긴 했지만 그래도 거의 비슷한 수치가 나오는 것을 확인하실 수가 있습니다. 앞서 말씀드렸던 예시와 같은 경우는 레이블을 기반으로 스트레이디 파일을 하기 때문에 사실상 분류 문제에 해당한다고 볼 수가 있는데요. 이번에는 연속적인 수치를 예측하는 회귀 문제에 적용할 수 있는 방법에 대해서 이야기해 보겠습니다. 우리도 똑같이 연속적인 이 타겟 변수 이 타겟 변수가 골고루 분할이 되면 좋겠다 이런 니즈가 있다고 했을 때 사용할 수 있는 방법으로는 비닝이 있습니다. 이 타겟 변수의 이 값을 구간으로 분리를 하여 각 구간에 매핑을 하는 거다라고 생각하시면 되는데요. 그래서 이 비닝된 이 레이블을 스트레딧 파이 인자로 전달을 해서 트레인 밸리드 테스트 데이터 셋의 이 레이블 값이 균일하게 분할되도록 하실 수 있습니다. 이전 단계에서 생성한 트레인 밸리드 테스트 데이터 셋을 모델 학습에 사용하기 위해서 여러분들이 하셔야 되는 것은 파이토치 데이터셋, 파이토치 데이터 로더로 만드는 것입니다. 데이터셋은 샘플 단위로 데이터를 꺼낼 수 있도록 도와주는 객체이고요. 데이터 로더는 미니 배치 셔플 혹은 학습에 최적화된 방식으로 데이터들을 공급하는 객체입니다. 또한 파이터치의 데이터 로더는 파이썬의 제너레이터 구현체로 구현됩니다. 그렇기 때문에 다음 예시와 같이 포문을 통해서 여러분들이 필요한 미니 배치만큼 데이터를 공급받을 수 있고 공급받은 데이터를 모델 학습 그리고 모델 검증 그리고 모델 테스트에 사용하게 됩니다. 다음은 학습 데이터를 준비할 때 주의해야 할 점에 대해서 이야기해 보겠습니다. 첫 번째로는 데이터 분할 비율입니다. 여러분들이 좀 궁금하실 부분일 거예요. 얼마만큼의 비율로 데이터를 분할해야 될까 인데 일반적으로는 8 대 1 대 1 정도를 사용을 합니다. 하지만 정답은 없습니다. 여러분들이 보유하고 있는 데이터의 수에 따라 그리고 또 도메인에 따라 다를 수 있습니다. 데이터가 충분히 많은 경우 검증 테스트 데이터가 통계적으로 신뢰할 수 있는 충분한 샘플들로만 구성되면 됩니다. 예시로는 극단적으로 트레인 밸리드 테스트의 비율이 98 대 1 대 1이 될 수도 있다라는 거고요. 데이터가 적은 경우에는 검증 테스트 데이터의 비율이 높아지면서 학습 데이터의 비율이 줄어들 수 있습니다. 예를 들어 트레인 밸리드 테스트의 비율이 6 대 2 대 2가 될 수 있고요. 여러분들이 학습 데이터 비율이 적어지는 문제를 보완하기 위해서 교차 검증을 통해 모델을 검증하기도 합니다. 교차 검증은 추후 강의에서 다뤄보도록 하겠습니다. 다음은 데이터 누수입니다. 모델을 학습하는 과정에서 예측하려는 타겟 정보가 직접적 혹은 간접적으로 노출되는 문제입니다. 모델 성능이 과장되어 측정되는 반면에 실제 환경에서는 좋은 성능이 나오지 않습니다. 여기서 직간접적으로 노출되는 문제라고 함은 밸리드 테스트 데이터의 정보가 트레인 데이터에 노출이 된다고 이해하시면 됩니다. 몇 가지 데이터 누수 사례에 대해서 확인을 해 보겠습니다. 예측 시점 이후의 정보를 포함하는 경우입니다. 예측 시점 이후 정보를 포함하여 모델을 학습시키면 검증 및 테스트 단계에서 좋은 성능을 보일 수 있지만 실제 운영 환경에서는 이러한 정보를 사용할 수 없거나 혹은 정보가 누락되어 있기 때문에 성능이 저하됩니다. 예를 들어 12월 20일에 수집된 교통량 로그를 통해 교통량 예측 모델을 구현하려고 합니다. 우리가 12월 20일 오후 2시에 교통량을 예측하기 위해 12월 20일 평균 온도를 입력 변수로 사용한다면 이는 데이터 누수에 해당합니다. 왜냐하면 12월 20일 오후 2시 시점에는 12월 20일 평균 온도를 알 수 없기 때문입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 2,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 1046,
      "char_count": 1914
    },
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c003_e3fb50",
      "content": "[기타] (3강) 학습, 검증, 테스트 데이터 이해하기(1)\n\n다. 예를 들어 12월 20일에 수집된 교통량 로그를 통해 교통량 예측 모델을 구현하려고 합니다. 우리가 12월 20일 오후 2시에 교통량을 예측하기 위해 12월 20일 평균 온도를 입력 변수로 사용한다면 이는 데이터 누수에 해당합니다. 왜냐하면 12월 20일 오후 2시 시점에는 12월 20일 평균 온도를 알 수 없기 때문입니다. 물론 12월 20일 오후 2시 시점에 오전 00시 시점부터 오후 2시 시점까지의 평균 온도를 사용할 수는 있지만 그것도 실제 학습할 때 사용하는 12월 20일 평균 온도와는 다른 값이 되겠죠. 다음으로는 타깃 변수의 직접적인 정보를 제공하는 경우입니다. 타겟 변수에 대해 직접적인 정보를 제공하는 데이터를 훈련 데이터셋에 포함하는 경우입니다. 예를 들어 특정 질병에 대한 환자의 진단을 예측하는 모델을 만들려고 합니다. 이때 환자의 현재 복용 중인 치료약을 변수로 사용하는 경우 해당 약물 정보는 진단 결과를 직접적으로 나타내므로 데이터 누수가 발생할 수 있습니다. 두 번째 예시입니다. 보험회사에서 사전에 유저 이탈을 막기 위해 유저의 해지 여부를 예측하는 모델을 만들려고 합니다. 이때 유저의 보험 해지를 변수로 사용하는 경우 모델이 유저가 해지했는지를 직접적으로 알 수 있기 때문에 데이터 수가 발생할 수 있습니다. 물론 이런 예시는 약간 극단적인 예시일 수 있지만 여러분들의 이해를 돕기 위해 추가한 점임을 참고해 주시면 감사하겠습니다. 다음으로는 가장 많이 실수할 수 있는 부분입니다. 전처리 과정에서 검증 테스트 데이터의 정보가 사용되는 경우입니다. 데이터 전처리 과정에서 전체 데이터의 통계를 사용하여 전 처리를 진행하면 데이터 누수가 발생합니다. 왼쪽의 예시를 먼저 보겠습니다. 여러분들이 데이터를 트레인 밸리드 테스트로 분할하기 이전에 스탠다드 스케일러를 기반으로 수치형 변수들에 대해 트랜스폼을 진행한다면 이미 밸리드 테스트로 분할될 데이터의 통계량 정보가 트레인 데이터 셋에 녹아 들어가게 됩니다. 이럴 때는 데이터 누수가 발생할 수밖에 없고요. 올바른 방법으로는 여러분들이 스탠다드 스킬러와 같이 특정 피처에 전체 행에 대해 어떤 통계량 값을 계산을 하고 그 통계량을 기반으로 연산을 하는 경우에는 데이터를 훈련 검증 테스트 데이터로 분할한 이후에 저 작업을 진행하셔야 됩니다. 그래서 오른쪽의 예시를 보면 데이터를 훈련 데이터와 검증 데이터로 먼저 분할을 하고요. 그다음 훈련 데이터에 대해 먼저 스케일링을 학습을 하고 변환을 합니다. 이후에는 검증 데이터에 대해서는 트랜스폼 트레인 데이터의 통계량을 기반으로 변환한 이제 이 스케일링을 적용만 하게 됩니다. 예시에는 누락이 되어 있지만 엑스 테스트에도 동일하게 스케일러 닷 트랜스폼을 통해서 스케일링 변환을 해주어야 됩니다. 다음으로는 똑같은 샘플이 훈련 데이터와 검증 테스트 데이터에 존재하는 경우입니다. 사실 이 부분은 우리가 데이터를 가져오는 단계에서 복제가 발생할 수 있게 되는데요. 이 복제가 발생하는 부분은 우리가 조인 쿼리를 잘못 작성했다거나 혹은 데이터를 적재하는 과정에서 중복이 발생하는 그런 문제가 있을 수 있습니다. 이런 경우에는 똑같은 샘플이 훈련 검증 테스트 데이터에 존재하는 경우 데이터 누수가 발생할 수 있고요. 예시를 보면 훈련 데이터 검증 데이터에 똑같은 샘플이 존재하는 것을 볼 수가 있습니다. 이런 경우에는 우리가 데이터를 가공할 때 반드시 디듀플리케이션 복제를 제거하는 연산을 하셔야 된다라는 점을 유념하시면 좋겠습니다. 다음은 데이터 불균형입니다. 분류 문제에서 클래스 간의 비율 차이가 많이 나는 경우를 의미합니다. 현실 세계에서도 흔하게 나타날 수 있는데요. 예를 들어서 특정 병을 분류한다거나 혹은 프라우드 디텍션 같은 테스크를 푸는 경우에도 그렇고요. 여러분들이 반도체와 같이 수율 99.9%가 나오는 이런 공정에서도 비슷한 문제가 발생할 수 있습니다. 사실 대부분의 현업 데이터는 인밸런스 되어 있다고 생각하시는 게 편합니다. 데이터 불균형이 왜 문제가 되는지 이야기해 보겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 3,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 1103,
      "char_count": 2012
    },
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c004_acec61",
      "content": "[기타] (3강) 학습, 검증, 테스트 데이터 이해하기(1)\n\n다. 다음은 데이터 불균형입니다. 분류 문제에서 클래스 간의 비율 차이가 많이 나는 경우를 의미합니다. 현실 세계에서도 흔하게 나타날 수 있는데요. 예를 들어서 특정 병을 분류한다거나 혹은 프라우드 디텍션 같은 테스크를 푸는 경우에도 그렇고요. 여러분들이 반도체와 같이 수율 99.9%가 나오는 이런 공정에서도 비슷한 문제가 발생할 수 있습니다. 사실 대부분의 현업 데이터는 인밸런스 되어 있다고 생각하시는 게 편합니다. 데이터 불균형이 왜 문제가 되는지 이야기해 보겠습니다. 모델이 다수 클래스에 맞춰 학습되어 소수 클래스 예측 성능이 떨어질 수 있기 때문입니다. 이는 모델이 불균형한 데이터에서 대부분 다수 클래스를 예측하기 때문에 발생하게 되는데요. 이 문제는 특정 테스크에서 조금 더 크게 두드러집니다. 예를 들어서 특정 질병을 예측한다거나 혹은 프라우드 디텍션이라거나 혹은 불량을 예측하는 테스크에서는 그 마이너한 이 소수의 클래스를 잘 잡는 게 잘 예측하는 게 매우 중요하기 때문에 이 데이터 불균형을 잘 해결을 해야 합니다. 또한 평가 지표를 왜곡시킵니다. 정확도와 같은 지표는 불균형 데이터에서 의미가 없을 수 있습니다. 예를 들어서 클래스 a가 95%, 클래스 비가 5%인 경우 항상 a를 예측해도 정확도가 95%가 됩니다. 그러나 이는 실제 성능이 좋은 것을 의미하는 것은 아닙니다. 다음으로는 모델의 일반화 성능을 저하시킬 수가 있습니다. 특정 클래스에 편향되게 학습된 모델은 새로운 데이터에 대한 일반화 성능이 떨어집니다. 이는 실제 환경에서 모델의 신뢰성을 저하시킬 수 있습니다. 데이터 불균형 문제를 해결하는 방법에 대해서 이야기해 보겠습니다. 먼저 분류입니다. 일반적으로 샘플링 기법을 통해서 우리가 데이터를 조정할 수 있습니다. 이러한 기법은 크게 언더 샘플링과 오버 샘플링으로 나눌 수 있습니다. 언더 샘플링은 많은 데이터 셋을 적은 쪽으로 맞추는 거, 오버 샘플링은 적은 데이터를 많은 쪽으로 맞추는 것을 이야기합니다. 이러한 언더 샘플링과 오버 샘플링을 텍스트 데이터 관점에서 다뤄보겠습니다. 먼저 언더샘플링입니다. 언더 샘플링이란 데이터 불균형 문제를 해결하기 위해 다수의 클래스의 데이터를 줄여서 소수 클래스의 비율과 맞추는 방법이라고 말씀을 드렸었는데요. 장점으로는 유의미한 실제 데이터만을 남겨서 학습 시간이 감소될 수 있습니다. 단점으로는 일부 데이터를 제거하면서 정보 손실이 발생할 수 있습니다. 대표적인 방법으로 랜덤 언더 샘플링을 이야기해 보겠습니다. 다수 클래스의 데이터를 무작위로 선택하여 제거를 합니다. 이를 통해 소수 클래스 비율과 맞추는 방법을 사용을 하는 건데요. 랜덤으로 샘플링 하기 때문에 처리 속도가 매우 빠르지만 샘플링 할 때마다 다른 결과가 나올 수가 있습니다. 또한 중요한 데이터 샘플이 제거될 수가 있겠죠. 다음으로는 리얼 미스입니다. 텍스트 데이터를 어떤 수치형 벡터로 만드는 인베딩을 통해서 변환을 시키고 벡터 공간에서 소수 클래스 데이터 포인트에 가장 가까운 다수 클래스 데이터를 선택해서 다수 클래스를 줄이는 방법입니다. 이제 그림으로 보시면 소수 클래스에 가까운 데이터 샘플만을 남기고 바깥쪽에 있는 데이터 샘플을 제거하는 것을 보실 수가 있습니다. 다음은 토맥 링스입니다. 토맥 링크는 서로 다른 클래스에 속하는 두 데이터가 서로 가장 가까운 경우를 이야기를 합니다. 어 방금 설명드렸던 리어 미스랑은 약간 다른 개념인데요. 동일하게 텍스트 데이터를 인베딩으로 변환하고 벡터 공간에서 이런 토믹 링크들을 찾습니다. 그중 다수 클래스에 속하는 데이터를 제거합니다. 이는 가까이에 있는 데이터들이 어찌 보면 노이즈라고도 우리가 판단할 수 있습니다. 그래서 가까이에 있는 이 클래스들 데이터 샘플들을 제거를 하고 어느 정도 좀 거리가 있는 데이터 샘플들만 남기는 방식입니다. 이렇게 되면 노이즈에 민감하지 않은 모델을 학습시킬 수가 있게 됩니다. 다음은 오버 샘플링입니다. 소수의 클래스를 데이터를 늘려서 다수의 클래스의 비율과 맞추는 방법을 말합니다. 장점 중의 하나는 정보 손실이 없습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 4,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 1120,
      "char_count": 2032
    },
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c005_db10d8",
      "content": "[기타] (3강) 학습, 검증, 테스트 데이터 이해하기(1)\n\n다. 그중 다수 클래스에 속하는 데이터를 제거합니다. 이는 가까이에 있는 데이터들이 어찌 보면 노이즈라고도 우리가 판단할 수 있습니다. 그래서 가까이에 있는 이 클래스들 데이터 샘플들을 제거를 하고 어느 정도 좀 거리가 있는 데이터 샘플들만 남기는 방식입니다. 이렇게 되면 노이즈에 민감하지 않은 모델을 학습시킬 수가 있게 됩니다. 다음은 오버 샘플링입니다. 소수의 클래스를 데이터를 늘려서 다수의 클래스의 비율과 맞추는 방법을 말합니다. 장점 중의 하나는 정보 손실이 없습니다. 단점으로는 과적합이 발생할 가능성이 있습니다. 왜냐하면 소수 클래스를 늘리는 방식에서 생길 수 있는 문제는 우리가 데이터를 생성을 할 때 그 소수 클래스를 기반으로 소수 클래스 데이터셋을 기반으로 만들기 때문에 이 소수 클래스가 충분히 이 클래스의 전체를 대표할 수 없는 샘플이라고 한다면 이제 과적하게 발생할 수 있다라고 설명을 드렸습니다. 언더 샘플링과 동일하게 오버 샘플링에도 랜덤 오버 샘플링 혹은 리샘플링이라는 기법이 있습니다. 소스 클래스 예제를 무작위로 중복시키는 간단한 방법으로 이 방법을 통해 소스 클래스의 데이터 양을 증가시켜 모델의 학습을 돕습니다. 예시를 보면 소수의 똑같은 데이터 샘플을 복사하는 방식으로 샘플링을 진행하게 됩니다. 다음은 이지 데이터 어그멘테이션이라는 방법인데요. 간단한 방법으로 소스 클래스의 데이터 양을 증가시켜 모델의 학습을 돕습니다. 이 방식에는 여러 가지가 있는데 첫 번째로는 동의어 대체가 있습니다. 단어를 랜덤으로 선택하여 동의어로 교체합니다. 예를 들어 원본으로 이 영화는 정말 감동적이었다 라는 것을 이 영화는 매우 감동적이었다로 변경할 수 있고요. 다음으로는 무작위 삽입이 있습니다. 문장 내 임의의 위치에 랜덤한 동의어를 삽입합니다. 원본으로 나는 책을 읽었다 증강된 샘플로 나는 어제 책을 읽었다. 다음으로는 무작위 삭제가 있습니다. 일정 확률로 랜덤하게 단어를 삭제합니다. 원본으로 맛있는 음식을 먹었다라는 문장에서 맛있는 이 제거된 음식을 먹었다가 됩니다. 다음은 무작위 교환입니다. 문장 내에서 두 개의 단어 위치를 서로 바꿉니다. 오늘 날씨가 매우 좋다를 날씨가 오늘 매우 좋다로 변경할 수 있습니다. 다음으로는 언 이지 데이터 어그멘테이션이라고 하는 건데요. 기존 문장의 특수 기호, 마침표, 쉼표, 느낌표, 물음표 등을 무작위 위치에 삽입해서 데이터를 증강시키는 방법입니다. 우리가 이전에 데이터를 정제하는 과정에서 마침표, 쉼표, 느낌표나 물음표와 같은 특수 기호 등을 삭제를 하거나 삭제하지 않을 수 있다라고 했었는데 이 방식도 동일한 맥락으로 항상 사용할 수 있는 건 아니고 사용할 수 있는 그런 조건들이 있을 거다 라고 생각하시면 되겠습니다. 다음은 백 트랜슬레이션입니다. 원본 문장을 다른 중간 언어로 번역한 뒤에 다시 원본 언어로 되돌아오는 과정을 통해 데이터를 증강시키는 방법입니다. 한글로 작성되어 있는 문장을 영어로 번역한 뒤 이 번역한 문장을 다시 한글로 되돌리면서 증강을 하는 방법입니다. 뭐 단순히 영어로 번역할 필요 없이 영어 말고도 일본어, 중국어, 혹은 뭐 다른 나라의 어떤 언어로 번역한 뒤에 돌아오는 방법으로 여러분들이 오버 샘플링을 진행하실 수가 있습니다. 다음으로는 LLM 기반 증강입니다. LLM을 이용해서 기존 텍스트의 맥락을 유지한 채 새로운 문장을 생성해서 데이터를 증강시키는 방법입니다. 원본으로 최근 날씨가 매우 춥다 증강된 버전으로 요즘 기온이 매우 낮다 이런 결과가 나올 수 있습니다. 엘엘엠을 이용하기 때문에 여러분들이 작성하는 프롬프트에 따라 증강되는 데이터 샘플의 품질이 달라질 수 있습니다. 다음은 회귀 문제를 간단하게 다뤄보겠습니다. 회귀 문제에는 데이터 불균형이 어떤 분류처럼 클래스 불균형이 있기보다는 타겟 변수가 한쪽으로 치우친 경우가 존재할 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 5,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 1061,
      "char_count": 1921
    },
    {
      "id": "transcript_기타_3강_학습_검증_테스트_데이터_이해하기1_c006_43e481",
      "content": "[기타] (3강) 학습, 검증, 테스트 데이터 이해하기(1)\n\n다. 다음으로는 LLM 기반 증강입니다. LLM을 이용해서 기존 텍스트의 맥락을 유지한 채 새로운 문장을 생성해서 데이터를 증강시키는 방법입니다. 원본으로 최근 날씨가 매우 춥다 증강된 버전으로 요즘 기온이 매우 낮다 이런 결과가 나올 수 있습니다. 엘엘엠을 이용하기 때문에 여러분들이 작성하는 프롬프트에 따라 증강되는 데이터 샘플의 품질이 달라질 수 있습니다. 다음은 회귀 문제를 간단하게 다뤄보겠습니다. 회귀 문제에는 데이터 불균형이 어떤 분류처럼 클래스 불균형이 있기보다는 타겟 변수가 한쪽으로 치우친 경우가 존재할 수 있습니다. 이러한 경우는 모델이 한쪽으로만 예측하는 경향이 있을 수가 있는데요. 예시를 보시면 제가 예전에 들었었던 사람의 어떤 키 혹은 사람의 이동 거리, 우리 국민의 이동 거리 등을 했을 때 왼쪽에 있는 것처럼 오른쪽으로 치우쳐진 분포가 보여지는 경우가 있고요. 혹은 왼쪽으로 치우쳐진 분포가 있는 경우가 있습니다. 이러한 경우에 학습을 할 때는 잠시 종모양 분포로 변환해서 학습합니다. 학습된 모델이 추론할 때는 다시 예측 값을 원래 분포로 변환을 하게 되는데요. 대표적인 방법으로는 로그 트랜스포메이션이 있습니다. 우리 왼쪽에 있는 예시를 보겠습니다. 오른쪽으로 치우쳐진 분포는 사실 우리가 다른 용어가 있습니다. 로그노말 분포라고 이야기를 하는데요. 저런 형태의 분포를 가진 변수에 우리가 로그를 취하면 가운데에 있는 종 모양의 분포로 변환이 되게 됩니다. 하지만 여기에서 주의할 점이 있습니다. 사실 우리가 예측하고 싶은 건 오른쪽으로 치우쳐진 분포의 스케일에서의 어떤 예측 값일 겁니다. 그러나 우리가 학습할 때 종 모양의 분포로 만들고 학습을 하게 되면 로그 스케일에서의 예측 값을 모델이 반환을 하게 되는데요. 여기에서 로그 스케일의 예측 값을 반환한 그 결과 값을 다시 익스포넨셜을 취해서 원 분포로 변경을 했을 때 실제 우리가 원하는 예측 값이 아닐 수 있습니다. 이 부분은 여러분들께 과제로 한번 드리도록 하겠습니다. 여러분들이 이제 고민해야 되는 포인트는 이겁니다. 우리가 오른쪽으로 치우쳐진 분포를 로그 트랜스폼을 통해 종 모양의 골고루 된 분포로 변경을 한 뒤 이 타겟 변수를 기반으로 모델을 학습한 다음 출력한 결과를 원 분포로 다시 되돌렸을 때 어떤 문제가 생길까 이거를 고민해 보시면 좋겠습니다. 네 이번 강의 요약을 하겠습니다. 첫 번째로 데이터셋 이해하기에서는 훈련 검증, 테스트 데이터셋에 대해서 이야기를 했었습니다. 훈련 데이터는 모델을 학습하기 위해 사용하는 데이터였고, 검증 데이터는 하이퍼 파라미터 튜닝 및 과적합 여부를 평가하기 위해 사용하는 데이터였습니다. 테스트 데이터는 모델의 일반화 성능을 평가하기 위해 사용하는 데이터입니다. 학습 데이터를 준비할 때 주의해야 할 점에 대해서도 이야기했었습니다. 데이터 분할 비율은 상황에 따라 비율을 조정할 수 있었습니다. 데이터 누수는 학습 단계에서 예측에 활용할 수 없는 정보가 직간접적으로 노출되는 경우에 발생할 수 있었고요. 데이터 불균형은 분류 문제에서 특정 클래스가 다른 클래스에 비해 현저하게 적거나 많은 경우에 발생할 수 있었습니다. 이번 강의 여기까지 하겠습니다. 들어주셔서 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(3강) 학습, 검증, 테스트 데이터 이해하기(1).json",
        "lecture_name": "(3강) 학습, 검증, 테스트 데이터 이해하기(1)",
        "course": "기타",
        "lecture_num": "3강",
        "lecture_title": "학습, 검증, 테스트 데이터 이해하기(1)",
        "chunk_idx": 6,
        "total_chunks": 7,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:d644d2ae576bb0ea834e81c89a548d9fade9b028bbcbada0aed8d21fc601685b"
      },
      "token_estimate": 889,
      "char_count": 1612
    }
  ]
}