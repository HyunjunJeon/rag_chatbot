{
  "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
  "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
  "course": "Generative AI",
  "total_chunks": 9,
  "chunks": [
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c000_f7351c",
      "content": "[강의 녹취록] 과목: Generative AI | 강의: 5강 | 제목: Image Generation 1 Image Generation  Models Overview\n\n안녕하세요. 고려대학교 산업경영공학부 강필성 교수입니다. 자 제너러티브 AI의 다섯 번째 강의 이미지 제너레이션 파트1 그래서 이미지 제너레이션에 대한 모델들을 개괄적으로 알아보는 시간을 갖도록 하겠습니다. 이미지 제너레이션 관련해서는 크게 세 가지 정도의 접근 방법이 있는데요. 제너러티브 어드버서리얼 네트워크 적대적 생성 신경망을 이용한 방식, 오토 인코더를 활용한 방식 그리고 최근에 가장 많이 널리 사용되는 디퓨전 모델을 사용한 방식이 있습니다. 각각의 방식들에 대해서 대표적인 모델들을 가볍게 한번 확인을 해보도록 하겠습니다. gnrativ adv스r 네트워크 지금부터는 줄여서 겐이라고 부르겠습니다. GA 기반의 방식은 오리지널 겐으로부터 시작해서 픽스토픽스, 사이클 겐, 스타겐, 프로그레시브 겐 스타이겐까지 이루어져 있습니다. 제너럴티브 어드버시럴 네트워크이라는 것은 판별자와 생성자를 적대적으로 학습시키는 모델 구조입니다. 판별자 디스크리미네이터는 입력 이미지가 생성된 이미지인지 진짜 이미지인지를 판별하는 겁니다. 그림상에서 봤을 때 초록색 실제 이미지와 파란색 생성된 이미지를 2개를 입력으로 받아서 제로원의 아웃풋을 받아내는 것이고요. 생성자는 잠재 변수인 z를 입력으로 받아서 학습 데이터의 분포에 가까운 이미지를 생성하는 것입니다. 그렇기 때문에 이 제너레이터가 잘 생성한 x 프라임이 정말로 원본 데이터와 같다면 디스크리미네이터 판별자는 이 둘 사이에 누가 진짜 그림이고 어떤 것이 가짜 그림인지를 잘 판별하지 못하게 되는 것이지요. 따라서 적대적 학습이다라고 하는 게 왜 나왔냐면은 판별자 d 디스크리미네이터는 데이터의 생성 여부 다시 말하면 진짜 이미지인지 생성된 이미지인지를 잘 판단하도록 학습을 시키는 것이고요. 생성자 g는 판별자가 생성 여부를 잘 판단하지 못하도록 학습을 시키는 겁니다. 그래서 이 그림에서의 로스의 이 에 겐 겐에서의 로스의 목적은 디스크리미네이터는 맥시마이즈를 시켜야 되고 제너레이터 관점에서는 미니마이즈를 시켜야 되는 건데 이 브 디와 쥐 디콤마 쥐라는 게 무엇이냐면 결국은 스가 실제 데이터의 분포일 때는 스를 넣었을 때의 디스크로네이터는 1에 가까운 값을 나타내야 되기 때문에 진짜 그림이라고 판별해야 되기 때문에 로그 디스 값이 크게 나타나야 되는 것이고요. 또한 여기서 스가 만약에 피 제트의 제라고 한다면 이거는 생성된 이미지니까 엑스를 이용해서 생성된 이미지를 가지고 디스크리미네이터에 넣었을 때는 굉장히 낮은 값 0에 가까운 값이 나타나야 됩니다. 그렇기 때문에 이 값이 커지게 되면 결국은 판별자가 이기는 것이고요. 지게 되면 작아지게 되면 생성자가 이기는 것인데 그림에서 보시는 바와 같이 모델 수렴 전에 서로 다른 모양을 가지고 있던 데이터의 분포가 모델 수렴 후에는 같아짐으로써 결국은 생성자와 이 판별자가 서로서로 공진화해서 어느 정도 수준에서 수렴을 하는 이러한 과정을 거치게 됩니다. 이러한 겐이 처음 등장했을 때 굉장히 센세이셔널 했고요. 여기에서부터 다양한 겐의 아류작들이 나옵니다. 첫 번째 컨디셔널 겐이라고 하는 거는 겐 학습에 조건을 주입해서 학습하는 구조입니다. 지금까지는 오리지널 개는 여기 보시는 그림에서의 파란색의 스만을 가지고 디스크리미네이터와 제너레이터를 학습을 했다면 그림에서 보시는 바와 같이 초록색의 y라는 컨디션을 주어서 우리가 원하는 무엇인가를 잘 적응할 수 있게 만드는 것이죠. 그래서 로스 함수를 보게 되면 아까는 그냥 여기가 스였다면 x gy y라는 조건을 주었을 때의 x에 대한 생성, 디스크메이터의 로그 프로버빌리티 그리고 y라는 조건을 주었을 때 스가 생성된 이미지를 가지고 디스크리미네이터에 적용했을 때의 로그 프로버빌리티 이걸 이용해서 로스 계산을 하게 됩니다. 픽스 투 픽스라는 것은 앞에서 보여주었던 컨디셔널 겐의 확장판으로서 이미지를 조건으로 이미지를 변환하는 방법입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 0,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1088,
      "char_count": 2000
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c001_0709cc",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 픽스 투 픽스라는 것은 앞에서 보여주었던 컨디셔널 겐의 확장판으로서 이미지를 조건으로 이미지를 변환하는 방법입니다. 그래서 이미지 2 이미지 트랜스플레이션이라고도 표현을 하고요. 학습을 위해서는 서로 매칭되는 페어드 이미지가 필요한데요. 그림에서 보시는 것처럼 레이블을 통해서 스트레 씬을 만들어 달라 그러면 여기에 있는 이 세그멘테이션 된 게 도로이거나 보라색 또는 파란색의 차량이거나 또는 주황색의 표지판이 있을 때 이걸 이용해서 이미지를 만들어 내 달라는 것이죠. 또는 엣지트 포터라고 하는 제일 마지막에 있는 예시 같은 경우에는 조건을 이렇게 인풋을 엣지가 짜여져 있는 그림에 그 스켈레톤으로 주면 이 안에 색깔을 채워 가지고 실제 실사와 비슷한 사진을 만들어 주는 것입니다. 이러한 픽스 트 픽스는 컨디셔널 겐 구조를 따르는 데 조건으로 이미지가 반영되는 것이고요. 그래서 아래 예시는 이미지 엣지 이미지를 조건으로 생성자를 통해서 만들어진 사진과 판별자 d는 생성된 사진 또는 실제 사진을 판별하도록 두 개의 조건을 같이 주는 것이죠. 조건을 줬을 때 왼쪽은 페이크가 되도록 만들어지고 오른쪽은 리얼이 되도록 이제 디스크리미네이터는 학습을 하는 거고 제너레이터는 그거를 디스크리미네이터가 잘 못 하도록 서로가 적대적으로 학습을 시키는 과정이 되겠습니다. 사이클 겐은 픽스 투 픽스 방식이 패어리 이미지가 필요하다는 단점을 보완한 방식입니다. 픽스 투 픽스는 아까 말씀드렸듯이 서로 간에 쌍이 되는 이미지가 필요한데 그렇기 때문에 많은 이미지를 확보하기가 어렵습니다. 사이클 겐은 이러한 단점을 극복하기 위해서 꼭 쌍이 아니더라도 언페어리 이미지를 통해서 사이클 컨시스턴스의 로스라는 것을 제안을 해서 학습을 시킵니다. 앞으로 여러분들께서 제 강의 또는 이후에 강의에서 컨시스턴시와 컨트라스티브라는 용어를 많이 접하게 되실 겁니다. 컨시스턴시는 쉽게 생각해서 일관성을 높여주는 정답이 없는 데이터에 대해서 어떠한 나에 대한 두 가지의 변형을 했을 때 그 둘 사이에 일관성이 높아지는 것을 의미하고요. 컨트라스티브는 나와 서로 다른 것들이 멀어지게 하는 목적이라고 생각하시면 용어를 이해하기가 쉬우실 겁니다. 그래서 여기서 보시는 것처럼 픽스트 픽스에서는 왼쪽 그림과 같이 XI와 YI라는 둘이 쌍으로 연결된 이미지들만을 학습할 수 있는 반면에 사이클 겐에서는 서로 간에 그냥 그림 또는 사진들 이미지가 뭐가 됐든지 간에 2개만 있으면 학습이 됩니다. 도대체 그게 어떻게 되는 것이냐 한번 말씀을 드려보면 여기서 예시부터 보여드리겠습니다. 언 페어드 이미지로 학습을 해서 이미지를 원하는 형태로 변환하는 것이 가능한 기법이라고 말씀을 드렸는데요. 이렇게 두 가지를 이렇게 학습을 하게 되면 사진을 모네였다가 포터로 바꾸는 것도 가능하지만 사진을 모네 스타일로 바꾸는 것도 가능하고요. 여기서 보시면 조건이 이 여름과 겨울 두 가지의 조건이 있는데 그러면 한여름 사진을 겨울 사진으로 바꾸는 것도 가능하고 겨울 사진을 여름으로 바꾸는 것도 가능해지는 것입니다. 이렇게 언페어드 이미지를 학습하기 위해서는 2개의 생성자인 지프와 판별자인 디스 dy가 학습에 사용이 되는데요. 이거를 구체적으로 조금 더 자세히 설명을 드리겠습니다. 사이클 겐에서의 학습을 위한 목적 함수는 겐의 로스와 컨시스턴스 사이클에 대한 로스로 구성이 됩니다. 겐에 대한 로스는 겐 학습을 위해서 어드버서리얼 트레이닝을 위한 로스가 되겠고요. 에어 사이클이라고 되어 있는 이 사이클에 대한 로스는 생성한 이미지를 다시 원본 이미지로 생성했을 때 그러니까 원본을 두 번 하는 것이죠. 했을 때 컨시스턴시를 유지하기 위한 로스입니다. 수식이 아닌 그림으로 한번 보겠습니다. 우선 x라는 이미지를 가지고 와라는 이미지로 만든 다음에 다시 와를 통해서 한 번 더 스로 변환을 했을 때 이 둘 사이의 차이가 최소한이 되어야 된다는 얘기입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 1,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1085,
      "char_count": 2000
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c002_8f18ce",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 겐에 대한 로스는 겐 학습을 위해서 어드버서리얼 트레이닝을 위한 로스가 되겠고요. 에어 사이클이라고 되어 있는 이 사이클에 대한 로스는 생성한 이미지를 다시 원본 이미지로 생성했을 때 그러니까 원본을 두 번 하는 것이죠. 했을 때 컨시스턴시를 유지하기 위한 로스입니다. 수식이 아닌 그림으로 한번 보겠습니다. 우선 x라는 이미지를 가지고 와라는 이미지로 만든 다음에 다시 와를 통해서 한 번 더 스로 변환을 했을 때 이 둘 사이의 차이가 최소한이 되어야 된다는 얘기입니다. 반대로 또 다른 쌍에 대해서는 와라는 이미지를 가지고 엑스로 만들었다가 다시 와로 복원을 했을 때 이 사이클에 대한 두 객체의 차이가 굉장히 작아야 된다는 말이죠. 그림을 하나씩 하나씩 나누어서 보겠습니다. 여기서 보시면 왼쪽에 있는 위의 그림에 대한 겐에 대한 로스를 먼저 말씀을 드리자면 첫 번째 겐에 대한 로스는 엑스를 통해서 쥐라는 함수를 통해서 와이와 비슷한 거를 생성했을 때 와와 생성된 스 지스 사이에 헷갈리게 하는 이 로스를 겟 로스로 만드는 것이고요. 반면에 두 번째 그림에서는 y를 입력으로 두어서 f라는 생성자를 이용해서 x에 가깝게 만들어 놓고 실제 스와 만들어진 스 사이에 디스크리미네이션을 잘 할 수 있도록 겐이 학습이 됩니다. 그래서 결국은 이 그림을 통해서 우리는 두 가지의 오리지널 겐이 학습이 된다라는 것을 알 수가 있습니다. 거기에 더해서 마지막으로 사이클로스 같은 경우에는 쥐라는 함수와 생성자와 프라는 생성자가 가지고 있는 내용인데요. 이것은 무슨 얘기냐면 순서를 잘 보십시오. 엑스를 통해서 먼저 쥐를 이용해서 한 번 와를 만든 다음에 다시 또 프함수라는 생성자를 씌우지요. 여기서 보시게 되자면 이렇게 한 번 가는 게 에프가 되는 것이 지가 되는 것이고요. 그리고 이렇게 한 번 다시 돌아오는 게 프가 되는 것입니다. 그랬을 때 이 둘 사이의 차이가 작으면 작을수록 좋기 때문에 1 l1 룸을 사용하는 것이고요. 마찬가지로 여기서도 역시 프라는 함수를 통해서 생성을 한번 하고 그렇게 생성된 이미지를 입력으로 해서 쥐라는 함수를 생성자를 통해서 한 번 더 학습을 했을 때 이 둘 사이에 로스가 줄어들어야 되기 때문에 엘1 로스를 한 번 더 사용하는 겁니다. 이런 식으로 해서 내가 한번 생성이 됐다가 다시 생성되는 역변환을 했다가 역변환을 하는 과정에서 원본과 역변환된 버전 사이가 매우 가까워야 된다. 컨시스턴시 로스의 개념입니다. 스타겐 같은 경우에는 여러 도메인을 생성 모델에 반영하기 위해서 많은 도메인별 생성 모델이 필요하고 학습 효율성이 떨어진다라는 이 단점을 사이클 겐의 단점을 보완한 것입니다. 그래서 모델 하나만으로 여러 도메인을 반영할 수 있는 구조를 제안하는 것이고요. 그림상에서 보시면 네 가지 정도의 도메인이 있을 때 이렇게 서로서로 각각을 바꿀 때 여기서 보시는 지원 쓰리 원 지투1, 지41 이런 것들이 있지 않습니까? 이게 전부 다 학습이 되어야 되는 모델인데 그게 아니라 다양한 도메인에 대해서 하나의 모델을 가지고 모두 반영하겠다라는 구조입니다. 이 스타인 같은 경우에는 사이클 계에서 제한된 사이클 컨시스턴스 로치와 도메인 클래시피케이션을 활용해서 여러 도메인을 반영할 수 있는 모델 구조를 제안한 방식이고요. 다시 한번 여기서 모델 학습을 위해서 어떤 식으로 목적 함수를 사용했는지 설명드리겠습니다. 세 가지를 사용을 합니다. 첫 번째는 겐의 일반적인 로스고요. 적대적 학습을 하기 위해서 필요한 로스고 두 번째는 도메인을 판단하기 위한 로스로써 클래시피케이션 로스라고 불리웁니다. 마지막으로는 사이클 컨시스턴스 오스를 그대로 사용하는 리컨스트럭션 로스입니다. 하나씩 보도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 2,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1020,
      "char_count": 1904
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c003_ff14bb",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 세 가지를 사용을 합니다. 첫 번째는 겐의 일반적인 로스고요. 적대적 학습을 하기 위해서 필요한 로스고 두 번째는 도메인을 판단하기 위한 로스로써 클래시피케이션 로스라고 불리웁니다. 마지막으로는 사이클 컨시스턴스 오스를 그대로 사용하는 리컨스트럭션 로스입니다. 하나씩 보도록 하겠습니다. 첫 번째 여기에 있는 이 로스 같은 경우에는 디스크리미네이터 로스라고 보이는데요. 주어진 이미지가 있을 때 여기서 시 프라임은 오리지널 도메인이고요. 시는 타겟 도메인이라고 했을 때 어떤 리얼 이미지와 페이크 이미지가 있을 때 이게 리얼인지 아닌지에 대해서 일반적인 이 겟 로스를 차용함과 동시에 밑에 보면 엘씨엘스r이라고 표현이 되어 있죠. 이 부분은 이 두 이미지가 어떤 도메인에 속하는지까지도 클래시피케이션을 해서 반영을 해 주는 방식입니다. 여기서도 마찬가지로 도메인 클래시피케이션이 추가가 되죠. 그러므로 인해서 겟 로스 플러스 사이클에 대한 부분들도 들어가는 겁니다. 디스크리미네이터는 이렇게 학습이 되는 것이고요. 제너레이션 같은 경우에는 겟 로스 더하기 클래시피케이션의 에프에 대한 페이크 이미지에 대한 클래시피케이션 로스 그리고 리컨스트럭션 로스 세 가지가 들어갑니다. 보시는 바와 같이 여기에는 이렇게 이제 사이클이 보이는데요. 타겟 도메인과 인풋 이미지를 줬을 때 우리가 원하는 도메인을 줬을 때 페이크 이미지를 재생성을 합니다. 그러면 이 페이크 이미지와 오리지널 도메인을 이렇게 주었을 때 얘네들이 리컨스트럭션을 다시 이미지를 하게 되는 것이고요. 이렇게 되었을 때 이 리컨스트럭션 된 이미지가 실제 인풋 이미지와 얼마나 가까운지까지도 로스에 반영하는 것입니다. 이게 페이크냐 아니냐를 잘 판별하지 못하게 해라. 클래스에 대한 부분을 판별을 잘 하게 해라. 그리고 리컨스트럭션을 미니마이즈 시켜라 이 세 가지의 로스가 함께 반영이 되어 있는 겁니다. 이렇게 학습에 반영된 스타겐을 통해서 여러 개의 도메인을 같이 넣게 되면 학습에 반영된 여러 도메인에 따라서 이미지 생성이 가능합니다. 보시는 바와 같이 인풋 이미지에 대해서 머리를 금발로 바꿔달라라는 도메인, 성별을 바꿔달라는 도메인, 좀 나이 들어 보이게 해달라는 도메인, 창백하게 해달라는 도메인 이거에 따라서 다 스타일이 바뀌고요. 또는 표정이 원래 표정에서 환한 표정, 즐거운 표정 또는 놀란 표정, 공포에 찬 표정으로 바꿔달라라고 했을 때 굉장히 그럴듯하게 비슷하게 바꿔주는 것을 보실 수가 있겠습니다. 이후에 나타난 것은 프로그레시브 겐입니다. 프로그래시브 겐은 고해상도 이미지는 결국은 굉장히 높은 해상도를 상대적으로 갖고 있기 때문에 이 고해상도 이미지 생성 모델을 학습하기 위해서는 많은 비용이 발생한다라는 단점이 있습니다. 그래서 고해상도 이미지를 생성하기 위해서 저해상도 이미지를 생성하는 것부터 해서 단계적으로 증가하는 모델입니다. 이러한 구조를 제안함으로써 적은 비용으로 빠른 수렴이 가능한 건데요. 여러 개를 하는데 왜 적은 비용으로 빠른 수렴이 가능하느냐 앞에서 만들어진 모델의 웨이트는 픽스 프리즈를 시키고 다른 웨이트만을 학습시키는 게 한 번에 처음부터 이만큼을 다 학습시키는 것보다 훨씬 더 효율적으로 학습이 된다라는 것을 알려주는 겁니다. 그래서 프로그래스 에는 16바이 16 이미지 해상도에서 32바이 32 이미지 해상도를 변화하는 모델 구조를 예시로 말씀을 드리면 여기 있는 것처럼 이미지 해상도를 키우는 과정에서 작은 해상도의 이미지와 큰 해상도의 이미지의 결과를 단순히 웨이티드 SM을 함으로써 계산하여 상하입니다. 여기 보시는 바와 같이 여기 있는 제너레이터와 디스크메이터에 대해서 원래 해상도의 개념과 그다음에 확장된 해상도에 따라서 1 마이너스 알파와 알파로 가중치가 부여되는 것을 보실 수가 있겠죠. 이걸 통해서 마치 딥러닝 예전 그 모델들에서 리지듀얼 커넥션을 사용하는 것처럼 활용하는 방식입니다. 마지막으로 스타일 겐입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 3,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1097,
      "char_count": 2017
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c004_fbb3c8",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 여기 보시는 바와 같이 여기 있는 제너레이터와 디스크메이터에 대해서 원래 해상도의 개념과 그다음에 확장된 해상도에 따라서 1 마이너스 알파와 알파로 가중치가 부여되는 것을 보실 수가 있겠죠. 이걸 통해서 마치 딥러닝 예전 그 모델들에서 리지듀얼 커넥션을 사용하는 것처럼 활용하는 방식입니다. 마지막으로 스타일 겐입니다. 스타일 겐 같은 경우에는 프로그레시브 겐 구조에서 스타일을 주입하는 방법을 제안한 모델이고요. 잠재 공간을 바로 사용하는 것이 아니라 매핑 네트워크인 프를 사용해서 변환된 w를 입력으로 사용합니다. 오른쪽 그림을 보면 굉장히 복잡한 구조처럼 보이지만 이게 어떤 의미냐라고 봤을 때 왜 스타일 겐이라는 개념이 이 매핑을 사용하느냐면 잠재 공간이라는 제티는 일반적으로는 가우시안 분포를 가져옵니다. 그렇기 때문에 데이터의 분포가 복잡하게 얽힌 형태로 구성이 되지만 학습 데이터의 분포가 사실 이미지 같은 경우는 두 인접한 픽셀 사이에서는 굉장히 그 픽셀에 대한 유사도가 높기 때문에 선형적으로 구성되어 있는 경우가 많지 그렇기 때문에 가우시안 분포를 가정한 잠재 공간은 데이터의 분포를 제대로 반영하지 못하는 문제점이 있습니다. 이것을 해결하기 위해서 매핑 펑션 f를 사용을 해서 데이터의 분포에 맞춰서 얽힘이 풀리도록 구성하는 거 그래서 원래 공간이 a라는 공간이라면 이걸 억지로 가우시안 공간에 이렇게 끼워 맞추면 이렇게 왜곡된 원 모양이 되는데 이 비를 사용하는 게 아니라 씨라는 것을 사용함으로써 뭔가 조금 더 명확하게 원래 공간과 인터폴레이션을 하는 것을 가능하게 하는 구조로 만드는 것이 바로 스타일링의 목적입니다. 그래서 이 스타일 개는 프로그레시브 겐 구조의 잠재 공간을 여기서 보시면 왼쪽 그림이 되겠죠 z를 변환하여 얻은 w에다가 어파인 트랜스포메이션을 적용하여 스타일을 계산하게 이 스타일 y는 어댑티브 인스턴스 노멀라이제이션이라고 그래가지고 각각의 평균과 표준 편차를 이용을 해서 반영을 하게 되는 것이고요. 이렇게 반영된 스타일들을 계속적으로 중간중간에 제너레이션 네트워크에 주입을 해줌으로써 스타일을 우리가 반영할 수 있게 하는 것입니다. 스타일 겐은 프로그레시브 겐 구조로 저해상도에서 고해상도까지 스타일을 반영해서 작은 부분부터 큰 부분까지 원하는 정도에 따라 스타일 변환이 가능하기 때문에 여기 보시는 그림처럼 어 검은 머리에 갈색 피부를 가진 성인 여성을 소스에서 보시는 바와 같이 어린 아이 금발의 흰 피부를 가진 어린아이의 스타일대로 바꿔 달라고 했을 때 굉장히 비슷하게 그 형태에 맞게 바꿔주는 것을 볼 수가 있고요. 반면에 굉장히 작은 부분에 대해서는 원본 이미지를 크게 변형하지 않는 선에서 일부분만을 변형을 함으로써 해당하는 그 변형의 정도를 조절할 수 있다라는 장점이 있습니다. 네 지금까지는 이제 겐 기반의 이미지 생성 모형을 말씀을 드렸고요. 지금부터는 오토 인코더 기반의 생성 모형 3가지를 말씀드리도록 하겠습니다. 우선 오토 인코더는 크게 인코더 g파이와 디코더 세터로 구성이 되는데요. 결국은 입력 이미지를 잠재 공간 저차원의 잠재 공간으로 변환을 했다가 다시 복원하도록 하는 학습 모델 구조입니다. 그래서 인코더인 지파이는 입력 이미지를 저차원의 잠재 공간으로 매핑해서 잠재변수 z로 변환을 하는 것이고요. 여기서 굉장히 중요하게 여러분들께서 기억하셔야 될 사항은 이 잠재변수 z라고 하는 것은 반드시 입력 데이터보다 차원이 낮아야 합니다. 그래야 정보의 압축과 효용성이 일어나고요. 그걸 다시 복원하는 과정에서 학습이 되게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 4,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 993,
      "char_count": 1830
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c005_58e1cc",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 우선 오토 인코더는 크게 인코더 g파이와 디코더 세터로 구성이 되는데요. 결국은 입력 이미지를 잠재 공간 저차원의 잠재 공간으로 변환을 했다가 다시 복원하도록 하는 학습 모델 구조입니다. 그래서 인코더인 지파이는 입력 이미지를 저차원의 잠재 공간으로 매핑해서 잠재변수 z로 변환을 하는 것이고요. 여기서 굉장히 중요하게 여러분들께서 기억하셔야 될 사항은 이 잠재변수 z라고 하는 것은 반드시 입력 데이터보다 차원이 낮아야 합니다. 그래야 정보의 압축과 효용성이 일어나고요. 그걸 다시 복원하는 과정에서 학습이 되게 됩니다. 디코더인 프세타는 잠재 변수를 입력으로 사용해서 원본 이미지를 복원하는 것이고요. 오토 인코더의 일반적인 세터와 파이라고 하는 것은 이제 학습이 되는 파라미터를 뜻하고 XI라는 객체를 주었을 때 g파이라는 인코더에 넣어서 잠재 변수를 만들어내고 잠재 변수를 f세타라고 하는 디코더에 투입을 다시 해서 원본을 복원했을 때 원래 데이터와 복원된 데이터 사이의 차이를 가장 최소화시키는 것. 그래서 그림에서 보시는 바와 같이 7을 입력으로 넣었을 때 7이 나오도록 하는 이러한 모델이 바로 오토 인코더가 되겠습니다. 모델 학습을 위해서는 주로 리컨스트럭션 로스라고 하는 이제 재구축 오차이고요. 이 재구축 오차는 쉽게 말해서 오리지널 나와 이 오토 인코더에 의해서 생성된 나 사이에 1 대 1로 픽셀 바이 픽셀로 그 차이를 계산해서 전부 썸을 스퀘어드 썸을 한 거라고 생각하시면 되겠습니다. 일반적으로는 민 스퀘어드 에러 스퀘어드 썸이고요. 민 앱솔루트 에러는 단순히 둘 사이의 절대값을 의미를 합니다. 베리에이셔널 오토 인코더 같은 경우에는 오토 인코더 구조와 동일하게 인코더와 디코더로 구성되어 있지만 잠재 공간의 분포 여기서의 핵심은 분포입니다. 보통 이제 가우시안으로 가정을 하죠. 가정해서 학습하는 구조입니다. 그래서 인코더를 학습을 했을 때 여기에서 z라는 것을 바로 학습하는 게 아니라 제가 가질 수 있는 가우시안 분포의 평균 분산을 학습을 하게 되고 일반적인 이제 노이즈를 주어서 이 세 가지를 합쳐서 z라는 것을 샘플링을 한 다음에 다시 디코딩을 하는 과정입니다. 그렇기 때문에 베리에이셔널 오토 인코더의 로스 함수 같은 경우에는 제가 주어졌을 때 스가 만들어질 확률이 가장 높은 부분 리컨스트럭션 로스 플러스 제트에 대한 세타의 프라버버리티와 엑스가 주어졌을 때의 잠재 공간에 대한 큐파이에 대한 확률 밀도 함수가 둘 사이가 매우 가까워져야 되는 컬백 라이블러 다이버전스까지도 함께 사용을 하게 됩니다. 여기서 보시는 바와 같이 사전에 정의한 잠재 공간에 대한 분포를 학습에 반영하기 위해서 케이어 다이브전스를 함께 정의한다라고 말씀드렸습니다. 세 번째 방식인 벡터 퀀타이스트, 베리에이셔널 오토 인코더 같은 경우에는 브큐브에라고도 표현하는데요. 연속적인 잠재 공간이 아닌 이산적인 잠재 공간을 가정해서 학습에 사용합니다. 이 말은 무슨 얘기냐면 사실은 모든 디지털은 이산적 공간에 존재합니다. 0 아니면 1의 값으로 표현이 되니까요. 그런데 아날로그로 되어 있는 연속적인 분포를 사용하는 것이 보다 비효율적일 수 있기 때문에 이산적인 잠재 공간 툭툭 끊기는 디스크립트 한 잠재 공간을 사용해서 이미지 또는 텍스트, 음성과 같은 데이터에 더 적합하게 사용하는 방식입니다. 그렇기 때문에 분포를 가정한다기보다는 여기서 보는 이라고 표현된 케이 개의 디 차원의 코드북을 사용을 하게 되고요. 이 코드북도 역시 학습을 하고자 하는 대상이 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 5,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 977,
      "char_count": 1814
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c006_96f854",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 이 말은 무슨 얘기냐면 사실은 모든 디지털은 이산적 공간에 존재합니다. 0 아니면 1의 값으로 표현이 되니까요. 그런데 아날로그로 되어 있는 연속적인 분포를 사용하는 것이 보다 비효율적일 수 있기 때문에 이산적인 잠재 공간 툭툭 끊기는 디스크립트 한 잠재 공간을 사용해서 이미지 또는 텍스트, 음성과 같은 데이터에 더 적합하게 사용하는 방식입니다. 그렇기 때문에 분포를 가정한다기보다는 여기서 보는 이라고 표현된 케이 개의 디 차원의 코드북을 사용을 하게 되고요. 이 코드북도 역시 학습을 하고자 하는 대상이 됩니다. 그래서 보시는 바와 같이 브큐브에는 학습을 해야 되는 파라미터가 세타와 파이에 더해서 코드북까지도 학습을 하게 되고요. 가장 왼쪽에 있는 부분들이 제큐라는 잠재 변수를 통해서 벡터를 통해서 스가 얼마나 잘 만들어지는지에 대한 리컨스트럭션 로스가 되겠고요. 여기서는 둘 사이에 스톱 그레디언트를 사용을 해서 실제로 제트 2가 주어졌을 때 스탑 그레디언트와 코드북 사이의 차이, 그리고 제이와 2가 주어졌을 때 스탑 그레디언트를 이용을 했을 때의 코드북의 차이 이 두 개의 차이를 최소화 시키는 것이 바로 브큐브에의 목적이 되겠습니다. 브큐브에에서는 정의한 잠재 공간인 코드북은 사전에 정의된 케익의 인베딩으로 활용을 하고요. 모델 학습을 위해서 리컨스트럭션 로스에 추가로 이와 인코더를 스탑 그레디언트를 통해서 따로 계산을 하는 그러한 과정을 거치게 됩니다. 마지막으로는 디퓨전 기반의 모델에 대해서 간략하게 설명을 드리도록 하겠습니다. 크게 4가지를 설명드릴 건데요. 디디피엠, 디디아엠 CFG 엘디엠 이렇게 4가지입니다. 디디피엠은 디노이징 디퓨전 프로버블리스틱 모델로지의 약자고요. 입력 이미지를 포워드 프로세스를 통해서 잠재 공간으로 변환을 하고 리버스 프로세스를 통해서 복원하는 구조입니다. 포드 프로세스는 점진적으로 가우시안 노이즈를 추가해서 잠재 공간으로 매핑하는 과정이라면 그래서 이 제로에서부터 이렇게 잠재 공간으로 매핑하는 과정이고 리버스 프로세스는 포드 프로세스에서 추가된 노이즈를 추정해서 제거하는 과정이라고 보시면 되겠습니다. 그래서 노이즈를 이제 프레딕션 하는 모델을 통해서 역방향으로 원래 그러면 원본 이미지가 x 제로라고 할 때 티 번째 스텝에서의 스티는 사실은 원본 이미지를 전혀 형체를 알아볼 수 없는 그냥 노이즈 이미지죠. 그런데 이 중간중간에 조금씩의 노이즈가 들어가 있다라고 가정을 하는 것이고 그 노이즈를 예측해서 제거를 한다면 이러한 스티 시점에서의 노이즈 이미지에 대해서 순차적으로 노이즈를 제거하면서 원본 이미지가 만들어질 수 있다라는 것을 의미하는 바입니다. 두 번째인 디디아이엠은 디노이징 디퓨전 인포리스 모델입니다. 사람들은 항상 게으릅니다. 게으르다는 얘기는 뭐냐면요 필요하지 않고 하지 않아도 될 것을 안 하려고 하는 경향이 있다는 뜻입니다. 연구도 마찬가지입니다. 이 디디피엠에서의 프로세스는 사실 엑스 제로에서부터 스3까지 가는 모든 스텝에 대해서 이미지를 노이즈를 추가해서 생성을 하고 다시 노이즈를 제거를 해야 됩니다. 그래서 디디피엠에서의 추정 방식을 약간 변화시키는데요. 그거는 인플리싯이라고 하는 거는 디디아이엠은 확률적인 모델링을 통해서 노이즈를 제거하는 것이 아니라 디터미니스틱한 샘플링 프로세스를 일부 과정에서 적용을 합니다. 이 말은 무슨 얘기냐면 확률적이라는 얘기는 왼쪽 그림에서 x1이 주어졌을 때 x2는 확률적으로 무엇이 만들어질지를 계산을 하기 때문에 스투는 항상 고정이 아닙니다. 그렇지만 오른쪽 그림에서의 ddim 인퍼런스 과정에서 보면 여기에서 예를 들어 스1에서 스2까지는 엑스 제로와 스원이 주어졌을 때 스2가 만들어지는 것을 만약에 확정을 한다면 이러한 과정에서는 그대로 여기에 있는 그 프로블리스틱하게 만들어지는 게 아니라 디터미니스틱하게 만들어질 수 있다는 얘기죠. 그렇기 때문에 스원만 주어지면 스2까지 그리고 x3까지를 한 번에 샘플링 할 수 있다라는 얘기입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 6,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1109,
      "char_count": 2039
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c007_a5aedc",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 그렇지만 오른쪽 그림에서의 ddim 인퍼런스 과정에서 보면 여기에서 예를 들어 스1에서 스2까지는 엑스 제로와 스원이 주어졌을 때 스2가 만들어지는 것을 만약에 확정을 한다면 이러한 과정에서는 그대로 여기에 있는 그 프로블리스틱하게 만들어지는 게 아니라 디터미니스틱하게 만들어질 수 있다는 얘기죠. 그렇기 때문에 스원만 주어지면 스2까지 그리고 x3까지를 한 번에 샘플링 할 수 있다라는 얘기입니다. 그래서 일부만 리버스 프로세스를 적용할 수 있음으로 인해서 모든 과정에서 리버스 프로세스를 적용하는 것에 비해서 훨씬 효율적으로 추정이 가능하다라는 뜻입니다. 이렇게 어 난 마크 이안 디퓨전 프로세스를 적용을 해서 전체 프로세스의 서브셋만을 이용함으로써 매우 좋은 성능을 보였다라는 게 실험적으로 입증이 되어 있습니다. 여기 보시는 것처럼 에타를 기준으로 봤을 때 여기에 있는 0으로 되어 있는 게 ddim인데 실제로 프로세스를 리버스 프로세스를 충분히 모든 과정에서 하지 않더라도 모든 과정에서 한 주황색의 그 성능보다 매우 좋은 성능을 나타내는 것을 보실 수가 있겠습니다. 세 번째는 클래시파이어 가이드라인입니다. 디퓨전 모델은 이제 그 겐보다 처음에 나왔을 때는 성능이 좋지 못했습니다. 그런데 요 디퓨전 모델은 비스 겐 온더 이메이지 신테티스라는 논문에서 연구에서 GA보다 더 나은 이 피데리티라고 이제 표현을 할 건데요. 데이터의 품질을 보이기 위해서 모델의 구조를 업데이트를 하고 클래시 파이어 가이던스를 제공해서 소타를 달성한 모형입니다. 클래시 파이어 가이던스라는 것은 뭐냐 하면 백업 프로세스에서 xt 마이너스 1의 노이즈를 추정할 때 학습한 클래식 파이어의 기울기를 통해서 임의의 클래스 y로 샘플링을 가이드하는 데 사용이 됩니다. 여기서 스케일은 이제 크면 클수록 특정한 클래스의 샘플링할 강도를 높게 주겠다라는 것이고요. 두 사진의 차이는 처음에 보면 스케일이 작을 때는 이 이미지의 중간 단계에서 강아지가 샘플링이 생성이 되지만 강아지가 아닌 것들도 생성이 되는 것을 보실 수가 있는데 이 스케일을 크게 두면 특정한 범주의 가중치를 높게 주어서 이미지 생성이 유도가 되기 때문에 강아지에 해당하는 그림들이 주로 만들어지는 것을 보실 수가 있겠습니다. 디디아이엠에 클래식 파이어 가이던스를 적용하기 위해서 스코어 기반의 컨디셔닝 트릭을 적용하는데요. 스코어 펑션은 노이즈를 제거하는 과정 그러니까 원래 노이즈가 엄청 많은 데이터에서 원본 이미지로 가는 과정에서 클래스 와를 조건부로 주입을 합니다. 그러니까 포드 프로세스는 그대로 놔두고 백워드 프로세스 과정에서 이 파란색으로 되어 있는 부분이 스코어 펑션인데 이게 다시 한번 표현하자면 y가 주어졌을 때 클래스가 주어졌을 때 x가 추정될 확률 를 가이던스로 제공한다라는 얘기죠. 그래서 이거에 대한 그레디언트를 직접적으로 명시적으로 제공해 줌으로써 해당하는 클래스가 원본 데이터에서 만들어지도록 유도하는 것입니다. 이렇게 했을 때 이 컨디셔닝 스코어 펑션은 사실은 베이스 룰에 의해서 보시는 바와 같이 스티기 와가 이런 식으로 이제 전개가 될 수가 있는데 수식적으로 중요한 부분을 어렵다고 생각을 하시면 이 수식에 대해서는 하나하나 이해하시면 더할 나위 없이 좋지만 여기서 의미하고자 하는 건 이것입니다. x를 입력받아 클래스 y를 반환하는 클래시파이어 피 세타를 사용하자. 그래서 여기 있는 피세타라는 클래시 파이어가 학습이 되는 겁니다. 이렇게 학습이 된 클래시 파이어를 이용을 해서 여기에 있는 이 가이던스를 제공하는 것이고요. 이 앞에 감마라고 하는 스케일 팩터를 추가해 줌으로써 얼마나 그 클래스의 종속적이 될 것인지 또는 강하게 제작을 할 것인지에 대한 스케일을 조정할 수가 있게 됩니다. 그런데 이러한 클래식 파이어 가이던스의 문제점은 기존 디퓨전 파이프라인에 별도로 클래식 파이어 모델이 추가돼서 조금 더 복잡해진다라는 이제 문제점이 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 7,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1091,
      "char_count": 2013
    },
    {
      "id": "transcript_generative_ai_generative_ai_5강_image_generat_c008_f42b69",
      "content": "[Generative AI] [Generative AI] (5강) Image Generation 1 Image Generation  Models Overview\n\n다. 이렇게 학습이 된 클래시 파이어를 이용을 해서 여기에 있는 이 가이던스를 제공하는 것이고요. 이 앞에 감마라고 하는 스케일 팩터를 추가해 줌으로써 얼마나 그 클래스의 종속적이 될 것인지 또는 강하게 제작을 할 것인지에 대한 스케일을 조정할 수가 있게 됩니다. 그런데 이러한 클래식 파이어 가이던스의 문제점은 기존 디퓨전 파이프라인에 별도로 클래식 파이어 모델이 추가돼서 조금 더 복잡해진다라는 이제 문제점이 있습니다. 또한 모든 스텝에 대해서 클래식 파이어가 필요하다라는 문제점도 발생하죠. 아까 말씀드렸듯이 우리는 모두 연구 목적으로도 역시 게으르다라고 말씀드렸죠. 이거를 이제 하지 않을 방법을 사람들이 생각합니다. 그래서 나온 게 씨프지 클래식 파이어 프리 가이던스입니다. 클래식 파이어 프리 가이던스에서는 클래식 파이어의 가이던스 식을 조건부와 조건이 없는 컨디셔널 그리고 언컨디셔널 스코어로 분해를 합니다. 바꿔 말하면 왼쪽에 있는 이 산식이 클래식 파이어 가이던스인데요. y가 주어졌을 때 t라는 xt라는 시점에서의 이 이미지가 발생할 확률이고요. 그거에 대한 그레디언트를 적용을 하는 것인데 이거를 조금 더 분해를 해 보면 이런 식으로 표현이 됩니다. 컨디셔널 스코어라는 거는 와라는 것에 대한 부분이고요. 언컨디셔널 스코어는 와이가 들어가 있지 않은 부분입니다. 또한 여기서는 피 세타라는 부분은 결국은 우리가 이미 학습을 했던 제너레이터에 대한 부분이기 때문에 따로 클래식 파이어를 만들어 낼 필요가 없다라는 뜻입니다. 그래서 클래시 파이어 프리 가이던스에서는 노이즈 레벨마다 핵심은 이렇습니다. 클래시 파이어를 학습하지 말자. 그래도 클래스에 대한 가이던스를 줄 수 있다. 어떻게 줄 수 있느냐 실질적으로 클래식 파이어에 의해서 주어지는 이 가이던스는 제너레이터 모델을 이용해서 컨디셔널 스코어와 언컨디셔널 스코어로 분해해서 사용을 할 수가 있고 이 컨디셔널 스코어와 언컨디셔널 스코어에 대해서 가중치를 제공함으로 인해서 여기서는 이제 감마로 표현이 되는데 감마는 1 플러스 w라고 생각하시면 되겠습니다. 이렇게 가중치를 부여함으로써 조절을 할 수 있게 되었다라는 의미입니다. 마지막으로 레이턴트 디퓨전 모델입니다. 레이턴트 디퓨전 모델은 디퓨전 모델을 학습할 때 이미지를 사용하는 것이 아니라 원본 이미지를 사용하는 것이 아니라 인코더를 통해서 추출된 저차원의 잠재 변수를 사용합니다. 그렇기 때문에 고해상도 이미지 학습에 보다 적은 비용으로 학습이 가능하게 되고요. 클래식 파이어 프리 가이던스 방식을 통해서 이미지 생성의 컨디션을 반영할 수 있습니다. 특히 이 부분에 있어서는 이 컨디션을 어텐션 메커니즘 트랜스포머의 어텐션 메커니즘을 적용함으로 인해서 다양한 생성 테스크에 적용이 가능해집니다. 아까 말씀드렸듯이 어떤 주어진 컨디션이 있을 때 시멘틱 맵으로 만들어라 리프레젠테이션을 만들어라 이미지가 있다 이런 것들을 전부 하나의 쿼리처럼 취급을 해서 키와 밸류들에 대한 고으로써 어텐션 스코어를 만들고 이렇게 만들어진 크로스 어텐션을 컨디션 임베딩에다가 반영함으로써 우리는 이 조건에 맞는 이 이미지를 더욱더 잘 생성할 수 있는 환경이 조성이 되는 겁니다. 그래서 레이턴트 디퓨전 모델에서의 예시를 보시면 텍스트 투 이미지 스테티스라는 거는 여기에 텍스트를 조건으로 줬을 때 어떻게 하면 만들어지는지에 대한 부분을 보여주는 것이고요. 레이아웃 투 이미지라는 것은 컨디션을 레이아웃으로 준 겁니다. 이 정도의 레이아웃이 있을 때 이미지를 만들어줘라고 하면은 오른쪽과 같은 이미지들이 생성이 되는 것이고요. 슈퍼 레졸루션 같은 경우에도 해상도를 높이는 것, 인페인팅 같은 경우에는 해당하는 부분에 대해서 그림을 채워 넣는 것 이러한 다양한 조건들을 레이턴트 디퓨전 모델에서는 어텐션 크로스 어텐션 기능을 통해서 구현해 주고 있다라는 얘기지요. 여기까지 해서 다섯 번째 강의인 이미지 생성에 대해서 설명을 드렸습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview.json",
        "lecture_name": "[Generative AI] (5강) Image Generation 1 Image Generation  Models Overview",
        "course": "Generative AI",
        "lecture_num": "5강",
        "lecture_title": "Image Generation 1 Image Generation  Models Overview",
        "chunk_idx": 8,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:fa56be7d46ccf965b809a6ab87c2db17ce4f95f4f37b908a5517d05e95657686"
      },
      "token_estimate": 1106,
      "char_count": 2026
    }
  ]
}