{
  "source_file": "7강_경사하강법이랑_친해져보자.json",
  "lecture_name": "7강_경사하강법이랑_친해져보자",
  "course": "AI Math",
  "total_chunks": 8,
  "chunks": [
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c000_dee773",
      "content": "[강의 녹취록] 과목: AI Math | 강의: 7강 | 제목: 경사하강법이랑_친해져보자\n\n네 안녕하세요 에스메릭스포 알티패션 인텔레전스 7강 경사강법이랑 더 친해져 보자. 네 시간 강의를 맡게 된 고려대학교 통계학과 임성빈 교수입니다. 네 지난 시간에 저희는 미분이라는 개념을 배우고 이걸 통해서 1차원 함수에서의 경사하강법 알고리즘을 한번 간단하게 배워보았는데요. 이번 시간에는 다차원 공간에서의 경사 하강법을 한번 공부해 보도록 하겠습니다. 지난 시간에 저희는 미분의 개념을 한번 살짝 배워보았습니다. 그래서 어떤 함수 f가 주어져 있을 때 그 함수가 주어진 점 x 콤마 FX에서의 접선의 기울기가 바로 미분의 개념이다라고 설명을 드렸었습니다. 이때 한 점에서의 접선의 기울기 값을 알면은 어느 방향으로 점을 움직여야 함수 값이 증가하는지 감소하는지 알 수 있다고 말씀드렸는데요. 딥러닝처럼 손실 함수의 최소화를 하는 알고리즘의 경우에는 함수의 최솟값을 구하는 게 목표이기 때문에 함수 값을 감소하는 방향으로 움직여야 됩니다. 그러므로 미분 값을 빼서 경사강법을 동작해서 함수의 극속값을 찾는 방향으로 우리가 알고리즘을 사용하게 된다라고 설명을 드렸습니다. 네 그러므로 그림에서 보시다시피 함수가 이렇게 어 증가하다가 감소하는 모양이나 또는 감소하다가 증가하는 모양이나 우리가 미분 값을 계산해서 빼는 방향으로 이동하면은 이 함수에 감소하는 방향으로 우리가 최적화를 수행할 수 있다라고 설명을 드렸었습니다. 네 근데 지난 시간에 배운 미분의 개념은 저희가 함수가 1차원인 경우에 즉 입력 값이 우리가 어떤 어 숫자가 아니라 만약에 이제 벡터인 경우에는 우리가 어떤 식으로 이 미분을 사용해야 될지를 고민을 한번 해봐야 되겠습니다. 즉 미분은 변수의 움직임에 따른 함수 값의 변화를 측정하는 도구인데 만약에 입력이 더 이상 숫자가 아니라 벡터인 경우 이 경우를 우리가 이제 다변수 함수라고 하게 되는데요. 이 경우에는 우리가 미분을 앞서 배웠던 정의대로 사용하기가 좀 쉽지 않습니다. 왜 그러냐면은 어 이 분모에 들어가는 부분에서 벡터를 넣는 것이 불가능하기 때문에 그렇습니다. 그래서 이 경우에는 그냥 미분의 개념을 쓰지 않고 우리가 이제는 편미분 즉 파셜 디퍼렌시에이션이라는 개념이 있습니다. 파셜 디퍼렌시에이션이란 벡터가 입력인 상태에서 그 벡터의 모든 구성 성분에 대해서 이동시키는 것이 아니라 어떤 특정 방향에서의 우리가 구성 성분만 변화량을 주고 그 방향에서의 미분 값을 계산하는 거를 우리가 편미분이라고 합니다. 즉 수식을 한번 보시면요. 이 이아라는 벡터를 우리가 이제 사용하게 될 텐데 이 이아 벡터는 아이 번째 값만 1이고 나머지는 0인 단위 벡터입니다. 즉 우리가 원한 팩터처럼 생각해 보시면 될 텐데요. 아이 번째 인덱스에 해당하는 값만 1이고 나머지는 0인 벡터이기 때문에 사실상 이 벡터는 아이 번째의 구성 성분만 더해지고 나머지는 변화가 없게 만들어 주는 역할을 합니다. 그래서 주어진 입력 변수 스에서 우리가 이 단위 벡터 2아에다가 숫자 치를 스칼라 곱을 해서 더해주게 되면은 이 스라는 벡터에서 오로지 아이 번째 변수만 치만큼 이동시키게 되고요. 그거를 다시 x 지점에서의 함수 값을 빼주면 우리가 분자는 함수 값의 변화량을 우리가 볼 수가 있게 됩니다. 이때 분모는 단위 벡터를 쓰지 않고 앞서 스칼라 곱 때 사용했던 h를 써주게 됩니다. 치는 스칼라이기 때문에 숫자이죠. 그러므로 분자와 분모 모두 다 잘 정리가 되고 이때 h를 0으로 보내는 극한을 계산해 주게 되면은 x의 i번째 방향에서의 편미분을 계산할 수가 있게 됩니다. 이거를 수식으로 쓸 때는 우리가 미분처럼 f프라임을 쓰지 않고 스아 번째 방향으로의 미분이다라는 기호를 써서 우리가 라운드라는 기호를 쓰고요. 그리고 x아라는 아이 번째 방향의 변화량이다라는 기호를 우리가 밑에다 써 줍니다. 이렇게 써주게 되면은 우리가 아 스아이번째 방향으로 편미분을 취했구나라고 이해를 할 수가 있게 됩니다. 그럼 구체적인 예시를 한번 살펴보도록 하겠습니다. 함수 프가 스랑 와라는 두 개의 변수를 입력으로 받는다고 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 0,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 1111,
      "char_count": 2022
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c001_ee306d",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 이거를 수식으로 쓸 때는 우리가 미분처럼 f프라임을 쓰지 않고 스아 번째 방향으로의 미분이다라는 기호를 써서 우리가 라운드라는 기호를 쓰고요. 그리고 x아라는 아이 번째 방향의 변화량이다라는 기호를 우리가 밑에다 써 줍니다. 이렇게 써주게 되면은 우리가 아 스아이번째 방향으로 편미분을 취했구나라고 이해를 할 수가 있게 됩니다. 그럼 구체적인 예시를 한번 살펴보도록 하겠습니다. 함수 프가 스랑 와라는 두 개의 변수를 입력으로 받는다고 하겠습니다. 즉 엑스 제곱 더하기 2 스와 더하기 3 더하기 코사인 x 플러스 2y라는 함수를 받는다고 할게요. 이때 저희는 어 편미분을 계산해 볼 건데요. 편미분을 계산할 때는 먼저 어느 방향으로 미분을 취할지를 정해줘야 됩니다. 저희 같은 경우에는 한번 스 방향으로 편미분을 계산한다고 하겠습니다. 그러면은 라운드 기호 그리고 x를 취해서 편미분을 계산하게 될 텐데 이 경우에는 y를 변수로 생각하지 않고 마치 산수로 생각하고 x에 대한 미분을 계산해 주면 됩니다. 이렇게 계산했을 때 결과가 2x 더하기 2와 빼기 사인 x 플러스 2y가 나오게 되는데요. 여러분들께서 손으로 직접 계산해 보셔도 되지만 지난 시간에 알려드렸던 심블릭 파이썬 즉 심파이를 이용해 가지고 저희가 계산을 또한 해볼 수가 있게 됩니다. 그래서 심파이에 폴리랑 그리고 심파이의 코사인 이 두 개의 함수를 이용해 가지고 앞서 이 다음 함수랑 그리고 삼각 함수 부분을 먼저 정의를 해 주게 된 후에 심파이의 디아프 함수를 이용해서 우리가 똑같이 미분을 엑스에 대해서 취할 수가 있게 됩니다. 이때 얻은 결론이 왼쪽에서 살펴본 구해낸 수식과 같다는 것을 보실 수가 있게 됩니다. 이와 같이 심파이를 이용해서 우리가 어떤 기호적인 기분을 계산하는 건 1차원 함수뿐만 아니라 우리가 다차원 함수에서도 똑같이 사용할 수 있다는 점을 기억하시면 좋겠습니다. 만약에 와에 대한 편미분을 계산하고 싶으시면은 어 이 코드에서 스만 와로 바꿔주시면은 와에 대한 미분도 여러분이 계산하실 수 있습니다. 네 이렇게 각 변수의 구성 성분에 대한 편미분을 각각 계산해 볼 수가 있는데요. 그러면 어떤 벡터 스가 주어졌을 때 그 벡터가 디 차원이라고 네 저희가 상상해 보겠습니다. 그러면 스는 엑스원 그리고 스투 점점점 스디로 이루어져 있는 숫자들로 이루어져 있는 벡터가 될 텐데요. 이때 각 엑스원 스투 그리고 스디까지의 편미분을 각자 계산해 볼 수가 있게 됩니다. 그러면 라운드 x1 프 그리고 라운드 스투프 그리고 라운드 스디프 이런 식으로 각각 다 계산해 볼 수가 있게 되는데요. 디게의 즉 입력 벡터 스의 차원만큼의 편미분을 쭉 모아서 저희가 또한 벡터로 만들어 볼 수가 있겠습니다. 그러면 이 벡터를 우리는 그라디언트 벡터 어 영어로는 그레디언트라고 하죠 그렇죠 이 그레디언트 벡터라고 정의를 하게 되고요. 이게 사실은 저희가 앞선 강의에서 배웠던 그레디언트의 의미랑 같은 의미입니다. 네 만약에 1차원인 경우에는 사실상 어떤 이 첫 번째 파셜 즉 라운드 스1 프만 계산하는 개념이기 때문에 원래 1차원에서의 미분과 같은 의미가 되고요. 우리가 이 그레디언트 벡터를 이용해 가지고 앞서 배웠던 경사 하강법 그리고 경사 상승법에 둘 다 일반적으로 사용할 수가 있게 됩니다. 참고로 그레디언트 벡터를 쓰실 때는 이렇게 삼각형을 거꾸로 돌린 기호를 쓰게 되는데 이 기호를 나블라 기호라고 부릅니다. 그래서 그레디언트 벡터를 쓸 때는 우리가 나블라라는 수식을 쓴다라는 점 기억하시면 좋겠습니다. 그렇다면 그레디언트 벡터가 실제로 어떤 모양으로 생기게 되는지를 한번 살펴보도록 하겠습니다. 네 이 그림은 저희가 어떤 함수 f가 이렇게 주어져 있을 때 이거를 3차원 그래프로 한번 그려본 것이고요. 그리고 3차원에서 그레디언티터가 어떻게 표시되는지를 한번 그려보았습니다. 네 그래서 함수 f가 유라 같이 3차원 공간에서 어떤 곡면으로 그려지게 될 텐데 그레디언트 벡터를 한번 표시해 보시면은 우선 그레디언트 벡터가 이와 같이 이스 콤마 4와이로 나온다는 거를 볼 수가 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 1,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 1105,
      "char_count": 2026
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c002_48a995",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 그렇다면 그레디언트 벡터가 실제로 어떤 모양으로 생기게 되는지를 한번 살펴보도록 하겠습니다. 네 이 그림은 저희가 어떤 함수 f가 이렇게 주어져 있을 때 이거를 3차원 그래프로 한번 그려본 것이고요. 그리고 3차원에서 그레디언티터가 어떻게 표시되는지를 한번 그려보았습니다. 네 그래서 함수 f가 유라 같이 3차원 공간에서 어떤 곡면으로 그려지게 될 텐데 그레디언트 벡터를 한번 표시해 보시면은 우선 그레디언트 벡터가 이와 같이 이스 콤마 4와이로 나온다는 거를 볼 수가 있게 됩니다. 이때 그레디언트 벡터에다가 음수를 취해 준 거를 한번 그려보면요. 이와 같은 모양이 생긴다는 것을 보실 수가 있게 됩니다. 그럼 잘 보시면 어떤 함수의 이 곡면에서 최소화되는 방향으로 향하는 이 어떤 화살표들의 모임을 보실 수가 있게 됩니다. 그레디언트 벡터는 어떤 함수가 주어져 있을 때 그 함수 프가 근방에서 어떤 극속값으로 향하는 지점을 보여주는 이런 벡터들의 방향을 여러분이 보실 수가 있습니다. 이거를 조금 더 구체적으로 이해해 보기 위해서 한번 등고선 컨트어를 그려서 해석해 보도록 하겠습니다. 이 함수 f의 함수 값의 고절을 파악하기 위해서 저희가 한번 이렇게 등고선을 그려 보았는데요. 이렇게 노란색 부분에 가까울수록 함숫값이 큰 영역이고 그리고 이렇게 파란색 영역으로 갈수록 함숫값이 0 근처에 작은 지점이 되겠습니다. 그리고 이 함수의 특성상 x가 0이고 y가 0일 때 최솟값이 되겠습니다. 그래서 이 지점이 최솟값의 영역에 해당하게 될 텐데요. 그레디언트 벡터 자체만 그려보시면 어 이렇게 0 콤마 0 지점에서 어느 쪽 방향으로 가야 증가하는지를 보여주는 화살표를 그려볼 수가 있게 됩니다. 그래서 그냥 그레디언트 벡터를 보시면 어떤 각 지점에서 가장 빨리 증가하는 방향으로 간다라고 이해하시면 되는데 그레디언트에다가 마이너스를 취해 주게 되면은 거꾸로 가장 빨리 감소하는 방향으로 향한다라고 여러분들이 이해하실 수가 있겠습니다. 지금 이 함수의 경우에는 변수가 2개인 3차원 공간에서 곡면을 보여드리긴 했는데요. 이 기하학적 해석은 여러분이 다차원 공간에서도 똑같이 사용하실 수 있는 것이기 때문에 지금 그림으로 보시는 이 해석을 어 일반적으로 다차원 즉 엔 차원에서도 여러분이 똑같이 쓰실 수 있다라는 거를 직관적으로 이해하시면 좋겠습니다. 그래서 어떤 한 시작 지점에서 우리가 그레디언트 팩터를 따라 흐르게 되면은 이렇게 최소점으로 가장 빨리 감소하는 방향으로 흐르는 거를 여러분이 살펴보실 수 있게 됩니다. 네 그럼 이제 이 경사강법 알고리즘을 앞서 배웠던 1차원 알고리즘에서 다차원으로 한번 바꿔보도록 하겠습니다. 사실 큰 차이는 없습니다. 보시면 그레디언트를 계산하는 함수가 이제는 더 이상 미분값을 계산하는 함수가 아니라 그레디언트 벡터를 계산하는 함수로 변경된 것만 있고요. 그리고 우리가 알고리즘 종료 조건을 계산을 할 때 이제는 절대값을 계산하는 파트가 아닌 벡터이기 때문에 이 그레디언트 벡터의 놈을 계산하는 걸로 변경을 해 주게 됩니다. 노음은 우리가 벡터 시간에 배웠지만 절대 값처럼 사용할 수 있다고 말씀드렸는데요. 이때 이 그레디언트 벡터가 알고리즘 종료 조건보다 클 때 동작 계속 동작을 시킨다라는 조건은 우리가 앞서 배웠던 1차 원 공간에서의 경사 방법과 같은 원리입니다. 네 사실상 이 부분만 차이가 있고 나머지는 모두 똑같이 동작을 하게 됩니다. 여러분들께서 기억하셔야 될 점은 우리가 경사하강법 알고리즘에서 미분 값을 계산할 때 사용하는 이 배려블에 들어가는 입력 벡터의 차원과 그레디언트 함수를 이용해서 계산된 이 그래드 벡터의 차원은 같아야 된다는 것입니다. 즉 여러분이 업데이트를 해야 될 대상과 그리고 업데이트를 할 때 필요한 정보인 그레디언트 벡터의 차원도 2개가 모두 일치를 해야 경사 강법이 동작할 수 있기 때문에 이 부분을 여러분들께서 기억하시면 좋겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 2,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 1068,
      "char_count": 1939
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c003_e44dc2",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 네 사실상 이 부분만 차이가 있고 나머지는 모두 똑같이 동작을 하게 됩니다. 여러분들께서 기억하셔야 될 점은 우리가 경사하강법 알고리즘에서 미분 값을 계산할 때 사용하는 이 배려블에 들어가는 입력 벡터의 차원과 그레디언트 함수를 이용해서 계산된 이 그래드 벡터의 차원은 같아야 된다는 것입니다. 즉 여러분이 업데이트를 해야 될 대상과 그리고 업데이트를 할 때 필요한 정보인 그레디언트 벡터의 차원도 2개가 모두 일치를 해야 경사 강법이 동작할 수 있기 때문에 이 부분을 여러분들께서 기억하시면 좋겠습니다. 네 그래서 함수 FX가 x 제곱 더하기 2와 제곱인 네 2차원 함수일 때 아 이게 지금 스가 아니고 스코마 와인데요. 2차원 함수일 때 경사 강법으로 최소점을 찾는 코드입니다. 보시면 이렇게 절대값을 쓰는 것이 아니라 리니어 알지브라 함수 안에 노이라는 우리가 함수를 써서 노름 값을 계산할 수 있다고 말씀을 드렸는데요. 이때 계산된 그레디언트를 가지고 종료 조건을 설정해서 경사강법을 수행하는 알고리즘이 되겠습니다. 그래서 그 결과를 보시면은 이렇게 최소점이 굉장히 0 근처의 작은 값으로 도달한 걸 보실 수가 있는데요. 이 함수의 경우에는 스랑 와가 둘 다 0일 때 최솟값을 가지기 때문에 최소 지점을 잘 찾은 것을 여러분이 살펴보실 수가 있습니다. 네 그러면은 우리가 앞서 배운 어 다차원 공간에서의 경사 강법을 이용을 해서 이번에는 선형 회귀 분석에서 한번 어떻게 사용되는지를 살펴보도록 하겠습니다. 우리가 앞서 배웠던 선형 회귀 분석 예시에서는요. 우리가 유사한 역행렬 즉 수도 인버스를 이용해서 이 선형 모델의 l2 노름을 최소화하는 함수 데이터의 어떤 y 값과 그리고 선형 모델의 함수 값에 해당하는 이 와 h 즉 추론하는 예측값의 차이를 l2 nr을 최소화하는 우리가 계수 베타를 찾는 이 선형 모델 즉 선형 회기를 배워봤었는데요. 우리가 앞선 예제에서는 이와 같이 유사역 행렬을 이용해서 어 계수 베타를 찾는 걸 배워보았습니다. 그렇다면 이 예시에서 우리가 한번 경사 하강법을 이용해서 똑같이 베타를 찾을 수 있는지를 한번 배워보도록 하겠습니다. 이걸 사용할 수 있는 이유는 사실 우리의 목적식이 와랑 그리고 선형 모델의 예측 값인 와 헤 이 두 개의 벡터의 차이의 엘트노른 값을 최소화하는 베타를 찾는 최적화 식이 바로 목적식이기 때문에 우리가 여기서 사실상 경사광법을 쓸 수 있기 때문에 우리가 한번 적용해 볼 수가 있습니다. 그렇다면 경사 강법으로 이 선형 회귀 계수를 찾는 거를 한번 살펴보도록 하겠습니다. 앞서 보신 것처럼 선형 회귀의 경우 목적식은 데이터의 어떤 그라운트루스에 해당하는 와랑 그리고 입력 데이터 애들의 행렬인 x와 그리고 개수 벡터에 해당하는 베타를 행렬 곱했을 때 이 둘 차이의 l2 노름을 최소화하는 것이 목적식입니다. 즉 이거를 최소화하는 베타를 찾는 게 목적식인데요. 그렇다면 이 엘투 노름에 해당하는 우리가 그레디언트 벡터를 찾아야 되겠죠 즉 우리의 목적식은 이 수식이니까 이 수식을 최소화하는 어떤 변수 베타를 즉 최소화하는 최적의 베타를 찾는 것이 목적이기 때문에 베타에 대해서 그레디언트를 우리가 계산을 해 줘야 됩니다. 그러므로 이 경우에는 각 베타에 해당하는 계수들의 숫자인 베타 원 그리고 베타 디까지의 편미분을 모두 계산해야 되는 것이죠. 어 이때 여러분들께서 한 가지 보셔야 될 게 l2 노름을 계산할 때는 사실 어떤 구성 성분들의 제곱의 합에 제곱근을 취해서 계산을 해 줬는데 사실 이 제곱근을 취한 부분을 미분하는 게 뭔가 좀 계산할 때 좀 약간 방해가 될 수 있는 부분이 있습니다. 그래서 이 경우에는 제곱근이 아닌 어 엘투노룸의 제곱을 목적식으로 해서 최소화를 해도 됩니다. 우선은 엘투노룸에 들어있는 스퀘어 즉 제곱근을 고려해서 한번 미분 값을 계산해 보도록 하겠습니다. 연습의 목적으로 한번 끈기를 가지고 계산을 한번 해봅시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 3,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 1058,
      "char_count": 1938
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c004_40cbe6",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 그래서 이 경우에는 제곱근이 아닌 어 엘투노룸의 제곱을 목적식으로 해서 최소화를 해도 됩니다. 우선은 엘투노룸에 들어있는 스퀘어 즉 제곱근을 고려해서 한번 미분 값을 계산해 보도록 하겠습니다. 연습의 목적으로 한번 끈기를 가지고 계산을 한번 해봅시다. 네 수식이 좀 복잡해 보이지만 하나하나 뜯어내 보면은 여러분들께서 또 이해해 보실 수가 있는데요. 먼저 와 빼기 x 베타를 저희가 이와 같이 수식으로 풀어서 써 보았습니다. 시그마 제는 14부터 d까지의 합이 들어있는 이 부분은 사실 행렬곱을 표현하는 부분입니다. 즉 아이 번째 행에 행벡터에 들어가 있는 이 각각의 구성 성분들과 그리고 개수 벡터에 해당하는 이 두 벡터들 간의 내적을 계산하는 부분이 이 부분인데요. 지난 시간에 배웠듯이 우리가 행렬의 행렬 곱은 앞서 x의 행벡터와 베타에 해당하는 열 벡터 간의 내적을 계산하는 거라고 배웠습니다. 그리고 이 내적 값을 i번째 구성 성분에 해당하는 데이터의 그라운트루스 값과 빼서 이 둘 사이 간의 제곱을 다 더해준 후 우리가 이제 어 루트를 씌워서 평균값을 계산해 주게 되는데요. 네 이때 주의하실 점은 이 엘트 노름을 계산하는 부분이 바로 이 수식 부분에 해당한다는 것입니다. 그리고 이거를 케 번째 베타로 미분하는 부분이 이 수식이 되는 것인데 우리가 그러면은 k번째 베타로 미분하는 즉 편미분을 취하는 거는 어디에 해당하게 되냐면은 이 베타 제들 중에서 k번째의 계수 베타 k에 적용이 되는 것이겠죠. 네 계산 과정을 조금 생략해 보면 이거의 계산 결과는 이와 같이 나오게 됩니다. 그래서 y 빼기 x 베타에 l2 노름이 분모에 들어가게 되고 그리고 다시 y백의 x 베타 이 부분도 분자에 들어가게 되는데 보시면 데이터 행렬에 해당하는 이 부분이 어 와 백의 스 벡터에 곱해지는 거를 볼 수가 있습니다. 이 부분이 뭐냐면 스라는 데이터 행렬에 케 번째 열 벡터 부분이 있습니다. 그 열 벡터의 전치에 해당하는 것이죠. 즉 열 벡터를 전치시키면 행벡터가 되죠. 그래서 이 부분은 우리가 데이터 행렬의 케번째 열 벡터를 행벡터로 바꾼 부분이구나라고 이해하시면 되겠습니다. 그래서 이게 분자에 들어가고 그리고 와 백의 스베타 l2 노름 값이 분모에 들어가는 게 우리가 구하고자 했던 이 목적식 l2 노름의 베타 k에 대한 미분 값이 되는 것입니다. 네 이렇게 한 k에 대한 편미분을 구하게 되면요. 우리가 다른 베타 원서부터 베타 디까지의 미분값 즉 편미분 값을 모두 계산할 수가 있고 이걸 통해서 그레디언트 벡타를 전부 다 구할 수가 있게 됩니다. 우리가 케번째 베타 케에 대해서 편미분을 계산했기 때문에 일반적으로 다른 베타원 그리고 베타 디까지의 편미분도 모두 계산할 수가 있게 되는 것이죠. 이거를 조금 더 깔끔하게 우리가 행렬로 표시를 해 주게 되면은 네 아까 전에 k번째 열 벡터를 사용했던 거를 우리가 다시 데이터 엑스의 전체 행렬의 전지를 사용해서 이용할 수가 있게 됩니다. 그럼 우리는 이걸 가지고 그레디언트 벡터를 계산할 수가 있게 되는 것이죠. 네 복잡해 보이지만 사실 이 계산은 스랑 베타 간의 행렬 곱을 어 우리가 계수 베타에 대해서 미분한 결과인 이 엑스의 전치 행렬만 곱해주는 것이구나라고 이해하시면 되겠습니다. 그래서 이와 같이 계산한 그레디언트 벡터를 이용을 해 가지고 저희가 목적식을 최소화하는 계수 베타를 구하는 경사 강법 알고리즘을 이렇게 밑에서와 같이 쓸 수가 있게 됩니다. 보시게 되면은 와 빼기 x 베타에 l2 노름 값에다가 베타에 대한 그레디언트를 취해서 빼주게 돼 있죠. 참고로 여기 있는 베타 티는요 우리가 아직 수렴하기 직전에 즉 업데이트하는 과정에서의 계산되는 베타라고 보시면 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 4,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 998,
      "char_count": 1843
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c005_766c37",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 그래서 이와 같이 계산한 그레디언트 벡터를 이용을 해 가지고 저희가 목적식을 최소화하는 계수 베타를 구하는 경사 강법 알고리즘을 이렇게 밑에서와 같이 쓸 수가 있게 됩니다. 보시게 되면은 와 빼기 x 베타에 l2 노름 값에다가 베타에 대한 그레디언트를 취해서 빼주게 돼 있죠. 참고로 여기 있는 베타 티는요 우리가 아직 수렴하기 직전에 즉 업데이트하는 과정에서의 계산되는 베타라고 보시면 됩니다. 즉 아직 최적화되어 있지 않은 베타이고 우리는 베타를 찾는 과정에서 계속 최적화를 수행하게 되는데 그때 티 번째 스텝에서의 찾은 베타가 되는 것이고요. 이 베타 티를 또 업데이트해서 베타 티 플러스 1을 구하고 다시 베타 티 플러스 1을 집어넣어서 미분을 계산하고 또 다음 베타를 구하는 방식으로 베타를 업데이트하다 보면은 우리가 최종적으로 얻게 되는 베타가 찾고자 하는 선형 회귀의 개수가 되는 것입니다. 앞서 계산했던 수식을 이용해서 방금 전에 그레디언트 부분에다가 집어넣어 주시게 되면은 최종적인 경사강법 알고리즘은 이와 같이 표현이 되는데 여기에 음수 부분이 붙었기 때문에 음수 부분이 이 부분과 만나서 플러스로 바뀐다라는 거를 여러분이 보실 수가 있겠습니다. 네 근데 사실 l2 노름의 경우에는 앞서 이미 설명드렸던 것처럼 제곱근 부분이 있는데요. 사실 이 제곱근 부분을 사용하지 않고 그냥 제곱의 합 부분만 이용해서 저희가 미분을 계산을 해도 괜찮습니다. 이 경우에는 사실 계산식이 조금 더 간결해지기 때문에 다음과 같이 엘투 노름을 계산해서 분모에 집어넣는 항이 사라지게 됩니다. 네 이 경우에는 경사강법 알고리즘이 좀 더 간단해지게 되겠죠. 네 이와 같이 업데이트를 해도 즉 경사 강법 알고리즘을 수행을 해도 최적화되는 베타를 찾는 것은 가능합니다. 네 그래서 이 부분을 이용을 해서 저희가 선형 회귀 알고리즘을 위한 경사 강법 알고리즘을 한번 제안해 볼 수가 있는데요. 이번에는 입력에 해당하는 데이터가 데이터 행렬인 x도 필요하고 그리고 그라운트루스인 y도 이제 필요하게 됩니다. 그리고 이번에는 종료 조건이 어떤 미분값이 0보다 작게 어떤 0에 가까워지는 것의 종지 조건이 아니라 학습 업데이트 횟수를 의미하는 이 티를 인풋으로 받도록 하겠습니다. 이번에는 l2 노름을 계산하는 함수 노름이 똑같이 이제 필요하게 됩니다. 이때 앞서 살펴본 경사하강법 종료 조건과 달리 주어진 학습 횟수 기간만큼 우리가 경사 하강법 업데이트를 하라는 걸로 바꿔 놨기 때문에 이 경우에는 미분값이 0에 도달하지 않아도 종료를 하게 되는데요. 사실 선형 회귀 알고리즘의 경우에는 어 키를 충분히 늘리기만 해도 앞서 살펴본 종료 조건이랑 비슷하게 미분값이 0에 가까운 지점에서 멈출 수가 있게 됩니다. 네 그리고 우리가 앞서 살펴본 선형 회기에서의 경사광법 그레디언트 벡터를 계산하는 공식을 이용해서 우리가 이제 그레드를 계산할 수가 있는데요. 우리가 직접적으로 그레디언트 함수를 계산하는 거를 써도 되지만 이렇게 앞서 살펴본 수식을 이용해 가지고 우리가 그레디언트를 계산할 수가 있습니다. 다시 한번 살펴보시면 우리가 스랑 그리고 베타랑 행렬곱을 취해준 다음에 와에서 빼준 거를 우리가 에러라고 표기를 하고요. 그리고 이 에러와 원래 데이터의 트랜스포드를 다시 행렬 곱해서 마이너스를 취한 부분이 앞서 엘투 노름의 제곱에다가 베타에 대한 그레이디언트를 취한 것과 같다고 했습니다. 즉 2항을 계산하는 것인데요. 이걸 이용을 해서 베타를 업데이트해 주는 알고리즘을 짜주면은 우리가 선형 회귀 알고리즘을 위한 경사 강법 알고리즘을 쓸 수가 있게 됩니다. 네 그래서 이걸 이용해 주는 방식으로 우리가 오른쪽 코드와 같이 실제로 선형의 길을 위한 경사 하강법 알고리즘을 쓸 수가 있게 되는데요. 보시면 우리가 유사 역행렬을 굳이 구하지 않아도 경사 하강법 알고리즘을 통해서 적절한 회귀 계수를 계산할 수 있다는 거를 살펴볼 수가 있게 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 5,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 1072,
      "char_count": 1949
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c006_625fff",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 즉 2항을 계산하는 것인데요. 이걸 이용을 해서 베타를 업데이트해 주는 알고리즘을 짜주면은 우리가 선형 회귀 알고리즘을 위한 경사 강법 알고리즘을 쓸 수가 있게 됩니다. 네 그래서 이걸 이용해 주는 방식으로 우리가 오른쪽 코드와 같이 실제로 선형의 길을 위한 경사 하강법 알고리즘을 쓸 수가 있게 되는데요. 보시면 우리가 유사 역행렬을 굳이 구하지 않아도 경사 하강법 알고리즘을 통해서 적절한 회귀 계수를 계산할 수 있다는 거를 살펴볼 수가 있게 됩니다. 네 지금 현재 보시는 이 알고리즘을 보시면은 어 그라운트로스에 해당하는 와를 만들 때 이렇게 벡터 1 2를 가지고 실제 그라운트루스에 해당하는 개수를 만들었고 그다음에 3이 인터세트에 해당하는 항입니다. 즉 1 2 3 선형 회기를 통해서 구하고자 했던 개수의 정답 값에 해당하는데요. 실제로 이 경사강법을 돌려서 나온 결과값을 보시면 네 원래 그라운트로스에 해당하는 개수들과 유사하게 나온 것을 보실 수가 있게 됩니다. 이처럼 경사하강법 알고리즘을 이용해서 우리가 유사 역행렬을 구하지 않아도 실제로 최적 개수를 구할 수가 있다라는 것 즉 선형 회귀를 수행할 수 있다라는 거를 우리가 배울 수가 있게 됩니다. 그렇다면 이런 알고리즘은 언제 우리가 쓰게 될까요? 만약에 데이터의 개수가 너무나 크게 돼버리면은 사실 앞서 배웠던 선형 대수학 시간에서 배우게 된 역행렬을 구하는 부분이나 에브디를 구하는 부분이 사실 계산적인 비용이 너무나 크게 듭니다. 이 경우에는 사실 어 SVD 알고리즘을 써서 유사 역행렬을 구하는 것보다는 이와 같이 경사 하강법으로 우리가 선형 회귀를 돌리는 게 조금 더 경제적으로 효율적인 방향이 될 수가 있습니다. 그러므로 우리가 스케일러빌리티를 고려했을 때는 어떤 선형대수에서 사용되는 기법을 쓸 수도 있지만 이렇게 경사하강법 알고리즘을 이용해서 회귀 계수를 계산할 수 있다는 부분도 여러분이 잘 기억하시면 좋겠습니다. 한 가지 유의하실 점은 경사하강법 알고리즘에서는 이제 학습 유래라고 부르는 러닝 웨이트와 그리고 학습 횟수에 해당하는 부분이 하이퍼 파라미터라고 부르는 즉 우리가 알고리즘을 설정해 줄 때 먼저 중요하게 결정해 줘야 되는 부분이 되기 때문에 어 이 값을 적절하게 사용해 주지 않으면은 알고리즘이 수렴하지 않을 수도 있게 됩니다. 가령 아까 같은 경우에는 학습 횟수를 5천 정도로 설정했는데요. 만약에 학습 횟수를 한 100 정도로만 줄여서 시행해 주게 되면은 원래 목표했던 개수를 구하지 못하고 엉뚱한 값을 구하게 된다라는 걸 보실 수 있게 됩니다. 네 이 경우에는 경사 하강법이 제대로 동작하지 못한 케이스라고 보실 수가 있겠죠 이 경우는 사실 경사 하강법 알고리즘이 수렴하기 전에 종료된 부분이기 때문에 어 우리가 적절한 값을 구하지 못한 것이고요 이런 경우에는 학습 횟수를 늘려주는 것이 해법이 되겠습니다. 하지만 어떤 경우에는 학습률을 조정해야 되는 경우도 있기 때문에 문제에 따라서 해법이 달라진다는 사실을 기억하시면 좋겠습니다. 그렇다면은 경사 하강법은 만능일까요? 우리가 최적화를 수행할 때 이렇게 미분 값을 알고 있으면은 최적 지점을 항상 찾을 수 있는 것처럼 보이는데요. 이론적으로는 경사 하강법은 미분 가능하고 그리고 함수가 볼록한 형태일 때 적절한 학습률이랑 학습 횟수를 선택했을 때는 수렴을 보장할 수는 있습니다. 볼록한 함수란 극속값에 해당하는 지점이 오로지 한 점에 해당하는 걸 의미하는데요. 자 이 경우에는 그레디언트 팩터가 항상 최소점을 향하게 되어 있습니다. 그래서 앞서 살펴본 선형 회귀의 경우에는 목적식에 해당하는 함수가 회귀 계수 베타에 대해서 볼록 감시이기 때문에 알고리즘을 충분히 돌리면 즉 학습 횟수를 충분히 늘리면 수렴은 사실 보장하게 되어 있습니다. 그러나 비선형 회귀 문제의 경우에는 목적식이 볼록하지 않을 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 6,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 1054,
      "char_count": 1913
    },
    {
      "id": "transcript_ai_math_7강_경사하강법이랑_친해져보자_c007_38c3d4",
      "content": "[AI Math] 7강_경사하강법이랑_친해져보자\n\n다. 볼록한 함수란 극속값에 해당하는 지점이 오로지 한 점에 해당하는 걸 의미하는데요. 자 이 경우에는 그레디언트 팩터가 항상 최소점을 향하게 되어 있습니다. 그래서 앞서 살펴본 선형 회귀의 경우에는 목적식에 해당하는 함수가 회귀 계수 베타에 대해서 볼록 감시이기 때문에 알고리즘을 충분히 돌리면 즉 학습 횟수를 충분히 늘리면 수렴은 사실 보장하게 되어 있습니다. 그러나 비선형 회귀 문제의 경우에는 목적식이 볼록하지 않을 수가 있습니다. 즉 계수 베타나 또는 파라미터에 해당하는 어떤 이제 세타 이런 애들에 대해서 수렴성이 항상 보장되지는 않기 때문에 이런 경우에는 경사강법으로 항상 최적 파라미터 또는 최적 개수를 찾는 것이 어려울 수 있습니다. 네 특히 이제 딥러닝을 사용하는 경우에는 목적 식이 대부분은 볼록 함수가 아니기 때문에 경사 하강법으로 항상 최적의 패러미터를 찾는 것이 보장되지는 않는다는 거를 여러분들께서 유의하셔야겠습니다. 그러므로 경사 하강법의 문제점을 좀 정리를 해보면요. 그 값에 도달하면 더 이상 업데이트를 할 수 없는 부분이 있는데 이게 볼록 함수인 경우에는 문제가 없지만 볼록하지 않은 즉 넌 컨벡스 함수인 경우에는 문제가 생깁니다. 그리고 이런 경우는 딥러닝 학습 부분에서 충분히 발생할 수 있는 문제이기 때문에 우리가 굉장히 고민을 해봐야 되는 문제인데요. 그 값에 도달하지 않더라도 그레디언트 벡터가 0 근처로 떨어지면 최적화가 느려지기 때문에 우리가 딥러닝에서처럼 손실 함수가 어떤 파라미터에 대해서 넌 컨백스인 경우에는 경사강법을 그대로 쓰시면 안 되고 우리가 이제 변형된 알고리즘을 써야 되는 것입니다. 다음 시간에는 바로 이 변형된 알고리즘을 한번 배워보도록 하겠습니다. 수고하셨습니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "7강_경사하강법이랑_친해져보자.json",
        "lecture_name": "7강_경사하강법이랑_친해져보자",
        "course": "AI Math",
        "lecture_num": "7강",
        "lecture_title": "경사하강법이랑_친해져보자",
        "chunk_idx": 7,
        "total_chunks": 8,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:76cfb11e7dd7891a36261bbafecfd44f7be5b3f602c87aa905d9f7ebc43fa222"
      },
      "token_estimate": 490,
      "char_count": 885
    }
  ]
}