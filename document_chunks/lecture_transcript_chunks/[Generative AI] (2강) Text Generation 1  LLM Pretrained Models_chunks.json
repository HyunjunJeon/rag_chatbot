{
  "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
  "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
  "course": "Generative AI",
  "total_chunks": 6,
  "chunks": [
    {
      "id": "transcript_generative_ai_generative_ai_2강_text_generati_c000_a630ff",
      "content": "[강의 녹취록] 과목: Generative AI | 강의: 2강 | 제목: Text Generation 1  LLM Pretrained Models\n\n이번 두 번째 강의에서는 텍스트 제너레이션의 첫 번째 파트로서 엘엘엠의 프리트레인드 모델 다시 말하면 대규모 언어 모델의 사전 학습된 모델에 대해서 간략하게 말씀을 드리도록 하겠습니다. 크게 두 가지로 구성이 되어 있는데요. 라즐 랭귀지 모델의 기본적인 개념 모델 아키텍처, 사전 학습 태스크 그리고 사전 학습을 위해서 만들어지는 코퍼스 데이터를 설명을 드릴 것이고요. 두 번째는 이렇게 만들어진 LLM을 추가적으로 학습을 하는 데 있어서의 인스트럭션 튜닝에 대한 부분 설명해 드리겠습니다. 이 인스트럭션 튜닝은 크게 h 리스와 세이프티에 대한 부분 그리고 슈퍼바이즈 파인 튜닝에 해당하는 부분, 마지막으로 리퍼스먼트 러닝 바이 휴먼 피드백 해서 알치프라고 표현하는 이 부분에 대해서 설명을 드리고 마무리를 짓도록 하겠습니다. 라즐 랭귀지 모델의 기본 파트에서는 개념과 구조 및 사전 학습 그리고 학습 코퍼스에 대해서 설명을 드릴 건데요. 우선 라즈 랭귀지 모델의 개념부터 말씀드리겠습니다. 라즈 랭귀지 모델은 범용적인 테스크 수행이 가능한 언어 모델입니다. 사전 학습 데이터와 파라미터의 수가 매우 큰 모델들을 통합적으로 지칭하는 것이고요. 여기서의 사전 학습 데이터는 온라인상에 수집 가능한 최대한의 텍스트 데이터를 의미를 합니다. 예를 들어 라마 같은 모델은 학습 데이터로 4테라바이트를 사용을 하고 있고요. 파라미터의 수는 하드웨어상 학습 가능한 최대한의 미지수 파라미터입니다. 라마 같은 경우에는 사이즈마다 좀 다르지만 체 빌리언 70억 개에서 sixt 5 n 650억 개까지 있겠죠. 그래서 여기서 보시면 오픈 API에 챗gpt부터 GPT 시리즈로 대표되는 모델들, 메타의 라마 세븐빌리언부터 세븐티빌리언까지의 모델들, 미스트럴 에아의 모델 그리고 업스테이지 우리나라의 솔라 모델 이런 것들이 전부 엘엠이라고 보시면 되겠습니다. 프리트레인드 엘램 같은 경우에는 지피티 1 2 또는 버트 같은 모델들이 있는데 이 랭귀지 모델들은 다운스트림 테스크별로 파인튜닝을 통해서 목적별로 모델을 구축을 합니다. 하나의 모델을 이용해서 하나의 테스크를 해결하는 것이 목적인 반면에 라즐 랭귀지 모델 같은 경우에는 지피티 3 4 또는 챗gpt 라마 미스트랄 같은 것들이 있고요. 사전 학습과 파인튜닝을 통해서 범용 모델을 구축하는 것이 목적입니다. 여기서 다시 한 번 강조를 해 드리자면 차이는 사전 학습 때 랭귀지 모델 LLM이 아닌 그냥 랭귀지 모델 소규모 모델 같은 경우에는 목적별로 모델을 구축을 하는 것입니다. 반면에 LLM 라즈 랭귀지 모델은 사전 학습과 파인튜닝을 통해서 범용 목적을 가지는 모델을 구축하는 것이라서 하나의 모델을 이용해서 다양한 테스크를 해결하는 것 시간의 흐름에 따라서 실제 태스크들을 어떻게 풀어 왔는지에 대해서 간략하게 아래에 있는 그림을 통해서 이해하실 수 있을 겁니다. 또한 제로 퓨샷 러닝이라는 거는 LLM 범용 모델이 동작할 수 있는 원리입니다. 얘는 모델을 추가로 학습을 하는 게 아니라 내가 엘엘엠에다가 어떠한 데이터를 입력으로 주느냐 그리고 그걸 어떻게 구성하느냐에 따라서 테스크를 다양하게 수행할 수 있다는 뜻이죠. 예시를 보면 중간에 있는 라지 랭귀지 모델은 동일한 모델입니다. 그런데 위에 있는 감성 분석과 같은 테스크를 하기 위해서 우리는 이거에 대해서 아래 입력 데이터는 영화에 대한 리뷰야 리뷰를 잘 읽고 감성을 분석해 줘라는 인스트럭션과 함께 리뷰를 주었을 때 이거를 감성이 무엇인지를 긍정인지 부정인지를 LLM이 뱉어낼 수도 있고요. 또는 이러한 뉴 뉴스 기사들을 주고서 해당 본문을 잘 읽고 요약해 주는데 요약문을 두세 문장으로만 구성을 해야 되고 뉴스의 전반적인 내용이 담겨 있어야 된다라고까지 구체적으로 주니까 여기에 대해서 업스테이지는 자체 개발한 언어 모델을 아스컵에 적용했다. 그 결과 지피티4 대비 비슷한 속도와 성능을 보이고 있었다고 합니다라고 두 문장으로 요약을 해 주는 것을 볼 수가 있겠죠. 다시 한 번 강조하자면 가운데 있는 라즈 랭귀지 모델은 바뀐 게 없습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
        "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
        "course": "Generative AI",
        "lecture_num": "2강",
        "lecture_title": "Text Generation 1  LLM Pretrained Models",
        "chunk_idx": 0,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f63c7c67912824fc974a6644ad1541969a0782c0009c55125941d0c2c6fc91"
      },
      "token_estimate": 1112,
      "char_count": 2065
    },
    {
      "id": "transcript_generative_ai_generative_ai_2강_text_generati_c001_12bcb4",
      "content": "[Generative AI] [Generative AI] (2강) Text Generation 1  LLM Pretrained Models\n\n다. 그 결과 지피티4 대비 비슷한 속도와 성능을 보이고 있었다고 합니다라고 두 문장으로 요약을 해 주는 것을 볼 수가 있겠죠. 다시 한 번 강조하자면 가운데 있는 라즈 랭귀지 모델은 바뀐 게 없습니다. 과거의 랭귀지 모델들은 감성 분석용 랭귀지 모델, 뉴스 요약용 랭귀지 모델이 있어야 되는데 그게 아니라 라즐 랭귀지 모델 하나를 이용해서 여러 가지의 테스크를 수행할 수 있다라는 것을 다시 한 번 강조해 드리겠습니다. 제로 샷과 퓨샷의 차이는 뭐냐면 제로 샷은 모델이 프롬프트만으로 테스크를 이행해서 수행합니다. 위에 있는 예시처럼 영화에 대한 감상인데 감성을 분석해 줘 라고 해놓고 리뷰만 던져주고 끝납니다. 굉장히 어려운 테스크죠 퓨샷 러닝이라는 거는 모델이 프롬프트와 데모 스트레이션 여기서 예시가 되겠죠. 데몬스트레이션을 몇 가지를 두고 태스크를 이해해서 수행시키는 것입니다. 여기에 있는 예시에서는 데몬스트레이션으로 리뷰에 대한 감성을 긍정적인 리뷰 하나와 부정적인 리뷰를 예시로 주는 것입니다. 아무래도 똑같은 테스크라면 제로 샷보다는 퓨샷의 성능이 높을 것입니다. 모델이 능력이 충분할 경우에는 이 데모스트레이션을 통해서만 성능 향상이 가능하다라는 연구 결과도 있습니다. 여기서는 보시면 이그젠퍼를 컨텍스트에 몇 개를 주느냐 하나도 주지 않느냐 또는 뭐 10개까지 주느냐에 따라서 1.3빌리언 13빌리언 175빌리언 파라미터가 있을 때 제로샷과 원샷과 퓨샷을 봤을 때 원 샷 또는 퓨샷에서 175빌리언 짜리 모델이 급격한 성능의 향상을 나타내는 것을 보실 수가 있겠습니다. 이 말은 무슨 얘기냐 모델의 사이즈가 커지면 컨텍스트 안에서 사용자가 입력만 잘 구성하는 것으로만으로도 우리가 원하는 태스크들을 일정 수준 이상 수행할 수 있게 된다라는 얘기입니다. 여기서 다시 한번 프롬프트라는 거는 제로 또는 퓨샷 러닝이 가능하게 하는 엘엠의 입력 구성 방식입니다. 구성 요소는 크게 세 가지고요. 테스크 디스크립션 무엇을 수행해야 되는지에 대한 묘사입니다. 두 번째 데몬스트레이션 수행할 테스크에 대한 예시 바꿔 말하면 입력과 출력의 쌍입니다. 인풋은 실제 테스크를 수행할 입력 데이터입니다. 이 프롬프트를 어떻게 구성하느냐에 따라서 모델 성능이 굉장히 크게 변화한다라는 다양한 연구 결과들이 있습니다. 모델의 아키텍처 같은 경우에는 두 가지의 모델을 주로 사용을 합니다. 트랜스포머 구조를 변형하는 방식으로서 인코더 디코더 구조를 사용하는 경우도 있고요. 디코더 온의 구조를 사용하는 경우도 있는데 인코더 디코더 구조는 입력을 이해하고 모델을 생성하는 부분을 분리합니다. 그래서 입력 이해는 인코더를 통해서 처리하고 문장을 생성하는 것은 디코더를 통해서 처리합니다. 왼쪽에 있는 그림이 바로 입력을 통해서 인코더를 통해서 입력을 이해하는 파트와 여기에 있는 부분처럼 디코드를 통해서 문장을 생성하는 파트입니다. 반면에 디코드 온리 구조 같은 경우에는 단일 모델을 통해서 이해와 생성을 하는 것이고요. 지피티 시리즈는 바로 이 디코드 온리 구조가 되겠습니다. 프리 트레인 테스크 같은 경우에는 모델 구조별로 사전 학습 태스크가 상이한데요. 인코더 디코더 구조 같은 경우에는 스펜 cri이라는 방법론을 사용을 하기도 하는데 이거는 과거에 이제 티5에서 제한된 사전 학습 테스크고요. 손상된 입력 문장의 일부를 복원하도록 생성하는 겁니다. 그래서 인코더에서는 열심히 배우면 뭐 뭐 뭐 될 거예요라고 해서 좋은 개발자를 마스킹을 해 놓고 디코더 구조에서는 이 좋은 개발자라는 것을 이제 보여준 다음에 이 인코더를 통해서 학습했을 때 열심히 배우면 좋은 개발자 될 거예요라는 부분들이 나올 수 있도록 만들어 주는 것입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
        "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
        "course": "Generative AI",
        "lecture_num": "2강",
        "lecture_title": "Text Generation 1  LLM Pretrained Models",
        "chunk_idx": 1,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f63c7c67912824fc974a6644ad1541969a0782c0009c55125941d0c2c6fc91"
      },
      "token_estimate": 1020,
      "char_count": 1883
    },
    {
      "id": "transcript_generative_ai_generative_ai_2강_text_generati_c002_4ca7d6",
      "content": "[Generative AI] [Generative AI] (2강) Text Generation 1  LLM Pretrained Models\n\n다. 그래서 인코더에서는 열심히 배우면 뭐 뭐 뭐 될 거예요라고 해서 좋은 개발자를 마스킹을 해 놓고 디코더 구조에서는 이 좋은 개발자라는 것을 이제 보여준 다음에 이 인코더를 통해서 학습했을 때 열심히 배우면 좋은 개발자 될 거예요라는 부분들이 나올 수 있도록 만들어 주는 것입니다. 요 스펜 커럽션 같은 경우에는 입력 문장 중에 임의의 스팸 여기서는 좋은 개발자를 이제 마스킹을 하고요. 마스킹 한 다음에 마스킹 아이디를 부여를 하고 스펜 커럽션 된 문장을 인코드에 입력을 한 다음에 마스킹된 아이디 다시 말하면 여기에서의 디코더에는 반대로 열심히 배우면과 될 거예요를 없애버리고 좋은 개발자만 디코더에 입력을 하는 것이죠. 그러면 이 디코더는 복원된 문장을 생성하도록 하는 것입니다. 이걸 통해서 입력 문장의 이해와 문장의 생성 능력을 학습하게 됩니다. 디코더 온리 구조 같은 경우에는 gpt1에서 처음 제안된 사전 학습 테스크고요. 단순히 입력된 토큰을 기반으로 해서 다음 토큰을 예측하는 것을 수행하는 겁니다. 랭귀지 모델링은 문장 단위로 토큰을 입력하고 매 토큰마다 다음 토큰을 예측하도록 학습을 해서 이전 입력을 바탕으로 다음 토큰을 생성하는 것이죠. 그래서 처음에 열심히라는 단어만 주었을 때는 배우면 이라는 단어를 예측하도록 하는 것이고요. 열심히 배우면 좋은 개발자가라는 것까지 정보를 주면 될 거예요라는 것을 이제 생성하도록 유도하는 구조입니다. 최근의 모델 구조들은 대부분의 LLM은 코우즈 디코더 구조를 사용하고 있고요. 내부 구조는 거의 대부분 트랜스포머의 디코더 구조입니다. 그래서 프리 트레이닝 방식은 대부분 넥스트 토큰 프레딕션을 수행을 하고요. 이 방식이 구현 방식과 연산이 효율적이라고 알려져 있습니다. 모델 크기 관점에서는 2020년 지피티3 이후 모델 크기가 점차 확장이 되고 있는데 최근에는 굉장히 큰 크기의 모델들도 등장을 했죠. 뭐 엠티 엔엘지 같은 경우에는 530밀리언 개를 가지고 있기 때문에 굉장히 큰 사이즈라고 보실 수가 있겠습니다. 그래서 이제 관건은 이러한 대형 모델을 충분히 훈련시키고 학습시키기 위한 사전 학습용 코퍼스를 구축하는 것이 되겠습니다. 코퍼스를 구축하는 절차는 다음과 같습니다. 코퍼스는 다시 정의하자면 사전 학습을 위해서 대량의 텍스트 데이터 집합을 만들어 두는 것입니다. 이거를 그냥 원시 데이터에서부터 바로 학습을 시킬 수 있는 게 아니기 때문에 온라인상에서 수집된 최대한 많은 데이터를 먼저 원시 데이터에 저장을 합니다. 블로그 게시글, 뉴스 서적 댓글, 웹 문서, 커뮤니티 게시글 등이 여기에 포함이 됩니다. 그런데 원시 데이터에서는 학습에 불필요한 데이터가 존재합니다. 욕설과 혐오 표현은 제거를 해줘야 되겠고요. 중복돼서 뭐 좋아요 좋아요 좋아요라는 게 100만 건이 있다라고 해서 언어 모델이 학습이 더 잘 되는 건 아닙니다. 그래서 중복 데이터를 제거하거나 또는 굉장히 민감한 개인 정보가 포함된 데이터를 제거하는 이러한 방식을 통해서 줄여가는 원래 원시 데이터의 일부분만을 필터링하는 과정을 거치게 됩니다. 이렇게 하면 이제 학습을 하는 데 있어서 어떠한 현상이 발생할 수가 있냐면 라즐 랭귀지 모델이 코퍼스 내에 존재하는 데이터를 암기하는 현상을 나타냅니다. 메모라이제이션이라고도 표현을 하는데요. 코퍼스 내에 중복하여 등장한 데이터를 쉽게 암기하기 때문에 모델 크기가 특히 클수록 암기 능력이 향상이 됩니다. 그렇기 때문에 데이터 정제를 수행하지 않을 경우에는 혐오 표현이라든지 개인 정보 등의 부적절한 답변도 도출이 가능하다라는 문제점이 있어서 이러한 메모라이제이션은 방지해야 된다라고 보시면 되겠습니다. 원시 코퍼스 내에 그 공개된 원시 코퍼스를 분석을 해보면 데이터 내에 최대 300만 회 이상 무의미한 중복 문장이 확인이 됩니다. 아래에 있는 예시들을 보자면 점만 많이 있다든지 슬래시만 많이 있다든지 물음표만 많이 있다든지 이런 것들은 전혀 의미가 없는 게 되겠죠. 이런 것들은 학습 자원을 소모할 뿐이지 모델 학습에 유의미한 도움이 되지는 않습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
        "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
        "course": "Generative AI",
        "lecture_num": "2강",
        "lecture_title": "Text Generation 1  LLM Pretrained Models",
        "chunk_idx": 2,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f63c7c67912824fc974a6644ad1541969a0782c0009c55125941d0c2c6fc91"
      },
      "token_estimate": 1117,
      "char_count": 2065
    },
    {
      "id": "transcript_generative_ai_generative_ai_2강_text_generati_c003_62bc0b",
      "content": "[Generative AI] [Generative AI] (2강) Text Generation 1  LLM Pretrained Models\n\n다. 원시 코퍼스 내에 그 공개된 원시 코퍼스를 분석을 해보면 데이터 내에 최대 300만 회 이상 무의미한 중복 문장이 확인이 됩니다. 아래에 있는 예시들을 보자면 점만 많이 있다든지 슬래시만 많이 있다든지 물음표만 많이 있다든지 이런 것들은 전혀 의미가 없는 게 되겠죠. 이런 것들은 학습 자원을 소모할 뿐이지 모델 학습에 유의미한 도움이 되지는 않습니다. 그래서 정제 작업을 통해서 학습 효율을 극대화시킬 필요성이 생기게 됩니다. 개인 정보 같은 경우에는 데이터 내에 그 확인을 해봤더니 최대 천만 회 이상의 개인 정보가 포함이 된다라는 것을 알았다고 합니다. 여기서의 개인 정보라는 것은 전화번호, 이메일 주소, 아이피 주소 등입니다. 이거는 아까 말씀드렸던 메모라이제이션을 통해서 모델이 암기를 해서 서비스를 배포하는데 이러한 개인 정보를 유출할 가능성도 높아집니다. 따라서 정제를 통해서 반드시 이러한 개인 정보가 암기되지 않도록 학습을 방지해야 합니다. 두 번째 파트로는 인스트럭션 튜닝에 대해서 설명을 드리겠습니다. 인스트럭션 튜닝에서는 크게 헬프 프루니스와 세이프티 관련된 내용, 슈퍼라이즈드 파인 튜닝에 대한 내용 그리고 다이렉트 프리퍼런스 옵티미제이션에 대한 내용을 설명을 드리겠습니다. LLM은 대형 코퍼스로 학습되었기 때문에 굉장히 놀라운 능력을 보유하고 있습니다. 앞서 말씀드렸던 설명해 드렸던 제로 또는 퓨샷 러닝을 통해서 입력된 프롬프트 내의 정보만으로 테스크를 학습하고 수행할 수 있기 때문에 존재하지 않는 어휘에 대해서도 문장을 생성할 수 있는 능력이 있습니다. 프롬프트에서 t da FL d들이라는 저도 발음하기도 어려운데 이러한 의미에 아무런 의미 없는 사전에 존재하지 않는 단어를 주고서 이게 이러한 의미를 한다 정의가 이렇다 라고 해서 점프 업 앤 다운을 굉장히 빨리 하는 겁니다. 위아래로 빨리 뛴다 이러한 단어를 이용해서 문장을 예시를 만들어 줘라라고 하면 GPT는 만들어집니다. 지금은 많이 정지가 됐지만 처음에 챗gpt가 나왔을 때 굉장히 놀림을 받았던 예시 중에 하나도 세종대왕의 맥북 사건에 대해서 알려줘라고 했을 때 실제로 세종대왕이 한글을 창제하다가 신하들에게 화가 나서 가지고 있던 맥북을 집어던졌다 이런 문장을 생성해 준 사례가 있었어요. 그렇기 때문에 여기에서는 엘엠이 생성해 내는 이 결과물을 무조건 믿을 수만은 없다라는 것입니다. 그래서 세이프티 관점에서는 온라인 상에 존재하는 혐오 차별 위험 표현이 포함될 수가 있기 때문에 이런 것들을 이제 걸러야 되고 이런 게 반드시 걸러져야지만이 활용과 서비스화의 걸림돌이 없어지게 됩니다. 예를 들어서 왜 무슬림들은 테러리스트냐라는 질문에 대해서 gpt3 같은 경우에 여러 가지 이제 답변들을 만약에 해주게 되는데요. 사실은 답변을 해주는 게 아니라 그러한 질문은 부적절합니다라고 해서 답변을 거부하는 게 더 맞겠지요. 바꿔 말씀드리자면 혐오 차별 위험 또는 특정 질병에 대한 잘못된 조언 또는 진단 같은 경우에는 생성을 하지 않아야 됩니다. 그래서 gpt3와 같이 예전에는 우리가 암을 치료하려면 어떻게 해야 돼라고 했을 때 아무거나 막 인터넷에 존재하는 어떤 정보들을 이용해서 아웃풋을 뱉어줬다면 셀프티가 반영된 이 답변 같은 경우에는 암이란 어떤 것이고 그 치료 방법에는 굉장히 다양한 방식이 있는데 여기에서는 적절하게 당신의 상황을 모르기 때문에 완벽한 치료법은 또는 트리트먼트에 대해서는 재현해 줄 수가 없습니다라고 답변을 우회하는 게 더 필요하다는 뜻이죠. 그래서 헬프 프로미스라는 개념은 엘엠이 사용자의 다양한 입력에 적절한 답변을 생성할 수 있어야 한다라는 뜻이고요. 기존 프리 트렌드 엘엠 같은 경우에는 입력과 출력이 훈련된 테스크로 구성되기 때문에 이러한 해프니스에 대한 이슈가 없는데 엘엘엠은 사용자가 원하는 광범위한 입력에 대해서 출력을 생성해야 되기 때문에 그렇습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
        "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
        "course": "Generative AI",
        "lecture_num": "2강",
        "lecture_title": "Text Generation 1  LLM Pretrained Models",
        "chunk_idx": 3,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f63c7c67912824fc974a6644ad1541969a0782c0009c55125941d0c2c6fc91"
      },
      "token_estimate": 1070,
      "char_count": 1978
    },
    {
      "id": "transcript_generative_ai_generative_ai_2강_text_generati_c004_c8096f",
      "content": "[Generative AI] [Generative AI] (2강) Text Generation 1  LLM Pretrained Models\n\n다. 예를 들어 프롬프트에서 6살 꼬마한테 달 착륙 과정을 설명해 줘라고 했을 때 지피티3 같은 경우에는 보시는 바와 같이 이제 그 별 의미 없는 문장을 설명을 해주는데 헤어포메이스 같은 경우에는 여기에 대해서 이와 관련해서 조금 더 유용한 정보들을 이렇게 꼬마가 이해할 수 있을 정도의 수준에서 설명을 해주게 되는 것이죠. 이러한 이슈들은 인스트럭션 튜닝을 통해서 이제 구현을 할 수가 있게 됩니다. 사전 학습은 이전 단어를 바탕으로 단순히 다음 단어를 예측하는 것만을 목적으로 하지만 인스트럭션 튜닝 같은 경우에는 사용자의 광범위한 입력에 대해서 안전하면서 도움이 되는 적절한 답변이 되도록 하는 파인 튜닝 과정입니다. 그렇기 때문에 인스트럭션 튜닝 과정은 크게 세 가지 단계로 이제 알치프 논문 기준입니다. 세 가지 단계로 구성이 되는데 첫 번째는 슈퍼바이즈 파인 튜닝, 광범위한 사용자 입력에 대해서 정해진 문장을 생성하도록 한 번 파인 튜닝을 합니다. 두 번째는 리워드 모델이라는 게 있는데요. 엘엘엠이 그렇게 해서 이 SFT를 통해서 만들어지는 생성물에 대해서 사람이 선호도를 판별할 수 있는 이러한 모델에 대해서 파인튜닝을 하고 알엘치프 같은 경우에는 광범위한 사용자 입력에 대해서 인간이 선호하는 답변을 출력하도록 이제 강화 학습을 하는 것입니다. 하나씩 설명을 드리겠습니다. 슈퍼바이드 파인 튜닝 에프티 같은 경우에는 학습 데이터는 프롬프트는 사용자의 매우 다양한 요청입니다. 도메인과 입력 형태들이 자유로워야 되고요. 데모스트레이션 같은 경우에는 해당 요청에 대한 적절한 답변을 사람이 직접 만든 겁니다. 이 데모스트레이션에서는 세이프티와 헤어토리스를 만족할 수 있도록 이제 가이드라인이 주어지는 것이고요. 이 슈퍼바이즈 파인트리닝의 목적은 엘엘엠에게 사용자 입력에 적절히 답변을 하도록 입력과 출력사항을 주어서 지도 학습을 시키는 것입니다. 두 번째 단계는 리워드 모델링인데요. 리워드 모델링은 엘엘엠이 이렇게 슈퍼바이트 파인튜닝을 통해서 만들어 낸 답변에 대해서 사람의 선호도를 모델링하는 것입니다. 리워드 모델링은 엘엘엠의 생성문이 해프니스는 얼마나 충족을 했고 세프티는 얼마나 충족을 했는지에 대한 만족도를 점수로 산출합니다. 학습 데이터는 프롬프트는 사용자의 요청이 되겠고요. 데몬스트레이션은 생성한 답변 후보 레이팅은 사람이 판단한 프롬프트에 대한 데몬스트레이션의 적절성이 되겠습니다. 학습 방법 같은 경우에는 프롬 소프트와 데모스트레이션을 입력으로 해서 사용자의 요청과 엘르램의 후보를 입력으로 해서 레이팅을 산출하도록 학습하는 겁니다. 그렇기 때문에 레이팅은 5점 척도 또는 뭐 10점 척도라고 한다면 1부터 10까지의 오디널 스케일이기 때문에 랭킹 기반의 학습 방법론을 사용합니다. 여기서 보시는 것처럼 예시처럼 왜 무슬림들은 테러리스트가 되는 거야라고 했을 때 쿨 안에 테러를 저지르라고 써 있기 때문입니다라고 하는 거는 셀프 티 측면에서 맞지 않고 일부 테러리스트가 본인들의 입맛에 맞게 쿨안을 해석하기 때문입니다가 더 적합한 모양입니다. 그러면 리워드 모델은 두 개의 데모 스트레이션에 대해서 아래쪽 데모스트레이션이 위쪽 데몬 스트레이션보다 더 적절한 답변이라고 웨이팅을 매기는 그러한 과정을 학습시켜 주는 것입니다. 마지막 단계인 RLH 리 퍼스트 러닝 바이 휴먼 피드백이라는 거는 엘엘엠이 사람의 선호도가 높은 답변을 생성하도록 학습하는 과정입니다. 사람의 선호도는 리워드 모델이 높은 점수를 부여하는 답변이 될 것이고요. 슈퍼파이드 파인 튜닝을 통해서 답변을 생성하면 리워드 모델이 레이팅을 부여합니다. 이 레이팅을 높이는 방향으로 SFT 모델을 학습을 하게 될 것이고 여기서는 프록시멀 폴리시 옵티미네이션이라고 하는 PPO 알고리즘을 사용을 하게 되는데 이 알고리즘 자체에 대한 설명은 조금 이 강의의 범위를 벗어나기 때문에 생략하도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
        "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
        "course": "Generative AI",
        "lecture_num": "2강",
        "lecture_title": "Text Generation 1  LLM Pretrained Models",
        "chunk_idx": 4,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f63c7c67912824fc974a6644ad1541969a0782c0009c55125941d0c2c6fc91"
      },
      "token_estimate": 1078,
      "char_count": 1977
    },
    {
      "id": "transcript_generative_ai_generative_ai_2강_text_generati_c005_d7bed5",
      "content": "[Generative AI] [Generative AI] (2강) Text Generation 1  LLM Pretrained Models\n\n다. 사람의 선호도는 리워드 모델이 높은 점수를 부여하는 답변이 될 것이고요. 슈퍼파이드 파인 튜닝을 통해서 답변을 생성하면 리워드 모델이 레이팅을 부여합니다. 이 레이팅을 높이는 방향으로 SFT 모델을 학습을 하게 될 것이고 여기서는 프록시멀 폴리시 옵티미네이션이라고 하는 PPO 알고리즘을 사용을 하게 되는데 이 알고리즘 자체에 대한 설명은 조금 이 강의의 범위를 벗어나기 때문에 생략하도록 하겠습니다. 예시만 보면 왜 무슬림들은 테러리스트가 되는 거야라고 질문을 했을 때 쿨 안에 테러를 저지르라고 써 있기 때문입니다라고 하는 거는 굉장히 선호도가 낮았죠. 그러면 레이팅을 0.1을 주면 이 피피오 알고리즘을 통해서 비슷한 질문이 나왔을 때는 이러한 답변을 어지간하면 내지 못하도록 유도하도록 학습시키는 게 바로 알치프입니다. 다시 한번 크게 정리를 하자면 알치프 학습을 위해서 데이터셋 및 모델이 이제 필요한데 데이터 구축 시에는 세이프티와 헬프 포니스 반영으로 막대한 비용이 소모가 됩니다. 또 LLM 학습으로 인해서 막대한 GPU 비용도 소모가 됩니다. 이 아래에 있는 그림에서의 예시를 보시면 슈퍼와이즈 파인튜닝 같은 경우에는 데이터셋을 13k이니까 만 3천여 개의 데이터셋을 사용을 해서 gpt3 모델을 학습을 했고요. 랭킹 모델 리워드 모델 같은 경우에도 33만 3천 개의 데이터셋을 사용을 해서 슈퍼바이드 파인튜닝 모델을 사용을 하고 그리고 PPO 데이터셋을 알치프에서는 사용을 하고 지피티3와 리워드 모델을 활용을 하기 때문에 세 가지의 스텝에서 전부 모두 데이터 셋과 모델을 많이 사용을 한다라는 이제 그 이슈가 존재하기는 합니다. 그러나 이러한 슈퍼바이스 파인 튜닝과 알엘치프를 적용을 했을 때는 사용자 지시를 따르도록 모델의 파인튜닝을 했을 때 인스트럭션 팔로잉 능력이 굉장히 향상이 되어 있고요. 사용자의 지시에 대한 호응도가 상승을 하고 거짓 정보인 할루시네이션에 대한 생성 빈도도 감소했다라는 연구 결과가 있습니다. 또한 슈퍼바이즈 파인튜닝과 RLH프를 학습을 할 경우에는 사전 학습 ln보다 다양한 지표에서 많이 개선이 되었습니다. 인스트럭션을 팔로잉하는 능력이라든지 커스터머를 어시스턴트 하는 능력 또는 할루시네이션 관련해서 감소시키는 능력들이 모두 향상이 되었고, 이를 통해서 라즐 랭귀지 모델은 모델의 크기도 중요하지만 인스트럭션 튜닝 방법론이 매우 중요하다는 것을 다시 한번 알 수가 있습니다. 정리를 하겠습니다. 엘엘엠은 대량의 코퍼스로 학습된 파라미터가 매우 많은 언어 모델입니다. 제로와 퓨샷 러닝은 입력 데이터 내에 주어진 정보로 새로운 테스크를 해결하는 능력이고요. 기존의 파인튜닝 방식 모델 패러다임과는 다른 엘엘엠을 활용하는 방식입니다. 그리고 엘엘엠을 광범위하게 활용할 수 있는 배경이 되는 거죠. 반면 학습 데이터를 암기하는 능력이 매우 탁월하기 때문에 욕설, 사회적 편향, 개인 정보 등의 사전 학습 코퍼스를 전처리하는 것이 매우 필수적입니다. 또한 인스트럭션 팔로잉 같은 경우에는 사용자 입력에 대해서 적절한 출력을 생성하는 능력이기 때문에 LLM 같은 경우는 사용자의 입력에 맞추어서 유용하고 안전한 답변을 생성해야 됩니다. 인스트럭션 튜닝은 인스트럭션 팔로잉 능력 향상을 위한 파인 튜닝 방법론으로서 슈퍼바이저 파인 튜닝과 알엘치프를 통해서 인스트럭션 팔로잉 능력을 향상시킬 수가 있습니다. 다만 슈퍼바이저 파인 튜닝과 알엘치프 학습 구축 시에는 매우 큰 비용이 발생할뿐더러 섬세한 설계가 필요하다는 점을 주의해 주시면 좋겠습니다. 이로써 두 번째 강의를 마치도록 하겠습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models.json",
        "lecture_name": "[Generative AI] (2강) Text Generation 1  LLM Pretrained Models",
        "course": "Generative AI",
        "lecture_num": "2강",
        "lecture_title": "Text Generation 1  LLM Pretrained Models",
        "chunk_idx": 5,
        "total_chunks": 6,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:53f63c7c67912824fc974a6644ad1541969a0782c0009c55125941d0c2c6fc91"
      },
      "token_estimate": 994,
      "char_count": 1843
    }
  ]
}