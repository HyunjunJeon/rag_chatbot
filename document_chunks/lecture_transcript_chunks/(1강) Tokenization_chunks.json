{
  "source_file": "(1강) Tokenization.json",
  "lecture_name": "(1강) Tokenization",
  "course": "NLP",
  "total_chunks": 5,
  "chunks": [
    {
      "id": "transcript_nlp_1강_tokenization_c000_3ae1e5",
      "content": "[강의 녹취록] 과목: NLP | 강의: 1강 | 제목: Tokenization\n\n안녕하세요. 이재경입니다. 이번 수강은 주민 반이 바라는 처리에 있어서 가장 첫 번째 단계를 수행하는 처리다. 기어진 텍스트를 특별한 단위로 쪼개서 일반 피타스 데이터를 만들어주는 것인데요. 이렇게 아나나 캐릭터 단위로 지어진 텍스트를 쪼개는 과정 포스터마이제이션에 대해 살펴보겠습니다. 구체적으로 이번 수업에서는 주어진 선수들이 시간 단위로 쪼아낸 테크나이제이션의 개념과 구체적인 방법론들이 어떻게 등장하는지 그리고 그 장단점을 살펴보고 캐리터 반이나 워드 바이 테크널리제이션 기법들의 단점을 모두 살린 실제 멜리스에 있는 아이포트 인터링이라는 테크날제이션 기법의 동작 과정 살펴보겠습니다. 기본적으로 다양한 데이터는 각 파트에서 지어진 캐릭터나 단어들의 일련의 시퀀스로 사용됩니다. 이러한 과정에서 실천하기 위해서는 지어진 텍스트를 어떤 기준으로 기울기 해서 변화 처리 모델의 입장에서 각 한 스텝에서 지어지는 캐릭터나 워드 혹은 서브 어드 러버로 테크나이제이션에서 보이는 것처럼 이렇게 서브 어드러로 쪼겨나가는 과정이라고 볼 수가 있고요. 그러한 테크나이제이션의 결과를 만들어 주는 각 사이 점마다 주어지는 이러한 입력 단위를 저희는 하나하나를 패턴이라고 부릅니다. 그러면 이러한 패턴의 경우 어떤 사전 혹은 도회지 머리가 변이되어 있을 때 그 사전에 변이된 하나의 카테터 대리이라고 볼 수 있습니다. 그러면 이를 연합 벡터를 보통 표현할 수 있습니다. 그리고 이러한 테크나이제이션의 방식은 각 테크 반응에 따라 워드 단위 혹은 캐리터 단위 그리고 그 중간 단계로서 서브 워드 단위의 테크날리제이션이라는 방식들이 존재하게 됩니다. 그중 먼저 워드 레벨 혹은 단어 단위의 테크널레이션을 좀 더 자세히 살펴보면 일반적으로 지어진 테스트를 가야 단위로 나눌 때는 이렇게 CTD를 기준으로 주문하는 경우가 일반적입니다. 그렇지만 언어에 따라서는 서로 다른 의미를 가진 자녀가 띄어쓰기 없이 비어져 나오는 경우도 존재하고요. 그리고 이 전에 그 의미 단위를 기준으로 하는 회의 형태라는 것을 개별 단어적으로 보고 지어진 설치의 테크나이제이션을 사용할 수 있습니다. 그러면 이 경우 여기 있는 한 한국 문과 예술의 전 첫 번째 한국편에서는 나라는 의미를 가지는 호칭 그리고 두 번째 타석에서는 기어라는 대사 혹은 기어라는 의미를 가지는 이러한 능이라는 단어를 하나의 소통으로 처리할 수 있을 것이고요. 마음 전에 나타는 이러한 띄어쓰기도 3월 독립된 단어를 본다면 세 번째 상처에서는 이러한 수기에 해당하는 공백 또한 하나의 단어로서 이 기업이 수상의 능력으로 볼 수 있을 것입니다. 그런데 이러한 단어 단위 포티마이제이션에 대한 문제점이 하나 있는데요. 이는 저희가 학습 단계에서 사전 혹은 고체 블러리를 구축할 때 즉 여기서의 사전이라는 것은 지어진 학습 데이터 안에서 발견된 모든 종류의 유니크한 단어들이 다 모아서 그 각각의 하나하나의 특정 카테고리를 확산하고 이를 통해 학습 데이터에서 나타난 각각의 패턴들이 이 사전에 기반한 카테고리컬 베리의 형태의 연합 벡터를 나타내는 과정이라고 생각할 수 있는데요. 학습이 완료된 모델을 대상으로 시험 단계에서는 어떤 새로운 응용 문버를 들어봤을 때 저희가 학습 단계에서 보지 못한 단어가 나오게 되면 사전에 정의되어 있지 않은 단계이기 때문에 이를 적절한 원아 세터 혹은 기업이 사천공이라는 대국은 경제를 실현할 수 없게 됩니다. 따라서 이를 보완하기 위한 현실적인 방법으로서 그것은 학습 단계에서 사전에 구축할 때 이를 획득하는 것을 보지 못할 단어이 여전히 어떤 하나의 파트된데에 기여해서 연합 독서를 인코딩 하기 위해서 사전에 미리 이거는 엠메이라는 단어에 해당하는 카테고리를 하나 만들어 주세요. 저희 사전상에 정의되지 않은 단어가 들어왔을 때 이들 모두 다 이러한 법령인 사포들에 해달리는 폭행으로 처리하는 것입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(1강) Tokenization.json",
        "lecture_name": "(1강) Tokenization",
        "course": "NLP",
        "lecture_num": "1강",
        "lecture_title": "Tokenization",
        "chunk_idx": 0,
        "total_chunks": 5,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c05481d0c7ce9b49ee7c553faa4af8c3c23b95c8d1a699ebda02d63c2de0c07c"
      },
      "token_estimate": 1069,
      "char_count": 1931
    },
    {
      "id": "transcript_nlp_1강_tokenization_c001_c76474",
      "content": "[NLP] (1강) Tokenization\n\n다. 따라서 이를 보완하기 위한 현실적인 방법으로서 그것은 학습 단계에서 사전에 구축할 때 이를 획득하는 것을 보지 못할 단어이 여전히 어떤 하나의 파트된데에 기여해서 연합 독서를 인코딩 하기 위해서 사전에 미리 이거는 엠메이라는 단어에 해당하는 카테고리를 하나 만들어 주세요. 저희 사전상에 정의되지 않은 단어가 들어왔을 때 이들 모두 다 이러한 법령인 사포들에 해달리는 폭행으로 처리하는 것입니다. 그러면 패스 한 것에 불과하는 이러한 법령에 해당하는 사람의 경우 그 나름대로 각각의 어떤 특정한 의미를 받을 수 있을 텐데 이런 의미를 적절히 나타내지 못하고 모두가 불목적으로 이러한 범행인 행위를 하려 하는 경우 해당 테스트의 의미를 우리 딥러닝 매뉴들이 잘 이해하지 못할 것이고요. 따라서 우리가 쓰고자 하는 패스트를 실현하는 데 있어서 큰 반응의 의미를 가져갈 수 있을 것입니다. 그래서 이렇게 문제가 되는 애는 패턴이 저는 아이렌 도페밀로리 패턴이라고도 부르고요. 이로 인해 나타나는 방금 말씀드린 문제점들을 아이렌 도표 블록이 문제가 될 보게 됩니다. 두 번째 종류의 테크놀리게이션 방법은 바로 캐릭터 혹은 활자 반응을 토크나이제이션입니다. 이러한 방식의 장점인 가령 같은 알파벳을 공유하는 여러 언어들 예를 들어 영화나 스토리나 등이 있겠고 그러면 이런 아동들에게 공통적으로 문과 단위의 패턴 지성이 적용될 수 있다는 장점을 가지게 됩니다. 또한 이 경우 캐릭터 단위의 파장을 생각해 낼 때 가령 알파벳을 쓰는 영어의 경우 a부터 100회까지 알파벳을 이미 4단사를 봤다고 파트 혹은 레드를 가늠하게 되면 패스트 파이에서는 그 어떤 텍스트가 주어진다 하더라도 이렇게 a부터 100개까지의 알파벳이라는 이 활자의 변이에서 벗어날 수는 없기 때문에 알은 대체 물가 문제 혹은 고의 문제는 근본적으로 발생하지 않게 됩니다. 그렇지만 이러한 캐릭터다를 테크하기 위한 비법은 바나바를 패턴하기 위해서는 기업 동일한 인간이 나타나 고 패턴의 벽체가 지나치게 많아져서 우리 딥러닝 모델이 처리해야 하는 입력 시퀀스의 길이가 상당히 길어짐으로 인해서 개선량이 많아지거나 혹은 개선된 GP의 리세트를 이익 문제를 처리하는 것이 불가능해질 수 있다는 관점을 가집니다. 그리고 또 다른 문제는 이어진 문장에 각 상위 시설에서 나타나는 어떤 특별한 사정만으로 인터뷰 할 때 이러한 각각의 토크의 정보가 어떤 유의미한 정보를 내포하는 것이 바람직할 텐데요. 다만 바나다미 코트네이 션이 되는 토크으로 취급되는 바나가 각각 어떤 의미나 뜻을 가진 바로 볼 수 있지만 캐릭터 단위의 테크니제이션을 보면 그 캐릭터 자체가 어떤 유의미한 정보를 가진다고 보기 어렵고요. 그 캐릭터들의 어떤 특별한 순서에 조합을 통해 만들어진 만화 형태가 되었을 때 이곳에 어떤 유의미한 정보를 가진다고 볼 수 있을 것입니다. 따라서 일반적으로 이러한 캐릭터 단위의 테크나이제이션을 사용했을 경우는 저희가 쓰고자 하는 다양한 바이네커리 테스트에 있어서 그 성능이 그다지 높지 않은 모습을 보여주었습니다. 다음에 말씀드린 네 번째 테크나이제이션 방법은 앞에서 워드 레벨 그리고 테리 환자들의 테크나이제이션에 대한 적절한 절편함으로써 기업인 패스들이 서브어드의 단위를 구분하고 테크나이제이션을 하는 것입니다. 스퍼 세팅이라는 단어를 생각해 볼 때 여기 있는 이 단어는 앞에 입지 그리고 퍼세트 그리고 그다음에 알렌이라는 이 대개의 의미 단위로서 세계로 생각할 수 있을 것이고요. 그래서 이 앞에 있는 소리라는 것은 무언가 이장으로라는 의미를 가지는 자체라는 것을 저희가 알고 있고요. 프로세스라는 단어 또한 어떤 것을 처리한다라는 의미를 가지고 있고, ing의 경우 현재 주문이나 어떤 영상화를 만들어주는 역할을 하고 있습니다. 따라서 이런 방식으로 주어진 하나의 단어라 하더라도 각각의 어떤 의미를 가지는 이러한 짜개어진 단어를 쪼개서 주어진 단어를 실천하기 하게 되면 한 단어를 대상으로도 사단감에 포함된 여러 의미들을 효과적으로 고려하는 식으로 소형이 구성될 것입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(1강) Tokenization.json",
        "lecture_name": "(1강) Tokenization",
        "course": "NLP",
        "lecture_num": "1강",
        "lecture_title": "Tokenization",
        "chunk_idx": 1,
        "total_chunks": 5,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c05481d0c7ce9b49ee7c553faa4af8c3c23b95c8d1a699ebda02d63c2de0c07c"
      },
      "token_estimate": 1106,
      "char_count": 2003
    },
    {
      "id": "transcript_nlp_1강_tokenization_c002_851c97",
      "content": "[NLP] (1강) Tokenization\n\n다. 따라서 이런 방식으로 주어진 하나의 단어라 하더라도 각각의 어떤 의미를 가지는 이러한 짜개어진 단어를 쪼개서 주어진 단어를 실천하기 하게 되면 한 단어를 대상으로도 사단감에 포함된 여러 의미들을 효과적으로 고려하는 식으로 소형이 구성될 것입니다. 그런데 이러한 서브드 레벨의 세터마이제이션을 수행할 때 생기는 이제는 어떤 이런 특정한 의미를 가지는 서브 에이 과연 어떤 것들이 있는가에 대한 정보가 사전에 주어져 있지 않다는 것입니다. 이를 해결하기 위해 학습 데이터상에서 빈번하게 나타나는 서브 에릭을 중심으로 우리의 사전을 구축하는 방법을 생각해 볼 수 있을 것입니다. 그중에 한 예시가 여기에는 아트 인포드이라는 기법이 되겠는데요. 이는 저희가 다음 슬라이드에서 좀 더 자세히 살펴볼 예정이고요. 그래서 이 바이크 인텔드를 넘어가기 전에 이러한 서브드 로델의 테크나이제이션 비법의 단점을 먼저 말씀드리면 캐릭터 단만으로 사용하는 테크나이제이션 방법에 비해서는 이어진 동일한 텍스트를 테크나이제이션 하고 났을 때 그 생기는 시퀀스의 길이가 확연히 적을 것이고요. 또한 일반적으로 서브 어드 레벨의 패크나이제이션 기법들은 어떤 다수의 캐릭터들로 이루어진 서브의 기능을 포함할 뿐만 아니라 캐릭터 버전 캐피노니제이션에서 처럼 각각의 다른 캐릭터들도 저희 사전에서 기본적으로 포함되는 전략을 취하기 때문에 앞에서 보신 프리컬러벌의 코트나드 의상에서 가지던 인권처럼 오호부의 문제가 전혀 발생하지 않는다는 장점을 가지게 됩니다. 또한 이렇게 오호 무역 문제가 어떤 것에도 의미 단위로 잘 나타낼 수 있는 그러한 서브워드 정도로 주어진 테스트를 테크나이제이션 했을 때 해당 다양한 처리 모델이 다양한 테스트들에 걸쳐서 실질적으로 앞에서 봤던 두 가지 테크나이제이션 기법들보다 더 좋은 성능을 보여주었고요. 이로 인해 많은 거래 처리 딥러닝 모델들에서 이러한 서브드 레벨로 컬터나이제이션 기법을 사용하고 있습니다. 그러면 이러한 서브드 레벨의 컬터나이제이션은 결제 방법론 중 하나인 바이트와 인터뷰 및 동작 과정을 좀 더 자세히 살펴보겠습니다. 외의 기업은 학습 데이터가 다양한 민관으로 이루어져 있을 때 그러한 학습 데이터에서 근거한 변화들이 여기에는 코가 있고 그 해당 단 인더스트가 이렇게 존재했다고 생각을 해 보겠습니다. 그러면 저희는 이러한 바이트 인쇄중에서 화선을 구축하는 첫 번째 단계로서 이 주어진 학습 데이터에서 나타난 이러한 단일 캐릭터들을 이상적으로 우리 사전상에 등록해 주게 됩니다. 하늘에는 이렇게 사전상에 등록된 단일 캐릭터들을 사용했다. 기업의 학습 데이터를 포크나이제이션을 꺼내든 이렇게 2개의 패턴들을 특정 순서로 이어붙인 글이 1인 패킹 쌍을 만들었고요. 그 각각의 패킹 쌍에 대해 이 학습 데이터에서 나타난 등대치를 계산하게 됩니다. 그랬을 때 여기에 있는 ST라는 토크 사이 저희 학습 데이터에서 10가지 등 대치를 가장 높은 블리스를 보인 것을 알 수 있고요. 이렇게 가장 높은 블리스를 보인 패턴을 따면 저희 사정단은 추가적인 새로운 서브 워드로 등록해 주게 됩니다. 그렇게 해서 현재 이전 단계까지 등록되었던 태풍들을 가지고 가장 중대 수가 높은 태풍 땅에 해당이 극지에 해당하는 서버 드가 우리 사전이 14점이었고요. 이렇게 높은 동물들을 가지는 세포는 땅을 투자하는 이유는 곧 그러한 서브워드가 어떤 유의미한 의미를 가졌기 때문에 그렇게 높은 빈도수로 우리 학습 데이터 상에서 등장했다고 가정할 수 있기 때문이고요. 앞에서 예시로 든 그것이 어떤 특정 의미를 가지기 때문에 아마 학교 데이터 상에서도 뛰어난 과제에 등장했을 것이라고 생각할 수 있습니다. 바로 이러한 집단에서부터 이렇게 높은 디백스의 서브 지지를 중심으로 우리 사장을 구축하게 되는 것이 아트 소주의 핵심 아이디어가 되겠습니다. 파란색으로 표시된 t라는 단어 장까지 우리 사장을 추가했을 때 다음 단계로서 방금 식사한 그런 라는 태풍까지 고려해서 새로운 아는 사이드를 만들어 주고 그리고 그에 대한 빈백질이 우리 학습 데이터 하면서 새롭게 대상이 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(1강) Tokenization.json",
        "lecture_name": "(1강) Tokenization",
        "course": "NLP",
        "lecture_num": "1강",
        "lecture_title": "Tokenization",
        "chunk_idx": 2,
        "total_chunks": 5,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c05481d0c7ce9b49ee7c553faa4af8c3c23b95c8d1a699ebda02d63c2de0c07c"
      },
      "token_estimate": 1118,
      "char_count": 2018
    },
    {
      "id": "transcript_nlp_1강_tokenization_c003_18cbc8",
      "content": "[NLP] (1강) Tokenization\n\n다. 바로 이러한 집단에서부터 이렇게 높은 디백스의 서브 지지를 중심으로 우리 사장을 구축하게 되는 것이 아트 소주의 핵심 아이디어가 되겠습니다. 파란색으로 표시된 t라는 단어 장까지 우리 사장을 추가했을 때 다음 단계로서 방금 식사한 그런 라는 태풍까지 고려해서 새로운 아는 사이드를 만들어 주고 그리고 그에 대한 빈백질이 우리 학습 데이터 하면서 새롭게 대상이 됩니다. 유리한 그리고 반동산 피라는 패턴을 옆에 쌍으로 만들었을 때 이렇게 빨간색으로 표시된 것처럼 그 해당 빈도 수가 총 9번으로 가장 많은 분들을 보인 것을 알 수 있고요. 그러면 이러한 이 스피라는 것이 저의 사진상의 새로운 서브 에들이 추가되게 됩니다. 그러면 지금 만든 사전에 새롭게 추가된 단어들을 가지고 또 다시 담장 또는 폭행 짬 등을 고려하고 이라는 초록색 14전 패턴을 포함해서 이렇게 폭행의 점에 대한 분리 시키를 업데이트하고 그러면 그런 정보에서 가장 높은 빈도 블록체인 태풍의 땅은 현재는 에오이기 때문에 이번에는 이 에오라는 서버가 데이터 전산이 효과를 등록하게 됩니다. 그러면 이렇게 추가된 에이라는 단어를 결국은 저희가 고려하는 전액 상을 추가적으로 가늠해서 해당 중대시의 세입을 얻고 있다면 그다음에 추가할 아마 땅은 바로 이 7번에 관한 가장 높은 빈도수를 보여주는 15 이게 다가 될 것이고요. 그러면 이 태풍 땅에 관한 우리 서버에 대해 더욱 더 추가적으로 사전에 준비할 수밖에 없습니다. 그러면 이 과정은 저희가 사전에 미리 정해둔 스태블러 사이즈 하자만 독립할 수 있는 가면 최대 패킹 혹은 간의 플로를 개발할 때까지 반복적이고요. 이경우 저희가 허용한 최대 10 미러리 사이즈가 가령 15였다고 한다면 여기에서 ow까지 사전에 등록하게 되면 15개의 토큰들이 이미 사전에 등록되어 있기 때문에 이것으로 사전에 구축하는 정보가 끝나는 것입니다. 그러면 어떤 주어진 새로운 입력 테스트가 들어왔을 때 이를 포크나이제이션 하게 될 정도가 열차 하나의 입증에 힘 이렇게 구축된 서브드 레더를 사용할 경우 많은 캐릭터들도 사전에 등록되어 있고 그리고 그 단일 캐릭터들이 일부로 포함된 둘 다 긴 길이의 서브 드기에 우리 사전에 등록되어 있기 때문에 주어진 텍스트를 커스터마이제이션 할 수 있는 여러 가지 다양한 방법들이 존재할 수 있다는 것입니다. 그렇지만 저는 캐릭터 개스를 에서 긴 지이의 서브어드가 이 동사를 우리가 짧은 서브하드나 혹은 다른 캐릭터들을 단위로 인터뷰했을 때보다 더 유의미한 정보를 가진다고 볼 수 있을 텐데요. 과연 아세사나 식물은 투입할 것인을 얘기해서 타일에 나타나는 pie라는 부분으로 t 그리고 r 그리고 이 이렇게 많은 캐릭터를 나이도 현하 데이션을 표현할 수도 있겠지만 만약 저희 사정을 PR 이라는 서브워드가 등록이 되어 있는 경우 이 4개의 캐릭터를 묶어서 하나의 작품으로서 테크나이제이션을 하는 것이 해당 의무를 보다 더 잘 나타낼 수 있을 것입니다. 이러한 아이디어를 바탕으로 어떤 서브어드 러버의 사전이 이렇게 구축되어 있을 때 기업의 입력 테스트를 테크하데이션 하는 방식은 저희가 이 사전에 등록된 각각의 상품들을 캐릭터 그리기 기준으로 이렇게 나눠놨습니다. 바로 이렇게 비리가 일자리인 태풍들 일자리인 태풍들 그리고 그가 한자리인 태풍들이 이렇게 화전상에 재미 보고 있는 사람입니다. 그러면 바로 기업인 인력 패스트가 높은 미니스트라는 것으로 주어졌을 때 이 기업인 패스트를 대상으로 보통 7개 사를 이어 나가면서 바로 가장 처음으로 등장한 소리사가 팬이었지만 이온으로 시작하는 우리 403에 등록된 태권도이고 먼저 우리가 가장 긴 태권도 즉 우리가 함께 있는 태권도 중에서 20에서부터 시작된 이 앤이 더이 이것이 곧 우리가 함께하는 기업의 인력 상을 진단한 서브 에드가 될 텐데요. 이 오미더블리가 우리 사진상에 등록되어 있는가를 확인해 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(1강) Tokenization.json",
        "lecture_name": "(1강) Tokenization",
        "course": "NLP",
        "lecture_num": "1강",
        "lecture_title": "Tokenization",
        "chunk_idx": 3,
        "total_chunks": 5,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c05481d0c7ce9b49ee7c553faa4af8c3c23b95c8d1a699ebda02d63c2de0c07c"
      },
      "token_estimate": 1049,
      "char_count": 1923
    },
    {
      "id": "transcript_nlp_1강_tokenization_c004_e698e1",
      "content": "[NLP] (1강) Tokenization\n\n다. 그러면 바로 기업인 인력 패스트가 높은 미니스트라는 것으로 주어졌을 때 이 기업인 패스트를 대상으로 보통 7개 사를 이어 나가면서 바로 가장 처음으로 등장한 소리사가 팬이었지만 이온으로 시작하는 우리 403에 등록된 태권도이고 먼저 우리가 가장 긴 태권도 즉 우리가 함께 있는 태권도 중에서 20에서부터 시작된 이 앤이 더이 이것이 곧 우리가 함께하는 기업의 인력 상을 진단한 서브 에드가 될 텐데요. 이 오미더블리가 우리 사진상에 등록되어 있는가를 확인해 됩니다. 이 오미더블리는 우리 사전에 등록되어 있지 않기 때문에 길이 상처 서버드로 되어 있는 그리 인터링 아이템이 없게 되고요. 그러면 그가 일자리인 것 즉 11자지인 이 캐릭터 스타트가 우리 사전상이 동립되 있는가를 확인하게 됩니다. 그러면 이거 사정상은 그러면 이 경우 사전 동의 없이 등록되어 있지 않기 때문에 이제는 우리가 일자리인 패턴들을 대상으로 인터팅을 하면 결국 이 앤이라는 다른 캐릭터 바로 이 캐릭터를 사용해서 포크나이제이션을 꺼내 버립니다. 자 그러면 그 다음으로 등장하는 캐릭터가 1년인데 그러면 여기서도 이 사진 상에서 우리가 작동 중인 한 자리에 사모들이 생각하면 이이가 될 텐데요. 이이가 역시 우리 사장님이 없고요. 그리고 이가 2자리인 이블의 경우도 우리 사장님이 없기 때문에 여기 있는 2대 이 가미 커리터 길이까지 내려와서 이라는 아미 커리터로 테크라이즈 구성이 되고요. 그다음에 더 위에서는 역시 우리가 한 자리를 체크해 보면 wes가 될 텐데 역시 우리 혈변상은 우리가 한 뼈인 해당 토크는 등록되어 있지 않고요. 그는 길이가 2 뼈인 더블류 이익까지를 고려할 때도 이 사전에 꼭 등록되어 있지 않고 저는 결국 때 다른 캐릭터 까이 내려가서 이 일 한 캐릭터를 사전에는 이 폭탄으로 통해서 포즈를 하게 되고 그러면 그 다음에 등장하는 이의 제 연기와 길이가 가장 긴 us에 해당하는 이 서브 어드가 사전상에 등록되어 있는가를 확인해 보면 여기서는 이제 매핑이 되어서 해당 서버에서도 등록된 것으로 알 수 있습니다. 그래서 이 경우 ESG라는 우리가 가장 비 우리 사전 사이 높은 운동은 이 소행으로 인쇄용이 되는 것입니다. 그러면 그 결과 이어진 리스트라는 입력 텍스트에 대한 폭 모두 이상 결과는 1 그리고 2 그리고 거리 그리고 유스비라는 총 4개의 패턴들로 인코딩이 될 것입니다. 지금까지 저희는 아트 인터뷰 혹은 이를 줄여서 BP이라고 불리는 서버 블레버의 테크나이제이션 기법을 알아봤고요. 실제 이 피 알고리즘에 서는 디피나 기타 라즐 레니즘 모델들에서 가장 많이 사용되고 있는 알고리즘이 되겠고요. 그 외에도 다른 부위의 서브 어질러벨 테크놀리지이션 기법들도 현재합니다. 과연 그 한 의지로서 월드피스의 경우는 단순히 군대세에 반한 것이 아니라 어떤 조건부 항의 기반해서 해당 확률 값이 가장 큰 캐릭터나 서브워드 케어를 사전에 추가하는 방식이고요. 또한 월드 피스를 협상한 컨테피스의 경우 각 서브워드가 문장 내에서 띄어쓰기 동작 여부 혹은 어떤 단어의 사명과 공간을 동반했는지 요구한 정보를 구별해서 우리 사전상의 등록 해심으로써 사전에 구축하는 것이 됩니다. 마지막으로 그 단어를 요약해서 말씀드리면 딥러닝을 통한 자매 처리에 있어서 가장 첫 번째 단계인 기존 테스트 데이터를 막상 컵에 해당하는 단위를 쪼개 나가는 테크나이제이션의 동향이 펼쳐졌고요. 워드 레벨 그리고 페리컬러의 테크널리레이션 기법의 특징과 장단점을 알아봤습니다. 그리고 이 두 가지 방법의 장점들을 결합한 그래서 실제 최근에 라디 렌리즘 모델들에서 많은 사람들이 있는 사이 딜레버의 테크 알리이즘과 이에 대한 대체 알고리즘인 바이스트 패드 알고리즘을 공부해 보았습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "(1강) Tokenization.json",
        "lecture_name": "(1강) Tokenization",
        "course": "NLP",
        "lecture_num": "1강",
        "lecture_title": "Tokenization",
        "chunk_idx": 4,
        "total_chunks": 5,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:c05481d0c7ce9b49ee7c553faa4af8c3c23b95c8d1a699ebda02d63c2de0c07c"
      },
      "token_estimate": 1011,
      "char_count": 1855
    }
  ]
}