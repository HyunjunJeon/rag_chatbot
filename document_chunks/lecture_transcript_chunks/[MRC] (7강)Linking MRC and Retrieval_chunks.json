{
  "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
  "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
  "course": "MRC",
  "total_chunks": 9,
  "chunks": [
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c000_fcf6ce",
      "content": "[강의 녹취록] 과목: MRC | 강의: 7강 | 제목: Linking MRC and Retrieval\n\n네 안녕하세요. 7강 시작하도록 하겠습니다. 7강은 엠알씨와 리트리버를 연결하는 방법에 대해서 알아보도록 하겠습니다. 네 오늘 섹션은 크게 세 가지로 나눠져 있는데요. 먼저 오픈 도메인 퀘스천 앤서레잉에 대해서 조금 더 자세히 알아보고 두 번째 섹션에서는 오디qa를 접근하는 하나의 방법론인 리트이브와 리더를 연결하는 어프로치에 대해서 알아보도록 하겠습니다. 섹션 3에서는 이런 방법론에 대한 최근의 이슈들과 또 다른 어프로치들에 대해 조금 더 자세히 알아보도록 하겠습니다. 네 섹션 1위입니다. 오픈 도메인 퀘스천 엔서링은 결국에는 MRC와 조금 근본적인 차이점이 있는데요. 저희가 렉처 1 2 3에서 봤듯이 MRC는 사실 지문이 주어진 상황에서 질의응답을 하는 그런 문제입니다. 보시다시피 서울특별시에 대한 문서가 주어진다고 한다면 이 가정이 있었을 때 그리고 저희가 질문을 서울의 지디피는 세계 몇 위야라고 했었을 때 주어진 지문 내에서 해당 질문에 답을 찾는 문제인 거죠. 그래서 저희가 이 문제를 해결하기 위해서 렉처 1 2 3에서 버트를 활용해서 어떻게 이런 문제를 포뮬레이트 할 수 있을지를 알아보았습니다. 이에 반해서 MRC와 다르게 오픈 도메인 퀘스천 엔서링은 비슷한 형태지만 조금 달라지는 게 있는데요. 바로 저 서퍼링 에비던스 또는 지문 부분이 주어지는 것이 아니라 특정해서 주어지는 것이 아니라 좀 더 넓은 의미로 넓게 위키피디아 또는 웹 전체가 주어지게 됩니다. 이렇기 때문에 똑같은 형태로 질문을 한다고 하더라도 저희가 봐야 하는 문서의 크기는 하나가 아닌 위키피디아 같은 경우 예를 들면 영어 영어 위키피디아 같은 경우 약 300만 개의 문서가 있는데요. 이와 같이 문제 자체가 스케일이 커지게 됩니다. 하지만 다른 거는 똑같다고 볼 수 있어요. 풋 인풋 형태도 비슷하고 인풋도 마찬가지로 질문이고 아웃풋도 마찬가지로 간결한 답변이 되겠죠. 실제로 이런 오디 QA를 이용해서 서비스를 하고 있는 그런 회사들이 많은데요. 가장 대표적인 예는 구글이 될 것 같아요. 구글에서 검색을 하시면은 물론 원래 초기에는 구글에서 검색을 했었을 때 문서들만 나왔지만 이제 최신 모던 서치 엔진으로 넘어오게 되면서 구글이나 다른 회사도 포함을 해서 연관 문서뿐만 아니라 질문에 대한 답도 같이 제공을 하게 돼 있습니다. 물론 오디큐에이라는 그런 도메인 이런 문제는 사실 어제 오늘 또는 최근 몇 년에 관심을 가졌던 문제는 아니고요. 꽤 예전부터 저희가 저희 커뮤니티에서 사실은 다뤘던 문제이긴 합니다. 음 보시면 사실 99년도부터 적어도 이런 문제들이 포뮬레잇 됐다는 걸 아실 수가 있는데요. 당시에는 텍스트 리트리벌 컨퍼런스라는 그런 학회에서 QA 트랙 에서 많이 다뤘던 그런 문제거든요. 그래서 이 연관 문서만 반환하는 인포메이션 위트리벌하고 조금 차이점은 거기서 더 나아가서 숏텐서를 실제로 서포팅 에비던스와 문서를 말하는 거죠. 이 답을 담고 있는 문서를 같이 되돌려주는 형태로 이제 학회에 큐에 트랙이 진행이 됐었습니다. 그래서 크게 당시에는 세 가지 파트로 구성이 돼 있었는데 첫 번째로는 퀘스천 프로세싱이었고 두 번째로는 패시지 미추리벌이고 세 번째로는 앤서 프로세싱이죠. 사실 근본적으로는 최근의 어프로치랑 어떻게 보면 크게 다르지 않다고 볼 수도 있겠네요. 그래서 보시면 퀘스천 프로세싱은 결국은 어떻게 하면 우리가 질문을 잘 이해할 수 있을까에 대한 부분인데요. 물론 당시에는 딥러닝이나 그런 어드벤스 된 모델 방법론이 존재하지 않았기 때문에 질문으로부터 키워드를 선택해서 엔서 타입을 셀렉트하는 방식이 거의 유일했습니다. 예를 들면은 이 질문은 답변의 형태가 장소여야 한다 또는 장소 중에서도 나라여야 한다 이런 것들을 분류를 미리 해 준 것이죠. 그 당시에는 이런 것들을 뭐 키워드나 룰 베이스의 그런 방법론으로 정의를 하도록 했습니다. 두 번째는 패세지 미트리버인데요. 사실 이거는 현재 방법론과 아주 많이 다르지 않습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 0,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 1082,
      "char_count": 1993
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c001_407e52",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 예를 들면은 이 질문은 답변의 형태가 장소여야 한다 또는 장소 중에서도 나라여야 한다 이런 것들을 분류를 미리 해 준 것이죠. 그 당시에는 이런 것들을 뭐 키워드나 룰 베이스의 그런 방법론으로 정의를 하도록 했습니다. 두 번째는 패세지 미트리버인데요. 사실 이거는 현재 방법론과 아주 많이 다르지 않습니다. 기존의 아알 방법들을 활용해서 연관된 문서를 뽑고 그 문서에서 문서가 너무 기니까 문서를 패시지 단위로 자른 다음에 그다음에 이런저런 프로세싱을 했는데요. 예를 들면 네임드 엔티티나 패시지 내에 퀘스천 단어의 개수 등과 같은 이런 핸드 크래프트 피처를 활용을 했던 거죠. 하지만 크게 봤었을 때는 사실 상당히 비슷하다고 볼 수 있겠죠. 저희가 지금 현재도 tfidf나 bm25 같은 것을 많이 쓰고 있고 이런 것들은 사실 그 당시에도 존재했던 그런 그런 이제 메소드이기 때문에 어떻게 보면 좀 꽤 오랫동안 애용되고 있는 그런 방법론이라 볼 수 있겠습니다. 마지막 3번 엔서 프로세싱은 이런 피처들을 활용해서 그리고 또한 다른 휴스틱을 활용해서 클래스 바이어를 만드는데 결국에는 주어진 퀘스천에 어떤 패시지가 답이 될지를 클래스 바이어 하는 문제로 보는 거죠. 이 부분은 조금 요즘과 다르다고 볼 수 있겠는데요. 최근에는 MRC라 함은 단순하게 패시지를 선별하는 것뿐만이 아니라 패스 시대에 어떤 스팸이 답변이 되는지를 맞추는 걸로 좀 많이 진화한 걸 보면 좀 많이 다르다고 볼 수도 있겠고요. 당시에는 스팸 레벨에서 어떤 그런 답을 낼 수 있을 정도의 기술력이 없었기 때문에 이런 정도까지 가지 못했던 것이기도 하고 또한 저희가 뭐 기술이 발전하면서 더 그 패시지 전체를 읽는 것보다 부분적으로 읽는 것들에 대한 커스터머들의 니즈를 유저들의 니즈를 좀 더 파악을 했다고 볼 수도 있겠습니다. 이런 차별점이 존재하지만 말씀드리고 싶었던 부분은 오디큐에라는 도메인은 생각보다 오래된 도메인인 거죠. 99년도부터 사실 많은 관심을 가졌던 그런 분야니까요. 2011년에는 조금 많이 많은 분들이 아시는 그런 상당히 큰 뉴스가 있었는데요. 아이비엠에서 만들었던 왓슨이라는 AI가 당시에는 딥큐에이 프로젝트라 불렸죠. 제어파드라는 그런 티브 퀴즈쇼에서 우승을 했었습니다. 사실 티브 퀴즈쇼라고 함은 사람들이 참가하는 티비 퀴즈쇼는 아니었고 따로 이런 AI랑 대결하는 그런 퀴즈쇼를 만들었던 건데 물론 원래 제오파디라는 퀴즈쇼는 사람들끼리 컴피 하는 거긴 합니다. 이제 거기서 이제 이런 이런 AI 엔진이 참여할 수 있도록 따로 좀 그런 기회를 만들었던 것이죠. 그래서 당시에 제어팔디의 챔피언들과 같이 붙어서 보시다시피 왓슨이라는 AI가 우승을 한 걸 알 수가 있는데 사실 2011년도에 꽤 오랜 시간이 지나서 이런 프로젝트가 진행이 됐던 건 맞지만 실제로 그 파이프라인을 들여다보면은 방금 보여드렸던 이제 트랙 큐에이나 좀 더 예전에 했던 방법론과 아주 많이 다르지 않습니다. 당시에도 아직까지 딥러닝이 활용되기 전이었고 여러 가지 피처들과 이 피처들을 위해 쌓아 올린 SPM 같은 초기 머신 러닝 모델들을 활용을 해서 답변들을 추출할 수 있었다는 것을 저희가 지금은 알 수가 있는데요. 그 후로는 정말 많은 발전들이 있었죠. 사실 특히나 2012년 2013년에 딥러닝의 발전으로 인해서 컴퓨터 비전 쪽이 완전히 완전히 패러다임이 바뀌게 되고 2014년 15년도를 통해서 사실 엔엘피 쪽도 마찬가지로 완전한 패러다임의 시프트가 일어납니다. 그에 힘입어 와 가지고 힘입어서 2016년 7년부터는 정말 많은 종류의 퀘스천 앤서링 그리고 오픈 도메인 퀘스천 앤서링 모델들이 나오기 시작했는데요. 보시다시피 특히 2017년을 기점으로 삼는다고 한다면은 저희가 3년 사이에 많은 모델들이 나왔고 또 성능 향상도 아주 높은 거를 알 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 1,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 1026,
      "char_count": 1911
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c002_b2e6de",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 그에 힘입어 와 가지고 힘입어서 2016년 7년부터는 정말 많은 종류의 퀘스천 앤서링 그리고 오픈 도메인 퀘스천 앤서링 모델들이 나오기 시작했는데요. 보시다시피 특히 2017년을 기점으로 삼는다고 한다면은 저희가 3년 사이에 많은 모델들이 나왔고 또 성능 향상도 아주 높은 거를 알 수가 있습니다. 특히나 여기서 보시다시피 스쿼드 같은 경우도 맨 왼쪽에 있는 스쿼드라는 데이터셋 같은 경우도 28.4점에 시작해 가지고 53점까지 올라가는 걸 볼 수가 있고 웹 큐라는 맨 오른쪽의 데이터 셋만 해도 뭐 19점 17점 이 정도의 점수에서 시작해서 보시면 나중에 45점 이렇게까지 올라가는 걸 보실 수가 있습니다. 그래서 보시다시피 많은 발전이 있었고 이런 발전들을 레버러지를 하는 것이 여러분의 파이널 프로젝트에 사실 가장 중요한 목표입니다. 그러면 이런 여러 가지 방법론이 있지만은 오늘은 좀 가장 일반적인 방법론을 알아보려고 해요. 이번 렉처에서는 가장 일반적인 방법론을 알아보려고 하고 저희가 렉처 나인과 텐에서는 조금 다른 비주류긴 하지만 상당히 상당히 들여다볼 이유가 있을 수 있는 그런 방법론을 보도록 하겠습니다. 그래서 섹션 2는 가장 일반적인 방법론이라 불릴 수 있는 리트리버 rde 어프로치입니다. odqa에서 가장 많이 쓰이는 어프로치죠. 사실 저희가 지금까지 배웠던 것과 아주 관련성이 깊습니다. 왜냐하면 사실 위트리버 위드 어프로치는 아주 간단하게 말씀을 드리면 렉처 1 2 3에서 다뤘던 위더와 그리고 렉처 4 5 6에서 다뤘던 위트리버를 활용을 해서 두 개를 간단하게 연결시키면 만들 수 있는 이제 그런 모델이기 때문인데요. 보시다시피 먼저 리티버를 통해서 문서를 찾고 그리고 찾은 문서를 통해서 MRC 엔진을 이용하여 리딩 컴프레션 모델을 이용을 하여 최종 답안을 내는 것이 해당 방법의 접근 방식입니다. 실제로 어떻게 보면은 좀 위캡적인 측면이 있을 텐데요. 모두 다 배우신 내용들이니까요. 위트리버는 입력 값이 문서 셋과 질문이 되겠고요. 출력 값은 관련성이 높은 문서가 되겠습니다. 반면에 리더 같은 경우는 입력 값은 리트리버가 리트리버 한 문서들 또는 문서 하나가 되겠고요. 그리고 또 질문이 똑같이 들어오겠죠. 같은 질문이 들어오게 되고 출력 값은 최종 답안 보통은 프레이즈로 이루어져 있는 최종 답안이 되겠습니다. 그리고 학습 단계도 저희가 잘 알고 있죠 we트리ver 같은 경우는 이제 tfidf나 bm 2a 5 같은 경우는 일반적인 학습 그 레이블 된 데이터에 통한 학습 학습은 없고 좀 셀프 슈퍼바이즈 형태로 학습을 하게 되죠. 댄스 같은 경우는 실제로 큐에 시스템을 큐에 데이터셋을 활용을 해서 학습을 하도록 하도록 하고요. 이제 리더 같은 경우는 이 스쿼드 같은 엠알씨 데이터셋으로 학습을 하게 되고 그리고 저희가 사실 학습을 할 때 특히나 좀 더 양을 데이터를 좀 추가하고 싶다면 디스턴 슈퍼비전을 활용을 할 수가 있습니다. 그래서 여기서 디스턴스 슈퍼비전은 무엇이냐 사실 보통 일반적으로 저희가 MRC 데이터셋은 이제 질문과 답변이 딱 주어지는 건데 근데 이런 경우는 데이터 셋이 해당 지문이 주어지는 경우이지만 이제 보통 더 일반적으로 질문이랑 답은 있지만 그 답이 어느 질문에 있는지는 알지 못할 때가 있겠죠. 조금 더 일반적인 질문들이라고 볼 수가 있겠죠. 그래서 이제 이런 데이터셋들이 여기 보여드린 것처럼 큐에이 트랙이나 웹 퀘션스나 위키 무비지 같은 경우인데요. 이런 경우는 사실 저희가 답변이 어디에 존재하는지가 주어지지 않기 때문에 저희가 직접 찾아야 합니다. 그게 디스텐트 슈퍼비전의 의미인데요. 답변을 보고 이제 찾는 거죠. 그래서 위키피디아에서 먼저 질문을 던져서 리트리버를 이용해서 관련성 높은 문서를 먼저 검색을 합니다. 그다음에 너무 짧거나 긴 문서나 뭐 질문에 고유 명사를 포함하지 않는 등 부적합한 문서는 제거를 하고요. 그다음에 앤서가 이작 매치로 들어 있지 않은 문서도 제거를 합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 2,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 1062,
      "char_count": 1983
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c003_030fa5",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 그게 디스텐트 슈퍼비전의 의미인데요. 답변을 보고 이제 찾는 거죠. 그래서 위키피디아에서 먼저 질문을 던져서 리트리버를 이용해서 관련성 높은 문서를 먼저 검색을 합니다. 그다음에 너무 짧거나 긴 문서나 뭐 질문에 고유 명사를 포함하지 않는 등 부적합한 문서는 제거를 하고요. 그다음에 앤서가 이작 매치로 들어 있지 않은 문서도 제거를 합니다. 남은 문서 중에서 질문과 연관성이 가장 높은 단락을 서포팅 에비던스로 사용을 하게 되죠. 물론 실제로 이 답변이 맥락상 질문의 답변을 줄 수 있는 지문이 아닐 수도 있어요. 하지만 많은 경우는 이런 룰을 통해서 필터하고 남은 것들은 아마도 그 해당 질문에 답변을 주는 맥락이겠죠. 이제 그런 가정을 가지고 디스턴 슈퍼비전을 진행을 합니다. 이제 예를 들면 이런 디스턴 슈퍼비전을 활용을 해서 실제로 데이터셋에 많이 쓰이고 있고 즉 이런 슈퍼비전을 활용을 하면 조금 더 퀘스천이랑 앤서 패세시 페어를 많이 만들거나 또는 없는 경우에도 퀘스천이랑 앤서 페어만 있는 경우에도 위키피디아에서 관련된 패시지를 가져와서 만들 수가 있습니다. 마지막으로 인퍼런스인데요. 인퍼런스 같은 경우는 리트리버가 질문과 가장 관련성이 높은 5개의 문서를 출력하고요. 그다음에 wider는 이 5개의 문서라는 건 물론 숫자를 늘릴 수도 있고 줄일 수도 있겠죠. 경우에 따라서 wd는 이 5개의 문서를 읽고 답변을 예측을 하고요. 그다음에 예측한 답변 중에서 가장 스코어가 높은 것을 최종 답으로 사용을 합니다. 이제 이런 방법론에 대해서 몇 가지 이슈와 또 몇 가지 더 개선하기 위한 최근 추가적인 방법론들에 대해 좀 말씀을 드려보겠습니다. 일단은 먼저 중요한 이슈 중 하나는 사실 저희가 패시지 지문 즉 MRC에 건네주는 그 단위를 항상 지문이나 패시지로 지칭을 했지만 이 패세지라는 단위가 사실 위키 PDR 상에서는 엄밀하게 정의가 안 돼 있다는 건데요. 여러 가지 방법이 있겠죠. 예를 들면 저희가 그 패시지 단위를 좀 크게 보면 문서 단위로 볼 수도 있을 거고요. aricrs 조금 더 작게 보면은 페어 그래프로 볼 수도 있고요. 또는 더 작게 보면 센텐스로 볼 수 있겠는데요. 이렇게 했을 때 당연히 위키피디아 같은 경우도 개수가 좀 많이 다르죠. 알티클 기준으로 했을 때는 위키피디아에 대략 500만 개 정도의 문서가 존재를 하고요. 페어 그래프 단위로 쟀을 때는 3천만 개 정도 그리고 문장 단위로 쟀을 때는 대략 7600만 개 정도 존재를 합니다. 그래서 아래 보시다시피 문서 전체는 이렇게 잣나무라는 문서 전체를 얘기하는 것이고 센텐스랑 패래퍼는 각각의 단락 문장과 단락을 명시한다고 보시면 됩니다. 그리고 이런 그래뉴얼러리티에 따라서 리튜브 단계에서 몇 개의 문서 즉 몇 개의 패시지를 넘길지를 정해야 하겠죠 당연히 케이가 달라질 수밖에 없고요. 알티클인 경우는 5개 정도면 충분할 수 있겠지만 페어 그래프면 조금 더 많이 필요하겠죠 예를 들면 29 그리고 센텐스 같은 경우는 좀 더 많이 78 이렇게 비율대로 갈 수가 있겠습니다. 물론 꼭 비율대로 갈 필요는 없겠지만 일반적으로 그렇게 많이 하는 것 같고요. 어 그래서 여기에 모델 같은 경우도 보시면은 k가 rticl인 경우는 5 8g 인 경우는 2 9 ST스인 경우는 dh인 걸 볼 수가 있고 성능에 좀 차이가 있긴 하지만 그래도 이 정도의 수준에서 어느 정도 뭐 특히 알 기준으로는 나름 비슷한 걸 보실 수 있습니다. 물론 케이를 늘리면 늘릴수록 성능이 올라가는 경우도 있고요. 항상 그렇진 않습니다. 그래서 이런 것들이 사실 여러분이 챌린지를 하실 때 잘 튜닝을 해야 될 문제일 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 3,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 969,
      "char_count": 1828
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c004_6414ff",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 물론 꼭 비율대로 갈 필요는 없겠지만 일반적으로 그렇게 많이 하는 것 같고요. 어 그래서 여기에 모델 같은 경우도 보시면은 k가 rticl인 경우는 5 8g 인 경우는 2 9 ST스인 경우는 dh인 걸 볼 수가 있고 성능에 좀 차이가 있긴 하지만 그래도 이 정도의 수준에서 어느 정도 뭐 특히 알 기준으로는 나름 비슷한 걸 보실 수 있습니다. 물론 케이를 늘리면 늘릴수록 성능이 올라가는 경우도 있고요. 항상 그렇진 않습니다. 그래서 이런 것들이 사실 여러분이 챌린지를 하실 때 잘 튜닝을 해야 될 문제일 수 있습니다. 또한 저희가 학습을 할 때 하나의 패시지만 볼 것인지 아니면 여러 개의 패시지를 볼 것인지도 결정을 해야 할 텐데요. 현재 저희가 k개의 패시지들을 리더가 각각 확인하고 특정 엔서스펜에 대해서 예측 점수를 나타내는데요 그리고 이 중 가장 높은 점수를 가진 엔서스펜을 고르도록 하게 돼 있죠. 그런데 이런 경우에는 저희가 각 위출된 패시지에 대한 직접적인 비교라고 볼 수가 없고 사실 어떻게 보면은 비교 자체를 하지 못한다고 볼 수가 있겠죠 간접적인 비교라고 볼 수가 있는 것이고 그래서 사실 저희가 전체를 한 번에 보는 것이 필요할 수 있습니다. 그래서 사실 그런 맥락에서 멀티 패시지를 활용을 해서 학습에 활용하는 경우가 많은데 이제 이 경우는 리튜브 된 패셋을 전체를 하나의 패셋으로 취급하고 리더 모델이 그 안에서 엔서스펜 하나를 찾도록 하는 거죠. 물론 이런 경우는 또 단점이 문서가 너무 길어지므로 지피유에 더 많은 메모리를 할당해야 하고요. 처리해야 하는 연산량도 많아지기 때문에 이것도 마찬가지로 모델을 만드시고 튜닝 하실 때 고민을 많이 하셔야 할 그런 에스펙트일 것 같습니다. 또 마지막으로는 저희가 오늘 나눌 얘기 중 마지막으로는 리트리브 된 모델에서 추출된 탑케 패세시들의 점수가 있겠죠 리트리버가 점수를 매겨서 탑케를 랭킹하는 것이니 그 점수가 있을 텐데 그냥 그대로 넘겨주게 되면은 이 페어 gf가 리트리버 입장에서는 조금 차이가 있었다 하더라도 어느 게 더 우선이 됐다 하더라도 리더 입장에서는 그거에 대한 정보를 전혀 없이 최종 답안을 내게 됩니다. 그래서 경우에 따라서는 이 탑 케 패세시들의 스코어를 리더 스코어에 넣어 줌으로써 최종 엔서를 고를 때 최종 엔서를 특히 랭킹을 할 때 패시지 스코어 패세지 위튜벌 스코어까지 같이 합했을 때 더 성능이 잘 나오는 경우도 최근에 많이 연구가 됐었고요. 그래서 이것도 마찬가지로 최종 프로젝트를 하실 때 고려하시면 좋을 것 같습니다. 오늘 저희가 실습에서 해볼 내용은 1강부터 3강까지 만들었던 MRC 모델과 4강부터 6강까지 만들었던 리트리버 모델을 연결해 보는 그런 실습을 할 텐데요. 오늘은 저희가 스파스 리트리벌 모델을 활용을 해서 MRC 모델에 연결하는 실습을 할 거고요. 다음 렉처에서는 저희가 5강 때 다뤘던 댄스 리트리벌 모델을 활용을 해서 연결하는 그런 실습도 해보도록 하겠습니다. 아마 오늘 하는 내용들은 많은 분들이 실제로 이미 벌써 하셨던 그런 것들이라 좀 더 위협적인 측면이 클 테니 좀 편하게 따라오실 수 있을 것 같아요. 자 항상 여느 때와 같이 원 타임에서 먼저 GPU로 설정을 해 주시도록 하고요. 자 먼저 필요한 패키지들을 인스톨 하도록 할게요. 저희 카페 페이스 할 거고요. 보시다시피 데이터 셋과 트랜스포머스 그리고 티큐디엠을 저희가 설치를 하는데 이거를 저희가 그냥 사일런스로 설치하도록 하겠습니다. 그럼 아무런 아웃풋이 나오지 않고 설치가 완료가 됩니다. 그래서 뭐 길게 나오는 거 싫어하시면 이렇게 하시면 될 것 같고요. 네 설치 완료됐죠. 그다음에 저희가 몇 가지 필요한 패키지들 라이브러리들을 임포트할 텐데 아마 다 익 익숙하실 거예요. 랜덤이나 넘파이 피피 프린트는 저희가 프린트를 예쁘게 하기 위해서 하는 유틸리티이고요. 밑에 보시면 데이터셋에서 데이터셋과 매트릭을 불러오기 위한 펑션들 그리고 사이킬 런에서 tfidf를 수행하기 위해서 tfidf 백터라이저를 인포트 해오도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 4,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 1084,
      "char_count": 2020
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c005_4e8a91",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 그럼 아무런 아웃풋이 나오지 않고 설치가 완료가 됩니다. 그래서 뭐 길게 나오는 거 싫어하시면 이렇게 하시면 될 것 같고요. 네 설치 완료됐죠. 그다음에 저희가 몇 가지 필요한 패키지들 라이브러리들을 임포트할 텐데 아마 다 익 익숙하실 거예요. 랜덤이나 넘파이 피피 프린트는 저희가 프린트를 예쁘게 하기 위해서 하는 유틸리티이고요. 밑에 보시면 데이터셋에서 데이터셋과 매트릭을 불러오기 위한 펑션들 그리고 사이킬 런에서 tfidf를 수행하기 위해서 tfidf 백터라이저를 인포트 해오도록 하겠습니다. 네 인포트 완료됐고요. 자 그리고 저희가 많이 쓰는 데이터셋 퀄 코드를 또 한번 불러오도록 할게요. 네 다운로드 완료가 됐고요. 그리고 저희가 4강에서 다뤘던 스파스 리트리버를 다시 불러오도록 할게요. 물론 다시 저희가 학습을 해야 되지만 제가 하나하나 하나 디테일하게 넘어가는 거는 여기서 하지 않고 혹시 필요하시면 다시 4강을 보시는 걸 추천해 드릴게요. 네 이렇게 하셔가지고 뭐 간단하게 설명만 좀 드리면 보시다시피 콜퍼스를 먼저 데이터 셋에서 트레인 데이터에서 컨텍스트를 가져와서 셋업을 하고 그다음에 실제로 저희가 이거를 이번에 밸리데이션 데이터와 좀 합쳐가지고 토크나이저를 통해서 토크나이즈를 한 다음에 여기 정리를 해주죠. 토크나이저를 토크나이저 펑션을 정리를 해준 다음에 그리고 엔그램 같은 경우는 1과 바이그램까지만 쓰는 거로 유니그램과 바이그램만 쓰는 걸로 해서 매트릭스를 저희가 트레인 한 다음에 핏 트랜스폼 한 다음에 얻어줍니다. 그러면 이 에피 메트릭스는 이 콜퍼스에 대한 모든 티파이더의 벡터를 담고 있는 그런 매트릭스가 되는 것이죠. 네 완료가 됐고요. 자 다음 그다음에 저희가 백토라이저에서 해당 쿼리에 관련된 문서를 가져오는 그런 펑션을 정의하도록 할게요. 보시다시피 이 펑션에서는 인풋으로 벡터 라이저와 그리고 쿼리가 들어오고요. 케는 탑케이 몇 개의 문서를 가져올지고 디폴트는 1로 돼 있습니다. 그래서 이제 보시면은 쿼리 벡터는 쿼리를 벡터 라이즈 넣어줘서 벡터로 바꿔주고요. 이 쿼리 벡터를 활용을 해서 먼저 이 쿼리 벡터가 서메이션이 0이 아닌 거 확인을 해요. 0인 경우에는 저희가 서치를 할 수 없기 때문에 어설션 에러가 발생을 하도록 해 주고요. 즉 이 경우는 저희가 쿼리 벡터 중에서 어떠한 단어도 저희가 학습할 때 썼던 보캡에 존재하지 않는 경우인 거죠. 이제 사실상 답이 안 나오는 경우라고 보시면 될 것 같아요. 이런 경우는 저희가 질문을 던졌지만 질문에 대한 답이 존재하지 않는 거죠. 문서에 문서 셋에 그다음에 이너 프로덕트를 그 저희 쿼리 벡터와 그리고 스퍼스 매트릭스를 곱해서 구해주게 됩니다. 그래서 곱해준 다음에 리절트는 결국은 이너 프로덕트 스코어들의 모든 문서에 대해서 계산한 값이고요. 그다음에 이 값을 솔팅을 해줘가지고 가장 높은 값 먼저 올 수 있도록 마이너스를 앞에 해줘서 알그 솔트를 하게 되고요. 알그 솔트는 당연히 인덱스가 나오기 때문에 이 인덱스를 다시 위저드 데이터에 인덱싱 해줌으로써 결국 그 순서대로 데이터를 다시 olgen이즈 하게 해주고 마지막으로는 이거에 해당되는 d 아이디들을 가져오게 되는 거죠. 그래서 d 스코어들과 d 아이디들을 탑 케만큼만 돌려주도록 돼 있습니다. 디폴트가 1이기 때문에 이런 경우 케이가 바뀌지 않는 한 하나만 나오게 되겠죠. 물론 리스트의 형태로 나오기 때문에 프로세싱 할 땐 조심하셔야 됩니다. 네 펑션을 정리를 해줬고요. 자 한번 저희가 한번 좀 테스트를 해볼게요. 테스트를 해볼 텐데 저희가 일단 원하는 질문을 한번 넣어보도록 하겠습니다. 예를 들면 예를 들면 제가 쿼리를 미국의 대통령은 누구인가 이렇게 질문을 던져볼게요. 그리고 리튜브를 할 텐데 겟 엘러벤 닥이라는 펑션을 활용을 해서 액터라이저를 활용을 하고 그렇죠 그리고 쿼리도 넣고 케이는 하나만 가져와 볼게요. 그다음에 저희가 이거에 대한 프린트 그 답변을 프린트 해 볼 텐데 이거는 좀 편리한 형태로 제가 카페 페이스 하도록 하겠습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 5,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 1094,
      "char_count": 2017
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c006_f302b2",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 네 펑션을 정리를 해줬고요. 자 한번 저희가 한번 좀 테스트를 해볼게요. 테스트를 해볼 텐데 저희가 일단 원하는 질문을 한번 넣어보도록 하겠습니다. 예를 들면 예를 들면 제가 쿼리를 미국의 대통령은 누구인가 이렇게 질문을 던져볼게요. 그리고 리튜브를 할 텐데 겟 엘러벤 닥이라는 펑션을 활용을 해서 액터라이저를 활용을 하고 그렇죠 그리고 쿼리도 넣고 케이는 하나만 가져와 볼게요. 그다음에 저희가 이거에 대한 프린트 그 답변을 프린트 해 볼 텐데 이거는 좀 편리한 형태로 제가 카페 페이스 하도록 하겠습니다. 자 그러면 이 답변에 대한 문서는 보시다시피 영국과 미국의 관계는 대략 400년 400년 정도 소급된다. 1607년 영국은 제임스 타운이라고 명명된 북미 대륙 최초의 상주 식민지를 세우기 시작하였고 이게 쭉 가서 나중에 대통령에 대한 얘기가 나오는지 한번 보죠. 미국의 대통령 나오죠 근데 실제로 대통령이 누구인지에 대한 얘기는 나오지 않죠 물론 클린턴이 대통령이긴 하지만 보시다시피 어느 정도 관련이 있는 문서를 주긴 하지만 답변이 있는지 없는지는 사실은 항상 보장되지는 않겠죠. 그래서 tk가 k가 1이 아니라 k가 1보다 높은 높이 설정을 해야 되는 경우가 있는 거고요. 저희가 리트리버가 작동을 그래도 하는 것을 확인을 했으니 이제 MRC 모델을 불러오도록 할게요. 사실 MRC 모델은 저희가 지금처럼 학습을 하기엔 너무 오래 걸려서 실제로 이미 학습된 모델을 가져오도록 하겠습니다. 콜 쿼드에 학습된 모델이 온라인상에 특히 허깅페이스 트랜스포머 라이브러리 베이스 그쪽 데이터베이스에 있는 것들이 있어 가지고 그중 하나를 가져오도록 할게요. 일단 먼저 저희 인포트를 하도록 할 텐데 여러 가지 필요한 것들 이거는 여러 번 보셔가지고 이제 익숙하실 것 같아요. 토치를 임포트하고 트랜스포머즈에서 오토 컴피그랑 오토 모델 폴 커션 앤스링 그리고 오토토크나이저를 인포트하도록 하겠습니다. 그다음에 저희가 가져올 모델 이름은 버트 베이스 멀티링구얼로 콜코드에 학습한 모델입니다. 상림 리라는 아이디를 가진 분께서 이거를 업로드 해 주셨고요. 다음으로 저희가 MRC 모델을 정의할 텐데 이건 똑같은 방법으로 하시면 돼요. 예전에 저희가 렉처 2에서 봤던 것처럼 이렇게 오트 모델 포 쿼션 앤스링 프롬 프리트레인 해서 모델 이름을 여기 적어주시고 그다음에 프리 트레인 모델인데 마찬가지로 모델 이름을 적어 주시고 토크 와이저랑 이제 쿼션 앤스링 모델을 각각 불러오는 거죠. 패스트는 투 해주셔도 되고 안 해주셔도 됩니다. 그리고 엠알스 모델 같은 경우는 결국엔 이밸류에이션만 할 거기 때문에 저희가 이발을 해주고요. 학습은 다 돼 있는 거니까요. 그래서 이렇게 정리를 해주면 다운을 받겠죠 좀 시간이 걸릴 거고요. 다운 받는데 그 다음으로 한 700메가 정도 되는 걸 보실 수가 있는데 이게 버트 베이스지만 키 크기가 큰 이유 중 하나는 결국 멀티링고이라는 버트를 쓰기 때문에 단어가 워낙 이제 멀티링고하게 좀 있어서 용량이 좀 클 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 6,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 821,
      "char_count": 1528
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c007_ef1d5e",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 그리고 엠알스 모델 같은 경우는 결국엔 이밸류에이션만 할 거기 때문에 저희가 이발을 해주고요. 학습은 다 돼 있는 거니까요. 그래서 이렇게 정리를 해주면 다운을 받겠죠 좀 시간이 걸릴 거고요. 다운 받는데 그 다음으로 한 700메가 정도 되는 걸 보실 수가 있는데 이게 버트 베이스지만 키 크기가 큰 이유 중 하나는 결국 멀티링고이라는 버트를 쓰기 때문에 단어가 워낙 이제 멀티링고하게 좀 있어서 용량이 좀 클 수 있습니다. 네 다운로드 완료가 됐고요. 자 이제는 저희가 펑션 하나를 디파인 할 텐데 이 모델이랑 토크나이저랑 그리고 컨텍스트 퀘스션까지 다 인풋으로 받아서 저희가 답을 내는 펑션을 정의하도록 할게요. 이것도 보시면은 컨텍스트 퀘스천 모델 토크나이저 이렇게 가져오고요. 먼저 저희가 인코더 딕트라는 거를 정의할 텐데 이게 뭐냐면은 토크나이저로 퀘스천이랑 컨텍스트를 전부 다 인코드해서 버트에 넣어주는 방식이에요. 이거는 저희가 4강 때 봤던 것처럼 일반적인 MRC 모델은 퀘스천이랑 컨텍스트를 컨캣 한 다음에 인풋으로 넣어줘서 컨텍스트 쪽에서 답변을 구하는 방식을 취하죠. 그다음에 저희 그리고 저희가 뭐 여러 가지 패딩 관련된 부분들을 해주게 되는데 보시면은 인코딕트에서 인풋 아이디들을 가져와서 패딩을 넌 패딩 아이디들을 저희가 정의해 주게 되고요. 실제로 저희가 풀 텍스트 같은 경우는 이 넌 패드 아이디들을 디코드 했었을 때 토크 아이저로 나오는 거겠죠 즉 그런 패딩이나 또는 CLS 토큰 같은 걸 없애주는 거고요. 그다음에 인풋을 저희가 보시다시피 원래 저희가 잘 아는 것처럼 인풋 아이디랑 어텐션 마스크랑 토큰 타입 아이디를 이렇게 정의를 해 줍니다. 그리고 이 아이 인풋들을 모델에 넣어주게 되고요. 아웃 아웃풋이 나오면은 이 아웃풋은 어쨌든 어떤 스타트랑 엔드에 대한 로직 값들이겠죠 이 로직 값들을 엑스를 통해서 가장 높은 값들을 구해주게 되고 이제 알그맥스입니다. 이렇게 액시스를 일로 해주면 알그맥스가 같이 나오게 되고 어 실제로 알그맥스 나온다는 게 뒤에 이렇게 인다이스드 아이템까지 같이 가기 때문에 이제 가능한 거고요. 알그맥스가 수정 정정하도록 할게요. 그래서 저희가 결국에는 스타트랑 엔드 로짓을 통해서 실제 맥스 값들을 스타트 엔드 값을 갖게 되고요. 그다음에 여기서 답변을 이 스타트 엔드를 통해서 실제 인풋 토큰에서 스타트랑 엔드 쪽만 스팸을 가져와서 디코드를 해서 최종 답을 가져오게 됩니다. 이거를 이제 정리를 하시고요. 자 이제 한번 해봅시다. 자 저희가 이제 모든 필요한 펑션은 정해됐으니 먼저 하나의 컨텍스트를 저희가 정해볼게요. MRC를 한번 테스트해 보는 건데요. 이렇게 하나 한번 컨텍스트를 먼저 간단하게 저희가 정의를 해볼게요. 이렇게 보시면 컨텍스트는 아 네 타이퍼네요. 컨텍스트는 이렇게 나오는 걸 볼 수가 있고 위에 펑션을 이용을 해서 저희가 엔서를 한번 가져와 보도록 할게요. 그리고 한번 답을 한번 내보내도록 해보겠습니다. 자 보시면 조지 w 부시가 나오는 걸 볼 수가 있죠. 왜 그럴까요? 보시다시피 여기서 이 문서에서 쭉 읽다 보시면은 오늘날 영국과 미국은 가까운 계속 가다가 보시면 가령 토니 블레어와 빌 클린턴 및 이후 조지 w 부시 대통령 간의 관계에서 조지 w 부시가 대통령인 걸 알게 되는 거죠. 물론 이 질문에 사실 가장 큰 문제점은 미국의 대통령이 누구냐고 물어봤을 때 이게 의미가 현재의 대통령이 누군지 아니면 과거의 특정 시점에 대통령을 얘기하는 건지 사실 그게 불명확한 부분이 있어요. 사실 그런 이유가 있기 때문에 조지 w 부시가 틀린 답변은 아니지만 유일한 답변은 또 아니겠죠 어쨌든 저희 MRC 모델이 잘 답을 찾는 걸 알 수가 있고요. 실제로 문서 내에 답변이 있었죠 자 그럼 이제 저희가 MRC 모델도 잘 되고 tfi 모델로 잘 되니 이 두 개를 연결시켜서 오디큐에를 해볼 텐데요. 오디큐에이를 하는 거는 사실 그대로 연결만 하면 되겠죠. 저희가 이제 결국 그걸 할 수 있는 펑션을 정의를 할 텐데 이거는 정말 간단한 펑션입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 7,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 1089,
      "char_count": 2022
    },
    {
      "id": "transcript_mrc_mrc_7강linking_mrc_and_retrieva_c008_040e0d",
      "content": "[MRC] [MRC] (7강)Linking MRC and Retrieval\n\n다. 좀 프린팅하는 그런 것들이 좀 있긴 하지만 보시다시피 쿼리랑 콜퍼스랑 벡터라이저와 모델 토크 와이저 그리고 k를 가져오게 되고요. 먼저 저희가 맨 위에 했던 겟 엘레반 닥을 이용을 해서 문서를 가져오게 되고 이 문서를 실제로 문서와 아이디를 찾은 다음에 문서 아이디를 통해서 콜퍼스에서 해당 문서를 뽑습니다. 컨텍스트를 가져온 다음에 저희가 탑 1이기 때문에 하나만 가져 온다는 그 보장이 있고요. 그다음에 이 문서를 똑같은 방법으로 저희가 겟 앤서프롬 컨텍스트라는 펑션에 넣어서 답을 가져오게 되고 밑에는 그냥 프린팅을 예쁘게 해주는 거죠. 자 이걸 정리를 하고요. 자 이제 한번 넣어볼까요? 이 펑션이 정리된 다음부터는 저희가 예를 들면 이렇게 만들 수도 있습니다. 그 엔터 애니 퀘스천 형태로 저희가 이제 데모를 만들어 보는 거죠. 일종의 네 그러면 보시다시피 그리고 이제 이거를 오픈 도메인 QA라는 펑션을 이용을 해서 허리와 콜퍼스 벡터 라이저 MRC 모델 그리고 마지막으로 토크나이저와 k 이렇게 해주면 네 간단한 데모가 완성이 된 거죠. 그럼 저희가 뭐 똑같은 질문을 돌려볼게요 하면은 답변이 나오게 되고요. 나오죠 이거를 좀 더 재미있게 해볼 수 있는 방법은 이거를 그대로 가져와 가지고 와일루프에 넣어주시면 또는 와일 쿼리를 이렇게 정의를 해 주고 와일 쿼리 그만이라는 키워드가 나올 때까지 계속 돌려주는 거죠. 이거를 해볼게요. 한번 자 이렇게 되면은 저희가 똑같이 질문을 미국의 대통령은 누구인가 질문을 던질 수가 있고 자 답변이 나왔어요. 질문 답변은 조지 w 부시 그럼 예를 들면 미국의 대통령은 뭐 또 다른 질문을 해볼까요? 미국의 대통령은 언제 태어났어? 물론 올바른 답이 안 나올 수도 있습니다. 왜냐하면 저희가 지금 다루고 있는 문서의 크기가 콜코드라는 제한된 셋이기 때문에 실제로 여러분이 챌린지를 하실 때는 전체 콜 포스를 보셔야 되고 이거는 콜 코드만 지금 보고 있는 거죠. 문서 몇십 개 또는 몇백 개 정도만 답변을 보면은 답변이 안 나온 거를 저희가 볼 수가 있네요. 네 답변이 없다는 얘기겠죠 여기 안에서 그러면 또 다른 질문을 해볼까요? 카메라는 뭐야 저 답변이 안 나올 수도 있어요 200만 화석 하면 이렇게 나오죠 어쨌든 이런 식으로 저희가 질문을 던질 수 있는 걸 알 수가 있고 뭐 어쨌든 어느 정도 의미가 있는 답변이 나오는 걸 볼 수가 있고요. 그다음에 저희가 그만이야 누르면 그만하겠죠 그만이야 눌렀더니 그만을 했죠. 네 그만해라 눌렀을 때 그만 했는데 한 번 더 돌아가고 끝났네요. 아무튼 이런 식으로 저희가 odql을 간단하게 저희가 한번 만들어 봤고요. 보시다시피 문서를 리트리브하는 엔진과 문서를 다 하는 MRC 모델을 연결해서 저희가 이런 오디qa를 만들 수 있는 걸 확인하셨고, 이거를 결국 여러분이 프로젝트에서는 처음으 해볼 수 있는 것들일 것 같아요. 문서를 좀 더 개수를 많이 하면은 좀 더 실제로 오픈 도메인을 할 수 있을 거고 좀 먼저 해 볼 수 있는 거는 콜코드 내에 있는 문서만 가져와서 해보는 방법이겠죠. 그리고 다음 실습에서는 오늘 썼던 tfidf에서 더 개선이 된 댄스 인베딩을 활용한 리트리버를 다음 실습에서 저희가 다뤄보도록 하겠습니다. 네 그러면은 7강 실습 마치도록 하겠습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[MRC] (7강)Linking MRC and Retrieval.json",
        "lecture_name": "[MRC] (7강)Linking MRC and Retrieval",
        "course": "MRC",
        "lecture_num": "7강",
        "lecture_title": "Linking MRC and Retrieval",
        "chunk_idx": 8,
        "total_chunks": 9,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:6fa4ab76cd92ef308b86f3b40293dd1e4f298e1146e071734902e5d183320e13"
      },
      "token_estimate": 884,
      "char_count": 1658
    }
  ]
}