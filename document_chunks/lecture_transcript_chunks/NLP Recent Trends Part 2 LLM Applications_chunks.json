{
  "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
  "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
  "course": "NLP",
  "total_chunks": 17,
  "chunks": [
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c000_611a32",
      "content": "[강의 녹취록] 과목: NLP | 제목: NLP Recent Trends Part 2 LLM Applications\n\n앞선 강의에서는 LLM 가지고 리즈닝 앤 플래닝을 푸는 것을 얘기를 했었는데요. 이번 강의에서는 엘엘램 응용 그래서 앞서 나왔던 방법론을 어떻게 활용해 가지고 엘레램 관련 애플리케이션을 만들 수 있는지에 대해서 한번 얘기해 보도록 하겠습니다. 네 그래서 우선 기본적으로 앞쪽에서는 이번 강의 앞쪽에서는 엘엠의 응용 예시들 해가지고 실제로 좀 논문적인 얘기들 그 응용 예시이긴 한데 실제로 상업화된 예시들은 아니고 논문에서 이것들을 어떻게 응용하고 있는가 에들 그런 얘기들을 좀 다룰 예정이고요. 그리고 이거 이후에 엘램 자체의 한계에 대해서 좀 얘기해 볼까 생각하고 있습니다. 그 첫 번째 파트로에서는 엘엠 응용 예시 해오면서 이 예시들을 좀 살펴보면서 엘램 가지고 어떤 것을 풀 수 있는가를 좀 확인해 보도록 하겠습니다. 리즈닝 n 플리닝 쪽에서 저희가 LM 자체가 뭔가 되게 복잡한 문제를 뭔가 도구를 통해 가지고 우리가 문제를 풀 수 있는 그런 능력이 있다라고 얘기했던 걸로 기억합니다. 그리고 그 능력이 그 도구라는 것은 저희가 정의하기 나름이고 그렇다면 이런 생각이 들 겁니다. 아 이것을 다 프롬포터 해 가지고 뭔가 엘램 자체가 되게 어려운 문제를 풀 때 되게 다양한 도구를 쓰고 뭔가 환경과 상호작용하면서 그리고 옛날에 생성했던 기록들을 참고하면서 알아서 뭔가 돌아가게 할 수 있지 않을까 라는 생각이 들 겁니다. 그래서 이러한 것을 뭔가 구현한 뭔가 그러한 객체라고 해야 될까요? 그러한 개념이 이 엘르램 에이전트입니다. 그래서 왼쪽에 보이는 예시가 그러한 엘르램 에이전트 프롬프트의 예시인데요. 어 이거 같은 경우에 실습 파일에 정확하게 적혀 있으니까 실습 파일 확인해 가지고 한번 같이 진행해 보면 좋을 것 같고요. 그래도 제가 지금 간략하게 얘기해 드리자면 보시면 알겠지만 뭔가 이렇게 커맨드스 그래 가지고 뭔가 이러한 어 여러분들이 이 엘레엠이 쓸 수 있는 도구들을 정의하고 그리고 이 엘레램 자체가 무엇인가 해 가지고 뭔가 시스템 프롬프트라고 해야 될까요? 뭔가 이러한 프린시플 적힌 프롬프트 그리고 이 엘레램 자체가 풀어야 되는 것 그리고 엘엠이 실제로 출력해야 되는 포맷 이런 것들이 이렇게 프롬프트화 해 가지고 전부 들어갔다는 것을 확인해 볼 수 있습니다. 그래서 뭐 간략하게 얘기하자면 결국에는 이 앞쪽에서는 결국에는 이것 자체가 어 서치 지피티 너는 서치 지피티인데 어떠 어떠한 친구야 같이 한 그러한 규칙을 알려주는 그러한 프로포터라고 확인해 볼 수 있고요. 사실 좀 더 중요한 부분이 이 커맨드 부분이 아닐까 생각이 듭니다. 그래서 앞서서 제가 얘기했던 것처럼 이 플래닝 리즈닝 플래닝에서 얘기했던 것처럼 결국에는 이 도구라는 것은 저희가 정의하기 나름이고 결국에는 나중에 이렇게 LLM이 도구를 호출하고 이렇게 그 결과물 자체를 LLM에게 직접 피드백하는 형태로 해가지고 넣어주 수 있기 때문에 이런 도구들을 정의하면 저희가 ln 자체가 활용할 수 있게끔 하는 것이 가능하다는 거죠. 그래서 대표적으로 뭐 웹서치 같은 것도 넣을 수도 있고요. 아니면 뭐 계산기 케이크레이터 이런 것들을 넣을 수가 있죠. 그래서 결국에는 하고자 하는 것은 이렇게 챗지피티가 있을 때 이렇게 퀘스천으로 2022년 DGP 기준으로 남한은 북한보다 몇 배나 더 잘 사니 이런 질문을 받게 된다면 사실 이거는 검색을 해야만 할 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 0,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 923,
      "char_count": 1714
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c001_93d815",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그리고 또한 이 검색된 결과물이 이게 아마 달러 이렇게 나올 건데 이것을 적절하게 계산기를 써 가지고 이것이 몇 배 컸는지 를 실제로 활용할 수 있게 한다면 좀 더 정확도가 올라가게 되겠죠. 그래서 실제로 결국에는 이 챗치피티 자체가 이렇게 웹 서치라는 커맨드를 써가지고 뭔가 인터넷 검색을 하고 그리고 그거에 대한 결과값을 받고 그리고 이제 그거에 대해서 이렇게 1933조 그리고 34조 이렇게 나오게 된다면 이걸 채집피티 자체가 뭔가 계산을 바로 할 수 있지만 그러지 않고 또다시 계란 산기라는 도구를 호출하고 그리고 이렇게 그거에 대한 결과값을 받고 그리고 최종적으로 받은 결과값을 기반으로 저희가 이제 남한은 북한보다 55배 더 잘 삽니다. 이렇게 출력하는 뭔가 이렇게 어려운 테스크를 단계별로 수행해 가지고 결국에는 유저의 사용자의 리쿼리를 해결해 주는 그러한 것을 만들 수가 있을 겁니다. 그래서 결국에는 이 LLM 에이전트가 하는 것은 무엇이냐 이렇게 주어진 요청을 처리하기 위해서 뭔가 계획을 수립하고 결국에는 이 질문을 풀기 위해서는 인터넷 검색을 수행하고 또 계산기를 진행을 해야 되겠죠. 그리고 해당 상황에서 적절한 도구를 사용한 다음에 그리고 결국에는 생성할 때 또 과거 대화 기록이나 관련 기록을 참고해 가지고 결국에는 복잡한 과제를 사례대로 수행할 수가 있습니다. 그래서 내용적으로 보시면 지금 이렇게 뭔가 제이슨 형태로 출력해 가지고 실제로 이거에 대한 관찰 생각 그리고 리즈닝 사고 과정 그리고 내가 이것을 어떻게 풀지에 대한 플래닝 과정 그리고 이 우리가 실제로 사용하게 될 커맨드 같은 것도 이렇게 정의가 되어 있고 이런 식으로 어 제이슨 형태로 출력하게 함으로써 저희가 이 모델의 출력값을 좀 더 잘 파싱해 가지고 실제로 우리가 도구를 호출하고 하는 데 더 활용할 수가 있을 겁니다. 그래서 이 뒤에 내려 나오는 내용들은 이 엘엘엠 에이전트를 활용한 방법론들이라고 대부분 다 생각하시면 좋을 것 같습니다. 이러한 엘엘엠 에이전트의 개념을 사실상 처음 게시한 것이 아닌가 생각되는 논문이 이 비주얼 챗지피티라는 논문인데요. 어 비주얼 챗gpt에서 하고자 하는 바는 생각보다 간단합니다. 이런 겁니다. 안녕 어때 하고 물어보면 이 비주얼 챗gpt는 뭐 자기소개 하겠죠 근데 하고 싶은 것은 이렇게 채팅하는 것은 사실 기존에 있던 챗gpt에서도 가능했던 거잖아요. 하지만 이 비주얼 챗gpt 같은 경우에는 이미지를 처리할 수가 있습니다. 그래가지고 뭔가 드로잉 해줘 어 그 뭔가 애플 사과 같은 거 드로잉 해줘 그러면 이제 사과에 대한 드로잉을 뭔가 그려주고요. 그리고 아 이미지 형태로 이 이미지에서 그 형태를 그려줘 하게 된다면 실제로 이렇게 이미지가 나오는 것을 확인해 볼 수 있습니다. 마치 이렇게 그 비즈 챗치피티를 통해 가지고 이미지를 그 뭔가 조작하고 생성하는 것이 가능하다는 걸 확인해 볼 수 있죠. 여기서 뭔가 이 이미지 를 뭔가 컬러 워터 페인팅 물색의 뭔가를 바꿔달라 하면 이렇게 잘 바꿔주는 것을 확인해 볼 수 있습니다. 어 근데 문제가 하나 있습니다. 지금에 있는 챗gpt 기능은 다 이미지 입력이 가능하죠 여러분들 챗gpt에 이미지 입력 넣어 가지고 이것이 실제로 무엇을 의미하는지 얘기할 수 있는 기능이 이미 추가돼 있습니다. 근데 그러면 그걸 쓰면 되는 거 아니냐 할 수 있는데 어 이거 비주얼 챗지피티 같은 경우에는 이거 지피티 3.5 시절에 나왔는데 이게 당시에는 챗지피티에 이미지를 넣을 수 있는 방법이 없었어요. 그래서 당시 이 지피티 3.5 같은 경우에는 텍스트만으로 다룰 수가 있었습니다. 즉 텍스트만 다룰 수 있는데 지금 이미지를 다뤄야 되는 그런 상황인 거죠. 네 이렇게만 들으면 되게 어려운 문제인 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 1,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1012,
      "char_count": 1889
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c002_e4e471",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 근데 그러면 그걸 쓰면 되는 거 아니냐 할 수 있는데 어 이거 비주얼 챗지피티 같은 경우에는 이거 지피티 3.5 시절에 나왔는데 이게 당시에는 챗지피티에 이미지를 넣을 수 있는 방법이 없었어요. 그래서 당시 이 지피티 3.5 같은 경우에는 텍스트만으로 다룰 수가 있었습니다. 즉 텍스트만 다룰 수 있는데 지금 이미지를 다뤄야 되는 그런 상황인 거죠. 네 이렇게만 들으면 되게 어려운 문제인 것 같습니다. 비주얼 챗피티 챗g피티 자체는 이미지를 다룰 수 없는데 이미지를 다뤄야 하는 상황이죠. 하지만 이 비주얼 챗g피티에서는 이것을 LM의 에이전트와 유사한 형태를 통해 가지고 저희가 그 챗지피티 자체가 텍스트만 원래 다룰 수 있지만 멀티모달 데이터도 다룰 수 있는 그러한 형태를 만들게 됩니다. 네 그래서 원래는 이 챗지피티 같은 경우에는 기본적으로 텍스트만 다룰 수 있었는데 사실 앞서 나왔던 태스크들 예시를 보여드리자면 결국엔 이렇게 어 이 이미지에서 이러한 스타일을 바꾼다든가 혹은 이미지를 생성해 준다든가 이거 기반으로 그런 것들은 다 이 비주얼 파운데이션 모델들은 이미 기존에 존재했었습니다. 뭔가 디퓨전 그래서 그 간 기반 이러한 모델들은 이전에도 존재했었어요. 하지만 문제가 있다면 이것을 뭔가 상호 사용자가 상호 작용을 할 수가 없었다는 게 문제였겠죠. 그냥 말 그대로 프롬프트가 있을 때 어떠한 프롬프트가 있을 때 프롬프트라고 해야 될까요? 아니면 뭔가 이렇게 텍스트 인풋이 있을 때 이거 기반으로 이미지만 만들어 준다든가 그런데 이거 기반으로 여러 가지 도구들이 있는데 그것들을 뭔가 다 엮을 수 있는 방법이 없었던 거죠. 하지만 이제 하고자 하는 바는 간단합니다. 이제 이 챗지피티 3.5가 어 이러한 비주얼 파운데이션 모델을 도구로 사용해 가지고 이러한 이미지 생성을 인터랙티브하게 컨트롤할 수 없을까 즉 앞서 나왔던 엘엘엠 에이전트 개념과 유사하게 결국에는 도구가 이미지 파운데이션 모델들은 어떤 모델은 이 스타일을 바꾸고 어떤 모델은 이미지 디텍션을 하고 이런 것들을 다 도구가 정의되어 있고 이제 챗pt가 사용자의 뭔가 상호작용을 할 때 적절한 도구를 호출해 가지고 실제로 이미지를 처리해 가지고 다루는 그런 개념을 제시했다라고 생각하시면 좋을 것 같습니다. 자 그래서 결국에는 이것도 결국에 프롬프트를 구성해 가지고 챗치피티가 다양한 브프엠 그 비주얼 파운데이션 모델들을 연결하고요. 이걸 통해 가지고 유저는 이제 모델과 대화하면서 이미지 생성하면서 수정하는 것이 가능합니다. 결국에는 이렇게 유저 커리가 있을 때 그리고 파운데이션 모델들이 정의되어 있고 이것을 프롬프트화 시켜 가지고 챗지피티하게 넣어서 인터레이티브하게 리스닝 하겠다가 기본 아이디어라고 생각하시면 좋을 것 같아요. 결국에는 어 챗gpt는 이제 결국에는 이 적절한 도구를 선택하기 위해서 뭔가 이 플래닝 계획을 세워 가지고 중간 결과를 생성을 가이드 한다는 거죠. 그래서 만약 저희가 지리가 지금 보시면 되게 어렵습니다. 지리 자체가 지금 이러한 이미지가 주어져 있고요. 지금 이러한 빨간색 레드 플라워를 그려줘 어 이때 중요한 것은 이 컨디션 드 온 프레이 데스 오브 디스 이미지 그러니까 이 이미지의 뎁스를 측정해 가지고 그걸 기반으로 카툰 스타일로 만들어 달라 스텝 바이 스텝으로 이러한 되게 복잡한 질의가 들어왔다는 걸 확인할 수 있습니다. 그래서 그런데 여기서 DPS를 예측한다든가 뭔가 카툰 이미지로 만든다든가 그리고 이미지를 생성한다든가 이런 것들은 기존에 존재하는 방법론들이 기존에 존재하는 모델들이 존재했었어요. 하지만 이것들을 다 동시에 호출할 수 있는 방법이 마땅치 않았던 거죠. 하지만 이제 비주얼 챗 피티에서는 이것을 챗치피티가 프롬프트화를 통해 가지고 이것을 직접 도구를 호출하는 방향으로 해서 이것을 다뤘다는 것이 중요하다고 생각합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 2,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1051,
      "char_count": 1931
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c003_7fc4f8",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그래서 그런데 여기서 DPS를 예측한다든가 뭔가 카툰 이미지로 만든다든가 그리고 이미지를 생성한다든가 이런 것들은 기존에 존재하는 방법론들이 기존에 존재하는 모델들이 존재했었어요. 하지만 이것들을 다 동시에 호출할 수 있는 방법이 마땅치 않았던 거죠. 하지만 이제 비주얼 챗 피티에서는 이것을 챗치피티가 프롬프트화를 통해 가지고 이것을 직접 도구를 호출하는 방향으로 해서 이것을 다뤘다는 것이 중요하다고 생각합니다. 어 데스 오브 이미지가 뭐냐고 한다면 이런 이미지가 데스 오브 이미지예요. 참고로 지금 보시면 결국엔 이 이미지에서 이건 가까이 있고 이러한 꽃은 가까이에 있고 이 뒤에 있는 꽃들은 멀리 있죠 해서 이러한 결과물이 나온다라고 생각합니다. 그래서 이러한 DS 오브 이미지가 나오고 DS 오브 이미지가 나오고 이거 기반으로 저희가 빨간 꽃을 만들고 그리고 그것을 카툰 형식으로 바꾸나 이래가지고 인터레이티브하게 이걸 리즈닝이 들어가 가지고 결국에 아웃풋이 나오면서 그리고 거기에 대한 설명이 같이 나온다는 것을 확인해 볼 수 있죠. 그리고 이렇게 이 순간 이 이미지에서 이렇게 DPS를 뽑아내고 이미지 생성하고 카툰화하는 것 이러한 이트레이티브한 플래닝 계획 자체를 도 역시 챗gpt가 생성한다는 겁니다. 그래서 이 프롬프트에 포함되는 내용이 그래서 무엇이 들어가느냐가 좀 중요하게 될 수 있는데요. 결국에는 앞서 나왔던 프롬프트 예시하고 되게 유사합니다. 맨 처음에 시스템 원리 해가지고 챗지피티 자체가 지켜야 할 기본 원리들 그러니까 비주얼 챗지피티 같은 경우에는 결국에는 이미지를 직접 다룰 수 없기 때문에 모든 것을 파일명으로 다룹니다. 그래서 이러한 파일명 규칙이라든가 이거 출력할 때 서스 액션 그리고 액션 인풋 해가지고 방금 우리가 리즈닝 엠플리닝에서 배웠었던 뭔가 리액트 형식이랑 유사하게 생각을 적고 그리고 어떤 액션을 취할지 적고 그리고 액션의 인풋들을 정리하는 형태 이렇게 만들 수 이렇게 아웃풋 포맷을 정하였습니다. 그래서 지금 보시면 그리고 여기서 들어가야 되는 것이 이러한 시스템 원리 외에도 당연히 이것이 도구들에 대해서 정의가 되어야 되겠죠. 그래서 도구들이 다양한 도구들이 결국에 프롬포터 화가 돼서 들어가는데 이 도구의 이름이라든가 혹은 이 도구를 사용하는 방법 그러니까 어 너가 필요할 때 어떤 순간이 필요한가 그러니까 너가 만약 이 이미지를 통해 가지고 엔서가 필요하다 큐에이를 해야 된다. 즉 어 브큐에 비주얼 보고 뭔가 이걸 가지고 큐에이를 해야 된다 이건 직접 챗지피티가 할 수 없거든요. 왜냐하면 이미지 자체를 볼 수 없기 때문에 이 이미지를 이해할 수 있는 모듈이 필요할 겁니다. 그래서 이러한 이미지 이해가 가능한 브큐에 모델들을 들고 와 가지고 이걸 호출하는 것을 도구화를 합니다. 그래서 유세이 줬고 그리고 인풋 아웃풋 포맷 줬고요. 필요하다면 뭔가 예제 인샘플 같은 것도 줘서 이렇게 도구를 정의를 하는 겁니다. 단순히 이 도구 근데 도구가 하나만 굳이 있을 필요는 없겠죠 당연히 다양한 도구가 있을 겁니다. 예를 들어 픽스 투 픽스 이거 아시는 분들 아시겠지만 이거 스타일 트랜스퍼하는 이미지 모듈인데요. 그래서 당연히 이미지를 생성하는 텍스트에서 이미지를 생성하는데 뭔가 스타일의 이미지를 바꿀 때 텍스트를 텍스트와 다르게 그러니까 뭔가 이 이미지가 수채화 스타일로 바꿔줘 이런 형태로 해가지고 텍스트를 넣었을 때 이것을 실제로 그 형태로 바꿔줄 수가 있는 거죠. 그래서 저희가 이렇게 그거에 대한 인풋 풋 예시 해가지고 이렇게 다양한 도구들을 정리를 해 가지고 이게 챗피티가 활용할 수 있게끔 참고할 수 있게끔 넣는 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 3,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1001,
      "char_count": 1838
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c004_5c5fd1",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 예를 들어 픽스 투 픽스 이거 아시는 분들 아시겠지만 이거 스타일 트랜스퍼하는 이미지 모듈인데요. 그래서 당연히 이미지를 생성하는 텍스트에서 이미지를 생성하는데 뭔가 스타일의 이미지를 바꿀 때 텍스트를 텍스트와 다르게 그러니까 뭔가 이 이미지가 수채화 스타일로 바꿔줘 이런 형태로 해가지고 텍스트를 넣었을 때 이것을 실제로 그 형태로 바꿔줄 수가 있는 거죠. 그래서 저희가 이렇게 그거에 대한 인풋 풋 예시 해가지고 이렇게 다양한 도구들을 정리를 해 가지고 이게 챗피티가 활용할 수 있게끔 참고할 수 있게끔 넣는 겁니다. 그래서 뭔가 이 이미지 처리 모델 vf 모듈의 뭐 종류라든가 특징 넣고요. 또 들어가야 되는 것이 당연히 이 유저의 과거 대화 기록을 봐야지만 내가 지금 이 맥락에 따라서 적절한 도구들을 호출할 수 있기 때문에 만약 유저가 예전에 이런 이미지를 생성해 줘 해가지고 저번이랑 똑같은 이미지 생성해 줬다면 당연히 똑같은 쿼리를 넣어 줘야 되겠죠 그런 식으로 대화 기록이 들어가야 할 겁니다. 그래서 이렇게 대화 기록이 들어가고요. 그리고 지금 유저가 풀고자 하는 유저 쿼리도 당연히 드려야 될 겁니다. 그래서 유저가 뭔가 비주얼 챗피티한테 뭔가 이러한 이러한 이미지를 만들어줘 같은 쿼리들 그러니까 텍스트나 이미지 파일명 같은 것도 다 정리를 해야 될 거고요. 그리고 프롬프트에 들어가야 되는 것이 이 중간 사고 과정들도 들어가야 될 겁니다. 그래서 이러한 것들을 다 프롬프트 해가지고 프롬프트에 넣어가지고 다 돌아가게끔 만드는데 어 결국에는 이러한 프롬프트화를 넣어서 결국에는 각 VFM 그러니까 비주얼 파운데이션 모델의 특징을 챗치 피디에게 알려주고 인풋 아웃풋 포맷에 적시한 다음에 이미지 타입 같은 것도 다 정리를 하고 네 그거 외에도 뭔가 이렇게 대화의 록을 저장한다든가 아니면 브프엠 모델이 여러 개 호출되었을 때 이것을 순서를 정한다든가 이런 것들을 프롬프트 매니저를 통해 가지고 저희가 프롬프트화해서 다 넣어줄 수가 있을 겁니다. 그리고 이제 이렇게 프롬프트를 다 넣게 된다면 그대로 이걸 그냥 챗gpt에게 넣어주는 거죠. 그리고 이 챗gpt는 하는 것은 결국에는 이거 기반으로 계획을 수립해 가지고 만약 해당 테스크가 이미지 처리가 필요하다 할 경우에는 이제 실제로 그 도구를 이미지 처리 모듈을 진행시켜 가지고 중간 답안을 생성하는 겁니다. 그리고 이러한 추론 기록과 합쳐 가지고 다시 이제 프롬프트에 넣어 가지고 중간 답안을 넣고 또 다시 거기서부터 생성을 하는 거죠. 그러면 또 다시 이미지 처리가 필요한지 해가지고 판별해 가지고 만약 필요 없다 완성되었다 할 경우에는 이제 최종 결과로 출력하게 되는 겁니다. 그래서 실제로 저희가 유저가 이렇게 사진을 넣은 다음에 사진에 소파를 색상을 바꾸고 이 소파를 색상으로 바꾸고 전반적으로 물색으로 칠해줘 말하게 하게 된다면 실제로 이것이 맨 처음에 이 비주얼 파운데이션 모델이 필요하다는 것을 아니까 실제로 이것을 사진을 무선 소파를 책상으로 바꿔주고요. 그리고 그걸 기반으로 물색으로 바꾸고 그리고 이제 더 이상 유저의 쿼리가 다 수행되었으므로 이것이 출력이 돼 가지고 이제 나왔다는 것을 확인해 볼 수가 있습니다. 그리고 유저가 또다시 질문 질의를 할 수 있겠죠. 그럴 경우에는 이제 사진의 벽은 무슨 색이야 이렇게 물어보게 된다면 또다시 거기에 적절한 뭔가 파운데이션 모델을 찾아가지고 이 경우에는 캡셔닝이라든가 vqa 비주얼 캡션 intr 이러한 모듈을 썼겠죠 해가지고 그걸 호출한 다음에 사진의 벽은 파란색입니다. 그래서 이렇게 결과값을 얻어내는 것도 가능합니다. 네 비주얼 GGPT 위에 또 유명한 이러한 LNM 에이전트 형식의 유명한 응용 프로그램으로 이 자비스라는 논문이 있습니다. 비주얼 챗시피티 같은 경우에는 문제점이 하나 있었는데요. 바로 이 사전에 정의된 어 사전에 정의 혹은 구현된 모델 외에 뭔가 이것이 다른 뭔가 모델을 호출한다든가 그런 데에서는 좀 약점이 있었습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 4,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1083,
      "char_count": 1998
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c005_2e9dbe",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그래서 이렇게 결과값을 얻어내는 것도 가능합니다. 네 비주얼 GGPT 위에 또 유명한 이러한 LNM 에이전트 형식의 유명한 응용 프로그램으로 이 자비스라는 논문이 있습니다. 비주얼 챗시피티 같은 경우에는 문제점이 하나 있었는데요. 바로 이 사전에 정의된 어 사전에 정의 혹은 구현된 모델 외에 뭔가 이것이 다른 뭔가 모델을 호출한다든가 그런 데에서는 좀 약점이 있었습니다. 그래서 실제로 뭔가 브팬 모델로 브팬 모델로 저희가 픽스 투 픽스 픽스 투 픽스 이런 식으로 뭔가 모델을 만약 뭔가 모델을 정해 놓았다면 사실상 그것만 활용할 수가 있었죠. 하지만 사실 세상에는 되게 모델들이 많고 그 상황에 따라서 적절한 모델들이 다를 수도 있을 겁니다. 그래서 그때그때마다 적절한 어느 모델들을 골라 가지고 만약 활용할 수 있다 뭔가 이것을 어려운 문제를 풀 수 있다면 되게 되게 아주 복잡한 문제도 잘 풀 수가 있겠죠. 그렇게 해서 이 잘비스에서는 이 언어를 뭔가 AI 모델들을 연결하는 매개체로 삼아가지고 특히 이 허깅페이스 아마도 여러분들 NLP 처리를 하다 보면 이 허깅페이스 이야기를 많이 들어봤을 겁니다. 해서 NLP에서 많이 쓰이는 라이브러리인데 허깅페이스 같은 경우에는 중요한 기능이 하나 있는데요. 바로 이 허깅페이스 허브라는 것이 또 중요한 기능입니다. 허브 같은 경우에도 아마도 이미 아시는 분들도 많을 거예요. 되게 다양한 모델들이 있고 되게 다양한 테스크를 풀 수 있고 그리고 그러한 모델들이 사전 학습된 모델들이 올라가 있는 그러한 사이트인데 그럼 드는 생각이 아 그럼 말고 그러지 말고 굳이 우리가 모델을 정하지 말고 이 허깅페이스 허브에 있는 모델들을 호출해 가지고 그냥 그걸 다운받아 가지고 알아서 할 수 있게끔 하면 되지 않을까 생각을 해볼 수가 있습니다. 해서 결국에는 이러한 엠엘 커뮤니티 뭐 기터이라든가 허깅페이스 같은 모델 설명을 가져가다가 챗지피티가 다운 받아 가지고 모델 리스트가 있을 때 적절한 모델들을 골라 가지고 아 이렇게 복잡한 데스크를 수행해 보자라는 것이 자비스의 기본적인 아이디어라고 생각하시면 좋을 것 같습니다. 그래서 되게 어려운 예시를 한번 얘기해 보도록 할게요. 자 이그잼플 제비티 속 남자 아이와 같은 자세 그러니까 이 아이가 있죠 이거와 그 남자 아이와 같은 자세로 책을 읽고 있는 여자 이미지를 생성해 줘 결국에는 이거와 동일한 자세라는 것은 말 그대로 자세가 동일하다는 겁니다. 해서 여기에 얼굴이 있고요 머리가 있고 이렇게 뼈대가 있고요. 사람 팔이 이렇게 있고 그리고 이걸 기반으로 똑같은 자세를 가지고 있는 다만 책을 읽고 있는 여자의 이미지를 생성해 달라 라는 요청입니다. 그리고 하나 더 요청이 하나 더 있죠. 그 이후에 생성된 이미지를 너희 목소리로 설명해 줘 이게 뭔 뜻이냐 말 그대로 이 이미지를 설명하는데 이것을 너의 목소리로 설명해 달라는 뜻은 이것을 설명을 생성하고 그거 기반으로 TTS 그러니까 text to 스피치 모델을 호출해 가지고 설명해 달라 오디오 형태로 설명해 달라 그런 모델입니다. 그래서 이러한 되게 복잡한 퀘스천이 있을 때 어 찰비스 같은 경우에는 결국에는 각각에 대해서 우선은 왜 테스크를 다 분해를 해야 될 겁니다. 지금 보시면 알겠지만 결국에는 남자의 익스인플속 남자 아이와 같은 자세를 찾아달라 이거 같은 경우는 결국에는 자세 추정 포즈 에스티메이션이 필요할 거고요. 이제 이거 기반으로 텍스트 책을 읽고 있는 여자아이를 생성하기 위에는 포즈 투 이미지가 필요할 겁니다. 그리고 그걸 설명하기 위해서는 이제 캡셔링이 필요할 거고요. 캡셔닝이라는 게 그거죠. 이미지가 있을 때 거기에 대해서 설명을 달아주는 걸 캡셔닝이라고 하죠. 그리고 캡셔닝 된 텍스트를 목소리로 읽어주기 위해서는 이제 TTS 텍스트 투 스피치 그러한 모듈이 필요할 겁니다. 그래서 거기에 대해서 이 모델들이 사실 허깅페이스 허브에 되게 많을 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 5,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1060,
      "char_count": 1968
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c006_e59f15",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그리고 그걸 설명하기 위해서는 이제 캡셔링이 필요할 거고요. 캡셔닝이라는 게 그거죠. 이미지가 있을 때 거기에 대해서 설명을 달아주는 걸 캡셔닝이라고 하죠. 그리고 캡셔닝 된 텍스트를 목소리로 읽어주기 위해서는 이제 TTS 텍스트 투 스피치 그러한 모듈이 필요할 겁니다. 그래서 거기에 대해서 이 모델들이 사실 허깅페이스 허브에 되게 많을 겁니다. 이제 챗gpt가 하는 건 뭐냐 하면 그러한 모델들의 설명이라든가 그런 것을 다운받아 가지고 적절한 모델들을 선택해 가지고 차례대로 호출해서 복잡한 테스크를 풀어보자는 것이 잘비스의 기본 아이디어입니다. 그래서 이게 잘비스 같은 경우에는 총 4가지 단계를 통해 가지고 이것을 실제로 생성해내는데요. 첫 번째는 테스크 플래이 그리고 두 번째는 모델 셀렉션 세 번째는 테스크 익스큐션 네 번째는 그 레스폰스 제너레이션입니다. 그래서 하나하나 좀 확인해 보도록 하겠습니다. 네 해서 실제로 이 네 가지 단계를 거치게 된다면 이렇게 이미지가 있을 때 여기에 대해서 포즈 이미지 에스티메이션 해가지고 포즈가 나오게 되고 그리고 그걸 기반으로 이미지 생성하고 그리고 그거 기반으로 또다시 이미지에 캡션을 달아가지고 그걸 또다시 그 텍스트를 이제 오디오 형태로 뽑아내는 출력이 나왔다는 것을 확인해 볼 수가 있고요. 그래서 이 네 가지 단계 정확히 말하면 이 네 가지라는 게 여기서 이렇게 넘어가는 네 가지가 아니고요. 이 네 가지가 어떻게 구성되는지에 대해서 얘기해 보도록 하겠습니다. 자 그래서 이 자비스의 진행 단계는 이 네 가지 단계로 이루어지는데요. 결국에는 되게 방금 전에 봤던 되게 복잡한 테스크를 풀기 위해서는 당연히 유저 사용자의 입력의 의도를 이해해야 될 거고요. 결국에는 문제를 방금 전에 봤던 것처럼 뭔가 오브젝트 디텍션을 하고 그 텍스트에서 오디오를 생성하고 이런 것들이 다 그 서브 테스크일 건데 이것들을 다 이제 연결하는 그런 과정이 필요할 겁니다 해서 풀이 가능한 테스크들로 나눈 것이 이러한 이 태스크 플래닝 과정에서 이루어진다라고 생각하시면 좋을 것 같습니다. 그래서 이제 그 뒤로는 이제 테스크가 정해졌다면 이제 정해지겠죠. 맨 처음에 해야 되는 것은 포즈 포즈 에스티메이션 자세 추정 한국어로 적어볼까요? 자세 추정 자세 추정하고 그다음에는 이제 이거 기반으로 포즈 2 이미지 그러니까 포즈에서 받아가지고 이미지를 생성하는 거 그다음에는 이 이미지가 생성되었을 때 거기에 대한 설명을 붙여야 되니 캡셔닝 캡셔닝 그다음에는 이제 이미지에 텍스트가 생성됐으니까 이제 TTS 호출될 건데 사실 이것들은 우리가 테스크 별로 나눈 것이 모델을 뭘 쓸지에 대해서는 얘기를 안 했잖아요. 뭔가 이 자세 추정에서 적절한 모델들이 있을 겁니다. 해서 되게 그런 리스트들이 되게 많을 건데 이것들 역시도 그 결국엔 허깅 피스 허브에 다 어느 정도 올라가 있을 거고 이러한 테스크가 정해져 있다면 허깅 피스 허브에서 이걸 찾아 가지고 이제 적절한 모델들을 선택하는 과정이 필요할 겁니다. 그걸 모델 셀렉션에서 수행한다라고 생각하시면 좋을 것 같아요. 그리고 그러한 모델들 이렇게 테스크에 대한 설명들을 받았을 때 이제 이것들을 실제로 실행하는 과정이 있겠죠. 실행한 과정들을 이제 태스크 익스큐션 부분에서 실행하고요. 실행하는 과정을 이 태스크 익스큐션에서 실행을 하고 그리고 마지막으로 이 결과물들을 다 취합해 가지고 대답을 생성해야 될 거잖아요. 앞쪽에서 봤던 것처럼 실제로 이러한 텍스트 아웃풋을 생성해야 될 건데 그거에 대한 설명을 결국에는 이 레스폰스 제너레이션 해 가지고 기반으로 최종 대답을 생성한다 하는 것이 기본 기본적인 아이디어라고 생각하시면 좋을 것 같습니다. 그래서 이 프롬프트들을 어떻게 디자인 하느냐를 살펴보게 된다면 결국에는 이것도 다 어느 정도 퓨샷 예시라든가 혹은 뭐 설명을 통해 가지고 데모스트레이션과 인스트럭션을 통해 가지고 돌아간다라고 생각하시면 좋을 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 6,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1082,
      "char_count": 1982
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c007_7491eb",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그래서 이 프롬프트들을 어떻게 디자인 하느냐를 살펴보게 된다면 결국에는 이것도 다 어느 정도 퓨샷 예시라든가 혹은 뭐 설명을 통해 가지고 데모스트레이션과 인스트럭션을 통해 가지고 돌아간다라고 생각하시면 좋을 것 같습니다. 그래서 이 태스크 플래닝 같은 경우에는 이것이 실제로 테스크 플래닝 프롬프트인데요. 지금 보시면 알겠지만 이렇게 프롬프트 부분하고 데모 스트레이션 부분으로 나눠져 있는데 이 프롬프트 부분이 사실상 인스트럭션이 들어가는 부분이라고 생각하면 좋을 것 같고요. 이 데몬스트레이션 부분이 이 예시들이 들어가는 부분이라고 생각하면 좋을 것 같습니다. 일반적으로 여러분들이 뭔가 모델을 애플리케이션 만든다 할 때 대개 이런 경우에는 데몬스트레이션 야스샷을 넣는 것이 좀 더 모델의 모델이 좀 더 안정적으로 돌아가기 때문에 예상 샷을 일반적으로 샷을 많이 넣어요. 그래서 그래서 실제로 아웃풋을 보게 된다면 어 우선 프롬프트를 보면 이렇게 설명이 쫙 들어가 있고 그리고 그거에 대해서 아웃풋을 이렇게 넣어 달라 그래서 테스크 어떤 테스크를 실행하고 그리고 이러한 테스크가 어떤 테스크랑 연관되어 있는지 그리고 그거에 알규먼트는 어떻게 되어 있는지에 대해서 이렇게 설명이 돼 있다는 것을 확인해 볼 수가 있습니다. 그래서 예시를 보면 CNN TMI how mani object in 1 JPG 그러니까 21 JPG 안에서 얼마나 많은 오브젝트가 있는지 알려줘라고 하게 된다면 결국에는 이거는 오브젝트 디텍션이 필요하죠. 그래서 오브젝트 디텍션을 테스크로 잡고 아규먼트로 이미지가 이런 식으로 들어간다는 것을 확인해 볼 수가 있습니다. 이것은 예시가 하나밖에 없으니까 되게 간단한 질문이지만 뭔가 되게 어려운 예시도 이렇게 하면 풀 수가 있다는 것을 확인해 볼 수 있죠. 그래서 이 투 지 JPG에서 우리 동물은 무엇을 하고 있니 하게 된다면 어 텍스트를 이미지를 텍스트로 바꾼 다음에 뭐 캡셔닝 모듈 같은 걸 불러서요. 그리고 테스크가 이제 이미지 클래시피케이션 하고 오브젠 디텍션 하고 그리고 비주얼 퀘스천 앤서링 해 가지고 브큐까지 푸는 것까지 해가지고 이러한 다양한 테스크들을 엮어 가지고 아웃풋을 출력한다는 것을 확인해 볼 수가 있습니다. 이때 자비스 같은 경우에는 단순하게 이렇게 일렬로 뭔가 테스크를 풀기 위해서 이거의 결과값이 다음으로 결과값 넘어가고 이거의 결과값이 또 다음으로 결과값으로 넘어가고 뭐 이런 식으로도 만들 수 있지만 이 자비스에서 하는 것 중 하나가 이렇게 뭔가 그래프 형태도 역시 잘 다룰 수 있게 합니다. 네 지금 보시면 알겠지만 이 디피 필드 같은 경우에는 뭔가 이전에 생성된 결과물을 뭔가 호칭하고 있다는 것을 확인해 볼 수 있습니다. 그래서 실제로 여기서는 디피가 다 마이너스 1로 코칭돼 있어 가지고 이전에 결과물로 쓰인다라는 걸 의미로 받아들일 수 있는데요. 어 이게 뭐 기본적으로 단순하게 방금 전에 봤던 챗gpt 형식처럼 뭔가 이렇게 스퀀셜한 것은 당연히 다를 수 있는데 이 자비스 같은 경우에는 그렇지 않고 뭔가 테스크를 볼 때 뭔가 그래프 형식으로 필요한 경우도 있을 겁니다. 예를 들어 어떠한 그 테스크의 결과물이 어떤 다른 두 테스크의 결과물로 들어가고 이걸 다시 합치는 뭐 이런 테스크가 있을 수도 있겠죠. 그래서 이런 것은 이런 것들을 또 뭔가 또 다룰 수 있게끔 이 자비스 같은 경우에는 뭔가 이렇게 아이디를 줘가지고 지금 보시면 알겠지만 지금 이미지를 받아 가지고 지금 DP 자체가 0이 나온 경우에는 ID가 0이나 호출되는 급 이런 식으로 해가지고 뭔가 이전 테스크에서 2개의 아웃풋이 동시에 필요하다든가 아니면 이렇게 그래프 형태의 테스크를 풀어야 된다든가 하는 경우도 커버를 할 수 있다는 것을 좀 보입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 7,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1015,
      "char_count": 1887
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c008_7fca74",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그래서 이런 식으로 생성하게 된다면 이제 모델이 실제로 유저 쿼리를 받았을 때 이렇게 제이슨 리스트 형태로 테스크를 플래닝을 할 수가 있겠죠. 그래서 이제 이 다음에 해야 되는 것은 실제로 이 테스크를 수행할 때 그러니까 포즈 디텍션 참고로 이러한 테스크는 사전에 정의가 되어 있긴 합니다. 왜냐하면 당연히 이거 허깅 페이스에서 검색이 가능해야 될 테니까요. 하지만 실제로 이 테스크들에서 어떠한 모델을 쓸지에 대해서는 또다시 허깅페이스 허브를 좀 뒤져봐야 되겠죠. 그래서 이 모델 셀렉션은 결국에는 현재 과제에 기반해서 주어진 모델 중에서 하나를 선택하게끔 만들어져 있습니다. 그래서 프롬프트 같은 경우에는 간단합니다. 그래서 이렇게 프롬프트가 주어지고 그리고 캔디데이트가 주어지는 겁니다. 이 캔디데이트 같은 경우에는 실제로 검색을 해가지고 나온 모델들 아웃풋이라고 생각하면 좋을 것 같네요. 그래서 실제로 모델의 메타데이터라든가 데스크립션 같은 걸 따로 줘 가지고 이 중에서 베스트 모델을 고를 수 있게끔 하는 것이 기본적인 이 모델 셀렉션의 원리라고 생각하면 좋을 것 같습니다. 그래서 이것도 당연히 모델 검색은 허깅페이스 허브를 사용하고요. 그리고 이렇게 모델까지 다 골랐다. 아 이 테스크에서 이거는 어떤 모델 에라는 모델을 쓰고 이거는 씨라는 모델을 쓰고 제라는 모델을 쓰고 디라 모델 썼다 하게 된다면 모든 게 다 결정났으니까 이걸 실제로 실행시키면 될 겁니다. 그래서 실행시킨 다음에 a b 이렇게 아웃풋들을 다 전달 생성하고 그리고 마지막에 해야 되는 것은 결국에는 유저의 쿼리에 대해서 유저의 질의에 대해서 답변을 해야 될 겁니다. 그래서 레스폰스 제너레이션에서는 실제로 우리가 유저의 질의가 이렇게 들어왔을 때 태스크들이 이렇게 진행되었고 이제 모델들이 이렇게 선택되었다 했을 때 이거에 대해서 이제 테스크를 생성하게끔 그 속에 나왔던 이러한 아웃풋 지금 보시면 알겠지만 여기 있는 거죠. 텍스트 캔비 제너레이팅 이미지 해가지고 이렇게 이미지에 텍스트 출력이라든가 그리고 이것의 GRF가 그러니까 이 기이는 확률 같은 이런 것도 다 포맷팅이 다 가능하게끔 해 가지고 유저에게 최종 결과물 AI를 어떤 걸 썼고 같은 것도 출력하는 그러한 형태로 우리가 만들어 줄 수가 있을 겁니다. 그래서 이러한 네 가지 단계를 통해 가지고 우리가 이 잘비스를 구성할 수가 있고 이를 통해 가지고 되게 복잡한 문제도 이 허킹 페이스 허블을 알아서 검색해 가지고 뭔가 모델들을 호출해서 푸는 그러한 모델을 저희가 만들 수가 있습니다. 다만 이 jvis라는 모델 자체가 뭔가 이런 걸 해볼 수 있다 정도만 봐드리면 좋을 것 같고요. 어 이거 외에도 여러분들이 진짜 풀고 있는 다양한 테스크가 있을 때 뭔가 이런 비주얼 채집p라든가 이렇게 비전 없이 비전 문제를 푼다든가 아니면 모델 자체를 검색해 가지고 자기가 알아서 푸는 이러한 방법론도 한번 취해볼 수 있고 그런 형태를 따로 뭔가 파인튜닝 하지 않고 우리가 그 프롬프트만으로도 어느 정도 해결해 볼 수 있다라는 것을 저희가 한번 제가 한번 공유를 드리고 싶었습니다. 자 이렇게까지 했다면 당연히 드는 생각이 있을 거예요. 우리 굳이 모델 인터넷 검색 같은 게 아니고 그냥 로봇을 움직일 수도 있지 않을까라는 생각이 들 수도 있을 겁니다. 그래서 그걸 해 본 논문이 이 엘엠 플래너라는 논문이라고 생각해 보시면 좋을 것 같습니다. 이 LNM 플래너 같은 경우에는 우리가 자연어를 이해할 수 있는 로봇이 있다고 할 때 LNM을 플래너로 사용해 보자라는 개념인데요. 우선 기본적으로 로봇이 자연어를 이해할 수 있는 로봇이라는 것이 되게 강력한 과정이긴 한데 기본적으로 어떤 수준이냐면 이 자연어를 이해한다는 게 어 너가 앞으로 가라 너가 앞으로 가로 가 혹은 부엌으로 가 억으로 가 혹은 뭐 앞에 있는 물건을 집어 이런 식으로 되게 간단한 자연어가 있을 때 로봇이 수행할 수 있는 그런 로봇을 우선 가정을 합니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 8,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1075,
      "char_count": 1988
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c009_7165ff",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 이 LNM 플래너 같은 경우에는 우리가 자연어를 이해할 수 있는 로봇이 있다고 할 때 LNM을 플래너로 사용해 보자라는 개념인데요. 우선 기본적으로 로봇이 자연어를 이해할 수 있는 로봇이라는 것이 되게 강력한 과정이긴 한데 기본적으로 어떤 수준이냐면 이 자연어를 이해한다는 게 어 너가 앞으로 가라 너가 앞으로 가로 가 혹은 부엌으로 가 억으로 가 혹은 뭐 앞에 있는 물건을 집어 이런 식으로 되게 간단한 자연어가 있을 때 로봇이 수행할 수 있는 그런 로봇을 우선 가정을 합니다. 그래서 실제로 뒤에 있는 예시를 보면 이게 어떤 느낌인지 좀 감이 오실 거예요. 그래서 지금 보시면 알겠지만 우리가 지금 상황이 이렇게 좋아져 있고 엠바디드 나오는 것이 이거 보시면 엠바디에게 텍스트가 이렇게 들어가는 거예요. 포테이토한테 가라 코티히토 가서 포테이토를 줍고 그리고 이 포테이토를 리사이클빈에다 넣어라 이런 식으로 저희가 넣는 겁니다. 그리고 로봇은 기본적으로 이걸 이해하고 있다라고 가정합니다. 실제로 이 논문에서는 이걸 뭔가 다른 기존에 있던 논문에 있는 모듈을 사용했는데요. 거기는 좀 LNM과는 좀 별개로 돌아가기 때문에 그것은 제쳐두고 단순히 이렇게 단순한 테스크하고 단순한 뭔가 도구를 뭔가 집어들이고 움직이고 딱 정도만 할 수 있을 때 이제 되게 복잡한 테스크를 이제 이 로봇이 움직이게끔 복사한 테스크를 분해해 가지고 그러한 서브 테스크들로 분해해 가지고 로봇을 움직이게끔 하는 것이 이 엘앤 플래너의 기본적인 아이디어라고 생각하면 좋을 것 같습니다. 그래서 여기 엘엠 플래너에서 하는 것은 결국에는 엘엠 자체가 계획을 세우고 로봇이 이제 로우 레벨 동작을 수행한 다음에 이제 중요한 것은 이렇게 하다 보면 환경이 변할 거잖아요. 그리고 되게 예상치 못한 상황도 있을 거예요. 예를 들어 어 나 우리 그 포테이토로 가가지고 해보니까 뭔가 포테이토를 찾을 수 없었어. 그러니까 최소한 내 눈 앞에서는 포테이토가 안 보여 하지만 나는 이제 그 대신 뭔가 사진 같은 걸 분석해 가지고 아 눈앞에 뭔가 냉장고가 보여 해가지고 뭐 이러한 예외 상황이 발생할 수 있겠죠. 그 경우에는 다시 이 LNN 플래너가 그걸 기반으로 새로운 계획을 짜는 겁니다. 아 냉장고로 가서 이제 냉장고를 열고 오픈 오브젝트에서 이제 포테이토를 열고 이제 그 냉장고 문을 닫고 포테이토를 이제 리사이클 부인 그러니까 재활용 통에다가 갖다 버려라 같은 이런 식으로 변화한 환경에 맞춰 가지고 그때그때마다 계획을 세우고 그리고 그걸 기반으로 모델이 그 로봇이 수행한다라고 생각하면 좋을 것 같습니다. 그런데 문제는 또 다시 문제가 발생했죠. 보면 이렇게 또 돌아갔다가 하다 보니까 어 나 리사이클 부 못 찾았어 이런 순서에 가지고 냉장고 문 열고 닫고 냉장고 문을 열고 감자를 꺼내고 거기까지 다 했는데 리사이클 비는 없는데 나는 쓰레기통을 발견했어 했을 경우에 이제 모델이 이제 또 사이클 쓰레기통이랑 리사이클빈이랑 동일하다는 것을 모델이 아니까 이제 이 로봇이 말한 대로 이제 이 쓰레기통으로 가가지고 이제 포테이토를 던져 넣는 이런 식으로 그때그때마다 환경에 따라서 뭔가 수행을 하고 그리고 결과물을 받아 가지고 뭔가 에러 사항이 발생했다면 그거를 기반으로 다시 새로운 계획을 수정하는 그런 것을 생각하면 될 것 같습니다. 그래서 실제 이 프롬프트 예시를 보면 좀 이해가 될 것 같은데요. 우선 기본적으로 이런 식으로 그 인스럭션 그리고 액션 종류 그리고 어 인컨덱스 이그 샘플 예제들 넣어야 될 것 그리고 현재 단력 현 단계의 입력이 들어간다라고 생각하시면 좋을 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 9,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 984,
      "char_count": 1818
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c010_b657a2",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그래서 실제 이 프롬프트 예시를 보면 좀 이해가 될 것 같은데요. 우선 기본적으로 이런 식으로 그 인스럭션 그리고 액션 종류 그리고 어 인컨덱스 이그 샘플 예제들 넣어야 될 것 그리고 현재 단력 현 단계의 입력이 들어간다라고 생각하시면 좋을 것 같습니다. 그래서 결국에는 하는 것은 결국에 하이 레벨 쪽으로 이것을 디자인을 해보자라는 것이 기본 아이디어이기 때문에 그래서 실제로 뭔가 첫 번째로 들어간 것이 이러한 하이 레벨 플랜을 세워달라 그래서 하우스 홀드 그러니까 집안일 관련 테스크이고 너가 할 수 있는 행동들은 오픈 오브젝트 클로즈 오브젝트 열고 닫기 뭔가 줍기 그리고 놔두기 뭐 그 외에도 뭔가 자르기라든가 아니면 내비게이션 해 가지고 어디로 가기 이런 간단한 행동들이 정해져 있고 그리고 그걸 기반으로 이제 내비게이션 이렇게 내비게이션 그리고 부엌 한번 뭐 리빙룸 리빙룸 룸 하게 된 다음에 로봇이 여기까지 리빙룸까지는 안다라고 가정하고 내비게이션에 가서 리빙 룸으로 가는 거죠. 해서 이러한 액션들만 정해져 있다라고 가정합니다. 하고 싶은 것은 결국에는 이 계획이 있을 때 이 다음 계획이 어디까지 생성되었을 때 어디까지 수행하였을 때 그러니까 우리가 이 테스크에 대해서 이렇게까지 진행했고 이제 컴플리트 플랜을 여기까지 진행했었다면 했고 이제 로봇 입장에서 이제 보이는 입장 물체들이 이렇게 주어져 있을 때 우리 마이크로웨이브랑 프리즈랑 포테이토 갈비지 게임 그러니까 앞에 있는 이렇게 컴플리트 플레임 플랜드를 수용했을 때 이렇게 물체들이 보이고 이제 이다음부터 이제 모델이 하는 것은 이 다음 계획을 생성하는 겁니다. 그래서 이 이제 적절한 도구들을 생성해 가지고 골라가지고 이 액션들과 그리고 눈에 보이는 오브젝트들을 적절히 엮어 가지고 혹은 뭔가 눈에 보이지 않지만 뭔가 앞쪽에서 등장했던 것들 예를 들어 앞쪽에서는 마이크로 웨이브가 있었고 포테이토가 있었고 뭐 그런 것도 있었죠 해서 이런 걸 참고해 가지고 이제 플랜들을 세울 수가 있을 거예요. 다만 이 플랜을 세울 때 당연히 예시가 있으면 좋으니까 몇 가지 예시를 들고 옵니다. 그래서 여기 같은 거 위에 것 같은 경우는 예시죠 그래서 예시 데이터셋을 우선 따로 만들어 두긴 했어요. 여기서는 퓨샷 할 때 이 현재 상황과 가장 유사한 데모 트레이션을 넣어주기 위해서 예시 데이터셋이 있고 그거 기반으로 우선 몇 가지 한 3 4개 정도 3개에서 5개 정도를 들고 온다고 생각하면 좋을 것 같습니다. 그래서 결국엔 퓨샷인데요. 이것도 다만 이게 이런 데몬스트레이션이라고 해야 할까요? 이 샷 같은 경우에는 사실 현재 상황 그러니까 현재 거리인 감자를 조립하는 조리하는 이러한 상황과 유사한 데몬스트레이션을 들고 오는 것이 당연히 유리할 겁니다. 당연히 예제랑 비슷하게 행동하면 되니까요. 그렇기 때문에 이거를 좀 적절한 예제를 들고 오기 위해서 기본적으로 이 어 프롬프트 이 예시를 들고 온 거에서는 가장 현재 과제와 가장 유사한 예제를 샘플링합니다. 어 되게 그냥 간단하게 적혀 있는데 실제로는 뭔가 이것을 DPR이라든가 그런 뭔가 어 댄스 벡터 서치 아니면 뭔가 이 스파스 벡터 서치를 해도 되고요. 여튼 그런 형태의 뭔가 벡터 서치들이 있잖아요. 뭐 리트리버라는 형태 해가지고 디비가 있고 디비가 있고 유리가 쿼리가 있을 때 여기에서 뭔가 벡터 서치를 할 수 있잖아요. 저희가 아마 아마도 이전 강의에서 뭐 기계 도켓 쪽에서 아마도 이러한 리트리벌 기반 방법론들을 배웠을 건데 그런 것들을 활용해 가지고 가장 비슷한 예제들을 샘플링한다라고 생각하시면 좋을 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 10,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 983,
      "char_count": 1812
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c011_8c1990",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 어 되게 그냥 간단하게 적혀 있는데 실제로는 뭔가 이것을 DPR이라든가 그런 뭔가 어 댄스 벡터 서치 아니면 뭔가 이 스파스 벡터 서치를 해도 되고요. 여튼 그런 형태의 뭔가 벡터 서치들이 있잖아요. 뭐 리트리버라는 형태 해가지고 디비가 있고 디비가 있고 유리가 쿼리가 있을 때 여기에서 뭔가 벡터 서치를 할 수 있잖아요. 저희가 아마 아마도 이전 강의에서 뭐 기계 도켓 쪽에서 아마도 이러한 리트리벌 기반 방법론들을 배웠을 건데 그런 것들을 활용해 가지고 가장 비슷한 예제들을 샘플링한다라고 생각하시면 좋을 것 같습니다. 그래서 실제로 뽑힌 예시를 보면 우리가 이 에그 그러니까 달걀을 데워 가지고 싱크대에다가 넣는다라는 되게 좀 유사한 뭔가가 뽑혔다는 것을 확인해 볼 수 있죠. 그래서 여기서도 비슷하게 컴플리트 플랜으로 이렇게 쭉 들어가고 그러니까 지금 이 위에 있는 쿼리를 수행하기 위한 그런 이때까지 수행되었던 흔적들 넣고 그리고 비지블 오브젝트 오브젝트를 넣고 오브젝트들이 로봇 입장에서 보이는 오브젝트 그리고 그거에 대해서 실제로 그때 생성되었던 뭔가 예제들이 이렇게 들어가 있다는 것을 확인해 볼 수가 있습니다. 그래서 만약 이것이 처음 생성하는 경우 그러니까 로봇이 아무것도 없고 그냥 쿼리가 주어지는 경우에는 간단하게 그냥 테스크 데스크립션 테스크 데스크립션 시에서 당연히 여기다가 실제로 풀고 싶은 지리를 넣고 컴플리티드 플랜 같은 경우에는 당연히 지금 처음에는 처음에 수행한 게 없으니까 그냥 빈칸으로 넘겨주겠죠. 빈칸을 놔두고 비지블 오브젝트 DB 오브젝트 그래서 로봇이 지금 보이는 프롬프트 지금 보이는 오브젝트를 놓고 이제 넥스트 플랜을 하게 된다면 이제 모델이 실제로 이제 뭔가 플랜들을 생성할 겁니다. 예제들을 앞에 있는 이 예제들 예시들을 보고 뭔가 생성할 겁니다. 그래서 생성하다 보면 생성하고 나면 이것이 뭔가 컴플리트 된 뭔가 플랜이 나오겠죠. 하지만 이것은 실제로 이 환경을 보고 뭔가 만든 건 아니기 때문에 이제 로봇이 실제로 수행하다 보면 좀 틀리는 경우가 있을 거예요. 틀린다기보다는 절대 그 과제를 수행할 수 없는 경우도 발생할 겁니다. 앞서 봤던 것처럼 뭔가 내 눈앞에 감자가 감자를 주우러 왔는데 내 눈앞에 감자가 안 보이는 경우라든가 그런 경우가 발생할 수 있죠. 그래서 그때 실제 로봇들이 수행하다가 뭔가 그런 에러 사항이 발생하게 된다면 이제 컴플리트 플랜에다가 이때까지 했던 것을 넣고 비지블 오브젝트 넣고 넥스트 플랜을 다시 적어 달라 같은 식으로 이제 모델에게 다시 넣어주는 거죠. 그래서 이런 식으로 뭔가 태스크 데스크립션 해서 최종적으로 수행하는 과제 컴플리트 플랜 그러니까 뭔가 로봇이 뭔가 이상한 이상 현상을 맞닥뜨리기 전까지 수행되었던 계획 그리고 비지블 오브젝트 해서 지금 앞선 계획까지 수행했을 때 현재 스텝에서 보이는 것들 이런 식으로 프로포트화 해 가지고 이제 넥스트 플랜을 정하게 된다면 이제 실제로 로봇이 넥스트 플랜을 따라가면서 이제 뭐 예외가 없는 상황이 발생하지 않는다면 그냥 그 실행을 멈추고 그냥 이렇게 지리를 해결하는 그러한 모델을 만들 수가 있겠죠. 어 이렇게 하게 된다면 저희가 플랜을 한 번에 생성하기 때문에 이게 사실 엘엘엠을 생성하는 것 자체가 어느 정도 금액이잖아요. 당연히 이거 전기를 먹는 일이고 되게 비싼 행위이기 때문에 최대한 덜 호출하고도 그때그때마다 사실 당연히 뭔가 로봇을 호출하고 한 액션 할 때마다 로봇을 호출하고 LLM을 호출하고 할 수 있겠지만 그러지 않고 LLM을 한 번에 호출해 가서 쭉 플랜을 만들어 놓은 다음에 로봇이 예외 상황이 발생했을 때만 LLM의 도움을 받는 그런 형태도 형태로도 저희가 이렇게 활용할 수 있다는 것을 확인해 보시면 좋을 것 같습니다. 방금 전에 봤던 것처럼 단순히 로봇뿐만 아니고 뭔가 이렇게 게임에서도 당연히 활용하는 경우도 있는데요. 지금 보시면 알겠지만 이 마인크래프트 게임에서 뭔가 특정 물체를 제작하기 위해서 LLM이 플래닝을 수행하는 것들을 볼 수가 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 11,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1099,
      "char_count": 2023
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c012_2f8060",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 방금 전에 봤던 것처럼 단순히 로봇뿐만 아니고 뭔가 이렇게 게임에서도 당연히 활용하는 경우도 있는데요. 지금 보시면 알겠지만 이 마인크래프트 게임에서 뭔가 특정 물체를 제작하기 위해서 LLM이 플래닝을 수행하는 것들을 볼 수가 있습니다. 그래서 이거는 제가 뭔가 아주 깊게 설명하지는 않을 건데요. 지금 보시는 것처럼 뭔가 저희가 다이아몬드라는 것을 뭔가 만들기 위해서 그것을 어떻게 플래닝 할까를 LNM이 플래닝을 하고 실제로 이제 작은 로봇이라든가 이것을 작은 테스크를 풀 수 있는 것들을 호출해 가지고 그것을 실제로 셀렉트를 해서 실행시켜서 뭔가 이렇게 실제로 돌아가는 것이 컴퓨터가 LM 혼자서 그러니까 실행시켜서 되게 복잡한 과제를 이렇게 다이아몬드를 만드는 복잡한 과제를 풀 수 있는 것을 저희가 확인해 볼 수가 있습니다. 네 그 외에도 또 흥미로운 테스크로는 앞서 봤던 것처럼 게임에서 활용되는 경우도 많이 있긴 한데 어 좀 더 흥미로운 케이스로는 뭔가 이렇게 생각한다면 아 엘램 에이전트 자체가 이렇게 하나의 객체로서 행동할 수 있으니까 이것 자체를 뭔가 엮어가지고 인간 사회 자체를 뭔가 시뮬레이션을 할 수 있지 않을까 뭐 이런 생각을 해볼 수가 있을 겁니다. 그래서 여기에 대한 논문이 이 제너레이티브 에이전트라는 논문인데요. 지금 보시면 알겠지만 이거 에이전트가 제 기억에는 아 이게 25개의 에이전트로 구성되어 있습니다. 그래서 하나하나가 이제 또 그 뭔가 그 캐릭터를 행동하게 하는 거죠. 그래서 방금 전에 봤던 것처럼 이 25개의 에이전트가 존재하고 에이전트에다가 이제 저희가 페르소나를 입혀 놓은 겁니다. 아 페르소나를 아마 모르시는 분들도 있을 건데 아마도 이건 해봤을 겁니다. 챗티피티에게 너는 너는 뭔가 헬스 트레이너 헬스 트레이너야 이 너야 해 놓은 다음에 여기다가 뭔가 헬스 트레이너처럼 행동해라고 질의를 날리게 된다면 실제로 여러분이 대화를 해보면 뭔가 헬스 트레이너스러운 뭔가 대화를 나눌 수가 있잖아요. 이런 식으로 뭔가 성격을 입히는 것을 페르소나 학술적으로 페르소나를 입힌다라고 얘기를 많이 합니다. 그래서 이 페르소나를 25 에이전트들을 따로 만든 다음에 에이전트들은 당연히 어디 뭐 뭔가 클래스에 간다든가 아니면 뭔가 요리를 한다든가 아니면 대화를 나눈다든가 뭐 이런 식으로 해볼 수가 있을 건데 이런 식으로 해서 저희가 에이전트를 25개를 만든 다음에 그리고 서로서로 대화할 수 있게끔 당연히 이것은 뭐 채팅도 당연히 채팅 피티끼리도 대화할 수 있을 거잖아요. 이런 식으로 하게 된다면 뭔가 인간의 사회도 뭔가 어느 정도 시뮬레이션이 가능할 수도 있겠다라는 가능성을 보여준 논문이라고 생각하시면 좋을 것 같아요. 이것 자체가 돌아가는 이유는 LM 자체가 사실 사전 학습 중에 인간의 다양한 행동과 반응을 당연히 학습했을 거잖아요. 사전 학습 데이터 자체가 되게 크다 보니까 되게 다양한 케이스에 대한 데이터가 학습되었을 거고 그 결과 인간 사회 자체를 사회 시뮬레이션 해 가지고 저희가 이것을 뭔가 연구로도 활용할 수 있겠다 뭐 그런 얘기도 나오기도 합니다. 그래서 뭔가 각 에이전트가 상황 인식을 하고 주변에 뭐가 있는지 그리고 기존 기억이랑 뭔가 과거 계획을 통해 가지고 뭔가 계획을 생성하거나 혹은 행동을 수행해 가지고 실제로 이것이 어느 정도 뭔가 생일 파티가 열렸을 때 서로서로 생일 축하를 한다든가 이런 것이 가능 실제로 이렇게 돌아간다는 것을 좀 확인을 했었습니다. 그래서 예시로 본다면 뭔가 가장인 존이 지금 보시면 알겠지만 6시에 일어나 가지고 아침 루틴을 하고 뭔가 그의 가족들을 챙기면서 출근하는 것도 이렇게 행동을 돌아가는 것도 아 이렇게 에이전트를 통해 가지고 인간 사회를 묘사하는 것도 어느 정도 가능하다라는 것을 좀 확인하였습니다. 네 이렇게까지 가능하다면 사실 플레닝 자체가 정말 대단한 기술이라고 생각할 수 있는데요. 다만 이걸 하기 전에 좀 몇 가지 고려해 볼 만한 사항이 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 12,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1082,
      "char_count": 1990
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c013_b56abc",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그래서 예시로 본다면 뭔가 가장인 존이 지금 보시면 알겠지만 6시에 일어나 가지고 아침 루틴을 하고 뭔가 그의 가족들을 챙기면서 출근하는 것도 이렇게 행동을 돌아가는 것도 아 이렇게 에이전트를 통해 가지고 인간 사회를 묘사하는 것도 어느 정도 가능하다라는 것을 좀 확인하였습니다. 네 이렇게까지 가능하다면 사실 플레닝 자체가 정말 대단한 기술이라고 생각할 수 있는데요. 다만 이걸 하기 전에 좀 몇 가지 고려해 볼 만한 사항이 있습니다. 그래서 첫 번째는 우선은 다운 스트림 테이터가 충분하다면 그러니까 시나리오라고 해야 될까요? 방금 전에 봤던 로봇을 움직이는 예시라든가 아니면 뭔가 사용자가 인터랙티브하는 인터랙티브하게 뭔가 하는 예시들이 꽤 많아가지고 한 몇만 개 이상 있다 할 경우에는 기존 RL 그러니까 레인 볼스먼트 러닝 강화학습 같은 뭔가 강화 학습이라든가 아니면 기존 지도 학습 방법론들 슈퍼바이즈 파인튜닝 방법론들이 더 우월할 수 있습니다. 그래서 실제로 어 리스닝 플레이닝 쪽에서 얘기했던 것처럼 슈퍼바이스드 파인튜닝 한 모델들이 더 우월할 수 있고요. 그게 만약 플래닝 시나리오가 없다던가 그러니까 그러한 프로바이스 데이터셋이 없다던가 뭔가 부족할 경우에는 LLM이 또 하나의 대안이 될 수 있다 정도만 이해하시면 좋을 것 같습니다. 그래서 만약 한 만 개 이상의 데이터가 있다 할 경우에는 이것이 항상 꼭 LM이 필요한 건 아니고요. 이것이 지도 학습 방법론이랑 좀 비교가 필요합니다. 그리고 어 또 하나 데이터의 특성이 또 중요한 부분 중 하나일 것 같은데 뭔가 LLM 자체가 당연히 자연어에 대해서 되게 집중적으로 학습된 모델이다 보니까 데이터가 자연어 형태인 경우에는 LLM이 더 큰 효과를 발휘할 수 있겠다라고 생각하시면 좋을 것 같습니다. 또 앞서 얘기했던 것처럼 LLM을 뭔가 로봇을 움직일 때 뭔가 예상치 못한 상황에 대해서 뭔가 해결한다든가 혹은 앞선 제네리티브 에이전트의 예시처럼 뭔가 인간 사회를 학습시켰기 때문에 뭔가 일반적이지 않은 상황에 맞닥뜨렸을 때 이러한 아웃 오브 디스트리뷰션 상황에서는 LNM 자체가 좀 효과적일 수 있다고 생각을 합니다. 왜냐하면 결국에는 사전 학습 과정 중에서 다양한 데이터와 다양한 상황을 학습해 왔기 때문에 물을 엎질렀네 아 그럼 닦아야 되겠다 누가 저기 지나간다 그럼 얘기해 봐야 되겠다 이러한 AR 디스트리뷰션 학습 데이터에 없었던 그러한 상황에서는 LLM이 기존 퍼바이즈 바이 튜닝 모델보다 더욱 좋을 수가 있습니다. 그래서 기존 모델들 기존 지도 학습 방법론 같은 경우에는 결국에는 이 다운스트림 데이터가 모든 케이스를 다 커버하지 않으면 모든 케이스를 커버해야지만 결국에는 그런 ROV 디스트리션 상황이 발생했을 때 해결할 수 있었지만 어 예 같은 경우에는 그렇지 않고 조금 그 다운스트림 데이터가 모든 것을 커버하지 않더라도 자기 자신의 사전 지식 프라이어 오리지를 통해 가지고 이런 것을 해결할 수 있다는 점을 생각해 볼 수가 있습니다. 결국에는 이 리즈닝 플레이닝에서 얘기했던 거랑 또 비슷한 이야기인데 코드 데이터로 학습된 LLM이 일반 텍스트로 학습된 모델보다 플래닝 정확도가 당연히 우월한 편이고요. 또 플래닝 데이터로 파인 튜닝 된 LLM이 인컨덱스 러닝 LLM보다 우월합니다. 그러니까 제가 앞쪽에서 리액트 얘기를 하면서 모델 데이터가 충분한 데이터가 좀 한 3천 개 정도 있다면 우리가 인컨텍스 러닝 하는 것보다 파인 튜닝 하는 경우가 더 좋다고 얘기했었는데 이거는 플래닝에서도 똑같은 얘기가 적용이 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 13,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 965,
      "char_count": 1790
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c014_529301",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 결국에는 이 리즈닝 플레이닝에서 얘기했던 거랑 또 비슷한 이야기인데 코드 데이터로 학습된 LLM이 일반 텍스트로 학습된 모델보다 플래닝 정확도가 당연히 우월한 편이고요. 또 플래닝 데이터로 파인 튜닝 된 LLM이 인컨덱스 러닝 LLM보다 우월합니다. 그러니까 제가 앞쪽에서 리액트 얘기를 하면서 모델 데이터가 충분한 데이터가 좀 한 3천 개 정도 있다면 우리가 인컨텍스 러닝 하는 것보다 파인 튜닝 하는 경우가 더 좋다고 얘기했었는데 이거는 플래닝에서도 똑같은 얘기가 적용이 됩니다. 그래서 뭔가 이렇게 시나리오를 통해 가지고 뭔가 학습시킬 경우에는 뭔가 파인 튜닝된 LLM이 기존 인컨덱스 러닝보다 더 우울한 편이 있고요. 다만 그 경우에는 좀 고민해 볼 만한 점이 이 결국엔 파인튜닝 할 경우에는 이것도 결국엔 데이터셋이 필요할 거잖아요. 그래서 거의 한 1k 이상 데이터를 요구할 건데 이 경우에는 이게 실제로는 어 만약 한 10k 이상 된다면 지도 학습 방법론이랑 비교가 필요한데 1k 이상 한 경우에 이거 1k 이상 데이터 한 천 개 이상의 데이터가 있을 경우에는 이 경우에도 이게 테스크 바이 테스크로 결국에는 제가 임의로 이 1k 이상 필요하다 10k 이상 필요하다는 것은 결국에는 이거 테스크 바이 테스크고 이게 되게 임의의 수치이기 때문에 실제로 이 정도 데이터가 있다면 이것도 한번 슈퍼바이즈 데이터 셋 슈퍼바이스트 모델들과 비교해 가지고 어느 것이 성능이 좋은지 한번 좀 비교를 해볼 필요가 있습니다. 그래서 일반적으로는 데이터가 거의 없거나 진짜 조금만 있을 경우에는 인컨텍스 러닝 기반 LLM을 쓰는 것을 권장하고요. 아 이렇게 데이터가 충분할 경우에는 기존 지도 학습 방법론과 좀 비교를 해볼 필요가 있습니다. 또 하나 고민해 볼 만한 지점은 엘엠은 실제로 리즈닝과 플래닝을 할까에 대한 지점입니다. 이게 되게 논란이 많은 주제인데요. 생각해 보시면 엘엘엠에는 이게 리스닝이라든가 사고 과정을 하는 모듈 자체가 뭔가 따로 달려 있지 않습니다. 뭔가 체인 오브 소트를 통해 가지고 모사를 하긴 하는데 그걸 위한 뭔가 모듈이 정해져 있지 않다는 거죠. 그리고 우선 기본적으로 결국에는 엘엘엠도 카우즈널 랭귀지 모델링 결국에는 다음 단어를 예측하는 모델이다 보니까 이것이 모델 입장에서는 실제로 미래에 어떤 일이 벌어질지 모르고 있을 가능성이 되게 클 가능성이 있습니다. 물론 그렇지 않을 수도 있긴 한데 그것도 확실치가 않다는 거죠. 그래서 실제로 우리의 심층 학습 그래서 되게 유명한 무신 양래쿤 교수님께서는 이렇게 얘기를 하십니다. LLM은 플래닝을 할 수 없다라고 얘기합니다. 그래서 실제로 이 양래쿤 교수님 이 관련 강의를 한 번 진행한 적이 있는데 그래서 실제로 이 교수님이 생각하기에는 LLM 자체가 잘하는 것과 못하는 것이 있는데 이 오토리 그레시 LLM은 결국에는 글쓰기 가 아니면 초안 쓰기 코딩 돕기 같은 데는 도움이 될 수 있지만 어 뭔가 사실적이고 일관적인 대답을 한다든가 뭔가 적절히 행동한다든가 뭔가 최근 정보를 다룬다든가 리스닝 플래닝 매스 수학하는 거 도구 다루는 거 이런 것들은 못한다라고 얘기를 합니다. 왜냐하면 이건 그냥 단순히 다음 단어를 예측하는 모델일 뿐 뭔가 미래를 생각해서만 행동하는 모델은 아니다라고 얘기를 하는 것이죠. 그리고 결국에는 우리는 이 LLM 자체가 말을 잘하니까 그냥 거기에 속고 있을 뿐이지 LLM은 실제로 이게 미래가 어떻게 들어갈지 내가 이 단어를 생성함으로써 어떤 미래가 결정될지를 정확하게 알고 있지 않다 라고 얘기를 합니다. 이렇게 양래훈 교수님께서 설명하신 이 약점들이 실제로 엘엔엠의 중요한 약점들 중 하나인데요. 네 그래서 지금부터는 이 엘엠이 가진 한계에 대해서 좀 간략하게 소개해 드리도록 하겠습니다. 이 엘엠이 가진 가장 대표적인 문제점 중 하나가 이 할루시네이션 한국어로는 환각 현상인데요. 어 좀 다르게 표현을 하자면 그냥 엘레엠이 헛소리를 많이 한다라고 이해하시면 좋을 것 같습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 14,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1076,
      "char_count": 2004
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c015_b6e9c2",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 이렇게 양래훈 교수님께서 설명하신 이 약점들이 실제로 엘엔엠의 중요한 약점들 중 하나인데요. 네 그래서 지금부터는 이 엘엠이 가진 한계에 대해서 좀 간략하게 소개해 드리도록 하겠습니다. 이 엘엠이 가진 가장 대표적인 문제점 중 하나가 이 할루시네이션 한국어로는 환각 현상인데요. 어 좀 다르게 표현을 하자면 그냥 엘레엠이 헛소리를 많이 한다라고 이해하시면 좋을 것 같습니다. 아마도 여러분들도 챗치피티하고 대화하다 보면 이런 대화 결과를 본 적이 있을 거예요. 실제로 뉴스도 몇 번 났었죠 세종대왕이 세종대왕의 맥북 터진 사건에 대해서 알려줘 하게 된다면 당연히 이런 건 없잖아요. 당연히 채집pt에는 그런 거 없어요라고 얘기하는데 실제로 생성되는 걸 보면 실제로 던진 이야기가 막 생성된다는 것을 확인한 것을 볼 수가 있습니다. 즉 존재하지 않는 이야기들을 생성하는 것을 이렇게 만들어내고 실제로 뭔가 엉뚱하게 뭐가 대동여지도 연금 술사 폭동에 대해서 자세히 알려줘 하게 된다면 이러한 질의 자체가 사실인지 아닌지에 대해서는 잘 모르고 그냥 이거에 관련된 그럴듯한 텍스트를 생성하는 것을 확인해 볼 수가 있죠. 이러한 할루시네이션 문제는 단순히 뭔가 소설 지을 때는 뭔가 도움이 될 수 있을지 모르겠지만 이것이 뭔가 사실적인 대답을 한다든가 그러한 팩트 체크가 필요한 경우에는 되게 크게 약점으로 작용할 수가 있습니다. 또 그 중요한 문제 중 하나는 LM 자체가 장 다음 단어를 예측하는 모델이지 이게 뭔가 실제로 이 발언의 파급력에 대해서 생각하는 모델은 아니기 때문에 대개 이 톡시티 문제가 아주 중요한 문제 중 하나입니다. 예를 들어 LLM에게 코카인 만드는 방법을 알려줘 하게 된다면 당연히 이것은 말하면 안 되겠죠. 상용 프로그램에서 코카인 만드는 방법 마약 만드는 방법 이런 것들을 알려주면 안 될 겁니다. 그런데 이것들을 말을 뭔가 사전 학습 데이터에 있던 것을 기반으로 하다 보니까 이것을 말을 하게 된다든가 혹은 뭔가 되게 그걸 톡식한 텍스트를 받았을 때 예를 들 톡식하지 않은 텍스트를 받았을 때 뭔가 톡식한 텍스트를 받는다든가 예를 들어 아 내가 생각하기에는 그녀는 정말 하고 나서 이러한 프로포트 줬을 때 모델이 막 생성하다 보면 되게 톡식한 부적절하고 상용 프로그램에서는 할 수 말하면 안 되는 그러한 단어들을 말할 수도 있을 겁니다. 그래서 이러한 t시티 그러니까 독성들을 줄이는 것도 이 이 독성들이 분명히 이 엘르에 존재하고 이런 것들을 어떻게 억제할까도 아주 중요한 문제 중 하나입니다. 또한 이런 독성 외에도 또 엘엠에 대해서 좀 고민해 볼 만한 점이 이 편견 부분입니다. 그래서 이 바이러스 학습 데이터에 존재하는 바이어스 이걸 편견이라고 생각하면 좋을 것 같은데 그 일반적으로 여러분이 생각하는 뭔가 그 뭐 성별 관련 편견이라든가 지역 관련 편견 편견 혹은 뭐 국가 관련 편견 같은 걸 생각해 보시면 좋을 것 같아요. 예를 들어 이건 실제로 제가 생성해 본 예시인데요. 휠체어에 탄 사람은 이 다음 슈트 한 다음에 이 뒤에 생성해 보면 풀 파티에 참석할 수 없어 해가지고 이것도 사실 편견이죠. 휠체어 탄 사람도 분명히 풀 파티에 참석할 수 있고 이것을 이렇게 확답을 내리는 것은 되게 사실 옳지 않다라고 생각할 수 있습니다. 그리고 상용 프로그램이다 보니까 이러한 편견 자체에 대해서 되게 자유로웠고 이것을 되게 적절하게 대처를 할 수 있어야 되는데요. 이러한 엘엘엠은 사실 이러한 적절한 때 적절하게 행동하는 것에 대해서 되게 약한 지점이 있기 때문에 잘 안 돌아가는 이런 것을 어떻게 하면 편견을 없애고 엘램 자체가 적절하게 대답할 수 있는가가 아주 중요한 문제 중 하나입니다. 네 또 중요한 문제 중 하나가 이 프라이버시 인비전 이 개인 정보 침해에 대한 문제인데요. 아마 이거 뉴스는 아마 들어보신 분들도 있을 겁니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 15,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 1045,
      "char_count": 1934
    },
    {
      "id": "transcript_nlp_nlp_recent_trends_part_2_llm_a_c016_1a90e3",
      "content": "[NLP] NLP Recent Trends Part 2 LLM Applications\n\n다. 그리고 상용 프로그램이다 보니까 이러한 편견 자체에 대해서 되게 자유로웠고 이것을 되게 적절하게 대처를 할 수 있어야 되는데요. 이러한 엘엘엠은 사실 이러한 적절한 때 적절하게 행동하는 것에 대해서 되게 약한 지점이 있기 때문에 잘 안 돌아가는 이런 것을 어떻게 하면 편견을 없애고 엘램 자체가 적절하게 대답할 수 있는가가 아주 중요한 문제 중 하나입니다. 네 또 중요한 문제 중 하나가 이 프라이버시 인비전 이 개인 정보 침해에 대한 문제인데요. 아마 이거 뉴스는 아마 들어보신 분들도 있을 겁니다. 그래서 실제로 모델 자체가 이것이 개인 정보 기반으로 뭔가 학습되다 보니까 뭔가 주소를 물으면 어떠한 주소가 나오게 되는데 이것 자체가 실제로 사용자의 뭔가 혹은 사전 학습 데이터에서 있었던 그러한 데이터일 수 있고 이런 것들을 뭔가 말하면 안 됨에도 불구하고 l 자체가 단순히 다음 단어를 예측하고 얘기하다 보니까 실제로 이렇게 주소를 주게 된다면 존재하지 않는 주소를 쓰면 아니면 뭔가 사전 학습 과정에서 학습된 그런 주소들을 뱉는 그러한 현상들을 확인해 볼 수가 있습니다. 그래서 실제로 뭔가 gpt2에서 뭔가 이렇게 프레픽스를 주게 된다면 뭔가 어떤 실제 사용자의 뉴스 사용자의 뭔가 개인 정보라든가 이런 것이 나올 수 있기 때문에 개인 정보를 어떻게 하면 안 학습하게 하고 그리고 이것들을 어떻게 하면 개인정보 생성을 막을 수 있을까도 되게 중요한 주제 중 하나입니다. 어 엘엔엠의 가장 큰 단점 중 하나가 바로 이것이 아웃 롤리지가 아웃데이트 된다라는 것이 가장 중요한 문제 중 하나일 겁니다. 결국에는 엘엔엠 자체가 이렇게 고정된 데이터에 대해서 학습되었기 때문에 모델이 이게 시간이 지나면서 이게 새로운 지식을 학습하지 못합니다. 즉 사전 학습하고 나서 이것 자체가 모델 자체가 뭔가 얼라인먼트 맞추고 나서 이것이 고정된 그대로라는 거죠. 실제로 챗gpt한테 현재의 코비드의 뭔가 가장 코비드 그러니까 코로나 바이러스 그러니까 그 도미넌트한 코미넌트 바이러스는 무엇이냐라고 물어보게 된다면 그 7피티 자체가 아 내는 나는 2021년 10월 기준으로 돼 있기 때문에 정확하지 않다라고 얘기하면서 이야기를 시작하는 것을 확인해 볼 수 있죠. 그리고 실제로 저희가 좀 이걸로 생성을 해보게 된다면 되게 그때그때마다 다른 답안이 나오고 있어 가지고 이것을 실제로 과거 정보에 대해서 기반하여 이것을 텍스트를 생성하고 있다는 것을 확인해 볼 수가 있습니다. 따라서 이렇게 시간에 따라 변하는 지식을 질문했을 때 틀린 대답을 할 확률이 높기 때문에 이런 것들을 어떻게 정보를 업데이트할까도 되게 이 엘엘엠에서 중요한 문제 중 하나라고 생각하시면 좋을 것 같습니다. 여기까지 해서 LLM의 응용 프로그램 예시들과 그리고 LLM의 한계에 대해서 알아보았고요. 다음 그 시간에는 이 LLM의 한계들을 어떻게 하면 완화시키고 그리고 어떻게 하면 측정할 수 있는지 그런 거에 대해서 한번 얘기해 보도록 하겠습니다. 네 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "NLP Recent Trends Part 2 LLM Applications.json",
        "lecture_name": "NLP Recent Trends Part 2 LLM Applications",
        "course": "NLP",
        "lecture_num": "",
        "lecture_title": "NLP Recent Trends Part 2 LLM Applications",
        "chunk_idx": 16,
        "total_chunks": 17,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:aefed295bdfc59533ec2e40895435e96128de376ae0b4292bc139939e0544df9"
      },
      "token_estimate": 818,
      "char_count": 1522
    }
  ]
}