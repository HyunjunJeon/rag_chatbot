{
  "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
  "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
  "course": "RecSys 이론",
  "total_chunks": 10,
  "chunks": [
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c000_d0d929",
      "content": "[강의 녹취록] 과목: RecSys 이론 | 강의: 2강 | 제목: 추천 시스템 Basic 2\n\n안녕하세요. 이번 시간은 추천 시스템의 기초를 배워보는 추천 시스템 베이직 두 번째 강좌입니다. 저는 강의를 맡은 강사 이준원입니다. 먼저 앞으로 배울 9개의 이론 강의에서 다루게 될 추천 시스템의 다양한 기법들이 무엇이 있는지 간단하게 살펴보고 나서 오늘 다룰 내용은 크게 두 가지입니다. 먼저 추천 기법 가운데 가장 오래되고 기초적인 방법인 연관 분석을 학습합니다. 그리고 텍스트 데이터를 활용한 콘텐츠 기반 추천인 tfidf 기반 추천에 대해서 다루겠습니다. 이 두 가지 내용 연관 분석과 tfidf 기반 추천은 보통 학교나 대학원에서 추천 시스템을 처음 입문할 때 배우게 되는 내용입니다. 3강부터 쭉 배우게 되는 컬래버레이터 필터링 모델이 추천 시스템에서 가장 많이 사용되는 모델이긴 하지만 이론을 탄탄히 하는 측면에서 오늘 내용도 여러분들이 반드시 한 번쯤은 배우고 넘어가야 하는 내용입니다. 먼저 앞으로의 강의 동안 학습하게 될 추천 시스템의 기법의 전체 개요에 대해서 간단하게 살펴보겠습니다. 추천 이론 강의에서 배우게 될 내용은 다음과 같습니다. 먼저 지난 강의에서 가장 기본적인 인기도 기반 추천 에 대해서 배웠는데요. 이번 시간에는 연관 분석과 콘텐츠 기반 추천에 대해서 개념에 대해 다루게 되겠습니다. 그리고 3강 4강에서는 추천 시스템에서 가장 중요한 개념인 컬라버레이트 필터링을 배우게 됩니다. 그리고 5강에서 워드 투백을 응용한 아이템 투 백 레코멘데이션을 학습합니다. 이제 강의 개요를 보시면 한 가지 의문점이 생길 수도 있는데요. 딥러닝 관련 추천 모델을 6강부터 배우게 된다는 것입니다. 사실 부스트캠프 강의에서는 딥러닝에 관련된 내용을 주로 학습하지만 추천 시스템이라는 분야는 다른 컴퓨터 비전이나 자연어 처리와는 다르게 두 가지 이슈로 인해서 클래식한 머신러닝 기법을 여전히 많이 사용합니다. 첫 번째 이유로는 추천 시스템 분야는 물론 딥러닝 모델이 굉장히 좋은 성능을 내긴 하지만 다른 컴퓨터 비전이나 자연어 처리 분야에 비해서 머신 러닝 모델로부터 딥러닝 모델로 향상된 그 향상의 폭이 아주 드라마틱하지 않습니다. 또한 두 번째로 더 중요한 이유는 추천 시스템은 실제로 현업에서 많이 사용되는데 특히 많은 유저가 사용하는 서비스에서 큰 트래픽을 감당해야 하는 경우에 사용되는데요. 이때 모델의 인퍼런스 타임, 즉 모델을 서빙하는 레이턴시가 중요합니다. 상대적으로 딥러닝 모델이 무거운데요. 계산량도 아주 많죠 그에 비해 아주 뛰어난 성능을 보이지 않는 한 딥러닝 모델을 굳이 사용할 이유가 없습니다. 그렇기 때문에 여전히 클래식한 머신러닝 모델도 많이 사용하고 그래서 저희가 1강부터 5강에 이런 클래식한 머신러닝 모델을 많이 배우게 되는 것입니다. 이제 이러한 이유로 추천 시스템이라는 분야는 연구적으로 볼 때는 물론 딥러닝 모델을 사용하는 모델이 요즘엔 가장 뛰어난 성능을 보이고 많이 활용되기 시작했지만 아직도 현업에서는 다양한 엔지니어링과 응용이 더 중요한 경우가 많습니다. 참고로 이 8강과 10강에서 컨텍스트 어어 레코멘데이션이나 멀티 암드 밴딧에서 다루는 내용도 마찬가지로 딥러닝이 등장하기 이전 시대에 등장하는 기법을 배우게 되는데요. 이제 이러한 배경이 있다는 것을 염두에 두시고 앞으로의 강의를 수강하시면 도움이 될 것입니다. 네 그래서 처음 소개했던 추천 시스템의 가장 고전적인 방법론인 연관 분석에 대해서 학습합니다. 이 연관 분석에서 연관 규칙과 그 규칙을 탐색하는 알고리즘에 대해서 배워보겠습니다. 다음은 연관 규칙 분석입니다. 어소시에이션 룰 어널러시스 혹은 어소시에이션 롤 마이닝이라고 불리는데요. 흔히 장바구니 분석 혹은 서열 분석이라고 불립니다. 장바구니 분석이라고 불리는 이유는 소비자가 마트에 갔을 때 하나의 장바구니 안에 어떤 물건을 같이 담는지 그 물건 사이의 관계나 규칙을 찾고자 하는 접근에서 시작했기 때문인데요. 유저가 상품을 구매하는 하나의 연속된 거래 안에서 그 거래들 안에 있는 아이템 사이의 규칙을 발견하기 위해 적용하는 방법입니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 0,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1112,
      "char_count": 2025
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c001_2fd4a1",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 다음은 연관 규칙 분석입니다. 어소시에이션 룰 어널러시스 혹은 어소시에이션 롤 마이닝이라고 불리는데요. 흔히 장바구니 분석 혹은 서열 분석이라고 불립니다. 장바구니 분석이라고 불리는 이유는 소비자가 마트에 갔을 때 하나의 장바구니 안에 어떤 물건을 같이 담는지 그 물건 사이의 관계나 규칙을 찾고자 하는 접근에서 시작했기 때문인데요. 유저가 상품을 구매하는 하나의 연속된 거래 안에서 그 거래들 안에 있는 아이템 사이의 규칙을 발견하기 위해 적용하는 방법입니다. 예를 들면은 하나의 장바구니 안에서 맥주와 기저귀를 같이 구매하는 빈도가 얼마나 잦은지 혹은 컴퓨터를 산 고객이 같이 사는 상품이 무엇인지의 규칙을 찾는 것입니다. 네 그래서 이 주어진 트랜잭션 데이터에서 하나의 상품이 등장했을 때 같이 등장하는 다른 상품이 무엇인지를 찾는 규칙을 연관 규칙 분석이라고 볼 수 있는데요. 여기 왼쪽 밑에 있는 데이터를 봅시다. 간단한 데이터인데요. 총 5개의 트랜잭션 5개의 장바구니 데이터가 있고요. 각각의 데이터는 다음과 같이 아이템으로 이루어져 있습니다. 이 아이템은 2개 이상 1개 이상의 아이템이 집합으로 구성되어 있는데요. 이제 이 트랜잭션 데이터에서 연관 규칙을 다음에 오른쪽의 예시와 같이 찾는 것입니다. 기저귀를 샀을 때 맥주도 같이 산다라는 연관 규칙, 혹은 우유와 빵을 샀을 때 계란과 콜라를 같이 산다라는 연관 규칙 이 연관 규칙의 예시가 되는데요. 여기에 나와 있는 이 화살표는 인과관계가 아니라 이 규칙을 기술하는 하나의 어노테이션이지 맥주와 빵을 샀기 때문에 우유를 산다라는 인과관계를 의미하지는 않습니다. 방금 예시를 보신 것처럼 연관 규칙에서는 규칙과 연관 규칙이라는 것을 정의해야 하는데요. 먼저 규칙은 if 컨디션 dn 리졸트 그래서 어떤 컨디션에 해당하는 물건 혹은 아이템이 있을 때 리졸트에 해당하는 아이템이 같이 등장한다라는 규칙으로 표현합니다. 이런 다양한 규칙 가운데 어떤 두 사건 혹은 이 엔티시던트와 컨 컨시커던트라는 각각의 아이템 집합이 빈번하게 발생하는 어떤 프레시 홀드를 정하게 되고요. 그랬을 때 규칙 가운데 일부 기준을 만족하는 것이 연관 규칙이 됩니다. 그래서 빈번하게 발생하는 어떤 규칙을 의미합니다. 그래서 연관 규칙을 구성할 때는 엔티시던트와 컨셉퀀트가 필요하고 이를 구성하는 상품의 집합을 아이템 셋이라고 합니다. 이제 아이템 셋은 방금 언급한 것처럼 1개 이상의 집합으로 이루어져 있습니다. 그래서 이 밑에 있는 규칙을 보시면 빵과 버터라고 이루어진 집합 이 앤티시던트 그리고 컨시퀀트가 우유인데요. 이 규칙에서 특징은 앞에 있는 앤티시던트와 컨시퀀트가 서로 소를 만족해야 합니다. 즉 서로 겹치는 아이템이 없어야 한다는 것이죠. 이게 규칙의 기본적인 구성 룰입니다. 네 다음은 아이템 셋과 이 아이템 셋 가운데 트랜잭션 데이터에 자주 등장하는 페이퀀트 아이템 셋 빈발 집합에 대해 정의해 봅시다. 먼저 아이템 셋은 아까 전에 예시에서 언급했듯이 1개 이상의 아이템으로 이루어진 집합을 의미합니다. 그리고 k 아이템 셋은 그 아이템 셋의 집합 아이템의 개수가 케인 것을 의미하죠. 그래서 각각의 트랜잭션 데이터는 케 아이템 셋으로 이루어져 있습니다. 2개짜리 아이템 셋, 다섯개짜리 아이템 셋, 4개짜리 아이템 셋과 같은 케이 아이템 셋이 하나하나의 트랜잭션 데이터가 되고요. 이제 여기서 처음으로 등장하는 서포트라는 개념이 있습니다. 이 서포트는 전체 트랜잭션 데이터 가운데서 특정 아이템 셋이 등장하는 횟수를 의미합니다. 빵과 우유라는 집합의 서포트 카운트를 구해보면은 빵과 우유가 등장하는 게 총 하나 둘 셋 그렇기 때문에 서포트 카운트는 3이 되고요. 이 서포트 카운트를 이용해서 최종적으로 서포트를 계산합니다. 이 서포트가 연관 규칙에서 제일 중요한 개념 중의 하나입니다. 아이템 셋이 전체 트랜잭션에 등장하는 비율을 의미합니다. 그래서 위에서 구한 서포트 카운트를 전체 트랜잭션 데이터의 개수인 5로 나눠주는 0.6이 서포트가 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 1,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1100,
      "char_count": 2010
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c002_e7ba05",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 이 서포트는 전체 트랜잭션 데이터 가운데서 특정 아이템 셋이 등장하는 횟수를 의미합니다. 빵과 우유라는 집합의 서포트 카운트를 구해보면은 빵과 우유가 등장하는 게 총 하나 둘 셋 그렇기 때문에 서포트 카운트는 3이 되고요. 이 서포트 카운트를 이용해서 최종적으로 서포트를 계산합니다. 이 서포트가 연관 규칙에서 제일 중요한 개념 중의 하나입니다. 아이템 셋이 전체 트랜잭션에 등장하는 비율을 의미합니다. 그래서 위에서 구한 서포트 카운트를 전체 트랜잭션 데이터의 개수인 5로 나눠주는 0.6이 서포트가 됩니다. 그래서 각각의 아이템 셋에 대해서 모두 서포트를 계산할 수 있는데요. 이제 여기서 프리퀀트 아이템 셋이란 모든 아이템 셋 가운데 유저 이 알고리즘을 만드는 저희 저희가 지정한 어떤 미니멈 서포트 값 이상의 모든 아이템 셋을 의미합니다. 자 이제 다음과 같은 규칙이 있을 때 규칙이 같이 가질 수 있는 대표적인 척도 매트릭은 대표적으로 세 가지가 있습니다. 방금 언급했던 서포트와 컴피던스 리프트입니다. 먼저 x에서 y로 가는 하나의 규칙이 있다고 가정을 하고 x와 y는 아이템 셋이 될 것이고요. 이 n은 전체 트랜잭션 데이터의 개수 아까 토이 데이터에서는 5가 되겠죠. 서포트는 방금 전 슬라이드에서 다뤘듯이 전체 트랜잭션에서 아이템 셋이 등장하는 비율을 의미합니다. 서포트는 개별 아이템 셋 하나 x에 대해서도 정의할 수 있고 스에서 와로 가는 연관 규칙에 대해서도 정의할 수 있습니다. 그래서 스에 대한 서포트는 전체 트랜잭션을 분모로 두고 스 아이템 셋이 등장하는 트랜잭션의 개수가 되고요. 이 스에서 와로 가는 연관 규칙의 서포트는 분모는 마찬가지로 전체 트랜잭션 데이터이고요. 분자는 x와 y의 합집합 즉 x y가 모두 등장하는 트랜잭션 데이터를 분자에 두게 되겠죠. 그래서 이걸 확률로 계산하게 되면 확률은 x가 등장할 확률이 되고요. x에서 y로 가는 서포트는 x와 y가 동시에 모두 등장하는 확률이 됩니다. 그래서 항상 이 서포트 x 값은 서포트 스에서 와로 가는 연관 규칙보다 항상 크거나 같게 됩니다. 다음은 컴피던스입니다. 컴피던스는 직관적으로 설명하면 조건부 확률입니다. 스가 포함된 트랜잭션 가운데 와도 동시에 같이 포함하고 있는 트랜잭션의 비율이죠. 분모에는 엑스의 서포트 카운트가 들어가고요. 분자에는 x와 y가 동시에 등장하는 서포트 카운트가 들어가죠. 그래서 스의 서포트 분의 x에서 와로 가는 서포트로 나눠주게 되면은 이 값은 결국 스가 등장했을 때 와도 같이 등장하는 조건부 확률을 의미합니다. 그래서 이 컴피던스 값이 높을수록 연관 규칙들 가운데 추천을 하기에도 유의미한 어떤 유용한 규칙임을 알 수 있습니다. 마지막으로 리프트입니다. 리프트는 앞에 서포트나 컴피던스와 달리 0과 1 사이의 확률 값이 아닙니다. 1을 기준으로 1보다 낮은지 1보다 높은지에 따라서 의미가 달라지는데요. 먼저 수식을 살펴보면 와가 등장할 확률이 분모에 들어가고요. 분자에는 스가 등장했을 때 와가 등장할 확률이 들어가게 됩니다. 그래서 이 값이 리프트 값이 1이라는 것은 x와 y가 서로 독립된 그러니까 서로 영향을 주지 않는다는 것이고요. 리프트가 1보다 크다는 것은 x y가 양의 상관관계, 리프트가 1보다 작다는 것은 x y가 음의 상관관계를 갖는 것을 의미합니다. 그래서 분자에는 y가 등장할 확률이 들어가고 분모에는 y가 등장할 확률이 들어가고요. 분자에는 스가 주어졌을 때 y가 등장할 확률이 들어갑니다. 그래서 이렇게 수식으로 살펴보면 다소 이해하기 어렵기 때문에 다음 데이터를 통해서 서포트와 컴피던스와 리프트를 구하는 수식을 살펴봅시다. 자 다음과 같은 연관 규칙 가장 간단한 빵이 엔티스턴트고 계란이 컨시퀀트일 때 이 규칙에 대한 서포트와 컴피던스 리프트를 계산합시",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 2,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1027,
      "char_count": 1903
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c003_89ae1e",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 그래서 분자에는 y가 등장할 확률이 들어가고 분모에는 y가 등장할 확률이 들어가고요. 분자에는 스가 주어졌을 때 y가 등장할 확률이 들어갑니다. 그래서 이렇게 수식으로 살펴보면 다소 이해하기 어렵기 때문에 다음 데이터를 통해서 서포트와 컴피던스와 리프트를 구하는 수식을 살펴봅시다. 자 다음과 같은 연관 규칙 가장 간단한 빵이 엔티스턴트고 계란이 컨시퀀트일 때 이 규칙에 대한 서포트와 컴피던스 리프트를 계산합시다. 먼저 서포트는 전체 트랜잭션 데이터의 수인 5가 분모에 들어가고요. 빵과 계란이 동시에 등장하는 트랜잭션은 2번과 5번이 되겠죠. 그래서 0.4로 구할 수 있고요. 컨피던스는 이제 분모에는 스가 단독으로 등장하는 트랜잭션의 개수 2번, 4번 5번이 되고 분자에는 x와 y의 가 동시로 등장하는 트랜잭션 개수가 됩니다. 즉 3분의 2가 되겠죠. 그래서 이것을 확률로 계산해 보면 x가 등장할 확률 즉 0. 6이 되고요. x와 y가 동시에 계산할 확률은 위에서 계산한 서포트와 동일한 값이 됩니다. 그래서 0.66 즉 빵이 등장했을 때 계란도 같이 등장할 확률은 0.66이 됩니다. 마지막으로 리프트는 분모에 계란이 등장할 확률이 들어가고요. 분자에는 빵이 등장했을 때 계란이 등장할 확률 즉 컨피던스 값이 들어갑니다. 이 값을 계산해 보면 1.66 그래서 리프트 값은 1보다 커질 수도 있다는 것을 의미하죠. 그래서 이 말의 의미는 단순히 계란이 등장할 확률과 빵이 등장했을 때 계란이 같이 등장할 확률을 비교해서 빵이 등장하면은 평소에 계란이 등장할 확률보다 더 높은 값을 가진다. 즉 그 빵과 계란의 상관관계가 1보다 크다. 서로 양해 상관관계를 갖는다는 의미를 말합니다. 이렇게 해서 연관 규칙의 서포트 컴피던스 리프트를 구해 봤습니다. 이제 이 규칙을 사용해서 어떻게 추천을 한다고 했을 때 그 방법을 살펴봅시다. 일단 아이템의 개수가 많아질수록 가능한 아이템 셋의 개수, 그리고 그 그걸 가지고 만드는 룰의 개수는 기하급수적으로 많아지기 때문에 모든 룰을 사용할 수 없고 이 중에 유의미한 룰을 사용해야 합니다. 그래서 이 유의미한 룰을 필터링하는 방법이 아래와 같습니다. 먼저 미니멈 서포트 미니멈 컴피던스를 사용자가 지정해 주면 그 컴피던스와 서포트 값을 가지고 의미 없는 모든 룰을 스크린 아웃 합니다. 그래서 전체 트랜잭션 가운데 너무 적게 등장하는 아이템 셋이나 혹은 조건부 확률이 아주 낮은 룰을 필터링하는 것이죠. 이제 최종 추천을 제공하기 위한 척도로는 컨피던스가 아니라 리프트 값을 사용하는데요. 그 이유는 유저의 입장에서 리프트 값을 사용했을 때 질적으로 만족스러운 추천 결과를 얻을 수 있기 때문입니다. 왜 리프트를 사용하는 것이 질적으로 만족도가 높은지 예시를 통해 살펴봅시다. 먼저 우리가 가진 아이템 셋 x y z가 있다고 가정하고 이 x는 와인 오프너 와인을 딸 때 사용하는 오프너죠. 그 z는 생수 우리가 보통 마트에서 많이 구입하는 물건이죠. 이제 이랬을 때 각각의 스가 등장했을 때 와 즉 와인을 샀을 때 오프너를 살 확률이 0.1이라고 보고 와인을 샀을 때 생수를 살 확률은 0.2라고 봅시다. 그럼 이 수식만 봤을 때는 와인을 샀을 때 더 많이 사는 물건은 오프너가 아니라 와인 오프너가 아니라 생수가 되겠죠. 그런데 여기서 리프트 값을 계산하기 위해서 기존에 와인을 사지 않고 단순히 오프너를 살 확률 즉 y만 살 확률은 0.01이라고 두고 생수는 원래 많이 사기 때문에 0.2라고 했을 때 이 두 개를 통해서 리프트 값을 계산하면 와인에서 오프너로 가는 리프트는 0.01분의 0.1이기 때문에 10이 되고요. 와인을 샀을 때 생수를 살 리프트는 0.2분의 0.2 1이 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 3,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 986,
      "char_count": 1859
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c004_708d47",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 그럼 이 수식만 봤을 때는 와인을 샀을 때 더 많이 사는 물건은 오프너가 아니라 와인 오프너가 아니라 생수가 되겠죠. 그런데 여기서 리프트 값을 계산하기 위해서 기존에 와인을 사지 않고 단순히 오프너를 살 확률 즉 y만 살 확률은 0.01이라고 두고 생수는 원래 많이 사기 때문에 0.2라고 했을 때 이 두 개를 통해서 리프트 값을 계산하면 와인에서 오프너로 가는 리프트는 0.01분의 0.1이기 때문에 10이 되고요. 와인을 샀을 때 생수를 살 리프트는 0.2분의 0.2 1이 됩니다. 즉 리프트로 정렬을 했을 때는 이 두 번째 룰 즉 x에서 y로 가는 룰의 리프트가 10이 되죠. 그래서 일반적으로 와인을 샀을 때 오프너를 살 확률이 확 증가하게 되는 거. 원래 오프너를 살 확률은 0.01인데 와인을 사고 나서 오프너를 살 확률을 보니까 10배로 늘어나게 되는 거죠. 그래서 와인을 샀을 때 적당한 추천 아이템은 원래 생수는 와인과 관계없이 많이 사는 아이템이지만 오프너 같은 경우에는 그 살 확률이 10배로 증가하기 때문에 리프트를 사용해서 사용자가 아 와인 오프너를 추천받았을 때 더 양질의 추천 결과를 받았다라고 느낄 수 있습니다. 이제 여기까지가 연관 규칙과 이를 이용한 추천 방법이었고요. 이제 연관 규칙을 추천해 사용하기 위해서 주어진 데이터셋에서 모든 연관 규칙을 추출해야 합니다. 이것이 마이닝 어소시이션 루즈 모든 어소시에이션 연관 규칙을 탐색한다는 것인데요. 사실 이 연관 규칙을 탐색하는 부분이 이 연관 규칙 연관 분석에서 가장 중요하고 어려운 부분입니다. 일단 우리가 찾아야 하는 연관 규칙은 아까 언급한 것처럼 미니멈 서포트보다 큰 서포트 그리고 미니멈 컴피던스보다 큰 컴피던스의 모든 연관 규칙을 찾는 것인데요. 이렇게 모든 연관 규칙을 찾기 위해서 가장 무식하고 간단한 방법은 브루트포스 어프로치입니다. 말 그대로 모든 연관 규칙을 다 나열하고 그 모든 연관 규칙에 대해서 서포트와 컴피턴스 값을 구하고 그다음에 그 모든 연관 규칙의 서포트와 컴피던스에 대해서 이 미니멈 서포트와 미니멈 컴피턴스를 이용해서 위와 같은 조건을 만족하는 모든 룰을 다 푸루닝 하는 것이죠. 근데 이제 이렇게 하는 브루트포스 어프로치는 사실 굉장한 연산량과 시간을 요구하게 됩니다. 이 브루트포스 어프로치를 사용하여서 연관 규칙을 탐색하기 위해서는 모든 아이템 셋의 서포트를 다 계산하고 그중에 프리퀀트 아이템 셋을 추려야 한다는 것인데요. 이를 위해서 개별 트랜잭션 별로 모든 아이템을 풀 스캔하고 가능한 아이템 셋 별로의 모든 서포트를 다 계산해야 합니다. 그래서 계산량이 전체 트랜잭션 엔 곱하기 각각의 트랜잭션의 아이템 셋의 개수인 w 곱하기 m 이 m은 아이템 셋의 모든 가능한 개수입니다. 이제 여기서 가장 문제가 되는 부분이 바로 m인데요. 이 아이템 셋의 개수 가능한 아이템 셋의 개수가 익스포넨셜 스케일로 증가한다는 것이 만약에 아이템이 고유한 아이템의 개수가 10개만 돼도 그 아이템 셋의 개수는 2의 10승 1024로 엄청나게 많이 증가하게 되죠. 그래서 브루트포스 어프로치로 모든 아이템 셋의 서포트를 다 계산하는 것은 사실 계산량 측면에서는 거의 불가능한 일입니다. 그리고 만약에 이 모든 아이템 셋의 서포트를 다 계산했다고 하더라도 이 주어진 디계의 아이템으로 구성한 모든 연관 규칙의 개수는 아래와 같이 3의 d제곱 으로 익스포넨셜하게 증가합니다. 따라서 브루트포스로 가능한 모든 아이템 셋의 서포트를 계산하고 그다음 가능한 모든 연관 규칙을 다 계산해서 이 계산량을 가지고 미니멈 서포트와 미니멈 컨피던스를 프루닝 하는 방법은 제한 시간 안에 의미 있는 연관 규칙을 탐색할 수 없게 만듭니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 4,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 997,
      "char_count": 1855
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c005_570e8a",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 그리고 만약에 이 모든 아이템 셋의 서포트를 다 계산했다고 하더라도 이 주어진 디계의 아이템으로 구성한 모든 연관 규칙의 개수는 아래와 같이 3의 d제곱 으로 익스포넨셜하게 증가합니다. 따라서 브루트포스로 가능한 모든 아이템 셋의 서포트를 계산하고 그다음 가능한 모든 연관 규칙을 다 계산해서 이 계산량을 가지고 미니멈 서포트와 미니멈 컨피던스를 프루닝 하는 방법은 제한 시간 안에 의미 있는 연관 규칙을 탐색할 수 없게 만듭니다. 그래서 이 효율적으로 연관 규칙을 탐색하기 위해서는 결국 필요한 작업이 두 가지인데요. 먼저 프리퀀트 아이템 셋 제너레이션 즉 미니멈 서포트 이상의 모든 아이템 셋을 생성하는 것, 그 이후에 확 추려진 프리퀀트 아이템 셋을 가지고 룰 제너레이션 연관 규칙을 만드는 것이죠. 이 연관 규칙은 미니멈 컴피던스 이상의 연관 규칙을 생성해야 하는 것입니다. 여기서 가장 계산량이 많이 필요한 부분이 바로 1번 테스크 가능한 모든 아이템 셋의 서포트를 계산하는 것이죠. 그래서 이 프리퀀트 아이템 셋을 제너레이션 하는 알고리즘이 이 연관 분석에서 제일 중요한 부분인데요. 이 프리퀀트 아이템 셋을 효율적으로 생성하는 알고리즘은 다음과 같이 뭐 에프오리 알고리즘, 디치피 프피 그로스 등이 있습니다. 각각의 알고리즘은 효율적인 계산을 통해서 원래는 익스포넨셜 타임이 걸리는 아이템 셋 제너레이션 계산을 획기적으로 단축시킵니다. 사실 이 각각의 알고리즘을 다 이번 강의에서 다루지 않겠는데요. 에이 프리오리 알고리즘 정도는 따로 구글링을 통해서 여러분들이 한번 이해해 보시길 추천드립니다. 제 강의 자료 부록에도 이 에이 프리오리 알고리즘에 대한 간단한 설명이 첨부되어 있습니다. 네 그래서 지금까지 연관 분석, 연관 규칙 그리고 서포트 컴피던스 리프트의 개념을 다뤘고 이 연관 규칙이 어떻게 추천에 활용되는지를 학습했습니다. 그리고 마지막으로 연관 규칙 특히 프리퀀트 아이템 셋을 생성하는 부분 탐색하는 부분이 굉장히 시간이 오래 걸리는데 이 다음과 같은 이런 알고리즘을 통해서 효율적으로 탐색할 수 있다는 것도 간단히 다뤄봤습니다. 네 그럼 다음 파트로 넘어가겠습니다. 네 다음은 티프아디프를 이용한 콘텐츠 기반 추천입니다. 텍스트 데이터를 다루는 가장 기본적인 tfi디프 개념을 살펴보고요. 이 텍스트 데이터로 이루어진 아이템을 콘텐츠 기반 추천으로 제공하는 방법을 학습합니다. 네 콘텐츠 기반 추천의 기본 아이디어는 아래와 같습니다. 유저 스가 과거에 선호한 이 아이템과 비슷한 아이템을 찾아서 그 비슷한 아이템을 유저 스에게 추천해 주는 것입니다. 이때 비슷한 아이템의 기준이라는 것은 아이템의 성격, 어떤 아이템이 추천되느냐에 따라 달라지는데요. 예를 들면 영화 같은 경우에는 비슷한 영화 장르나 비슷한 감독이 만든 영화를 비슷한 아이템이라고 할 수도 있고요. 음악 같은 경우에는 같은 장르에 속해 있는 음악 혹은 같은 와유라는 아티스트가 만든 음악을 비슷한 아이템이라고 정의할 수도 있겠죠. 이와 같이 콘텐츠 기반 추천에서 비슷하다는 아이템 그 비슷하다는 정의는 그 아이템이 어떠한 정보 즉 아이템의 아이템 부가 정보가 어떤 걸로 이루어져 있는지에 따라서 달라지게 됩니다. 그림을 통해서 콘텐츠 기반 추천을 직관적으로 이해해 봅시다. 먼저 우리가 주어진 데이터가 이 유저가 이 빨간색 동그라미와 빨간색 세모를 이미 좋아한다는 데이터를 갖고 있다고 가정합시다. 이때 우리는 새로운 다른 아이템을 이 유저한테 추천해 줘야 하는데요. 이 아이템과 비슷한 시밀러 아이템을 찾아야 하는 것이 우리의 테스크입니다. 그러면 여기서 이 아이템이 가지고 있는 성질을 찾습니다. 이 성질은 빨간색이라는 색깔 정보가 있고요. 동그라미와 세모로 이루어져 있다는 정보가 있죠. 이제 이 정보를 가지고 유저 프로파일을 만듭니다. 즉 이 유저는 빨간색을 좋아하고 동그라미와 세모 아이템을 좋아한다고 유저 프로파일을 만드는 것이죠. 그 이후에 이 정보를 가지고 비슷한 아이템을 찾습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 5,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1100,
      "char_count": 2000
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c006_56242f",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 이때 우리는 새로운 다른 아이템을 이 유저한테 추천해 줘야 하는데요. 이 아이템과 비슷한 시밀러 아이템을 찾아야 하는 것이 우리의 테스크입니다. 그러면 여기서 이 아이템이 가지고 있는 성질을 찾습니다. 이 성질은 빨간색이라는 색깔 정보가 있고요. 동그라미와 세모로 이루어져 있다는 정보가 있죠. 이제 이 정보를 가지고 유저 프로파일을 만듭니다. 즉 이 유저는 빨간색을 좋아하고 동그라미와 세모 아이템을 좋아한다고 유저 프로파일을 만드는 것이죠. 그 이후에 이 정보를 가지고 비슷한 아이템을 찾습니다. 이 아이템은 빨간색과 비슷한 주황색이면서 동그라미를 가지고 있고요. 이 아이템은 세모는 아니지만 네모로 이루어진 빨간색 아이템이 되겠죠. 그래서 기존에 만들어진 이 유저 프로파일을 가지고 비슷한 아이템을 찾아서 최종적으로 이 비슷한 아이템을 유저한테 추천해 주는 방식이 콘텐츠 기반 추천입니다. 그래서 콘텐츠 기반 추천은 다음과 같은 장점을 갖습니다. 유저에게 추천을 할 때 다른 유저의 데이터가 필요하지 않습니다. 그 유저가 소비한 그 아이템의 데이터만 사용하기 때문이죠. 그리고 새로운 아이템이나 인기도가 낮은 아이템도 추천될 수 있습니다. 이것은 우리가 3강부터 배울 컬래버레이터 필터링 계열의 추천 모델은 새로운 아이템이나 소비가 덜 된 인기도가 적은 아이템이 추천될 확률이 굉장히 낮은데요. 그 씨프 컬라버레이트 필터링 모델에 비해서 가진 장점이라고 볼 수 있습니다. 왜냐하면 이 콘텐츠 기반 추천은 아이템이 가진 특징만 사용하기 때문에 인기도가 없더라도 그 아이템이 가진 특징이 비슷할 경우에 추천이 되기 때문입니다. 그리고 추천 아이템에 대한 설명이 가능한데요. 방금 전에 그 예시처럼 이 기존의 유저가 좋아했던 아이템이 빨간색이고 동그라미 세모다라는 정보를 가지고 다른 아이템을 찾아서 추천하기 때문에 이 추천이 왜 됐는지를 설명할 수 있다는 것이 콘텐츠 기반 추천의 또 다른 장점입니다. 반대로 단점도 있는데요. 먼저 아이템에 적합한 피처를 찾는 것이 가장 어렵습니다. 아까 이 콘텐츠 기반 추천은 아이템별로 사용할 수 있는 아이템의 부가 정보가 다르다고 얘기했는데요. 때문에 아이템이 어떤 것으로 이루어져 있느냐 텍스트 데이터인지 이미지 데이터인지에 따라서 각각에 맞는 피처 프리 프로세싱이 달라지기 때문에 이 과정이 상대적으로 시간이 많이 들고 어려운 기도 합니다. 또한 한 분야나 한 장르의 추천 결과가 계속 나올 수 있는데요. 이걸 오버 스페셜라이제이션이라고 했는데요. 아까 빨간색을 좋아했다고 했을 때 계속해서 빨간색 관련된 아이템만 나오게 되는 거죠. 이제 이것은 컬래버레이터 필터링 계열의 논문 모델에서는 그 빨간색이 아니더라도 다른 인기도가 많은 아이템이 추천될 수도 있는데요. 이것은 이제 콘텐츠 기반의 추천이 가진 장점이 단점으로도 발생할 수 있다는 것을 의미합니다. 그리고 다른 유저의 데이터를 활용할 수 없다는 것도 어떻게 보면 단점이 될 수 있는데요. 다른 유저가 사용했던 다양한 유저 아이템 상호 작용 정보를 사용하지 않고 오로지 그 아이템의 정보만을 활용하기 때문에 때로는 다른 유저의 선호도가 높은 아이템이 추천되는 경우가 이 콘텐츠 기반에서는 발생하지 않습니다. 다음은 아이템 프로파일입니다. 콘텐츠 기반 추천에서 제일 중요하다는 것이 아까 아이템이 가진 부가 정보 즉 아이템의 피처를 찾는 것인데요. 이 아이템의 피처를 가지고 아이템의 프로파일을 만들어야 합니다. 뭐 영화 이미지 SNS 각각 다양한 아이템에 대해서 가진 피처들은 이미 다양하게 이미 서로 다르다고 말씀을 드렸는데요. 이제 이 아이템이 가진 속성을 결국 어떻게 표현할까요? 가장 쉬운 형태는 수학적으로 표현하는 것이죠. 즉 벡터의 형태를 표현하는 것입니다. 이 벡터는 이미 앞에서 배우셨겠지만 하나의 피처가 어떤 한 개 이상의 디멘전으로 표현되고요. 이 벡터 값은 바이너리로 표현될 수도 있고 혹은 리얼 밸류 실수 값으로도 표현될 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 6,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1090,
      "char_count": 1982
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c007_34d058",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 뭐 영화 이미지 SNS 각각 다양한 아이템에 대해서 가진 피처들은 이미 다양하게 이미 서로 다르다고 말씀을 드렸는데요. 이제 이 아이템이 가진 속성을 결국 어떻게 표현할까요? 가장 쉬운 형태는 수학적으로 표현하는 것이죠. 즉 벡터의 형태를 표현하는 것입니다. 이 벡터는 이미 앞에서 배우셨겠지만 하나의 피처가 어떤 한 개 이상의 디멘전으로 표현되고요. 이 벡터 값은 바이너리로 표현될 수도 있고 혹은 리얼 밸류 실수 값으로도 표현될 수 있습니다. 이제 다양한 콘텐츠 다양한 아이템들이 있겠지만 이번 시간에는 텍스트 데이터로 이루어진 문서 그리고 텍스트 피처를 표현하는 제일 기본적인 개념인 tfidf를 가지고 콘텐츠 기반 추천을 수행해 보도록 하겠습니다. 이 문서의 경우에는 텍스트로 이루어져 있는데요. 텍스트는 단어들의 집합으로 표현할 수 있습니다. 그 각각의 가진 단어가 문서의 특징을 나타낸다고 할 수 있겠죠. 그렇다면 이 단어들 가운데 중요한 단어를 찾아야 하는데요. 중요한 단어에 대한 스코어링이 필요합니다. 그래서 단어에 대한 중요도를 나타내는 스코어가 바로 이 tfidf가 되는 거죠. 그 tfidf를 아래와 같이 정의했는데요. 텀 프리퀀시가 TF고요. 인퍼스 다큐먼트 프리퀀시가 IDF입니다. 문서 뒤에 등장하는 단어에 대해서 이 tfidf가 정의가 되는데요. 먼저 TF 그 단어가 문서에 자주 등장한다는 것은 이 단어가 그 문서에서 중요하다는 것을 의미하죠. 이것이 TF 뭐예요? 단순한 프리퀀시가 되겠죠 그리고 IDF 같은 경우에는 이 단어가 전체 문서에서는 적게 등장하는데 하필 그 문서에서 등장했다면 그만큼 그 단어는 그 문서의 중요한 단어가 될 수 있다는 것이죠. 그래서 이 TF와 IDF를 각각 구하고 이 둘을 곱해서 최종적으로 tfidf 값을 구하게 되는 것인데요. 이 tfif 값이 높다는 것은 이 단어 블가 문서 d에서 중요한 단어 즉 이 문서를 표현하는 중요한 피처라는 것을 의미합니다. 그래서 tfidf를 다음과 같은 수식으로 나타냈는데요. TF와 IDF를 각각 구해서 곱하면 됩니다. TF는 말 그대로 단어 w가 문서에 등장하는 횟수인데요. 이 횟수 값을 그대로 사용할 수도 있지만 아래와 같이 노멀라이제이션을 할 수도 있습니다. 만약에 문서의 길이가 서로 제각각 다를 경우에 절대적인 문서의 길이에 따라서 전체적인 텀 프리퀀시 값이 올라갈 수 있는데요. 이 경우에는 그 문서에서 가장 많이 등장한 단어의 개수로 나눠줘서 그 문서의 길이에 대한 값을 노멀라이제이션 해주기도 합니다. 그리고 IDF 값은 다음과 같은 수식 로그 n 분의 라지 n으로 나타나는데요. 이 라지 n 같은 경우에는 우리가 가지고 있는 전체 문서의 개수로 정해진 값이고요. NW는 전체 문서 가운데 그 단어 블가 등장한 문서의 개수입니다. 그래서 이 IDF 값은 단어 단어마다 정의가 되는 것이죠. 그리고 이 아디프 값은 변화가 크기 때문에 스무딩을 위해서 로가리듬을 사용합니다. 단순히 이 로가리드를 사용하지 않고 분수의 형태로만 사용하게 되면은 이 IDF의 변화의 폭이 크기 때문에 로그를 사용하는 것이죠. 자 그럼 다음 예시를 통해 티프idf의 계산이 어떻게 이루어지는지 이해해 봅시다. 다음과 같이 4개의 문서가 있고요. 이 문서를 이루는 고유한 단어의 개수는 6개만 있다고 가정합시다. 간단한 토의 데이터이죠. 이제 여기서 각 숫자의 의미는 문서에 등장한 단어의 횟수인데요. 이게 바로 곧 텀 프리퀀시 TF 값이 됩니다. 이제 여기서 IDF를 계산하는데요. IDF는 단어마다 생성되는 값이 되겠죠 그래서 전체 문서의 개수가 4인데요. 이 4 중에서 이 블1이라는 단어가 등장한 횟수는 두 번 등장했기 때문에 로그 2분의 4 그래서 이렇게 쭉 단어에 대해서 각각 IDF 값이 6개로 구해지게 됩니다. 이렇게 해서 tfidf가 각각 구해지면 이를 곱해서 최종적으로 각 문서의 단어에 대한 tfidf를 계산할 수 있습니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 7,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1032,
      "char_count": 1968
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c008_2c95b1",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 간단한 토의 데이터이죠. 이제 여기서 각 숫자의 의미는 문서에 등장한 단어의 횟수인데요. 이게 바로 곧 텀 프리퀀시 TF 값이 됩니다. 이제 여기서 IDF를 계산하는데요. IDF는 단어마다 생성되는 값이 되겠죠 그래서 전체 문서의 개수가 4인데요. 이 4 중에서 이 블1이라는 단어가 등장한 횟수는 두 번 등장했기 때문에 로그 2분의 4 그래서 이렇게 쭉 단어에 대해서 각각 IDF 값이 6개로 구해지게 됩니다. 이렇게 해서 tfidf가 각각 구해지면 이를 곱해서 최종적으로 각 문서의 단어에 대한 tfidf를 계산할 수 있습니다. 그래서 이 오른쪽에 있는 우측의 수식은 첫 번째 문서에 대해서 각각 tfidf를 다 계산한 값이고요. 이 전체 값이 tfidf의 값이 결국 최종적으로 이 d1이라는 문서를 표현하는 벡터가 됩니다. 그래서 계속해서 이 모든 문서에 대해서 각각 tfidf를 계산하면 다음과 같이 첫 번째 문서에 대한 벡터 즉 6개 워드에 대한 tfidf 값을 계산할 수 있고요. 이제 단어의 개수가 6개이기 때문에 이 아이템 프로파일 벡터는 6차원 벡터가 됩니다. 이 단어의 개수가 늘어날수록 즉 고유한 단어의 개수가 늘어날수록 이 tfidf 값은 점점 늘어나게 되겠죠. 자 이렇게 해서 아이템 프로파일 즉 각각의 문서에 대한 벡터 표현을 모두 구축했으니 우리가 해야 할 것은 이제 유저한테 아이템을 추천하는 것입니다. 유저한테 아이템을 추천하기 위해서는 유저 프로파일 구축을 해야 합니다. 이 유저 프로파일은 과거에 유저가 선호했던 아이템 리스트가 있다고 가정하고 아까처럼 유저가 빨간색 동그라미와 빨간색 네모를 선호했다는 그런 아이템 리스트가 있겠죠 그 각각의 아이템은 tfidf를 통해서 벡터로 표현됩니다. 이것이 아이템 프로파일이었죠. 그러면 각각의 유저의 아이템 리스트 안에 있는 아이템들의 모든 벡터를 다 통합하면 유저 프로파일이 됩니다. 여기서 이 통합하는 방법은 대표적으로 두 가지가 있는데요. 다음 예시를 통해 각각의 방법을 살펴봅시다. 자 이 데이터는 아까 전에 계산했던 4개의 아이템에 대한 tfidf 벡터인데요. 유저가 선호했던 아이템 리스트에 d1과 d3가 들어가 있다고 가정하고 이제 이 d1과 d3를 가지고 유저 프로파일 벡터를 만들게 됩니다. 가장 단순한 방법은 이 d1과 d3의 벡터, 즉 tfidf의 6차원 벡터가 있으면 이 두 벡터를 평균 내는 방법입니다. 그리고 여기서 좀 더 발전한 방법은 이 유저가 단순히 d1 d3를 선호한 것이 아니라 rd1 r d3와 같은 어떤 레이팅을 가지고 서로 다른 선호도를 나타냈다고 했을 때는 이 선호도 값을 가중치로 사용하여서 가중 평균을 낼 수도 있습니다. 조금 더 정확한 유저 벡터를 표현할 수 있겠죠 네 이제 유저 벡터를 구했으니 이 유저 벡터를 이용하여서 아이템을 찾아서 추천해야 되는데요. 이때 필요한 개념이 있습니다. 바로 시밀러리티 유사도 개념입니다. 유사도는 다음 강의인 이웃 기반 콜라베이트 필터링에서 자세히 다루지만 먼저 코사인 시밀러리티만 간단히 다루겠습니다. 코사인 시뮬러리티는 추천 시스템에서 가장 많이 사용되는 유사도입니다. 코사인 유사도는 같은 차원을 가진 두 가지의 벡터에 대해서 정의가 되는데요. 수식은 다음과 같이 각각의 벡터의 크기 분의 두 벡터의 내적으로 나타냅니다. 이 코사인 시뮬러이티는 직관적으로 표현하면 두 벡터의 각도를 의미합니다. 그래서 각도가 완전히 같을수록 1이 되고요. 서로 직교할수록 코사인 시뮬리이티는 0이 되고요. 그 각도가 완전히 서로 반대를 가르칠 때는 마이너스 1이 됩니다. 이제 이 코사인 유사도를 사용하여서 유저 벡터와 아이템 벡터의 거리를 계산해야 합니다. 아까 전에 유저 벡터는 아이템 벡터의 합으로 표현되어 있기 때문에 이 유저 벡터와 아이템 벡터의 차원은 이미 같죠 그렇기 때문에 유저 벡터와 아이템 벡터의 코사인 시뮬러리티를 계산할 수 있습니다. 이 유저 벡터와 아이템 벡터의 코사인 시뮬리티가 곧 이 유저가 아이템을 갖는 스코어가 됩니",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 8,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 1060,
      "char_count": 2005
    },
    {
      "id": "transcript_recsys_이론_recsys_이론_2강_추천_시스템_basic_2_c009_e6de22",
      "content": "[RecSys 이론] [RecSys 이론] (2강) 추천 시스템 Basic 2\n\n다. 이제 이 코사인 유사도를 사용하여서 유저 벡터와 아이템 벡터의 거리를 계산해야 합니다. 아까 전에 유저 벡터는 아이템 벡터의 합으로 표현되어 있기 때문에 이 유저 벡터와 아이템 벡터의 차원은 이미 같죠 그렇기 때문에 유저 벡터와 아이템 벡터의 코사인 시뮬러리티를 계산할 수 있습니다. 이 유저 벡터와 아이템 벡터의 코사인 시뮬리티가 곧 이 유저가 아이템을 갖는 스코어가 됩니다. 그래서 그 둘의 유사도가 클수록 해당 아이템이 유저에게 관련성이 높다는 것을 의미하고 해당 유저와 가장 유사한 아이템, 즉 이 스코어 값 코사인 시뮬리이티 값이 가장 큰 아이템부터 유저한테 추천이 되는 것입니다. 이렇게 해서 이 유저 아이템에 대한 스코어를 가지고 유저에게 엔게이 아이템을 추천해 줄 수 있는데요. 이것이 아까 지난 시간에 언급했던 탑 앤 레코멘데이션 문제가 되겠죠. 이제 이게 아니라 유저 u가 아이템 i에 대해서 가질 선호도나 평점을 정확하게 예측하고 싶다면 즉 예측 문제를 풀고 싶을 때는 어떻게 계산해야 할까요? 정확한 평점을 예측하기 위해서는 유저 프로파일을 따로 구축하지 않고 유저가 평점을 매긴 모든 아이템의 벡터를 활용해서 정확한 평점을 예측합니다. 여기서 우리가 새로 예측할 아이템을 아이 프라임이라고 했을 때 우리는 유저가 과거에 선호했던 i1부터 in과 그에 해당하는 벡터 그리고 거기에 해당하는 평점을 활용할 것입니다. 그래서 새로 예측할 i 프라임과 기존의 유저가 선호했던 이 i1부터 in 사이의 유사도를 구하고요. 그 유사도를 가중치를 사용하여서 과거에 각각의 선호했던 아이템들에 대한 평점 평점을 가지고 가중 평균을 매기게 됩니다. 다음 예시를 통해서 조금 더 쉽게 이해해 봅시다. 특정 유저가 이미 선호했던 아이템 영화가 3개가 있을 때 이 3개는 m1, m2 m3라고 가정하고 그럼 이 영화에 대한 평점 값이 있겠죠 이건 이미 유저가 과거에 내린 평점 값입니다. 그리고 이 영화에 대한 tfidf 벡터를 만들었다고 가정합시다. 4차원 벡터네요. 그래서 총 3개의 벡터가 있을 때 우리는 새로운 영화에 대한 예측 값을 구하고 싶습니다. 이때 이 새로운 영화에 필요한 값은 m4에 대한 또 다른 tfif 벡터가 되겠죠. 그럼 이때 이 엠4에 대해서 이 유저의 웨이팅을 어떻게 예측하냐 먼저 유저가 선호했던 m1, m2, m3에 대해서 각각 새로운 영화 엠4에 대한 코사인 시뮬레이티를 각각 계산해 주게 됩니다. 그래서 이 각각의 코사인 시뮬리이티가 가중평균의 가중치가 되고요. 그리고 유저가 m1에는 3.0 m2에는 2.5를 매겼기 때문에 여기에 해당하는 이 유사도와 이 값 그리고 이 레이팅과 이 유사도를 각각 곱하게 되면은 우측에 있는 것처럼 분모에는 가중치, 그리고 분자에는 가중치와 각각의 평점의 합이 합이 되겠죠. 최종적으로 이 3.2점이 이 유저가 새로운 영화 즉 엠4에 대한 평점을 얼마로 가질지에 대한 예측치를 계산할 수 있게 됩니다. 네 그래서 이상 추천 시스템 베이직 두 번째 강의를 모두 마쳤고요. 모두 들어주시느라 수고하셨습니다. 감사합니다.",
      "metadata": {
        "doc_type": "lecture_transcript",
        "source_file": "[RecSys 이론] (2강) 추천 시스템 Basic 2.json",
        "lecture_name": "[RecSys 이론] (2강) 추천 시스템 Basic 2",
        "course": "RecSys 이론",
        "lecture_num": "2강",
        "lecture_title": "추천 시스템 Basic 2",
        "chunk_idx": 9,
        "total_chunks": 10,
        "schema_version": "2.0.0",
        "pipeline_version": "2.0.0",
        "corpus_version": "2025.11.27",
        "processed_at": "2025-12-04T15:35:42Z",
        "source_hash": "sha256:efe51d596885a9512b34f8dbcbea6a38d7f434b6362537a55c5bd8b87bae5615"
      },
      "token_estimate": 824,
      "char_count": 1552
    }
  ]
}