"""
Adaptive RAG ì›Œí¬í”Œë¡œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ ì§‘í•©.

ê° ë…¸ë“œëŠ” RAG í”„ë¡œì„¸ìŠ¤ì˜ ê°œë³„ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
"""

from __future__ import annotations

from typing import Any

from langchain_core.runnables import Runnable
from langchain_core.retrievers import BaseRetriever
from langchain_core.messages import AIMessage

from naver_connect_chatbot.service.graph.state import AdaptiveRAGState
from naver_connect_chatbot.service.graph.types import (
    IntentUpdate,
    QueryAnalysisUpdate,
    RetrievalUpdate,
    AnswerUpdate,
    OODResponseUpdate,
)
from naver_connect_chatbot.service.agents.intent_classifier import (
    aclassify_intent,
    IntentClassification,
)
from naver_connect_chatbot.service.agents.query_analyzer import (
    aanalyze_query,
    QueryAnalysis,
)
from naver_connect_chatbot.service.agents.answer_generator import (
    get_generation_strategy,
)
from naver_connect_chatbot.service.tool.retrieval_tool import retrieve_documents_async, RetrievalResult
from naver_connect_chatbot.rag import ClovaStudioReranker
from naver_connect_chatbot.config import logger, settings


def _extract_text_response(response: Any) -> str:
    """
    LangChain ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if isinstance(response, AIMessage):
        if isinstance(response.content, str):
            return response.content
        return str(response.content)

    if hasattr(response, "content"):
        content = response.content
        if isinstance(content, str):
            return content
        return str(content)

    if isinstance(response, dict):
        output = response.get("output")
        if isinstance(output, str):
            return output
        content_val = response.get("content")
        if isinstance(content_val, str):
            return content_val

    if isinstance(response, str):
        return response

    return str(response)


async def classify_intent_node(state: AdaptiveRAGState, llm: Runnable) -> IntentUpdate:
    """
    ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤ (OUT_OF_DOMAIN ê°ì§€ í¬í•¨).

    Note:
        tools/function calling ëŒ€ì‹  llm.invoke(prompt) í˜•íƒœë¡œ ì§ì ‘ í˜¸ì¶œí•˜ì—¬
        CLOVA HCX-007ì˜ reasoning ëª¨ë“œì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë¶„ë¥˜ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸

    ë°˜í™˜ê°’:
        ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ê°€ í¬í•¨ëœ ìƒíƒœ ì—…ë°ì´íŠ¸ (domain_relevance í¬í•¨)
    """
    logger.info("---CLASSIFY INTENT---")
    question = state["question"]

    # aclassify_intent ì§ì ‘ í˜¸ì¶œ (ë‚´ë¶€ì—ì„œ ì—ëŸ¬ ì²˜ë¦¬)
    response = await aclassify_intent(question, llm)

    # domain_relevanceê°€ ë‚®ìœ¼ë©´ OUT_OF_DOMAINìœ¼ë¡œ ë³´ì •
    intent = response.intent
    domain_relevance = response.domain_relevance

    if domain_relevance < 0.3 and intent != "OUT_OF_DOMAIN":
        logger.info(
            f"Low domain_relevance ({domain_relevance:.2f}), "
            f"overriding intent from {intent} to OUT_OF_DOMAIN"
        )
        intent = "OUT_OF_DOMAIN"

    if intent == "OUT_OF_DOMAIN":
        logger.info(
            f"OUT_OF_DOMAIN detected: domain_relevance={domain_relevance:.2f}, "
            f"question='{question[:50]}...'"
        )

    return {
        "intent": intent,
        "intent_confidence": response.confidence,
        "intent_reasoning": response.reasoning,
        "domain_relevance": domain_relevance,
    }


async def analyze_query_node(state: AdaptiveRAGState, llm: Runnable) -> QueryAnalysisUpdate:
    """
    ì§ˆì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ê³  ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬ ë° ê²€ìƒ‰ í•„í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ ë…¸ë“œëŠ” Query Analysis, Multi-Query Generation, Filter Extractionì„ í†µí•©í•˜ì—¬:
    1. ì§ˆì˜ì˜ ëª…í™•ì„±, êµ¬ì²´ì„±, ê²€ìƒ‰ ê°€ëŠ¥ì„±ì„ í‰ê°€
    2. ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ 3-5ê°œ ìƒì„± (Multi-Query)
    3. ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ í•„í„° ì¶”ì¶œ (doc_type, course, etc.)

    Pre-Retriever ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ:
    - VectorDB ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•˜ì—¬ LLMì´ ì‹¤ì œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì•Œê³  í•„í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ í•¨
    - ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œëœ SchemaRegistryì—ì„œ ë°ì´í„° ì†ŒìŠ¤ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´

    Note:
        tools/function calling ëŒ€ì‹  llm.invoke(prompt) í˜•íƒœë¡œ ì§ì ‘ í˜¸ì¶œí•˜ì—¬
        CLOVA HCX-007ì˜ reasoning ëª¨ë“œì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë¶„ì„ ë° ì¿¼ë¦¬ ìƒì„±ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸

    ë°˜í™˜ê°’:
        ì§ˆì˜ ë¶„ì„, ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬, ê²€ìƒ‰ í•„í„°ë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---ANALYZE QUERY & GENERATE MULTI-QUERY & EXTRACT FILTERS---")
    question = state["question"]
    intent = state.get("intent", "SIMPLE_QA")

    try:
        # VectorDB ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì™€ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
        data_source_context = None
        try:
            from naver_connect_chatbot.rag.schema_registry import (
                get_data_source_context,
                get_schema_registry,
            )

            data_source_context = get_data_source_context(max_courses=10)

            # ë³„ì¹­ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (VectorDB ê¸°ë°˜ ë™ì  ìƒì„±)
            registry = get_schema_registry()
            if registry.is_loaded():
                alias_context = registry.get_alias_context_for_prompt()
                if alias_context:
                    data_source_context = f"{data_source_context}\n\n{alias_context}"

            logger.debug("Data source context with aliases loaded for query analysis")
        except Exception as e:
            logger.warning(f"Failed to load data source context: {e}")

        # aanalyze_query ì§ì ‘ í˜¸ì¶œ
        response = await aanalyze_query(question, intent, llm, data_source_context)

        # retrieval_filtersë¥¼ RetrievalFilters TypedDictë¡œ ë³€í™˜
        filters = {}
        if response.retrieval_filters:
            rf = response.retrieval_filters
            if rf.doc_type:
                filters["doc_type"] = rf.doc_type
            if rf.course:
                # Fuzzy + Alias í›„ì²˜ë¦¬ë¡œ ê³¼ì •ëª… í™•ì¥
                try:
                    from naver_connect_chatbot.rag.schema_registry import get_schema_registry

                    registry = get_schema_registry()
                    if not registry.is_loaded():
                        logger.warning(
                            "SchemaRegistry not loaded, using original course names"
                        )
                        filters["course"] = rf.course
                    else:
                        resolved_courses: list[str] = []
                        for course in rf.course:
                            try:
                                resolved = registry.resolve_course_with_fuzzy(course)
                                resolved_courses.extend(resolved)
                            except Exception as course_error:
                                logger.error(
                                    f"Failed to resolve course '{course}': {course_error}",
                                    exc_info=True,
                                )
                                resolved_courses.append(course)  # Fallback to original
                        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                        filters["course"] = list(dict.fromkeys(resolved_courses))
                        logger.info(
                            f"Course names resolved: {rf.course} â†’ {filters['course']}"
                        )
                except Exception as e:
                    logger.error(
                        f"Critical error in course fuzzy resolution: {e}",
                        exc_info=True,
                    )
                    filters["course"] = rf.course
            if rf.course_topic:
                filters["course_topic"] = rf.course_topic
            if rf.generation:
                filters["generation"] = rf.generation

        if filters:
            logger.info(f"Extracted retrieval filters: {filters}")

        # filter_confidence ì¶”ì¶œ
        filter_confidence = 1.0
        if response.retrieval_filters:
            filter_confidence = response.retrieval_filters.filter_confidence
            if filter_confidence < 0.5:
                logger.info(
                    f"Low filter confidence ({filter_confidence:.2f}), "
                    "clarification may be needed"
                )

        # ë¶„ì„ ê²°ê³¼ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        return {
            "query_analysis": {
                "clarity_score": response.clarity_score,
                "specificity_score": response.specificity_score,
                "searchability_score": response.searchability_score,
            },
            "refined_queries": response.improved_queries
            if response.improved_queries
            else [question],
            "original_query": question,
            "retrieval_filters": filters if filters else None,
            "filter_confidence": filter_confidence,
        }

    except Exception as e:
        logger.error(f"Query analysis error: {e}")
        return {
            "query_analysis": {"error": str(e)},
            "refined_queries": [question],
            "original_query": question,
            "retrieval_filters": None,
            "filter_confidence": 1.0,  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’
        }


async def retrieve_node(state: AdaptiveRAGState, retriever: BaseRetriever) -> RetrievalUpdate:
    """
    ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        retriever: ë¬¸ì„œ ê²€ìƒ‰ê¸°

    ë°˜í™˜ê°’:
        ê²€ìƒ‰ëœ ë¬¸ì„œì™€ í•„í„°ë§ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---RETRIEVE---")

    # ê°€ëŠ¥í•˜ë©´ ì •ì œëœ ì§ˆì˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    queries = state.get("refined_queries", [state["question"]])
    primary_query = queries[0] if queries else state["question"]

    # ìƒíƒœì—ì„œ í•„í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    filters = state.get("retrieval_filters")
    if filters:
        logger.info(f"Applying retrieval filters: {filters}")

    try:
        # ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
        result: RetrievalResult = await retrieve_documents_async(
            retriever,
            primary_query,
            filters=filters,
            fallback_on_empty=True,
            min_results=1,
        )

        logger.info(
            f"Retrieved {result.original_count} docs, "
            f"filtered to {result.filtered_count}, "
            f"filters_applied={result.filters_applied}, "
            f"fallback_used={result.fallback_used}"
        )

        return {
            "documents": result.documents,
            "context": result.documents,  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ë¥¼ ìœ„í•œ í•„ë“œ
            "retrieval_strategy": "hybrid",
            "retrieval_filters_applied": result.filters_applied,
            "retrieval_fallback_used": result.fallback_used,
            "retrieval_metadata": {
                "original_count": result.original_count,
                "filtered_count": result.filtered_count,
                "filters": filters,
            },
        }

    except Exception as e:
        logger.error(f"Retrieval error: {e}")
        return {
            "documents": [],
            "context": [],
            "retrieval_strategy": "hybrid",
            "retrieval_filters_applied": False,
            "retrieval_fallback_used": False,
        }


async def rerank_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ Clova Studio Rerankerë¡œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.

    Post-Retriever ë‹¨ê³„ë¡œ, ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ë„ë¥¼ ì¬í‰ê°€í•˜ì—¬
    ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ìƒìœ„ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ì¬ì •ë ¬ëœ ë¬¸ì„œë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---RERANK DOCUMENTS---")

    question = state["question"]
    documents = state.get("documents", [])

    if not documents:
        logger.warning("No documents to rerank")
        return {
            "documents": [],
            "context": [],
        }

    # Reranking ì„¤ì • í™•ì¸
    use_reranking = (
        settings.adaptive_rag.use_reranking if hasattr(settings, "adaptive_rag") else True
    )

    if not use_reranking:
        logger.info("Reranking disabled, skipping")
        return {
            "documents": documents,
            "context": documents,
        }

    try:
        # Clova Studio Reranker ì´ˆê¸°í™”
        reranker = ClovaStudioReranker(
            api_key=settings.clova_llm.api_key.get_secret_value()
            if settings.clova_llm.api_key
            else "",
            max_tokens=1024,
        )

        # Reranking ìˆ˜í–‰
        logger.info(f"Reranking {len(documents)} documents")
        reranked_docs = await reranker.arerank(
            query=question,
            documents=documents,
            top_k=min(len(documents), 10),  # ìµœëŒ€ 10ê°œê¹Œì§€ ìœ ì§€
        )

        logger.info(f"Reranked to {len(reranked_docs)} documents")

        return {
            "documents": reranked_docs,
            "context": reranked_docs,
        }

    except Exception as e:
        logger.error(f"Reranking error: {e}, using original documents")
        return {
            "documents": documents,
            "context": documents,
        }


async def generate_answer_node(state: AdaptiveRAGState, llm: Runnable) -> AnswerUpdate:
    """
    ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ (Reasoning ëª¨ë“œ í™œìš©).

    CLOVA HCX-007 ëª¨ë¸ì˜ Reasoning ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬:
    1. ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í†µí•´ ë‹µë³€ í’ˆì§ˆ í–¥ìƒ
    2. ìì²´ ê²€ì¦ì„ í†µí•´ í™˜ê° ë°©ì§€
    3. ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë…¼ë¦¬ì  ë‹µë³€ ìƒì„±

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ìƒì„±ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸ (Reasoning ì§€ì›)

    ë°˜í™˜ê°’:
        ìƒì„±ëœ ë‹µë³€ì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---GENERATE ANSWER (with Reasoning)---")
    question = state["question"]
    documents = state.get("documents", [])
    intent = state.get("intent", "SIMPLE_QA")

    try:
        # ì‚¬ìš©í•  ìƒì„± ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
        strategy = get_generation_strategy(intent)

        # Reasoning effort ì„¤ì • (intent ê¸°ë°˜)
        # COMPLEX_REASONING: high, EXPLORATORY: medium, SIMPLE_QA: low
        thinking_effort = "medium"  # ê¸°ë³¸ê°’
        if intent == "COMPLEX_REASONING":
            thinking_effort = "high"
        elif intent == "SIMPLE_QA":
            thinking_effort = "low"
        elif intent == "EXPLORATORY":
            thinking_effort = "medium"

        logger.info(f"Using thinking_effort: {thinking_effort} for intent: {intent}")

        # ìƒì„±ì— ì‚¬ìš©í•  ë¬¸ë§¥ì„ í¬ë§·í•©ë‹ˆë‹¤.
        context_text = "\n\n".join(
            [f"[ë¬¸ì„œ {i + 1}]\n{doc.page_content}" for i, doc in enumerate(documents)]
        )

        if not context_text:
            context_text = "ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        # ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. Reasoningì´ í™œì„±í™”ëœ LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ë©°, toolsëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        prompt = (
            "ë‹¹ì‹ ì€ Naver Boost Camp í•™ìƒë“¤ì—ê²Œ AI/MLì„ ê°€ë¥´ì¹˜ëŠ” ì¡°êµì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ë¬¸ë§¥ë§Œì„ ê·¼ê±°ë¡œ, ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•œ ë’¤ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
            f"question: {question}\n\ncontext:\n{context_text}"
        )

        response_raw = await llm.ainvoke(prompt)
        answer = _extract_text_response(response_raw)
        logger.info(f"Generated answer with reasoning: {answer[:100]}...")

        return {
            "answer": answer,
            "generation_metadata": {
                "strategy": strategy,
                "context_length": len(context_text),
                "thinking_effort": thinking_effort,
                "reasoning_enabled": True,
            },
            "generation_strategy": strategy,
        }

    except Exception as e:
        logger.error(f"Answer generation error: {e}")
        return {
            "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "generation_metadata": {"error": str(e)},
            "generation_strategy": "error",
        }


async def generate_ood_response_node(state: AdaptiveRAGState) -> OODResponseUpdate:
    """
    Out-of-Domain ì§ˆë¬¸ì— ëŒ€í•œ ì •ì¤‘í•œ ê±°ì ˆ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    AI/ML êµìœ¡ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ (ë‚ ì”¨, ìŒì‹, ì—¬í–‰ ë“±)ì— ëŒ€í•´
    ì •ì¤‘íˆ ê±°ì ˆí•˜ê³  ë„ì›€ ê°€ëŠ¥í•œ ì˜ì—­ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        OOD ê±°ì ˆ ì‘ë‹µì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---GENERATE OOD RESPONSE---")

    question = state.get("question", "")
    domain_relevance = state.get("domain_relevance", 0.0)

    # ì§ˆë¬¸ ë‚´ìš©ì„ ê°„ëµíˆ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
    question_preview = question[:50] + "..." if len(question) > 50 else question

    response = (
        f"ì£„ì†¡í•©ë‹ˆë‹¤. '{question_preview}'ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n\n"
        "ì €ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI êµìœ¡ ê³¼ì •ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
        "â€¢ **AI/ML ê°œë… ì„¤ëª…** - Transformer, CNN, RNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±\n"
        "â€¢ **ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬** - PyTorch, TensorFlow ì‚¬ìš©ë²•\n"
        "â€¢ **ì½”ë“œ êµ¬í˜„ ë°©ë²•** - ëª¨ë¸ í•™ìŠµ, ë°ì´í„° ì „ì²˜ë¦¬ ë“±\n"
        "â€¢ **ê°•ì˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸** - CV, NLP, RecSys ê°•ì˜\n"
        "â€¢ **ì‹¤ìŠµ/ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸**\n\n"
        "ìœ„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ¤–"
    )

    logger.info(
        f"OOD response generated for question: '{question_preview}' "
        f"(domain_relevance: {domain_relevance:.2f})"
    )

    return {
        "answer": response,
        "generation_strategy": "ood_decline",
        "workflow_stage": "completed",
        "is_out_of_domain": True,
    }


async def clarify_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ì‚¬ìš©ìì—ê²Œ ëª…í™•í™”ë¥¼ ìš”ì²­í•˜ëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    í•„í„° ì¶”ì¶œ ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ (filter_confidence < 0.5) í˜¸ì¶œë˜ì–´
    ì‚¬ìš©ìì—ê²Œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì¢í ìˆ˜ ìˆëŠ” ì„ íƒì§€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ëª…í™•í™” ìš”ì²­ ì‘ë‹µì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---CLARIFY FILTER---")

    question = state["question"]
    filters = state.get("retrieval_filters", {})
    courses = filters.get("course", []) if filters else []

    # ëª…í™•í™” ë©”ì‹œì§€ ìƒì„±
    clarification_parts = [
        "ì§ˆë¬¸ì„ ë” ì •í™•í•˜ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
    ]

    if courses:
        clarification_parts.append(f"'{question}'ì—ì„œ ì–¸ê¸‰í•˜ì‹  ê³¼ì •ì´ ë‹¤ìŒ ì¤‘ ì–´ëŠ ê²ƒì¸ê°€ìš”?\n")
        for i, course in enumerate(courses[:5], 1):
            clarification_parts.append(f"{i}. {course}")
        clarification_parts.append("\nì›í•˜ì‹œëŠ” ê³¼ì • ë²ˆí˜¸ë¥¼ ì•Œë ¤ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")
    else:
        clarification_parts.append(
            "ì–´ë–¤ ìë£Œì—ì„œ ì°¾ì•„ë³¼ê¹Œìš”?\n"
            "- **ê°•ì˜ìë£Œ** (PDF ìŠ¬ë¼ì´ë“œ)\n"
            "- **ë…¹ì·¨ë¡** (ê°•ì˜ ë‚´ìš©)\n"
            "- **ìŠ¬ë™ Q&A** (ì§ˆì˜ì‘ë‹µ)\n"
            "- **ì‹¤ìŠµ ë…¸íŠ¸ë¶** (ì½”ë“œ)\n"
            "- **ë¯¸ì…˜** (ê³¼ì œ)\n"
        )

    clarification_message = "\n".join(clarification_parts)

    return {
        "answer": clarification_message,
        "workflow_stage": "awaiting_clarification",
    }


def finalize_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ì›Œí¬í”Œë¡œë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ì¢…ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---FINALIZE---")

    # ì›Œí¬í”Œë¡œë¥¼ ì™„ë£Œ ìƒíƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    return {
        "workflow_stage": "completed",
    }
