"""
Adaptive RAG ì›Œí¬í”Œë¡œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ ì§‘í•©.

ê° ë…¸ë“œëŠ” RAG í”„ë¡œì„¸ìŠ¤ì˜ ê°œë³„ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
"""

from __future__ import annotations

from typing import Any

from langchain_core.runnables import Runnable
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage, SystemMessage, ToolMessage

from naver_connect_chatbot.service.graph.state import AdaptiveRAGState
from naver_connect_chatbot.service.graph.types import (
    IntentUpdate,
    QueryAnalysisUpdate,
    AgentUpdate,
    PostProcessUpdate,
    OODResponseUpdate,
)
from naver_connect_chatbot.service.agents.intent_classifier import aclassify_intent
from naver_connect_chatbot.service.agents.query_analyzer import aanalyze_query
from naver_connect_chatbot.config import logger


# =============================================================================
# Multi-turn ëŒ€í™” ì§€ì› ìœ í‹¸ë¦¬í‹°
# =============================================================================

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ìµœëŒ€ í„´ ìˆ˜ (ë©”ëª¨ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬)
MAX_HISTORY_TURNS = 5


def _format_chat_history(messages: list[BaseMessage], max_turns: int = MAX_HISTORY_TURNS) -> str:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        messages: BaseMessage ë¦¬ìŠ¤íŠ¸ (HumanMessage, AIMessage)
        max_turns: í¬í•¨í•  ìµœëŒ€ í„´ ìˆ˜ (ê¸°ë³¸ê°’: 5)

    ë°˜í™˜ê°’:
        í¬ë§·íŒ…ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ (í„´ ë²ˆí˜¸ í¬í•¨, ì¤‘ë³µ ë°©ì§€ ì§€ì‹œ í¬í•¨)
    """
    if not messages:
        return ""

    # ìµœê·¼ Ní„´ë§Œ ì‚¬ìš© (1í„´ = ì‚¬ìš©ì + AI ìŒ)
    recent_messages = messages[-(max_turns * 2) :]

    if not recent_messages:
        return ""

    history_lines = ["[ì´ì „ ëŒ€í™”]"]
    turn_num = 0
    for msg in recent_messages:
        if isinstance(msg, HumanMessage):
            turn_num += 1
            history_lines.append(f"[í„´ {turn_num}] ì‚¬ìš©ì: {msg.content}")
        elif isinstance(msg, AIMessage):
            # AI ì‘ë‹µì€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½ (500ìë¡œ í™•ì¥)
            content = msg.content
            if len(content) > 500:
                content = content[:500] + "..."
            history_lines.append(f"[í„´ {turn_num}] ì–´ì‹œìŠ¤í„´íŠ¸: {content}")

    history_lines.append(
        "[ì£¼ì˜] ìœ„ ëŒ€í™”ì—ì„œ ì´ë¯¸ ë‹µë³€í•œ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”. ìƒˆë¡œìš´ ì •ë³´ì— ì§‘ì¤‘í•˜ì„¸ìš”."
    )

    return "\n".join(history_lines) + "\n"


# =============================================================================
# ì‘ë‹µ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹°
# =============================================================================


def _extract_text_from_content(content: Any) -> str:
    """
    content í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Gemini thinking_level ì‚¬ìš© ì‹œ contentê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
    [{"type": "thinking", "text": "..."}, {"type": "text", "text": "actual answer"}]
    ì´ ê²½ìš° type="text" ë¸”ë¡ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if isinstance(content, str):
        return content

    if isinstance(content, list):
        text_parts = []
        for block in content:
            if isinstance(block, dict):
                # Gemini thinking block í˜•ì‹: {"type": "text", "text": "..."}
                if block.get("type") == "text":
                    text_parts.append(block.get("text", ""))
                elif "text" in block and "type" not in block:
                    # type í•„ë“œ ì—†ëŠ” ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¸”ë¡
                    text_parts.append(block["text"])
            elif isinstance(block, str):
                text_parts.append(block)
        if text_parts:
            return "\n".join(text_parts)
        # thinking ë¸”ë¡ë§Œ ìˆëŠ” ê²½ìš° (text ë¸”ë¡ ì—†ìŒ) â†’ ì²« ë²ˆì§¸ ë¸”ë¡ì˜ text ì‚¬ìš©
        for block in content:
            if isinstance(block, dict) and "text" in block:
                return block["text"]

    return str(content)


def _extract_text_response(response: Any) -> str:
    """
    LangChain ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Gemini thinking blocks (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ content)ë„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if isinstance(response, AIMessage):
        return _extract_text_from_content(response.content)

    if hasattr(response, "content"):
        return _extract_text_from_content(response.content)

    if isinstance(response, dict):
        output = response.get("output")
        if isinstance(output, str):
            return output
        content_val = response.get("content")
        if isinstance(content_val, str):
            return content_val

    if isinstance(response, str):
        return response

    return str(response)


async def classify_intent_node(state: AdaptiveRAGState, llm: Runnable) -> IntentUpdate:
    """
    ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤ (OUT_OF_DOMAIN ê°ì§€ í¬í•¨).

    Gemini with_structured_output()ì„ ì‚¬ìš©í•˜ì—¬ IntentClassificationì„ ì§ì ‘ ë°˜í™˜ë°›ìŠµë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë¶„ë¥˜ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸

    ë°˜í™˜ê°’:
        ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ê°€ í¬í•¨ëœ ìƒíƒœ ì—…ë°ì´íŠ¸ (domain_relevance í¬í•¨)
    """
    logger.info("---CLASSIFY INTENT---")
    question = state["question"]
    question_lower = question.lower().strip()

    # 1. íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í™•ì‹¤í•œ OUT_OF_DOMAIN ë¨¼ì € ì²˜ë¦¬ (LLM í˜¸ì¶œ ì—†ì´ ë¹ ë¥´ê²Œ)
    ood_patterns = {
        "greeting": [
            "ì•ˆë…•",
            "ë°˜ê°€",
            "í•˜ì´",
            "í—¬ë¡œ",
            "hello",
            "hi ",
            "hey",
            "ì˜ ì§€ë‚´",
            "ì¢‹ì€ ì•„ì¹¨",
            "ì¢‹ì€ ì €ë…",
        ],
        "self_intro": [
            "ì´ë¦„ì´ ë­",
            "ë„Œ ëˆ„êµ¬",
            "ë„ˆ ëˆ„êµ¬",
            "ë­˜ í•  ìˆ˜ ìˆ",
            "ë­˜ ë„ì™€ì¤„ ìˆ˜ ìˆ",
            "ì–´ë–¤ ë´‡",
            "ë¬´ìŠ¨ ë´‡",
            "ë­í•˜ëŠ” ë´‡",
            "ì†Œê°œí•´",
            "ìê¸°ì†Œê°œ",
            "who are you",
            "what can you do",
            "what's your name",
        ],
        "chitchat": [
            "ë­í•´",
            "ì‹¬ì‹¬",
            "ë°°ê³ íŒŒ",
            "ì¡¸ë ¤",
            "í”¼ê³¤",
        ],
        "off_topic": [
            "ë‚ ì”¨",
            "ì ì‹¬ ë©”ë‰´",
            "ì €ë… ë©”ë‰´",
            "ì•„ì¹¨ ë©”ë‰´",
            "ë©”ë‰´ ì¶”ì²œ",
            "ë§›ì§‘",
            "ì—¬í–‰",
            "ì£¼ì‹",
            "íˆ¬ì",
            "ì—°ì˜ˆ",
            "ìŠ¤í¬ì¸ ",
            "ì¶•êµ¬ ê²½ê¸°",
            "ì•¼êµ¬ ê²½ê¸°",
        ],
    }

    for pattern_type, patterns in ood_patterns.items():
        if any(pattern in question_lower for pattern in patterns):
            logger.info(f"Pattern-matched OUT_OF_DOMAIN ({pattern_type}): '{question[:50]}'")
            return {
                "intent": "OUT_OF_DOMAIN",
                "intent_confidence": 0.95,
                "intent_reasoning": f"Pattern matched: {pattern_type}",
                "domain_relevance": 0.0,
                # Multi-turn: ì‚¬ìš©ì ì§ˆë¬¸ì„ HumanMessageë¡œ ì €ì¥
                "messages": [HumanMessage(content=question)],
            }

    # 2. íŒ¨í„´ ë§¤ì¹­ì— ê±¸ë¦¬ì§€ ì•Šìœ¼ë©´ LLMìœ¼ë¡œ ë¶„ë¥˜
    response = await aclassify_intent(question, llm)

    # domain_relevanceê°€ ë§¤ìš° ë‚®ìœ¼ë©´ OUT_OF_DOMAINìœ¼ë¡œ ë³´ì • (Hard OOD ì„ê³„ê°’)
    intent = response.intent
    domain_relevance = response.domain_relevance

    if domain_relevance < 0.2 and intent != "OUT_OF_DOMAIN":
        logger.info(
            f"Low domain_relevance ({domain_relevance:.2f}), "
            f"overriding intent from {intent} to OUT_OF_DOMAIN"
        )
        intent = "OUT_OF_DOMAIN"

    if intent == "OUT_OF_DOMAIN":
        logger.info(
            f"OUT_OF_DOMAIN detected: domain_relevance={domain_relevance:.2f}, "
            f"question='{question[:50]}...'"
        )

    # Multi-turn: ì‚¬ìš©ì ì§ˆë¬¸ì„ HumanMessageë¡œ ì €ì¥
    return {
        "intent": intent,
        "intent_confidence": response.confidence,
        "intent_reasoning": response.reasoning,
        "domain_relevance": domain_relevance,
        "messages": [HumanMessage(content=question)],
    }


async def analyze_query_node(state: AdaptiveRAGState, llm: Runnable) -> QueryAnalysisUpdate:
    """
    ì§ˆì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ê³  ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬ ë° ê²€ìƒ‰ í•„í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ ë…¸ë“œëŠ” Query Analysis, Multi-Query Generation, Filter Extractionì„ í†µí•©í•˜ì—¬:
    1. ì§ˆì˜ì˜ ëª…í™•ì„±, êµ¬ì²´ì„±, ê²€ìƒ‰ ê°€ëŠ¥ì„±ì„ í‰ê°€
    2. ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ 3-5ê°œ ìƒì„± (Multi-Query)
    3. ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ í•„í„° ì¶”ì¶œ (doc_type, course, etc.)

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë¶„ì„ ë° ì¿¼ë¦¬ ìƒì„±ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸

    ë°˜í™˜ê°’:
        ì§ˆì˜ ë¶„ì„, ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬, ê²€ìƒ‰ í•„í„°ë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---ANALYZE QUERY & GENERATE MULTI-QUERY & EXTRACT FILTERS---")
    question = state["question"]
    intent = state.get("intent", "SIMPLE_QA")

    try:
        # VectorDB ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì™€ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
        data_source_context = None
        try:
            from naver_connect_chatbot.rag.schema_registry import (
                get_data_source_context,
                get_schema_registry,
            )

            data_source_context = get_data_source_context(max_courses=10)

            # ë³„ì¹­ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (VectorDB ê¸°ë°˜ ë™ì  ìƒì„±)
            registry = get_schema_registry()
            if registry.is_loaded():
                alias_context = registry.get_alias_context_for_prompt()
                if alias_context:
                    data_source_context = f"{data_source_context}\n\n{alias_context}"

            logger.debug("Data source context with aliases loaded for query analysis")
        except Exception as e:
            logger.warning(f"Failed to load data source context: {e}")

        # aanalyze_query ì§ì ‘ í˜¸ì¶œ
        response = await aanalyze_query(question, intent, llm, data_source_context)

        # retrieval_filtersë¥¼ RetrievalFilters TypedDictë¡œ ë³€í™˜
        filters = {}
        if response.retrieval_filters:
            rf = response.retrieval_filters
            if rf.doc_type:
                filters["doc_type"] = rf.doc_type
            if rf.course:
                # Fuzzy + Alias í›„ì²˜ë¦¬ë¡œ ê³¼ì •ëª… í™•ì¥
                try:
                    from naver_connect_chatbot.rag.schema_registry import get_schema_registry

                    registry = get_schema_registry()
                    if not registry.is_loaded():
                        logger.warning("SchemaRegistry not loaded, using original course names")
                        filters["course"] = rf.course
                    else:
                        resolved_courses: list[str] = []
                        for course in rf.course:
                            try:
                                resolved = registry.resolve_course_with_fuzzy(course)
                                resolved_courses.extend(resolved)
                            except Exception as course_error:
                                logger.error(
                                    f"Failed to resolve course '{course}': {course_error}",
                                    exc_info=True,
                                )
                                resolved_courses.append(course)  # Fallback to original
                        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                        filters["course"] = list(dict.fromkeys(resolved_courses))
                        logger.info(f"Course names resolved: {rf.course} â†’ {filters['course']}")
                except Exception as e:
                    logger.error(
                        f"Critical error in course fuzzy resolution: {e}",
                        exc_info=True,
                    )
                    filters["course"] = rf.course
            if rf.course_topic:
                filters["course_topic"] = rf.course_topic
            if rf.generation:
                filters["generation"] = rf.generation

        if filters:
            logger.info(f"Extracted retrieval filters: {filters}")

        # filter_confidence ì¶”ì¶œ
        filter_confidence = 1.0
        if response.retrieval_filters:
            filter_confidence = response.retrieval_filters.filter_confidence
            if filter_confidence < 0.5:
                logger.info(
                    f"Low filter confidence ({filter_confidence:.2f}), clarification may be needed"
                )

        # ë¶„ì„ ê²°ê³¼ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        return {
            "query_analysis": {
                "clarity_score": response.clarity_score,
                "specificity_score": response.specificity_score,
                "searchability_score": response.searchability_score,
            },
            "refined_queries": response.improved_queries
            if response.improved_queries
            else [question],
            "original_query": question,
            "retrieval_filters": filters if filters else None,
            "filter_confidence": filter_confidence,
        }

    except Exception as e:
        logger.error(f"Query analysis error: {e}")
        return {
            "query_analysis": {"error": str(e)},
            "refined_queries": [question],
            "original_query": question,
            "retrieval_filters": None,
            "filter_confidence": 1.0,  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’
        }


# =============================================================================
# Agent Node (Tool-based Retrieval)
# =============================================================================


def _build_agent_system_prompt(
    intent: str,
    domain_relevance: float,
    refined_queries: list[str],
) -> str:
    """
    Agent LLMì— ì „ë‹¬í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        intent: ë¶„ë¥˜ëœ ì§ˆë¬¸ ì˜ë„
        domain_relevance: ë„ë©”ì¸ ê´€ë ¨ì„± ì ìˆ˜ (0.0~1.0)
        refined_queries: ë¶„ì„ì„ í†µí•´ ìƒì„±ëœ ê°œì„  ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡

    ë°˜í™˜ê°’:
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    queries_hint = ""
    if refined_queries:
        queries_hint = (
            "\n\n## ì¶”ì²œ ê²€ìƒ‰ ì¿¼ë¦¬ (ì°¸ê³ ìš©)\n"
            + "\n".join(f"- {q}" for q in refined_queries)
        )

    return (
        "<role>ë‹¹ì‹ ì€ Naver Boost Camp AI Tech í•™ìƒë“¤ì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.</role>\n\n"
        "## ë„êµ¬ ì‚¬ìš© ì§€ì¹¨\n"
        "- `qdrant_search`: ë¶€ìŠ¤íŠ¸ìº í”„ êµìœ¡ ìë£Œ(ê°•ì˜, ë…¸íŠ¸ë¶, Slack Q&A, ë¯¸ì…˜) ê²€ìƒ‰\n"
        "- `web_search`: ì›¹ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰ (êµìœ¡ ìë£Œì— ì—†ëŠ” ì¼ë°˜ ê°œë…/ìµœì‹  ì •ë³´ìš©)\n\n"
        "## ê·œì¹™\n"
        "1. êµìœ¡ ìë£Œ ê´€ë ¨ ì§ˆë¬¸ì€ ë¨¼ì € `qdrant_search`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "2. êµìœ¡ ìë£Œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ `web_search`ë¡œ ë³´ì¶©í•˜ì„¸ìš”.\n"
        "3. ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¯¸ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë„êµ¬ ì—†ì´ ë°”ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "4. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "5. ë¬¸ì„œë¥¼ ì¸ìš©í•  ë•ŒëŠ” ëŒ€ê´„í˜¸ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ˆ: [ê°•ì˜ìë£Œ: CV ì´ë¡ /3ê°•]).\n"
        "6. 'ë¬¸ì„œ 1', 'ë¬¸ì„œ 2' ê°™ì€ ìˆœë²ˆ ì°¸ì¡°ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "7. ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¯¸ ë‹µë³€í•œ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        f"## í˜„ì¬ ì§ˆë¬¸ ë¶„ì„\n"
        f"- ì˜ë„: {intent}\n"
        f"- ë„ë©”ì¸ ê´€ë ¨ì„±: {domain_relevance:.2f}\n"
        f"{queries_hint}"
    )


async def agent_node(
    state: AdaptiveRAGState,
    llm: Runnable,
    tools: list,
) -> AgentUpdate:
    """
    LLMì— ë„êµ¬ë¥¼ ë°”ì¸ë”©í•˜ê³  í˜¸ì¶œí•˜ëŠ” ì—ì´ì „íŠ¸ ë…¸ë“œì…ë‹ˆë‹¤.

    LLMì´ í•„ìš”ì— ë”°ë¼ qdrant_search, web_search ë„êµ¬ë¥¼ ì„ íƒì ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
    tool_callsê°€ ìˆìœ¼ë©´ tools ë…¸ë“œë¡œ, ì—†ìœ¼ë©´ post_process ë…¸ë“œë¡œ ë¼ìš°íŒ…ë©ë‹ˆë‹¤.

    Multi-turn ì§€ì›:
    - ì´ì „ í„´ì˜ HumanMessage + AIMessage(ìµœì¢… ë‹µë³€ë§Œ) í¬í•¨
    - ì´ì „ í„´ì˜ ToolMessageëŠ” í•„í„°ë§í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì ˆì•½

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë„êµ¬ í˜¸ì¶œì´ ê°€ëŠ¥í•œ LLM (Gemini)
        tools: ë°”ì¸ë”©í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸

    ë°˜í™˜ê°’:
        messagesì— AIMessageê°€ appendëœ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---AGENT NODE---")
    messages = state.get("messages", [])
    intent = state.get("intent", "SIMPLE_QA")
    domain_relevance = state.get("domain_relevance", 1.0)
    refined_queries = state.get("refined_queries", [])

    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_content = _build_agent_system_prompt(intent, domain_relevance, refined_queries)

    # 2. LLMìš© ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    llm_messages: list[BaseMessage] = [SystemMessage(content=system_content)]

    # í˜„ì¬ í„´ ì‹œì‘ì  ì°¾ê¸° (ë§ˆì§€ë§‰ HumanMessage)
    current_turn_idx = 0
    for i in range(len(messages) - 1, -1, -1):
        if isinstance(messages[i], HumanMessage):
            current_turn_idx = i
            break

    # ì´ì „ í„´: HumanMessage + AIMessage(tool_calls ì—†ëŠ” ìµœì¢… ë‹µë³€ë§Œ)
    for msg in messages[:current_turn_idx]:
        if isinstance(msg, HumanMessage):
            llm_messages.append(msg)
        elif isinstance(msg, AIMessage) and not getattr(msg, "tool_calls", None):
            content = _extract_text_from_content(msg.content)
            if len(content) > 500:
                content = content[:500] + "..."
            llm_messages.append(AIMessage(content=content))

    # í˜„ì¬ í„´: HumanMessage + ë„êµ¬ í˜¸ì¶œ/ì‘ë‹µ ì „ë¶€ í¬í•¨
    for msg in messages[current_turn_idx:]:
        llm_messages.append(msg)

    # 3. LLM í˜¸ì¶œ (tools bind)
    llm_with_tools = llm.bind_tools(tools)
    response = await llm_with_tools.ainvoke(llm_messages)

    # 4. tool_call_count ì—…ë°ì´íŠ¸
    tool_call_count = state.get("tool_call_count", 0)
    if response.tool_calls:
        tool_call_count += 1
        logger.info(
            f"Agent requested {len(response.tool_calls)} tool call(s), "
            f"iteration {tool_call_count}"
        )
    else:
        answer_preview = _extract_text_from_content(response.content)[:100]
        logger.info(f"Agent produced final answer: {answer_preview}...")

    return {
        "messages": [response],
        "tool_call_count": tool_call_count,
    }


async def post_process_node(state: AdaptiveRAGState) -> PostProcessUpdate:
    """
    Agent ë£¨í”„ ì™„ë£Œ í›„ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.

    ì²˜ë¦¬ ë‚´ìš©:
    1. messagesì—ì„œ ë§ˆì§€ë§‰ AIMessage(tool_calls ì—†ëŠ” ê²ƒ)ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    2. Post-retrieval OOD ê°ì§€: ëª¨ë“  ë„êµ¬ê°€ "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ" + domain_relevance < 0.5
    3. ë‹µë³€ê³¼ ìƒì„± ë©”íƒ€ë°ì´í„° ë°˜í™˜

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ìµœì¢… ë‹µë³€ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---POST PROCESS---")
    messages = state.get("messages", [])
    domain_relevance = state.get("domain_relevance", 1.0)
    question = state.get("question", "")

    # ë§ˆì§€ë§‰ AIMessageì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ
    answer = ""
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            if not msg.tool_calls:
                answer = _extract_text_from_content(msg.content)
                break
            else:
                # max iterations ë„ë‹¬ â€” tool_callsëŠ” ìˆì§€ë§Œ í…ìŠ¤íŠ¸ë„ ìˆì„ ìˆ˜ ìˆìŒ
                text = _extract_text_from_content(msg.content)
                if text.strip():
                    answer = text
                    break

    if not answer:
        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    # Post-retrieval OOD ê°ì§€
    tool_msgs = [m for m in messages if isinstance(m, ToolMessage)]
    all_tools_empty = (
        tool_msgs
        and all("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ" in m.content for m in tool_msgs)
        and domain_relevance < 0.5
    )

    if all_tools_empty:
        logger.info(
            f"Post-retrieval soft decline: all tools empty + "
            f"low relevance ({domain_relevance:.2f})"
        )
        return {
            "answer": (
                f"'{question}'ì— ëŒ€í•´ êµìœ¡ ìë£Œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆìœ¼ë‚˜, "
                "ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
                "ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:\n"
                "- ì§ˆë¬¸ì— êµ¬ì²´ì ì¸ ê¸°ìˆ  ìš©ì–´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”\n"
                "- ë¶€ìŠ¤íŠ¸ìº í”„ ê°•ì˜ë‚˜ ê³¼ì œì™€ ê´€ë ¨ëœ ë§¥ë½ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”"
            ),
            "generation_metadata": {
                "strategy": "post_retrieval_soft_decline",
                "domain_relevance": domain_relevance,
            },
            "generation_strategy": "post_retrieval_soft_decline",
            "is_out_of_domain": True,
        }

    return {
        "answer": answer,
        "generation_metadata": {
            "strategy": "tool_based_agent",
            "tool_calls_count": state.get("tool_call_count", 0),
        },
        "generation_strategy": "tool_based_agent",
    }


async def generate_ood_response_node(state: AdaptiveRAGState) -> OODResponseUpdate:
    """
    Out-of-Domain ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    ì¸ì‚¬/ì¡ë‹´ì—ëŠ” ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ê³ , ê·¸ ì™¸ AI/ML êµìœ¡ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ”
    ì •ì¤‘íˆ ê±°ì ˆí•˜ê³  ë„ì›€ ê°€ëŠ¥í•œ ì˜ì—­ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        OOD ì‘ë‹µì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---GENERATE OOD RESPONSE---")

    question = state.get("question", "")
    domain_relevance = state.get("domain_relevance", 0.0)

    # íŒ¨í„´ ê°ì§€
    question_lower = question.lower().strip()

    # ì±—ë´‡ ìê¸°ì†Œê°œ íŒ¨í„´
    self_intro_patterns = [
        "ì´ë¦„ì´ ë­",
        "ë„Œ ëˆ„êµ¬",
        "ë„ˆ ëˆ„êµ¬",
        "ë­˜ í•  ìˆ˜ ìˆ",
        "ë­˜ ë„ì™€ì¤„ ìˆ˜ ìˆ",
        "ì–´ë–¤ ë´‡",
        "ë¬´ìŠ¨ ë´‡",
        "ë­í•˜ëŠ” ë´‡",
        "ì†Œê°œí•´",
        "ìê¸°ì†Œê°œ",
        "who are you",
        "what can you do",
        "what's your name",
    ]
    is_self_intro = any(pattern in question_lower for pattern in self_intro_patterns)

    # ì¸ì‚¬/ì¡ë‹´ íŒ¨í„´
    greeting_patterns = [
        "ì•ˆë…•",
        "ë°˜ê°€",
        "í•˜ì´",
        "í—¬ë¡œ",
        "hello",
        "hi ",
        "hey",
        "ì˜ ì§€ë‚´",
        "ë­í•´",
        "ì‹¬ì‹¬",
        "ì¢‹ì€ ì•„ì¹¨",
        "ì¢‹ì€ ì €ë…",
    ]
    is_greeting = any(pattern in question_lower for pattern in greeting_patterns)

    if is_self_intro:
        response = (
            "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” **ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech í•™ìŠµ ë„ìš°ë¯¸**ì…ë‹ˆë‹¤. ğŸ¤–\n\n"
            "ë¶€ìŠ¤íŠ¸ìº í”„ êµìœ¡ ê³¼ì •ì—ì„œ í•™ìŠµí•˜ì‹œë©´ì„œ ê¶ê¸ˆí•œ ì ì´ ìˆì„ ë•Œ ë„ì›€ì„ ë“œë¦¬ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡Œì–´ìš”.\n\n"
            "**ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ì˜ì—­:**\n"
            "â€¢ AI/ML ê°œë… ì„¤ëª… (Transformer, CNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±)\n"
            "â€¢ PyTorch, ë”¥ëŸ¬ë‹ ì½”ë“œ êµ¬í˜„ ë°©ë²•\n"
            "â€¢ ê°•ì˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸ (CV, NLP, RecSys)\n"
            "â€¢ ì‹¤ìŠµ ë° ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸\n\n"
            "í¸í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
        logger.info(f"Self-intro response generated for: '{question}'")
    elif is_greeting:
        response = (
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\n\n"
            "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦´ ìˆ˜ ìˆì–´ìš”:\n"
            "â€¢ AI/ML ê°œë… (Transformer, CNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±)\n"
            "â€¢ PyTorch, ë”¥ëŸ¬ë‹ ì½”ë“œ êµ¬í˜„\n"
            "â€¢ ê°•ì˜ ë‚´ìš© ë° ì‹¤ìŠµ/ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸\n\n"
            "í¸í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ¤–"
        )
        logger.info(f"Greeting response generated for: '{question}'")
    else:
        question_preview = question[:50] + "..." if len(question) > 50 else question
        response = (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. '{question_preview}'ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n\n"
            "ì €ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI êµìœ¡ ê³¼ì •ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            "â€¢ **AI/ML ê°œë… ì„¤ëª…** - Transformer, CNN, RNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±\n"
            "â€¢ **ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬** - PyTorch, TensorFlow ì‚¬ìš©ë²•\n"
            "â€¢ **ì½”ë“œ êµ¬í˜„ ë°©ë²•** - ëª¨ë¸ í•™ìŠµ, ë°ì´í„° ì „ì²˜ë¦¬ ë“±\n"
            "â€¢ **ê°•ì˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸** - CV, NLP, RecSys ê°•ì˜\n"
            "â€¢ **ì‹¤ìŠµ/ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸**\n\n"
            "ìœ„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ¤–"
        )
        logger.info(
            f"OOD response generated for question: '{question_preview}' "
            f"(domain_relevance: {domain_relevance:.2f})"
        )

    # Multi-turn: OOD ì‘ë‹µë„ AIMessageë¡œ ì €ì¥
    return {
        "answer": response,
        "generation_strategy": "ood_decline",
        "workflow_stage": "completed",
        "is_out_of_domain": True,
        "messages": [AIMessage(content=response)],
    }


async def clarify_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ì‚¬ìš©ìì—ê²Œ ëª…í™•í™”ë¥¼ ìš”ì²­í•˜ëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ëª…í™•í™” ìš”ì²­ ì‘ë‹µì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---CLARIFY FILTER---")

    question = state["question"]
    filters = state.get("retrieval_filters", {})
    courses = filters.get("course", []) if filters else []

    # ëª…í™•í™” ë©”ì‹œì§€ ìƒì„±
    clarification_parts = ["ì§ˆë¬¸ì„ ë” ì •í™•í•˜ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"]

    if courses:
        clarification_parts.append(f"'{question}'ì—ì„œ ì–¸ê¸‰í•˜ì‹  ê³¼ì •ì´ ë‹¤ìŒ ì¤‘ ì–´ëŠ ê²ƒì¸ê°€ìš”?\n")
        for i, course in enumerate(courses[:5], 1):
            clarification_parts.append(f"{i}. {course}")
        clarification_parts.append(
            "\nì›í•˜ì‹œëŠ” ê³¼ì • ë²ˆí˜¸ë¥¼ ì•Œë ¤ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
        )
    else:
        clarification_parts.append(
            "ì–´ë–¤ ìë£Œì—ì„œ ì°¾ì•„ë³¼ê¹Œìš”?\n"
            "- **ê°•ì˜ìë£Œ** (PDF ìŠ¬ë¼ì´ë“œ)\n"
            "- **ë…¹ì·¨ë¡** (ê°•ì˜ ë‚´ìš©)\n"
            "- **ìŠ¬ë™ Q&A** (ì§ˆì˜ì‘ë‹µ)\n"
            "- **ì‹¤ìŠµ ë…¸íŠ¸ë¶** (ì½”ë“œ)\n"
            "- **ë¯¸ì…˜** (ê³¼ì œ)\n"
        )

    clarification_message = "\n".join(clarification_parts)

    return {
        "answer": clarification_message,
        "workflow_stage": "awaiting_clarification",
    }


def finalize_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ì›Œí¬í”Œë¡œë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ì¢…ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---FINALIZE---")

    # ì›Œí¬í”Œë¡œë¥¼ ì™„ë£Œ ìƒíƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    return {
        "workflow_stage": "completed",
    }
