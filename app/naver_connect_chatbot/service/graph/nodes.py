"""
Adaptive RAG ì›Œí¬í”Œë¡œì—ì„œ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ ì§‘í•©.

ê° ë…¸ë“œëŠ” RAG í”„ë¡œì„¸ìŠ¤ì˜ ê°œë³„ ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
"""

from __future__ import annotations

from typing import Any

from langchain_core.runnables import Runnable
from langchain_core.retrievers import BaseRetriever
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage

from naver_connect_chatbot.service.graph.state import AdaptiveRAGState
from naver_connect_chatbot.service.graph.types import (
    IntentUpdate,
    QueryAnalysisUpdate,
    RetrievalUpdate,
    AnswerUpdate,
    OODResponseUpdate,
)
from naver_connect_chatbot.service.agents.intent_classifier import (
    aclassify_intent,
    IntentClassification,
)
from naver_connect_chatbot.service.agents.query_analyzer import (
    aanalyze_query,
    QueryAnalysis,
)
from naver_connect_chatbot.service.agents.answer_generator import (
    get_generation_strategy,
)
from naver_connect_chatbot.service.tool.retrieval_tool import retrieve_documents_async, RetrievalResult
from naver_connect_chatbot.rag import ClovaStudioReranker
from naver_connect_chatbot.config import logger, settings


# =============================================================================
# Multi-turn ëŒ€í™” ì§€ì› ìœ í‹¸ë¦¬í‹°
# =============================================================================

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ìµœëŒ€ í„´ ìˆ˜ (ë©”ëª¨ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬)
MAX_HISTORY_TURNS = 5


def _format_chat_history(messages: list[BaseMessage], max_turns: int = MAX_HISTORY_TURNS) -> str:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        messages: BaseMessage ë¦¬ìŠ¤íŠ¸ (HumanMessage, AIMessage)
        max_turns: í¬í•¨í•  ìµœëŒ€ í„´ ìˆ˜ (ê¸°ë³¸ê°’: 5)

    ë°˜í™˜ê°’:
        í¬ë§·íŒ…ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¬¸ìì—´

    ì˜ˆì‹œ:
        >>> messages = [HumanMessage(content="ì•ˆë…•"), AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”!")]
        >>> _format_chat_history(messages)
        "[ì´ì „ ëŒ€í™”]\nì‚¬ìš©ì: ì•ˆë…•\nì–´ì‹œìŠ¤í„´íŠ¸: ì•ˆë…•í•˜ì„¸ìš”!\n"
    """
    if not messages:
        return ""

    # ìµœê·¼ Ní„´ë§Œ ì‚¬ìš© (1í„´ = ì‚¬ìš©ì + AI ìŒ)
    # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœê·¼ 2*max_turnsê°œë§Œ ì„ íƒ
    recent_messages = messages[-(max_turns * 2):]

    if not recent_messages:
        return ""

    history_lines = ["[ì´ì „ ëŒ€í™”]"]
    for msg in recent_messages:
        if isinstance(msg, HumanMessage):
            history_lines.append(f"ì‚¬ìš©ì: {msg.content}")
        elif isinstance(msg, AIMessage):
            # AI ì‘ë‹µì€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
            content = msg.content
            if len(content) > 200:
                content = content[:200] + "..."
            history_lines.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")

    return "\n".join(history_lines) + "\n"


# =============================================================================
# ì‘ë‹µ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹°
# =============================================================================


def _extract_text_response(response: Any) -> str:
    """
    LangChain ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if isinstance(response, AIMessage):
        if isinstance(response.content, str):
            return response.content
        return str(response.content)

    if hasattr(response, "content"):
        content = response.content
        if isinstance(content, str):
            return content
        return str(content)

    if isinstance(response, dict):
        output = response.get("output")
        if isinstance(output, str):
            return output
        content_val = response.get("content")
        if isinstance(content_val, str):
            return content_val

    if isinstance(response, str):
        return response

    return str(response)


async def classify_intent_node(state: AdaptiveRAGState, llm: Runnable) -> IntentUpdate:
    """
    ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤ (OUT_OF_DOMAIN ê°ì§€ í¬í•¨).

    Note:
        tools/function calling ëŒ€ì‹  llm.invoke(prompt) í˜•íƒœë¡œ ì§ì ‘ í˜¸ì¶œí•˜ì—¬
        CLOVA HCX-007ì˜ reasoning ëª¨ë“œì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë¶„ë¥˜ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸

    ë°˜í™˜ê°’:
        ì˜ë„ ë¶„ë¥˜ ê²°ê³¼ê°€ í¬í•¨ëœ ìƒíƒœ ì—…ë°ì´íŠ¸ (domain_relevance í¬í•¨)
    """
    logger.info("---CLASSIFY INTENT---")
    question = state["question"]
    question_lower = question.lower().strip()

    # 1. íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í™•ì‹¤í•œ OUT_OF_DOMAIN ë¨¼ì € ì²˜ë¦¬ (LLM í˜¸ì¶œ ì—†ì´ ë¹ ë¥´ê²Œ)
    ood_patterns = {
        "greeting": [
            "ì•ˆë…•", "ë°˜ê°€", "í•˜ì´", "í—¬ë¡œ", "hello", "hi ", "hey",
            "ì˜ ì§€ë‚´", "ì¢‹ì€ ì•„ì¹¨", "ì¢‹ì€ ì €ë…",
        ],
        "self_intro": [
            "ì´ë¦„ì´ ë­", "ë„Œ ëˆ„êµ¬", "ë„ˆ ëˆ„êµ¬", "ë­˜ í•  ìˆ˜ ìˆ", "ë­˜ ë„ì™€ì¤„ ìˆ˜ ìˆ",
            "ì–´ë–¤ ë´‡", "ë¬´ìŠ¨ ë´‡", "ë­í•˜ëŠ” ë´‡", "ì†Œê°œí•´", "ìê¸°ì†Œê°œ",
            "who are you", "what can you do", "what's your name",
        ],
        "chitchat": [
            "ë­í•´", "ì‹¬ì‹¬", "ë°°ê³ íŒŒ", "ì¡¸ë ¤", "í”¼ê³¤",
        ],
        "off_topic": [
            "ë‚ ì”¨", "ì ì‹¬", "ì €ë…", "ì•„ì¹¨", "ë©”ë‰´ ì¶”ì²œ", "ë§›ì§‘",
            "ì—¬í–‰", "ì£¼ì‹", "íˆ¬ì", "ì—°ì˜ˆ", "ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ì•¼êµ¬",
        ],
    }

    for pattern_type, patterns in ood_patterns.items():
        if any(pattern in question_lower for pattern in patterns):
            logger.info(
                f"Pattern-matched OUT_OF_DOMAIN ({pattern_type}): '{question[:50]}'"
            )
            return {
                "intent": "OUT_OF_DOMAIN",
                "intent_confidence": 0.95,
                "intent_reasoning": f"Pattern matched: {pattern_type}",
                "domain_relevance": 0.0,
                # Multi-turn: ì‚¬ìš©ì ì§ˆë¬¸ì„ HumanMessageë¡œ ì €ì¥
                "messages": [HumanMessage(content=question)],
            }

    # 2. íŒ¨í„´ ë§¤ì¹­ì— ê±¸ë¦¬ì§€ ì•Šìœ¼ë©´ LLMìœ¼ë¡œ ë¶„ë¥˜
    response = await aclassify_intent(question, llm)

    # domain_relevanceê°€ ë‚®ìœ¼ë©´ OUT_OF_DOMAINìœ¼ë¡œ ë³´ì •
    intent = response.intent
    domain_relevance = response.domain_relevance

    if domain_relevance < 0.3 and intent != "OUT_OF_DOMAIN":
        logger.info(
            f"Low domain_relevance ({domain_relevance:.2f}), "
            f"overriding intent from {intent} to OUT_OF_DOMAIN"
        )
        intent = "OUT_OF_DOMAIN"

    if intent == "OUT_OF_DOMAIN":
        logger.info(
            f"OUT_OF_DOMAIN detected: domain_relevance={domain_relevance:.2f}, "
            f"question='{question[:50]}...'"
        )

    # Multi-turn: ì‚¬ìš©ì ì§ˆë¬¸ì„ HumanMessageë¡œ ì €ì¥
    return {
        "intent": intent,
        "intent_confidence": response.confidence,
        "intent_reasoning": response.reasoning,
        "domain_relevance": domain_relevance,
        "messages": [HumanMessage(content=question)],
    }


async def analyze_query_node(state: AdaptiveRAGState, llm: Runnable) -> QueryAnalysisUpdate:
    """
    ì§ˆì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ê³  ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬ ë° ê²€ìƒ‰ í•„í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ ë…¸ë“œëŠ” Query Analysis, Multi-Query Generation, Filter Extractionì„ í†µí•©í•˜ì—¬:
    1. ì§ˆì˜ì˜ ëª…í™•ì„±, êµ¬ì²´ì„±, ê²€ìƒ‰ ê°€ëŠ¥ì„±ì„ í‰ê°€
    2. ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ 3-5ê°œ ìƒì„± (Multi-Query)
    3. ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ í•„í„° ì¶”ì¶œ (doc_type, course, etc.)

    Pre-Retriever ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ:
    - VectorDB ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•˜ì—¬ LLMì´ ì‹¤ì œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì•Œê³  í•„í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ í•¨
    - ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œëœ SchemaRegistryì—ì„œ ë°ì´í„° ì†ŒìŠ¤ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´

    Note:
        tools/function calling ëŒ€ì‹  llm.invoke(prompt) í˜•íƒœë¡œ ì§ì ‘ í˜¸ì¶œí•˜ì—¬
        CLOVA HCX-007ì˜ reasoning ëª¨ë“œì™€ í˜¸í™˜ë©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ë¶„ì„ ë° ì¿¼ë¦¬ ìƒì„±ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸

    ë°˜í™˜ê°’:
        ì§ˆì˜ ë¶„ì„, ë‹¤ì¤‘ ê²€ìƒ‰ ì¿¼ë¦¬, ê²€ìƒ‰ í•„í„°ë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---ANALYZE QUERY & GENERATE MULTI-QUERY & EXTRACT FILTERS---")
    question = state["question"]
    intent = state.get("intent", "SIMPLE_QA")

    try:
        # VectorDB ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê°€ì ¸ì™€ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
        data_source_context = None
        try:
            from naver_connect_chatbot.rag.schema_registry import (
                get_data_source_context,
                get_schema_registry,
            )

            data_source_context = get_data_source_context(max_courses=10)

            # ë³„ì¹­ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (VectorDB ê¸°ë°˜ ë™ì  ìƒì„±)
            registry = get_schema_registry()
            if registry.is_loaded():
                alias_context = registry.get_alias_context_for_prompt()
                if alias_context:
                    data_source_context = f"{data_source_context}\n\n{alias_context}"

            logger.debug("Data source context with aliases loaded for query analysis")
        except Exception as e:
            logger.warning(f"Failed to load data source context: {e}")

        # aanalyze_query ì§ì ‘ í˜¸ì¶œ
        response = await aanalyze_query(question, intent, llm, data_source_context)

        # retrieval_filtersë¥¼ RetrievalFilters TypedDictë¡œ ë³€í™˜
        filters = {}
        if response.retrieval_filters:
            rf = response.retrieval_filters
            if rf.doc_type:
                filters["doc_type"] = rf.doc_type
            if rf.course:
                # Fuzzy + Alias í›„ì²˜ë¦¬ë¡œ ê³¼ì •ëª… í™•ì¥
                try:
                    from naver_connect_chatbot.rag.schema_registry import get_schema_registry

                    registry = get_schema_registry()
                    if not registry.is_loaded():
                        logger.warning(
                            "SchemaRegistry not loaded, using original course names"
                        )
                        filters["course"] = rf.course
                    else:
                        resolved_courses: list[str] = []
                        for course in rf.course:
                            try:
                                resolved = registry.resolve_course_with_fuzzy(course)
                                resolved_courses.extend(resolved)
                            except Exception as course_error:
                                logger.error(
                                    f"Failed to resolve course '{course}': {course_error}",
                                    exc_info=True,
                                )
                                resolved_courses.append(course)  # Fallback to original
                        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                        filters["course"] = list(dict.fromkeys(resolved_courses))
                        logger.info(
                            f"Course names resolved: {rf.course} â†’ {filters['course']}"
                        )
                except Exception as e:
                    logger.error(
                        f"Critical error in course fuzzy resolution: {e}",
                        exc_info=True,
                    )
                    filters["course"] = rf.course
            if rf.course_topic:
                filters["course_topic"] = rf.course_topic
            if rf.generation:
                filters["generation"] = rf.generation

        if filters:
            logger.info(f"Extracted retrieval filters: {filters}")

        # filter_confidence ì¶”ì¶œ
        filter_confidence = 1.0
        if response.retrieval_filters:
            filter_confidence = response.retrieval_filters.filter_confidence
            if filter_confidence < 0.5:
                logger.info(
                    f"Low filter confidence ({filter_confidence:.2f}), "
                    "clarification may be needed"
                )

        # ë¶„ì„ ê²°ê³¼ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        return {
            "query_analysis": {
                "clarity_score": response.clarity_score,
                "specificity_score": response.specificity_score,
                "searchability_score": response.searchability_score,
            },
            "refined_queries": response.improved_queries
            if response.improved_queries
            else [question],
            "original_query": question,
            "retrieval_filters": filters if filters else None,
            "filter_confidence": filter_confidence,
        }

    except Exception as e:
        logger.error(f"Query analysis error: {e}")
        return {
            "query_analysis": {"error": str(e)},
            "refined_queries": [question],
            "original_query": question,
            "retrieval_filters": None,
            "filter_confidence": 1.0,  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’
        }


async def retrieve_node(state: AdaptiveRAGState, retriever: BaseRetriever) -> RetrievalUpdate:
    """
    ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        retriever: ë¬¸ì„œ ê²€ìƒ‰ê¸°

    ë°˜í™˜ê°’:
        ê²€ìƒ‰ëœ ë¬¸ì„œì™€ í•„í„°ë§ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---RETRIEVE---")

    # ê°€ëŠ¥í•˜ë©´ ì •ì œëœ ì§ˆì˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    queries = state.get("refined_queries", [state["question"]])
    primary_query = queries[0] if queries else state["question"]

    # ìƒíƒœì—ì„œ í•„í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    filters = state.get("retrieval_filters")
    if filters:
        logger.info(f"Applying retrieval filters: {filters}")

    try:
        # ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
        result: RetrievalResult = await retrieve_documents_async(
            retriever,
            primary_query,
            filters=filters,
            fallback_on_empty=True,
            min_results=1,
        )

        logger.info(
            f"Retrieved {result.original_count} docs, "
            f"filtered to {result.filtered_count}, "
            f"filters_applied={result.filters_applied}, "
            f"fallback_used={result.fallback_used}"
        )

        return {
            "documents": result.documents,
            "context": result.documents,  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ë¥¼ ìœ„í•œ í•„ë“œ
            "retrieval_strategy": "hybrid",
            "retrieval_filters_applied": result.filters_applied,
            "retrieval_fallback_used": result.fallback_used,
            "retrieval_metadata": {
                "original_count": result.original_count,
                "filtered_count": result.filtered_count,
                "filters": filters,
            },
        }

    except Exception as e:
        logger.error(f"Retrieval error: {e}")
        return {
            "documents": [],
            "context": [],
            "retrieval_strategy": "hybrid",
            "retrieval_filters_applied": False,
            "retrieval_fallback_used": False,
        }


async def rerank_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ Clova Studio Rerankerë¡œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.

    Post-Retriever ë‹¨ê³„ë¡œ, ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ë„ë¥¼ ì¬í‰ê°€í•˜ì—¬
    ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ìƒìœ„ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ì¬ì •ë ¬ëœ ë¬¸ì„œë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---RERANK DOCUMENTS---")

    question = state["question"]
    documents = state.get("documents", [])

    if not documents:
        logger.warning("No documents to rerank")
        return {
            "documents": [],
            "context": [],
        }

    # Reranking ì„¤ì • í™•ì¸
    use_reranking = (
        settings.adaptive_rag.use_reranking if hasattr(settings, "adaptive_rag") else True
    )

    if not use_reranking:
        logger.info("Reranking disabled, skipping")
        return {
            "documents": documents,
            "context": documents,
        }

    try:
        # Clova Studio Reranker ì´ˆê¸°í™” (settings.rerankerì—ì„œ endpoint, api_key ë“± ë¡œë“œ)
        reranker = ClovaStudioReranker.from_settings(settings.reranker)

        # Reranking ìˆ˜í–‰
        logger.info(f"Reranking {len(documents)} documents")
        reranked_docs = await reranker.arerank(
            query=question,
            documents=documents,
            top_k=min(len(documents), 10),  # ìµœëŒ€ 10ê°œê¹Œì§€ ìœ ì§€
        )

        logger.info(f"Reranked to {len(reranked_docs)} documents")

        return {
            "documents": reranked_docs,
            "context": reranked_docs,
        }

    except Exception as e:
        logger.error(f"Reranking error: {e}, using original documents")
        return {
            "documents": documents,
            "context": documents,
        }


async def generate_answer_node(state: AdaptiveRAGState, llm: Runnable) -> AnswerUpdate:
    """
    ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ (Reasoning ëª¨ë“œ í™œìš©).

    CLOVA HCX-007 ëª¨ë¸ì˜ Reasoning ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬:
    1. ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í†µí•´ ë‹µë³€ í’ˆì§ˆ í–¥ìƒ
    2. ìì²´ ê²€ì¦ì„ í†µí•´ í™˜ê° ë°©ì§€
    3. ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë…¼ë¦¬ì  ë‹µë³€ ìƒì„±

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ
        llm: ìƒì„±ì— ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸ (Reasoning ì§€ì›)

    ë°˜í™˜ê°’:
        ìƒì„±ëœ ë‹µë³€ì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---GENERATE ANSWER (with Reasoning)---")
    question = state["question"]
    documents = state.get("documents", [])
    intent = state.get("intent", "SIMPLE_QA")

    # Multi-turn: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (í˜„ì¬ ì§ˆë¬¸ HumanMessage ì œì™¸)
    messages = state.get("messages", [])
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€(í˜„ì¬ ì§ˆë¬¸)ë¥¼ ì œì™¸í•œ ì´ì „ ëŒ€í™”ë§Œ í¬í•¨
    previous_messages = messages[:-1] if messages else []
    chat_history = _format_chat_history(previous_messages)

    if chat_history:
        logger.info(f"Multi-turn context: {len(previous_messages)} previous messages")

    try:
        # ì‚¬ìš©í•  ìƒì„± ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
        strategy = get_generation_strategy(intent)

        # Reasoning effort ì„¤ì • (intent ê¸°ë°˜)
        # COMPLEX_REASONING: high, EXPLORATORY: medium, SIMPLE_QA: low
        thinking_effort = "medium"  # ê¸°ë³¸ê°’
        if intent == "COMPLEX_REASONING":
            thinking_effort = "high"
        elif intent == "SIMPLE_QA":
            thinking_effort = "low"
        elif intent == "EXPLORATORY":
            thinking_effort = "medium"

        logger.info(f"Using thinking_effort: {thinking_effort} for intent: {intent}")

        # ìƒì„±ì— ì‚¬ìš©í•  ë¬¸ë§¥ì„ í¬ë§·í•©ë‹ˆë‹¤.
        context_text = "\n\n".join(
            [f"[ë¬¸ì„œ {i + 1}]\n{doc.page_content}" for i, doc in enumerate(documents)]
        )

        if not context_text:
            context_text = "ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        # Multi-turn í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # ì´ì „ ëŒ€í™”ê°€ ìˆìœ¼ë©´ í¬í•¨
        if chat_history:
            prompt = (
                "ë‹¹ì‹ ì€ Naver Boost Camp í•™ìƒë“¤ì—ê²Œ AI/MLì„ ê°€ë¥´ì¹˜ëŠ” ì¡°êµì…ë‹ˆë‹¤. "
                "ì£¼ì–´ì§„ ë¬¸ë§¥ê³¼ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬, ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•œ ë’¤ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                f"{chat_history}\n"
                f"[í˜„ì¬ ì§ˆë¬¸]\nquestion: {question}\n\ncontext:\n{context_text}"
            )
        else:
            prompt = (
                "ë‹¹ì‹ ì€ Naver Boost Camp í•™ìƒë“¤ì—ê²Œ AI/MLì„ ê°€ë¥´ì¹˜ëŠ” ì¡°êµì…ë‹ˆë‹¤. "
                "ì£¼ì–´ì§„ ë¬¸ë§¥ë§Œì„ ê·¼ê±°ë¡œ, ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•œ ë’¤ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
                f"question: {question}\n\ncontext:\n{context_text}"
            )

        response_raw = await llm.ainvoke(prompt)
        answer = _extract_text_response(response_raw)
        logger.info(f"Generated answer with reasoning: {answer[:100]}...")

        # Multi-turn: AI ì‘ë‹µì„ AIMessageë¡œ ì €ì¥
        return {
            "answer": answer,
            "generation_metadata": {
                "strategy": strategy,
                "context_length": len(context_text),
                "thinking_effort": thinking_effort,
                "reasoning_enabled": True,
                "has_chat_history": bool(chat_history),
            },
            "generation_strategy": strategy,
            "messages": [AIMessage(content=answer)],
        }

    except Exception as e:
        logger.error(f"Answer generation error: {e}")
        error_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return {
            "answer": error_answer,
            "generation_metadata": {"error": str(e)},
            "generation_strategy": "error",
            "messages": [AIMessage(content=error_answer)],
        }


async def generate_ood_response_node(state: AdaptiveRAGState) -> OODResponseUpdate:
    """
    Out-of-Domain ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    ì¸ì‚¬/ì¡ë‹´ì—ëŠ” ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ê³ , ê·¸ ì™¸ AI/ML êµìœ¡ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ”
    ì •ì¤‘íˆ ê±°ì ˆí•˜ê³  ë„ì›€ ê°€ëŠ¥í•œ ì˜ì—­ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        OOD ì‘ë‹µì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---GENERATE OOD RESPONSE---")

    question = state.get("question", "")
    domain_relevance = state.get("domain_relevance", 0.0)

    # íŒ¨í„´ ê°ì§€
    question_lower = question.lower().strip()

    # ì±—ë´‡ ìê¸°ì†Œê°œ íŒ¨í„´
    self_intro_patterns = [
        "ì´ë¦„ì´ ë­", "ë„Œ ëˆ„êµ¬", "ë„ˆ ëˆ„êµ¬", "ë­˜ í•  ìˆ˜ ìˆ", "ë­˜ ë„ì™€ì¤„ ìˆ˜ ìˆ",
        "ì–´ë–¤ ë´‡", "ë¬´ìŠ¨ ë´‡", "ë­í•˜ëŠ” ë´‡", "ì†Œê°œí•´", "ìê¸°ì†Œê°œ",
        "who are you", "what can you do", "what's your name",
    ]
    is_self_intro = any(pattern in question_lower for pattern in self_intro_patterns)

    # ì¸ì‚¬/ì¡ë‹´ íŒ¨í„´
    greeting_patterns = [
        "ì•ˆë…•", "ë°˜ê°€", "í•˜ì´", "í—¬ë¡œ", "hello", "hi ", "hey",
        "ì˜ ì§€ë‚´", "ë­í•´", "ì‹¬ì‹¬", "ì¢‹ì€ ì•„ì¹¨", "ì¢‹ì€ ì €ë…",
    ]
    is_greeting = any(pattern in question_lower for pattern in greeting_patterns)

    if is_self_intro:
        # ì±—ë´‡ ìê¸°ì†Œê°œ ì‘ë‹µ
        response = (
            "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” **ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech í•™ìŠµ ë„ìš°ë¯¸**ì…ë‹ˆë‹¤. ğŸ¤–\n\n"
            "ë¶€ìŠ¤íŠ¸ìº í”„ êµìœ¡ ê³¼ì •ì—ì„œ í•™ìŠµí•˜ì‹œë©´ì„œ ê¶ê¸ˆí•œ ì ì´ ìˆì„ ë•Œ ë„ì›€ì„ ë“œë¦¬ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡Œì–´ìš”.\n\n"
            "**ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ì˜ì—­:**\n"
            "â€¢ AI/ML ê°œë… ì„¤ëª… (Transformer, CNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±)\n"
            "â€¢ PyTorch, ë”¥ëŸ¬ë‹ ì½”ë“œ êµ¬í˜„ ë°©ë²•\n"
            "â€¢ ê°•ì˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸ (CV, NLP, RecSys)\n"
            "â€¢ ì‹¤ìŠµ ë° ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸\n\n"
            "í¸í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ˜Š"
        )
        logger.info(f"Self-intro response generated for: '{question}'")
    elif is_greeting:
        # ì¹œê·¼í•œ ì¸ì‚¬ ì‘ë‹µ
        response = (
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\n\n"
            "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦´ ìˆ˜ ìˆì–´ìš”:\n"
            "â€¢ AI/ML ê°œë… (Transformer, CNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±)\n"
            "â€¢ PyTorch, ë”¥ëŸ¬ë‹ ì½”ë“œ êµ¬í˜„\n"
            "â€¢ ê°•ì˜ ë‚´ìš© ë° ì‹¤ìŠµ/ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸\n\n"
            "í¸í•˜ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ¤–"
        )
        logger.info(f"Greeting response generated for: '{question}'")
    else:
        # ì¼ë°˜ OOD ê±°ì ˆ ì‘ë‹µ
        question_preview = question[:50] + "..." if len(question) > 50 else question
        response = (
            f"ì£„ì†¡í•©ë‹ˆë‹¤. '{question_preview}'ì— ëŒ€í•´ì„œëŠ” ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n\n"
            "ì €ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI êµìœ¡ ê³¼ì •ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            "â€¢ **AI/ML ê°œë… ì„¤ëª…** - Transformer, CNN, RNN, ì¶”ì²œ ì‹œìŠ¤í…œ ë“±\n"
            "â€¢ **ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬** - PyTorch, TensorFlow ì‚¬ìš©ë²•\n"
            "â€¢ **ì½”ë“œ êµ¬í˜„ ë°©ë²•** - ëª¨ë¸ í•™ìŠµ, ë°ì´í„° ì „ì²˜ë¦¬ ë“±\n"
            "â€¢ **ê°•ì˜ ë‚´ìš© ê´€ë ¨ ì§ˆë¬¸** - CV, NLP, RecSys ê°•ì˜\n"
            "â€¢ **ì‹¤ìŠµ/ê³¼ì œ ê´€ë ¨ ì§ˆë¬¸**\n\n"
            "ìœ„ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë„ì›€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ¤–"
        )
        logger.info(
            f"OOD response generated for question: '{question_preview}' "
            f"(domain_relevance: {domain_relevance:.2f})"
        )

    # Multi-turn: OOD ì‘ë‹µë„ AIMessageë¡œ ì €ì¥
    return {
        "answer": response,
        "generation_strategy": "ood_decline",
        "workflow_stage": "completed",
        "is_out_of_domain": True,
        "messages": [AIMessage(content=response)],
    }


async def clarify_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ì‚¬ìš©ìì—ê²Œ ëª…í™•í™”ë¥¼ ìš”ì²­í•˜ëŠ” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    í•„í„° ì¶”ì¶œ ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ (filter_confidence < 0.5) í˜¸ì¶œë˜ì–´
    ì‚¬ìš©ìì—ê²Œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì¢í ìˆ˜ ìˆëŠ” ì„ íƒì§€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ëª…í™•í™” ìš”ì²­ ì‘ë‹µì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---CLARIFY FILTER---")

    question = state["question"]
    filters = state.get("retrieval_filters", {})
    courses = filters.get("course", []) if filters else []

    # ëª…í™•í™” ë©”ì‹œì§€ ìƒì„±
    clarification_parts = [
        "ì§ˆë¬¸ì„ ë” ì •í™•í•˜ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
    ]

    if courses:
        clarification_parts.append(f"'{question}'ì—ì„œ ì–¸ê¸‰í•˜ì‹  ê³¼ì •ì´ ë‹¤ìŒ ì¤‘ ì–´ëŠ ê²ƒì¸ê°€ìš”?\n")
        for i, course in enumerate(courses[:5], 1):
            clarification_parts.append(f"{i}. {course}")
        clarification_parts.append("\nì›í•˜ì‹œëŠ” ê³¼ì • ë²ˆí˜¸ë¥¼ ì•Œë ¤ì£¼ì‹œê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.")
    else:
        clarification_parts.append(
            "ì–´ë–¤ ìë£Œì—ì„œ ì°¾ì•„ë³¼ê¹Œìš”?\n"
            "- **ê°•ì˜ìë£Œ** (PDF ìŠ¬ë¼ì´ë“œ)\n"
            "- **ë…¹ì·¨ë¡** (ê°•ì˜ ë‚´ìš©)\n"
            "- **ìŠ¬ë™ Q&A** (ì§ˆì˜ì‘ë‹µ)\n"
            "- **ì‹¤ìŠµ ë…¸íŠ¸ë¶** (ì½”ë“œ)\n"
            "- **ë¯¸ì…˜** (ê³¼ì œ)\n"
        )

    clarification_message = "\n".join(clarification_parts)

    return {
        "answer": clarification_message,
        "workflow_stage": "awaiting_clarification",
    }


def finalize_node(state: AdaptiveRAGState) -> dict[str, Any]:
    """
    ì›Œí¬í”Œë¡œë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        state: í˜„ì¬ ì›Œí¬í”Œë¡œ ìƒíƒœ

    ë°˜í™˜ê°’:
        ì¢…ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    logger.info("---FINALIZE---")

    # ì›Œí¬í”Œë¡œë¥¼ ì™„ë£Œ ìƒíƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    return {
        "workflow_stage": "completed",
    }
