{
  "version": "2.0.0",
  "created_at": "2025-12-09",
  "description": "Naver Connect Boost Camp RAG 평가 데이터셋 v2 - LLM-as-Judge 기반 확장 버전",
  "statistics": {
    "total_questions": 80,
    "by_category": {
      "in_domain": 50,
      "out_of_domain": 15,
      "edge_case": 15
    }
  },
  "categories": {
    "in_domain": {
      "concept": "개념/정의 질문",
      "implementation": "구현/코드 질문",
      "troubleshooting": "트러블슈팅 질문",
      "comparison": "비교/분석 질문",
      "course_specific": "과정 특정 질문",
      "source_specific": "자료 유형 특정 질문"
    },
    "out_of_domain": {
      "unrelated": "관련 없는 질문",
      "ambiguous": "불완전/애매한 질문",
      "hallucination_inducing": "할루시네이션 유도 질문",
      "boundary": "경계 질문"
    },
    "edge_case": {
      "multi_hop": "여러 문서 종합 필요",
      "temporal": "시간/순서 추론 필요",
      "negation": "부정/제외 조건 처리",
      "code_execution": "코드 동작 설명",
      "meta_question": "자료 메타정보 질문"
    }
  },
  "questions": [
    {
      "id": "in_concept_001",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "Transformer의 Self-Attention 메커니즘이 무엇인가요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP", "NLP 이론"]
      },
      "expected_topics": ["transformer", "attention", "self-attention", "query", "key", "value"],
      "ground_truth": {
        "answer_keywords": ["query", "key", "value", "scaled dot-product", "attention weight"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "in_concept_002",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "CNN에서 Convolution 연산의 원리를 설명해주세요",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV", "CV 이론", "level2_cv", "Computer Vision"]
      },
      "expected_topics": ["cnn", "convolution", "kernel", "filter", "feature map"],
      "ground_truth": {
        "answer_keywords": ["커널", "필터", "특징 맵", "stride", "padding"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["CV"]
      }
    },
    {
      "id": "in_concept_003",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "Collaborative Filtering과 Content-Based Filtering의 차이점은?",
      "expected_filters": {
        "doc_type": null,
        "course": ["RecSys", "RecSys 이론", "level2_recsys"]
      },
      "expected_topics": ["collaborative filtering", "content-based", "추천 시스템"],
      "ground_truth": {
        "answer_keywords": ["협업 필터링", "콘텐츠 기반", "유사도", "아이템", "사용자"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["RecSys"]
      }
    },
    {
      "id": "in_concept_004",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "AI Math에서 확률과 통계 기초 개념 정리해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "pdf"],
        "course": ["AI Math", "AI 수학"]
      },
      "expected_topics": ["probability", "statistics", "확률", "통계"],
      "ground_truth": {
        "answer_keywords": ["확률", "분포", "기댓값", "분산"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["AI Math"]
      }
    },
    {
      "id": "in_concept_005",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "Batch Normalization이 왜 학습에 도움이 되나요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["batch normalization", "regularization", "internal covariate shift"],
      "ground_truth": {
        "answer_keywords": ["정규화", "gradient", "수렴", "internal covariate shift"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_concept_006",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "Dropout의 작동 원리와 효과는 무엇인가요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["dropout", "regularization", "overfitting"],
      "ground_truth": {
        "answer_keywords": ["뉴런", "비활성화", "앙상블", "과적합", "정규화"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_concept_007",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "Word2Vec의 Skip-gram과 CBOW 차이점이 뭐예요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP", "NLP 이론"]
      },
      "expected_topics": ["word2vec", "skip-gram", "cbow", "embedding"],
      "ground_truth": {
        "answer_keywords": ["중심 단어", "주변 단어", "context", "target"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "in_concept_008",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "Gradient Vanishing 문제가 무엇이고 어떻게 해결하나요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["gradient vanishing", "deep network", "activation"],
      "ground_truth": {
        "answer_keywords": ["기울기 소실", "ReLU", "residual", "초기화"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_concept_009",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "GAN의 Generator와 Discriminator 역할 설명해줘",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV", "Computer Vision"]
      },
      "expected_topics": ["gan", "generator", "discriminator", "adversarial"],
      "ground_truth": {
        "answer_keywords": ["생성자", "판별자", "적대적", "진짜", "가짜"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["CV", "Deep Learning"]
      }
    },
    {
      "id": "in_concept_010",
      "category": "in_domain",
      "subcategory": "concept",
      "question": "LSTM에서 Gate들의 역할이 뭔가요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP", "NLP 이론"]
      },
      "expected_topics": ["lstm", "gate", "forget gate", "input gate", "output gate"],
      "ground_truth": {
        "answer_keywords": ["forget gate", "input gate", "output gate", "cell state"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["NLP", "Deep Learning"]
      }
    },
    {
      "id": "in_impl_001",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "PyTorch에서 DataLoader를 어떻게 사용하나요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["PyTorch", "파이토치 기초"]
      },
      "expected_topics": ["dataloader", "dataset", "batch", "pytorch"],
      "ground_truth": {
        "answer_keywords": ["Dataset", "DataLoader", "batch_size", "shuffle", "num_workers"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_impl_002",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "학습률 스케줄러를 구현하는 방법이 궁금해요",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["learning rate", "scheduler", "optimizer", "pytorch"],
      "ground_truth": {
        "answer_keywords": ["scheduler", "StepLR", "CosineAnnealing", "optimizer"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["PyTorch", "Deep Learning"]
      }
    },
    {
      "id": "in_impl_003",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "MRC 과제에서 BERT로 질의응답 모델 어떻게 구현하나요?",
      "expected_filters": {
        "doc_type": ["notebook", "weekly_mission"],
        "course": ["MRC"]
      },
      "expected_topics": ["mrc", "bert", "question answering", "qa"],
      "ground_truth": {
        "answer_keywords": ["BertForQuestionAnswering", "start_positions", "end_positions"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": ["MRC"]
      }
    },
    {
      "id": "in_impl_004",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "Data Engineering에서 데이터 파이프라인 구축 방법?",
      "expected_filters": {
        "doc_type": null,
        "course": ["Data Engineering", "Data Eng", "데이터 엔지니어링"]
      },
      "expected_topics": ["pipeline", "data engineering", "etl"],
      "ground_truth": {
        "answer_keywords": ["파이프라인", "ETL", "데이터 처리"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["Data Engineering"]
      }
    },
    {
      "id": "in_impl_005",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "PyTorch에서 Custom Dataset 클래스 만드는 방법 알려줘",
      "expected_filters": {
        "doc_type": null,
        "course": ["PyTorch"]
      },
      "expected_topics": ["dataset", "custom dataset", "pytorch", "__getitem__"],
      "ground_truth": {
        "answer_keywords": ["__init__", "__len__", "__getitem__", "Dataset"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_impl_006",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "모델 체크포인트 저장하고 불러오는 코드 작성 방법?",
      "expected_filters": {
        "doc_type": null,
        "course": ["PyTorch"]
      },
      "expected_topics": ["checkpoint", "save", "load", "state_dict"],
      "ground_truth": {
        "answer_keywords": ["state_dict", "torch.save", "torch.load", "checkpoint"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_impl_007",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "Hugging Face Transformers로 BERT 토크나이저 사용법",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP"]
      },
      "expected_topics": ["huggingface", "tokenizer", "bert", "transformers"],
      "ground_truth": {
        "answer_keywords": ["AutoTokenizer", "from_pretrained", "input_ids", "attention_mask"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "in_impl_008",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "이미지 데이터 증강(augmentation) 코드 예시 보여줘",
      "expected_filters": {
        "doc_type": ["notebook"],
        "course": ["CV", "Computer Vision"]
      },
      "expected_topics": ["augmentation", "transform", "albumentations"],
      "ground_truth": {
        "answer_keywords": ["transform", "RandomCrop", "Normalize", "augmentation"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["CV"]
      }
    },
    {
      "id": "in_impl_009",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "Multi-GPU 학습을 위한 DataParallel 사용법?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["multi-gpu", "dataparallel", "distributed"],
      "ground_truth": {
        "answer_keywords": ["DataParallel", "DistributedDataParallel", "cuda", "device"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_impl_010",
      "category": "in_domain",
      "subcategory": "implementation",
      "question": "WandB로 실험 로깅하는 방법 알려주세요",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["wandb", "logging", "experiment", "tracking"],
      "ground_truth": {
        "answer_keywords": ["wandb.init", "wandb.log", "config", "run"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["Deep Learning", "PyTorch"]
      }
    },
    {
      "id": "in_trouble_001",
      "category": "in_domain",
      "subcategory": "troubleshooting",
      "question": "CUDA out of memory 에러가 발생하는데 어떻게 해결하나요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["cuda", "memory", "gpu", "oom", "out of memory"],
      "ground_truth": {
        "answer_keywords": ["batch size", "gradient accumulation", "mixed precision", "메모리"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["PyTorch", "Deep Learning"]
      }
    },
    {
      "id": "in_trouble_002",
      "category": "in_domain",
      "subcategory": "troubleshooting",
      "question": "모델이 수렴하지 않고 loss가 발산하는데 원인이 뭘까요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["loss", "diverge", "학습", "수렴", "발산"],
      "ground_truth": {
        "answer_keywords": ["학습률", "learning rate", "gradient", "초기화", "normalization"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_trouble_003",
      "category": "in_domain",
      "subcategory": "troubleshooting",
      "question": "학습 중 NaN loss가 발생하는 원인과 해결법?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["nan", "loss", "gradient", "exploding"],
      "ground_truth": {
        "answer_keywords": ["gradient clipping", "학습률", "수치 안정성", "overflow"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_trouble_004",
      "category": "in_domain",
      "subcategory": "troubleshooting",
      "question": "Validation loss는 낮은데 Test 성능이 안 나와요",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["overfitting", "generalization", "test", "validation"],
      "ground_truth": {
        "answer_keywords": ["과적합", "데이터 분포", "augmentation", "regularization"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_trouble_005",
      "category": "in_domain",
      "subcategory": "troubleshooting",
      "question": "학습이 느린데 bottleneck을 찾는 방법이 있나요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["bottleneck", "profiling", "performance", "speed"],
      "ground_truth": {
        "answer_keywords": ["profiler", "DataLoader", "num_workers", "pin_memory"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_trouble_006",
      "category": "in_domain",
      "subcategory": "troubleshooting",
      "question": "RuntimeError: CUDA error 관련 디버깅 방법?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["cuda error", "debugging", "device"],
      "ground_truth": {
        "answer_keywords": ["CUDA_LAUNCH_BLOCKING", "device", "synchronize"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_compare_001",
      "category": "in_domain",
      "subcategory": "comparison",
      "question": "Adam과 SGD 옵티마이저의 장단점을 비교해주세요",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["adam", "sgd", "optimizer", "옵티마이저"],
      "ground_truth": {
        "answer_keywords": ["모멘텀", "adaptive", "learning rate", "수렴 속도", "일반화"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["Deep Learning", "PyTorch"]
      }
    },
    {
      "id": "in_compare_002",
      "category": "in_domain",
      "subcategory": "comparison",
      "question": "ResNet과 VGG 아키텍처의 차이점은?",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV", "CV 이론", "level2_cv"]
      },
      "expected_topics": ["resnet", "vgg", "skip connection", "residual"],
      "ground_truth": {
        "answer_keywords": ["skip connection", "residual", "깊이", "gradient", "vanishing"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["CV"]
      }
    },
    {
      "id": "in_compare_003",
      "category": "in_domain",
      "subcategory": "comparison",
      "question": "RNN, LSTM, GRU의 차이점을 설명해줘",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP"]
      },
      "expected_topics": ["rnn", "lstm", "gru", "sequential"],
      "ground_truth": {
        "answer_keywords": ["gate", "cell state", "hidden state", "long-term dependency"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["NLP", "Deep Learning"]
      }
    },
    {
      "id": "in_compare_004",
      "category": "in_domain",
      "subcategory": "comparison",
      "question": "Cross-Entropy Loss와 MSE Loss는 언제 각각 사용하나요?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["cross entropy", "mse", "loss function", "classification", "regression"],
      "ground_truth": {
        "answer_keywords": ["분류", "회귀", "확률", "classification", "regression"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "in_compare_005",
      "category": "in_domain",
      "subcategory": "comparison",
      "question": "YOLO와 Faster R-CNN의 장단점 비교",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV", "Computer Vision"]
      },
      "expected_topics": ["yolo", "faster rcnn", "object detection", "real-time"],
      "ground_truth": {
        "answer_keywords": ["속도", "정확도", "one-stage", "two-stage", "real-time"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["CV"]
      }
    },
    {
      "id": "in_compare_006",
      "category": "in_domain",
      "subcategory": "comparison",
      "question": "GPT와 BERT의 아키텍처 차이가 뭔가요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP"]
      },
      "expected_topics": ["gpt", "bert", "transformer", "encoder", "decoder"],
      "ground_truth": {
        "answer_keywords": ["encoder", "decoder", "autoregressive", "bidirectional", "masked"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "in_course_001",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "CV 강의에서 Object Detection 부분 설명해주세요",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "pdf"],
        "course": ["CV", "CV 이론", "level2_cv", "Computer Vision"]
      },
      "expected_topics": ["object detection", "yolo", "faster rcnn", "cv"],
      "ground_truth": {
        "answer_keywords": ["bounding box", "anchor", "YOLO", "R-CNN"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["CV"]
      }
    },
    {
      "id": "in_course_002",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "NLP 강의에서 BERT 파인튜닝 방법 알려주세요",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "pdf"],
        "course": ["NLP", "NLP 이론"]
      },
      "expected_topics": ["bert", "fine-tuning", "nlp", "pretrained"],
      "ground_truth": {
        "answer_keywords": ["fine-tuning", "pre-trained", "downstream", "classification"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "in_course_003",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "RecSys 3강에서 다룬 Matrix Factorization 내용 정리해주세요",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": ["RecSys", "RecSys 이론"]
      },
      "expected_topics": ["matrix factorization", "svd", "latent factor"],
      "ground_truth": {
        "answer_keywords": ["행렬 분해", "잠재 요인", "user matrix", "item matrix"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["RecSys"]
      }
    },
    {
      "id": "in_course_004",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "PyTorch 기초 강의에서 텐서 연산 부분 정리해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "pdf"],
        "course": ["PyTorch", "파이토치 기초"]
      },
      "expected_topics": ["tensor", "pytorch", "operation"],
      "ground_truth": {
        "answer_keywords": ["tensor", "shape", "연산", "broadcasting"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "in_course_005",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "AI Math 강의에서 경사하강법 설명한 부분 알려줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "pdf"],
        "course": ["AI Math", "AI 수학"]
      },
      "expected_topics": ["gradient descent", "경사하강법", "optimization"],
      "ground_truth": {
        "answer_keywords": ["gradient", "learning rate", "최적화", "미분"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["AI Math"]
      }
    },
    {
      "id": "in_course_006",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "NLP 강의에서 Attention 메커니즘 설명한 내용 찾아줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": ["NLP"]
      },
      "expected_topics": ["attention", "transformer", "nlp"],
      "ground_truth": {
        "answer_keywords": ["attention", "query", "key", "value", "weight"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "in_course_007",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "CV 강의 중 Semantic Segmentation 설명해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "pdf"],
        "course": ["CV", "Computer Vision"]
      },
      "expected_topics": ["semantic segmentation", "pixel", "fcn"],
      "ground_truth": {
        "answer_keywords": ["pixel", "classification", "FCN", "U-Net"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["CV"]
      }
    },
    {
      "id": "in_course_008",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "RecSys 강의에서 NCF(Neural Collaborative Filtering) 설명해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": ["RecSys"]
      },
      "expected_topics": ["ncf", "neural collaborative filtering", "embedding"],
      "ground_truth": {
        "answer_keywords": ["neural network", "embedding", "MLP", "GMF"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["RecSys"]
      }
    },
    {
      "id": "in_course_009",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "Data Engineering 강의에서 Airflow 사용법 알려줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": ["Data Engineering"]
      },
      "expected_topics": ["airflow", "dag", "workflow"],
      "ground_truth": {
        "answer_keywords": ["DAG", "task", "scheduler", "operator"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["Data Engineering"]
      }
    },
    {
      "id": "in_course_010",
      "category": "in_domain",
      "subcategory": "course_specific",
      "question": "MRC 강의에서 Dense Passage Retrieval 설명해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": ["MRC"]
      },
      "expected_topics": ["dpr", "dense retrieval", "passage"],
      "ground_truth": {
        "answer_keywords": ["dense", "retrieval", "embedding", "passage"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": ["MRC"]
      }
    },
    {
      "id": "in_source_001",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "Slack에서 GPU 관련 질문 중에 유용한 답변 있나요?",
      "expected_filters": {
        "doc_type": ["slack_qa"],
        "course": null
      },
      "expected_topics": ["gpu", "cuda", "memory"],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["slack_qa"]
      }
    },
    {
      "id": "in_source_002",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "실습 노트북에서 데이터 전처리 예제 코드 찾아줘",
      "expected_filters": {
        "doc_type": ["notebook"],
        "course": null
      },
      "expected_topics": ["preprocessing", "data", "전처리", "notebook"],
      "ground_truth": {
        "answer_keywords": ["pandas", "numpy", "transform"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["notebook"]
      }
    },
    {
      "id": "in_source_003",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "이번 주 미션 루브릭에서 평가 기준 뭐야?",
      "expected_filters": {
        "doc_type": ["weekly_mission"],
        "course": null
      },
      "expected_topics": ["mission", "rubric", "평가", "기준"],
      "ground_truth": {
        "answer_keywords": ["루브릭", "평가", "점수", "기준"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["weekly_mission"]
      }
    },
    {
      "id": "in_source_004",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "Slack QA에서 학습률 관련 질문 답변 찾아줘",
      "expected_filters": {
        "doc_type": ["slack_qa"],
        "course": null
      },
      "expected_topics": ["learning rate", "학습률", "slack"],
      "ground_truth": {
        "answer_keywords": ["학습률", "scheduler", "warm-up"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["slack_qa"]
      }
    },
    {
      "id": "in_source_005",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "PDF 강의 자료에서 backpropagation 설명 찾아줘",
      "expected_filters": {
        "doc_type": ["pdf"],
        "course": null
      },
      "expected_topics": ["backpropagation", "역전파", "gradient"],
      "ground_truth": {
        "answer_keywords": ["역전파", "chain rule", "gradient"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["pdf"]
      }
    },
    {
      "id": "in_source_006",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "노트북에서 모델 평가 메트릭 계산 코드 찾아줘",
      "expected_filters": {
        "doc_type": ["notebook"],
        "course": null
      },
      "expected_topics": ["metric", "evaluation", "accuracy", "f1"],
      "ground_truth": {
        "answer_keywords": ["accuracy", "precision", "recall", "f1"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["notebook"]
      }
    },
    {
      "id": "in_source_007",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "강의 트랜스크립트에서 regularization 설명 부분 찾아줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": null
      },
      "expected_topics": ["regularization", "정규화", "l1", "l2"],
      "ground_truth": {
        "answer_keywords": ["정규화", "L1", "L2", "weight decay"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["lecture_transcript"]
      }
    },
    {
      "id": "in_source_008",
      "category": "in_domain",
      "subcategory": "source_specific",
      "question": "미션 문서에서 제출 방법 안내 찾아줘",
      "expected_filters": {
        "doc_type": ["weekly_mission"],
        "course": null
      },
      "expected_topics": ["mission", "submission", "제출", "방법"],
      "ground_truth": {
        "answer_keywords": ["제출", "GitHub", "deadline", "형식"],
        "should_have_context": true
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": ["weekly_mission"]
      }
    },
    {
      "id": "ood_unrelated_001",
      "category": "out_of_domain",
      "subcategory": "unrelated",
      "question": "오늘 날씨 어때?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "politely_decline"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_unrelated_002",
      "category": "out_of_domain",
      "subcategory": "unrelated",
      "question": "맛있는 점심 메뉴 추천해줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "politely_decline"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_unrelated_003",
      "category": "out_of_domain",
      "subcategory": "unrelated",
      "question": "주말에 뭐하면 좋을까?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "politely_decline"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_unrelated_004",
      "category": "out_of_domain",
      "subcategory": "unrelated",
      "question": "오늘 주식 시장 어때?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "politely_decline"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_ambiguous_001",
      "category": "out_of_domain",
      "subcategory": "ambiguous",
      "question": "그거 어떻게 해?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "ask_clarification"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_ambiguous_002",
      "category": "out_of_domain",
      "subcategory": "ambiguous",
      "question": "에러나요",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "ask_clarification"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_ambiguous_003",
      "category": "out_of_domain",
      "subcategory": "ambiguous",
      "question": "그 강의에서 설명한 알고리즘이 뭐였죠?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "ask_clarification"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_ambiguous_004",
      "category": "out_of_domain",
      "subcategory": "ambiguous",
      "question": "저번에 말한 그거 다시 설명해줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "ask_clarification"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "ood_hallucination_001",
      "category": "out_of_domain",
      "subcategory": "hallucination_inducing",
      "question": "부스트캠프 10기 NLP 커리큘럼 알려줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_no_info"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": [],
        "hallucination_risk": "high"
      }
    },
    {
      "id": "ood_hallucination_002",
      "category": "out_of_domain",
      "subcategory": "hallucination_inducing",
      "question": "김철수 마스터님이 설명한 GPT-5 내용 정리해줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_no_info"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": [],
        "hallucination_risk": "high"
      }
    },
    {
      "id": "ood_hallucination_003",
      "category": "out_of_domain",
      "subcategory": "hallucination_inducing",
      "question": "2030년 부스트캠프 AI Tech 일정 알려줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_no_info"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": [],
        "hallucination_risk": "high"
      }
    },
    {
      "id": "ood_hallucination_004",
      "category": "out_of_domain",
      "subcategory": "hallucination_inducing",
      "question": "박영희 조교님이 올린 과제 피드백 내용 알려줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": [],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_no_info"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": false,
        "coverage": [],
        "hallucination_risk": "high"
      }
    },
    {
      "id": "ood_boundary_001",
      "category": "out_of_domain",
      "subcategory": "boundary",
      "question": "GPT-4o의 최신 업데이트 내용이 뭐야?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["gpt", "llm"],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_limitation"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": [],
        "note": "AI/ML 관련이지만 부스트캠프 자료에 없는 최신 정보"
      }
    },
    {
      "id": "ood_boundary_002",
      "category": "out_of_domain",
      "subcategory": "boundary",
      "question": "Anthropic Claude의 Constitutional AI 논문 설명해줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["claude", "anthropic", "ai safety"],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_limitation"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": [],
        "note": "AI 관련이지만 부스트캠프 커리큘럼 외 내용"
      }
    },
    {
      "id": "ood_boundary_003",
      "category": "out_of_domain",
      "subcategory": "boundary",
      "question": "Llama 3 모델 학습 방법 알려줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["llama", "meta", "llm"],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": false,
        "expected_behavior": "acknowledge_limitation"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": [],
        "note": "LLM 관련이지만 부스트캠프 자료에 없는 내용"
      }
    },
    {
      "id": "edge_multihop_001",
      "category": "edge_case",
      "subcategory": "multi_hop",
      "question": "CV에서 배운 CNN과 NLP에서 배운 Transformer의 공통점과 차이점은?",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV", "NLP"]
      },
      "expected_topics": ["cnn", "transformer", "cv", "nlp"],
      "ground_truth": {
        "answer_keywords": ["convolution", "attention", "local", "global"],
        "should_have_context": true,
        "expected_behavior": "multi_doc_synthesis"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["CV", "NLP"]
      }
    },
    {
      "id": "edge_multihop_002",
      "category": "edge_case",
      "subcategory": "multi_hop",
      "question": "RecSys와 NLP에서 모두 사용되는 Embedding 기법들을 비교해줘",
      "expected_filters": {
        "doc_type": null,
        "course": ["RecSys", "NLP"]
      },
      "expected_topics": ["embedding", "recsys", "nlp", "representation"],
      "ground_truth": {
        "answer_keywords": ["embedding", "representation", "dimension"],
        "should_have_context": true,
        "expected_behavior": "multi_doc_synthesis"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["RecSys", "NLP"]
      }
    },
    {
      "id": "edge_multihop_003",
      "category": "edge_case",
      "subcategory": "multi_hop",
      "question": "PyTorch 강의와 실습 노트북에서 다룬 모델 학습 과정을 종합해서 설명해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript", "notebook"],
        "course": ["PyTorch"]
      },
      "expected_topics": ["training", "pytorch", "model"],
      "ground_truth": {
        "answer_keywords": ["forward", "backward", "optimizer", "loss"],
        "should_have_context": true,
        "expected_behavior": "multi_doc_synthesis"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "edge_multihop_004",
      "category": "edge_case",
      "subcategory": "multi_hop",
      "question": "AI Math에서 배운 미분 개념이 딥러닝 backpropagation에 어떻게 적용되나요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["AI Math", "Deep Learning"]
      },
      "expected_topics": ["derivative", "backpropagation", "chain rule"],
      "ground_truth": {
        "answer_keywords": ["미분", "chain rule", "gradient", "역전파"],
        "should_have_context": true,
        "expected_behavior": "multi_doc_synthesis"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["AI Math", "Deep Learning"]
      }
    },
    {
      "id": "edge_temporal_001",
      "category": "edge_case",
      "subcategory": "temporal",
      "question": "NLP 강의 순서대로 주요 개념들을 정리해줘",
      "expected_filters": {
        "doc_type": ["lecture_transcript"],
        "course": ["NLP"]
      },
      "expected_topics": ["nlp", "sequence", "order"],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": true,
        "expected_behavior": "temporal_reasoning"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "edge_temporal_002",
      "category": "edge_case",
      "subcategory": "temporal",
      "question": "Object Detection 기법들의 발전 순서를 설명해줘 (R-CNN부터 YOLO까지)",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV"]
      },
      "expected_topics": ["r-cnn", "fast rcnn", "faster rcnn", "yolo", "evolution"],
      "ground_truth": {
        "answer_keywords": ["R-CNN", "Fast R-CNN", "Faster R-CNN", "YOLO"],
        "should_have_context": true,
        "expected_behavior": "temporal_reasoning"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["CV"]
      }
    },
    {
      "id": "edge_temporal_003",
      "category": "edge_case",
      "subcategory": "temporal",
      "question": "Transformer 이전의 Seq2Seq 모델들은 어떤 순서로 발전했나요?",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP"]
      },
      "expected_topics": ["seq2seq", "rnn", "lstm", "attention"],
      "ground_truth": {
        "answer_keywords": ["RNN", "LSTM", "Attention", "Transformer"],
        "should_have_context": true,
        "expected_behavior": "temporal_reasoning"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "edge_negation_001",
      "category": "edge_case",
      "subcategory": "negation",
      "question": "CNN을 제외한 이미지 처리 방법들을 설명해줘",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV"]
      },
      "expected_topics": ["image", "non-cnn", "vision transformer"],
      "ground_truth": {
        "answer_keywords": ["ViT", "MLP-Mixer", "transformer"],
        "should_have_context": true,
        "expected_behavior": "negation_handling"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["CV"]
      }
    },
    {
      "id": "edge_negation_002",
      "category": "edge_case",
      "subcategory": "negation",
      "question": "Adam이 아닌 다른 옵티마이저들의 특징을 알려줘",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["optimizer", "sgd", "adagrad", "rmsprop"],
      "ground_truth": {
        "answer_keywords": ["SGD", "RMSprop", "Adagrad", "momentum"],
        "should_have_context": true,
        "expected_behavior": "negation_handling"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["Deep Learning"]
      }
    },
    {
      "id": "edge_negation_003",
      "category": "edge_case",
      "subcategory": "negation",
      "question": "BERT 이외의 Pre-trained Language Model들을 비교해줘",
      "expected_filters": {
        "doc_type": null,
        "course": ["NLP"]
      },
      "expected_topics": ["gpt", "roberta", "xlnet", "electra"],
      "ground_truth": {
        "answer_keywords": ["GPT", "RoBERTa", "XLNet", "ELECTRA"],
        "should_have_context": true,
        "expected_behavior": "negation_handling"
      },
      "metadata": {
        "difficulty": "hard",
        "requires_reasoning": true,
        "coverage": ["NLP"]
      }
    },
    {
      "id": "edge_code_001",
      "category": "edge_case",
      "subcategory": "code_execution",
      "question": "노트북에 있는 학습 루프 코드가 어떤 순서로 실행되는지 설명해줘",
      "expected_filters": {
        "doc_type": ["notebook"],
        "course": null
      },
      "expected_topics": ["training loop", "forward", "backward", "step"],
      "ground_truth": {
        "answer_keywords": ["forward", "loss", "backward", "step", "zero_grad"],
        "should_have_context": true,
        "expected_behavior": "code_explanation"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["PyTorch"]
      }
    },
    {
      "id": "edge_code_002",
      "category": "edge_case",
      "subcategory": "code_execution",
      "question": "데이터 증강 파이프라인 코드의 실행 흐름을 설명해줘",
      "expected_filters": {
        "doc_type": ["notebook"],
        "course": ["CV"]
      },
      "expected_topics": ["augmentation", "transform", "pipeline"],
      "ground_truth": {
        "answer_keywords": ["transform", "Compose", "augmentation", "순서"],
        "should_have_context": true,
        "expected_behavior": "code_explanation"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": true,
        "coverage": ["CV"]
      }
    },
    {
      "id": "edge_meta_001",
      "category": "edge_case",
      "subcategory": "meta_question",
      "question": "현재 부스트캠프에서 제공하는 강의 과정들이 뭐가 있어?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["course", "curriculum", "lecture"],
      "ground_truth": {
        "answer_keywords": ["CV", "NLP", "RecSys", "AI Math", "PyTorch"],
        "should_have_context": true,
        "expected_behavior": "meta_info_retrieval"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "edge_meta_002",
      "category": "edge_case",
      "subcategory": "meta_question",
      "question": "어떤 종류의 자료들을 검색할 수 있어?",
      "expected_filters": {
        "doc_type": null,
        "course": null
      },
      "expected_topics": ["document type", "source"],
      "ground_truth": {
        "answer_keywords": ["강의 자료", "노트북", "Slack", "미션"],
        "should_have_context": true,
        "expected_behavior": "meta_info_retrieval"
      },
      "metadata": {
        "difficulty": "easy",
        "requires_reasoning": false,
        "coverage": []
      }
    },
    {
      "id": "edge_meta_003",
      "category": "edge_case",
      "subcategory": "meta_question",
      "question": "CV 강의 자료가 몇 개나 있어?",
      "expected_filters": {
        "doc_type": null,
        "course": ["CV"]
      },
      "expected_topics": ["count", "cv", "lecture"],
      "ground_truth": {
        "answer_keywords": [],
        "should_have_context": true,
        "expected_behavior": "meta_info_retrieval"
      },
      "metadata": {
        "difficulty": "medium",
        "requires_reasoning": false,
        "coverage": ["CV"]
      }
    }
  ],
  "coverage_summary": {
    "by_category": {
      "in_domain": {
        "concept": 10,
        "implementation": 10,
        "troubleshooting": 6,
        "comparison": 6,
        "course_specific": 10,
        "source_specific": 8
      },
      "out_of_domain": {
        "unrelated": 4,
        "ambiguous": 4,
        "hallucination_inducing": 4,
        "boundary": 3
      },
      "edge_case": {
        "multi_hop": 4,
        "temporal": 3,
        "negation": 3,
        "code_execution": 2,
        "meta_question": 3
      }
    },
    "by_course": {
      "CV": 12,
      "NLP": 14,
      "RecSys": 4,
      "PyTorch": 10,
      "AI Math": 3,
      "MRC": 2,
      "Data Engineering": 2,
      "Deep Learning": 12,
      "General": 21
    },
    "by_doc_type": {
      "slack_qa": 2,
      "lecture_transcript": 10,
      "pdf": 4,
      "notebook": 6,
      "weekly_mission": 3
    },
    "total_questions": 80
  }
}
