"""
Adaptive RAG ì›Œí¬í”Œë¡œ í†µí•© í…ŒìŠ¤íŠ¸

Hybrid Retrieverë¥¼ Adaptive RAG ì›Œí¬í”Œë¡œì— í†µí•©í•˜ì—¬
ì „ì²´ ì›Œí¬í”Œë¡œê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

from langchain_core.runnables.config import RunnableConfig
import pytest
from pydantic import SecretStr
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "app"))

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(PROJECT_ROOT / ".env")


@pytest.fixture
def embeddings():
    """OpenRouterEmbeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    sys.path.insert(0, str(PROJECT_ROOT / "app" / "naver_connect_chatbot" / "config"))
    from embedding import OpenRouterEmbeddings
    
    api_key = os.getenv("OPENROUTER_API_KEY")
    return OpenRouterEmbeddings(
        model="qwen/qwen3-embedding-4b",
        api_key=SecretStr(api_key)
    )


@pytest.fixture
def hybrid_retriever(embeddings):
    """Hybrid Retriever ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from naver_connect_chatbot.rag.retriever_factory import build_dense_sparse_hybrid_from_saved
    from naver_connect_chatbot.rag.retriever.hybrid_retriever import HybridMethod
    
    bm25_path = PROJECT_ROOT / "sparse_index" / "kiwi_bm25_slack_qa"
    qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
    collection_name = os.getenv("QDRANT_COLLECTION_NAME", "slack_qa")
    
    return build_dense_sparse_hybrid_from_saved(
        bm25_index_path=str(bm25_path),
        embedding_model=embeddings,
        qdrant_url=qdrant_url,
        collection_name=collection_name,
        weights=[0.5, 0.5],
        k=10,
        method=HybridMethod.RRF,
        rrf_c=60,
    )


@pytest.fixture
def llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from naver_connect_chatbot.config.llm import get_chat_model, LLMProvider
    
    # ì—¬ëŸ¬ LLM ì œê³µìë¥¼ ì‹œë„
    providers_to_try = [
        LLMProvider.NAVER_CLOUD,
        LLMProvider.OPENROUTER,
        LLMProvider.OPENAI,
    ]
    
    for provider in providers_to_try:
        try:
            llm_instance = get_chat_model(provider)
            print(f"\nâœ… {provider.value} LLM ì‚¬ìš©")
            return llm_instance
        except ValueError:
            continue
    
    # ëª¨ë“  ì œê³µì ì‹¤íŒ¨ ì‹œ
    pytest.skip("ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")


@pytest.mark.asyncio
async def test_adaptive_rag_graph_construction(hybrid_retriever, llm):
    """Adaptive RAG ê·¸ë˜í”„ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("1. Adaptive RAG ê·¸ë˜í”„ ìƒì„±")
    print("=" * 80)
    
    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph
    
    try:
        graph = build_adaptive_rag_graph(
            retriever=hybrid_retriever,
            llm=llm,
            fast_llm=llm,  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë™ì¼í•œ LLM ì‚¬ìš©
        )
        
        assert graph is not None, "ê·¸ë˜í”„ê°€ Noneì…ë‹ˆë‹¤"
        
    except Exception as e:
        pytest.fail(f"ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")


@pytest.mark.asyncio
async def test_simple_qa_workflow(hybrid_retriever, llm):
    """SIMPLE_QA ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("2. SIMPLE_QA ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph
    
    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        fast_llm=llm,
    )
    
    query = "PyTorch ì„¤ì¹˜ ë°©ë²•ì€?"
    print(f"\nğŸ” ì¿¼ë¦¬: {query}")
    
    try:
        result = await graph.ainvoke({
            "question": query,
            "max_retries": 2,
        }, config=RunnableConfig(run_name="test_simple_qa_workflow", tags=["test"], configurable={"thread_id": "test_simple_qa_workflow"}))
        
        assert "answer" in result, "Answer not generated"
        assert len(result["answer"]) > 0, "Answer is empty"
        assert "documents" in result, "Documents not retrieved"
        
    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


@pytest.mark.asyncio
async def test_retrieval_in_workflow(hybrid_retriever, llm):
    """ì›Œí¬í”Œë¡œ ë‚´ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("3. ì›Œí¬í”Œë¡œ ë‚´ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph
    
    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        fast_llm=llm,
    )
    
    try:
        result = await graph.ainvoke(
            {
                "question": "GPU ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²° ë°©ë²•",
                "max_retries": 1,
            },
            config=RunnableConfig(
                run_name="test_retrieval_in_workflow",
                tags=["test"],
                configurable={"thread_id": "test_retrieval_in_workflow"},
            ),
        )
        
        documents = result.get("documents", [])
        
        assert len(documents) > 0, "Documents not retrieved"
        
        # Hybrid ê²€ìƒ‰ì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert result.get("retrieval_strategy") == "hybrid", "Hybrid retrieval not used"
        
    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


@pytest.mark.asyncio
async def test_workflow_state_tracking(hybrid_retriever, llm):
    """ì›Œí¬í”Œë¡œ ìƒíƒœ ì¶”ì  í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("4. ì›Œí¬í”Œë¡œ ìƒíƒœ ì¶”ì  í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph
    
    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        fast_llm=llm,
    )
    
    try:
        result = await graph.ainvoke(
            {
                "question": "ë°ì´í„° ì¦ê°• ê¸°ë²•",
                "max_retries": 1,
            },
            config=RunnableConfig(
                run_name="test_workflow_state_tracking",
                tags=["test"],
                configurable={"thread_id": "test_workflow_state_tracking"},
            ),
        )
        
        # ì£¼ìš” ìƒíƒœ í•„ë“œ í™•ì¸
        assert "intent" in result, "Intent classification not performed"
        assert "documents" in result, "Documents not retrieved"
        assert "answer" in result, "Answer not generated"
        
    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


@pytest.mark.asyncio
async def test_answer_generator_structured_output(llm):
    """Answer Generator êµ¬ì¡°í™”ëœ ì¶œë ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("5. Answer Generator êµ¬ì¡°í™”ëœ ì¶œë ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    from naver_connect_chatbot.service.agents.answer_generator import (
        create_answer_generator,
        AnswerOutput,
    )
    from naver_connect_chatbot.service.graph.nodes import _coerce_model_response

    # Simple ì „ëµìœ¼ë¡œ ì—ì´ì „íŠ¸ ìƒì„±
    generator = create_answer_generator(llm, strategy="simple")

    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: What is 2+2?")
    print("ğŸ“ ì»¨í…ìŠ¤íŠ¸: Mathematics: 2+2 equals 4.")

    try:
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        response_raw = await generator.ainvoke({
            "messages": [{
                "role": "user",
                "content": "question: What is 2+2?\n\ncontext:\nMathematics: 2+2 equals 4."
            }]
        })

        # AnswerOutputìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
        response = _coerce_model_response(AnswerOutput, response_raw)

        # ê²€ì¦
        assert isinstance(response, AnswerOutput), f"Expected AnswerOutput, got {type(response)}"
        assert isinstance(response.answer, str), f"Expected str answer, got {type(response.answer)}"
        assert len(response.answer) > 0, "Answer is empty"

        print("\nâœ… êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„±ê³µ:")
        print(f"   - Type: {type(response).__name__}")
        print(f"   - Answer length: {len(response.answer)} characters")
        print(f"   - Answer preview: {response.answer[:100]}...")

    except Exception as e:
        pytest.skip(f"Answer generator test failed: {e}")


if __name__ == "__main__":
    # pytest ì‹¤í–‰
    pytest.main([__file__, "-v", "-s"])

