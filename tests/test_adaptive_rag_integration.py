"""
Adaptive RAG ì›Œí¬í”Œë¡œ í†µí•© í…ŒìŠ¤íŠ¸

Hybrid Retrieverë¥¼ Adaptive RAG ì›Œí¬í”Œë¡œì— í†µí•©í•˜ì—¬
ì „ì²´ ì›Œí¬í”Œë¡œê°€ ì •ìƒ ìž‘ë™í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

from langchain_core.runnables.config import RunnableConfig
import pytest
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "app"))

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(PROJECT_ROOT / ".env")


@pytest.fixture
def embeddings():
    """NaverEmbeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from naver_connect_chatbot.config.embedding import get_embeddings

    return get_embeddings()


@pytest.fixture
def hybrid_retriever(embeddings):
    """Hybrid Retriever ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from naver_connect_chatbot.rag.retriever_factory import build_dense_sparse_hybrid_from_saved
    from naver_connect_chatbot.rag.retriever.hybrid_retriever import HybridMethod

    bm25_path = PROJECT_ROOT / "sparse_index" / "unified_bm25"
    qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
    collection_name = os.getenv("QDRANT_COLLECTION_NAME", "naver_connect_docs")

    return build_dense_sparse_hybrid_from_saved(
        bm25_index_path=str(bm25_path),
        embedding_model=embeddings,
        qdrant_url=qdrant_url,
        collection_name=collection_name,
        weights=[0.5, 0.5],
        k=10,
        method=HybridMethod.RRF,
        rrf_c=60,
    )


@pytest.fixture
def llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    from naver_connect_chatbot.config.llm import get_chat_model

    try:
        llm_instance = get_chat_model()
        return llm_instance
    except ValueError:
        # CLOVASTUDIO_API_KEY ë“± í•„ìˆ˜ ì„¤ì •ì´ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.
        pytest.skip("ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")


@pytest.fixture
def reasoning_llm():
    """Reasoning LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (medium effort)"""
    from naver_connect_chatbot.config.llm import get_chat_model

    try:
        llm_instance = get_chat_model(
            model="HCX-007",
            use_reasoning=True,
            reasoning_effort="medium",
        )
        return llm_instance
    except ValueError:
        pytest.skip("ì‚¬ìš© ê°€ëŠ¥í•œ Reasoning LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")


@pytest.mark.asyncio
async def test_adaptive_rag_graph_construction(hybrid_retriever, llm, reasoning_llm):
    """Adaptive RAG ê·¸ëž˜í”„ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("1. Adaptive RAG ê·¸ëž˜í”„ ìƒì„±")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    try:
        graph = build_adaptive_rag_graph(
            retriever=hybrid_retriever,
            llm=llm,
            reasoning_llm=reasoning_llm,
            debug=True,
        )

        assert graph is not None, "ê·¸ëž˜í”„ê°€ Noneìž…ë‹ˆë‹¤"

    except Exception as e:
        pytest.fail(f"ê·¸ëž˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")


@pytest.mark.asyncio
async def test_simple_qa_workflow(hybrid_retriever, llm, reasoning_llm):
    """SIMPLE_QA ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("2. SIMPLE_QA ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    query = "PyTorch ì„¤ì¹˜ ë°©ë²•ì€?"
    print(f"\nðŸ” ì¿¼ë¦¬: {query}")

    try:
        result = await graph.ainvoke(
            {
                "question": query,
                "max_retries": 2,
            },
            config=RunnableConfig(
                run_name="test_simple_qa_workflow",
                tags=["test"],
                configurable={"thread_id": "test_simple_qa_workflow"},
            ),
        )

        assert "answer" in result, "Answer not generated"
        assert len(result["answer"]) > 0, "Answer is empty"
        assert "documents" in result, "Documents not retrieved"

    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


@pytest.mark.asyncio
async def test_retrieval_in_workflow(hybrid_retriever, llm, reasoning_llm):
    """ì›Œí¬í”Œë¡œ ë‚´ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("3. ì›Œí¬í”Œë¡œ ë‚´ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    try:
        result = await graph.ainvoke(
            {
                "question": "GPU ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²° ë°©ë²•",
                "max_retries": 1,
            },
            config=RunnableConfig(
                run_name="test_retrieval_in_workflow",
                tags=["test"],
                configurable={"thread_id": "test_retrieval_in_workflow"},
            ),
        )

        documents = result.get("documents", [])

        assert len(documents) > 0, "Documents not retrieved"

        # Hybrid ê²€ìƒ‰ì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert result.get("retrieval_strategy") == "hybrid", "Hybrid retrieval not used"

    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


@pytest.mark.asyncio
async def test_workflow_state_tracking(hybrid_retriever, llm, reasoning_llm):
    """ì›Œí¬í”Œë¡œ ìƒíƒœ ì¶”ì  í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 80)
    print("4. ì›Œí¬í”Œë¡œ ìƒíƒœ ì¶”ì  í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    try:
        result = await graph.ainvoke(
            {
                "question": "ë°ì´í„° ì¦ê°• ê¸°ë²•",
                "max_retries": 1,
            },
            config=RunnableConfig(
                run_name="test_workflow_state_tracking",
                tags=["test"],
                configurable={"thread_id": "test_workflow_state_tracking"},
            ),
        )

        # ì£¼ìš” ìƒíƒœ í•„ë“œ í™•ì¸
        assert "intent" in result, "Intent classification not performed"
        assert "documents" in result, "Documents not retrieved"
        assert "answer" in result, "Answer not generated"

    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


@pytest.mark.asyncio
async def test_answer_generator_structured_output(llm):
    """Answer Generator LCEL ì²´ì¸ í…ŒìŠ¤íŠ¸ (tools ë¯¸ì‚¬ìš©)"""
    print("\n" + "=" * 80)
    print("5. Answer Generator LCEL ì²´ì¸ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    from naver_connect_chatbot.service.agents.answer_generator import (
        create_answer_generator,
    )

    # Simple ì „ëžµìœ¼ë¡œ ì²´ì¸ ìƒì„±
    generator = create_answer_generator(llm, strategy="simple")

    print("\nðŸ§ª í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: What is 2+2?")
    print("ðŸ“ ì»¨í…ìŠ¤íŠ¸: Mathematics: 2+2 equals 4.")

    try:
        # LCEL ì²´ì¸ ì‹¤í–‰ (ìƒˆ ì¸í„°íŽ˜ì´ìŠ¤)
        response_raw = await generator.ainvoke(
            {
                "question": "What is 2+2?",
                "context": "Mathematics: 2+2 equals 4.",
            }
        )

        # AIMessageì—ì„œ content ì¶”ì¶œ
        response = response_raw.content if hasattr(response_raw, "content") else str(response_raw)

        # ê²€ì¦
        assert len(response) > 0, "Answer is empty"

        print(f"   - Type: {type(response).__name__}")
        print(f"   - Answer length: {len(response)} characters")
        print(f"   - Answer: {response}")

    except Exception as e:
        pytest.skip(f"Workflow execution failed: {e}")


# ============================================================================
# Data-Driven RAG Test Cases
# ============================================================================


# ì§ˆì˜ ìœ í˜•ë³„ í…ŒìŠ¤íŠ¸ ë°ì´í„°
RAG_TEST_CASES = [
    # (ì§ˆì˜, ì˜ˆìƒ intent, ì˜ˆìƒ doc_type ížŒíŠ¸, ì„¤ëª…)
    pytest.param(
        "PyTorchì—ì„œ í…ì„œë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€?",
        "SIMPLE_QA",
        None,
        "ê¸°ë³¸ ê°œë… ì§ˆë¬¸",
        id="simple_qa_pytorch",
    ),
    pytest.param(
        "CV ê°•ì˜ì—ì„œ CNN ì•„í‚¤í…ì²˜ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "SIMPLE_QA",
        ["lecture_transcript", "pdf"],
        "ê³¼ì • íŠ¹ì • ì§ˆë¬¸",
        id="course_specific_cv",
    ),
    pytest.param(
        "RecSysì—ì„œ collaborative filteringê³¼ content-basedì˜ ìž¥ë‹¨ì ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”",
        "COMPLEX_REASONING",
        None,
        "ë¹„êµ ë¶„ì„ ì§ˆë¬¸",
        id="complex_comparison",
    ),
    pytest.param(
        "NLP ë¶„ì•¼ì˜ ìµœì‹  íŠ¸ë Œë“œì™€ ë°œì „ ë°©í–¥ì€?",
        "EXPLORATORY",
        None,
        "íƒìƒ‰ì  ì§ˆë¬¸",
        id="exploratory_nlp",
    ),
    pytest.param(
        "Slackì—ì„œ GPU ê´€ë ¨ ì§ˆë¬¸ ì¤‘ì— CUDA ì—ëŸ¬ í•´ê²°í•œ ë‹µë³€ ìžˆë‚˜ìš”?",
        "SIMPLE_QA",
        ["slack_qa"],
        "Slack íŠ¹ì • ì§ˆë¬¸",
        id="slack_specific",
    ),
]

# í•„í„° ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
FILTER_EXTRACTION_CASES = [
    # (ì§ˆì˜, ì˜ˆìƒ doc_type, ì˜ˆìƒ course í‚¤ì›Œë“œ)
    pytest.param(
        "CV ê°•ì˜ìžë£Œì—ì„œ ResNet ì„¤ëª…",
        ["pdf"],
        ["CV"],
        id="filter_cv_pdf",
    ),
    pytest.param(
        "NLP ë…¹ì·¨ë¡ì—ì„œ Transformer ì•„í‚¤í…ì²˜",
        ["lecture_transcript"],
        ["NLP"],
        id="filter_nlp_transcript",
    ),
    pytest.param(
        "PyTorch ì‹¤ìŠµ ë…¸íŠ¸ë¶ ì°¾ì•„ì¤˜",
        ["notebook"],
        ["PyTorch"],
        id="filter_pytorch_notebook",
    ),
    pytest.param(
        "ë¯¸ì…˜ì—ì„œ ê°ì²´ íƒì§€ ê³¼ì œ",
        ["weekly_mission"],
        None,
        id="filter_mission",
    ),
]


@pytest.mark.asyncio
@pytest.mark.parametrize("query,expected_intent,expected_doc_types,description", RAG_TEST_CASES)
async def test_rag_query_types(
    hybrid_retriever,
    llm,
    reasoning_llm,
    query,
    expected_intent,
    expected_doc_types,
    description,
):
    """
    ë‹¤ì–‘í•œ ì§ˆì˜ ìœ í˜•ì— ëŒ€í•œ RAG ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸.

    ë°ì´í„° ê¸°ë°˜ íŒŒë¼ë¯¸í„°í™” í…ŒìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ ì§ˆì˜ íŒ¨í„´ì„ ì»¤ë²„í•©ë‹ˆë‹¤.
    """
    print(f"\n{'=' * 80}")
    print(f"RAG Test: {description}")
    print(f"Query: {query}")
    print(f"Expected Intent: {expected_intent}")
    print(f"Expected Doc Types: {expected_doc_types}")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    try:
        result = await graph.ainvoke(
            {"question": query, "max_retries": 1},
            config=RunnableConfig(
                run_name=f"test_rag_{description}",
                tags=["test", "data-driven"],
            ),
        )

        # ê¸°ë³¸ ê²€ì¦
        assert "answer" in result, f"Answer not generated for: {query}"
        assert len(result.get("answer", "")) > 0, f"Empty answer for: {query}"
        assert "documents" in result, f"Documents not retrieved for: {query}"

        # Intent ê²€ì¦ (í—ˆìš© ì˜¤ì°¨)
        actual_intent = result.get("intent", "")
        print(f"Actual Intent: {actual_intent}")
        # IntentëŠ” ì •í™•ížˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ë¨ (LLM íŒë‹¨ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìžˆìŒ)

        # ë¬¸ì„œ ê²€ìƒ‰ ê²€ì¦
        documents = result.get("documents", [])
        print(f"Retrieved {len(documents)} documents")
        assert len(documents) > 0, f"No documents for: {query}"

        # doc_type í•„í„°ê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ížŒíŠ¸ê°€ ìžˆëŠ” ê²½ìš°)
        if expected_doc_types:
            doc_types_found = {
                doc.metadata.get("doc_type") for doc in documents if doc.metadata
            }
            print(f"Doc types found: {doc_types_found}")

        print(f"âœ… Test passed: {description}")

    except Exception as e:
        pytest.skip(f"RAG test failed for '{description}': {e}")


@pytest.mark.asyncio
async def test_multi_course_filter_or_condition(hybrid_retriever, llm, reasoning_llm):
    """
    ë‹¤ì¤‘ course í•„í„° OR ì¡°ê±´ í…ŒìŠ¤íŠ¸.

    "CV ê°•ì˜"ì™€ ê°™ì€ ì• ë§¤í•œ ì§ˆì˜ê°€ ì—¬ëŸ¬ courseë¡œ í™•ìž¥ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 80)
    print("Multi-Course Filter OR Condition Test")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    # ì• ë§¤í•œ ê³¼ì •ëª… ì§ˆì˜
    query = "CV ê°•ì˜ì—ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë°©ë²•"

    try:
        result = await graph.ainvoke(
            {"question": query, "max_retries": 1},
            config=RunnableConfig(run_name="test_multi_course_filter"),
        )

        # í•„í„°ê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        filters_applied = result.get("retrieval_filters_applied", False)
        retrieval_filters = result.get("retrieval_filters")

        print(f"Query: {query}")
        print(f"Filters applied: {filters_applied}")
        print(f"Retrieval filters: {retrieval_filters}")

        # course í•„í„°ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if retrieval_filters and "course" in retrieval_filters:
            course_filter = retrieval_filters["course"]
            print(f"Course filter: {course_filter}")
            assert isinstance(course_filter, list), "Course filter should be a list"
            # ì—¬ëŸ¬ CV ê´€ë ¨ ê³¼ì •ì´ í¬í•¨ë  ìˆ˜ ìžˆìŒ
            print(f"Number of courses in filter: {len(course_filter)}")

        # ë¬¸ì„œ ê²€ìƒ‰ í™•ì¸
        documents = result.get("documents", [])
        assert len(documents) > 0, "No documents retrieved"
        print(f"Retrieved {len(documents)} documents")

        print("âœ… Multi-course filter test passed")

    except Exception as e:
        pytest.skip(f"Multi-course filter test failed: {e}")


@pytest.mark.asyncio
async def test_clarification_workflow(hybrid_retriever, llm, reasoning_llm):
    """
    Clarification ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸.

    enable_clarification=Trueì¼ ë•Œ ë‚®ì€ confidence ì§ˆì˜ê°€ clarifyë¡œ ë¼ìš°íŒ…ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 80)
    print("Clarification Workflow Test")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    # Clarification í™œì„±í™”
    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
        enable_clarification=True,
        clarification_threshold=0.5,
    )

    # ì• ë§¤í•œ ì§ˆì˜ (ë‚®ì€ filter_confidence ì˜ˆìƒ)
    query = "ê·¸ ê°•ì˜ì—ì„œ ì„¤ëª…í•œ ì•Œê³ ë¦¬ì¦˜ì´ ë­ì˜€ì£ ?"

    try:
        result = await graph.ainvoke(
            {"question": query, "max_retries": 1},
            config=RunnableConfig(run_name="test_clarification"),
        )

        print(f"Query: {query}")
        print(f"Filter confidence: {result.get('filter_confidence', 'N/A')}")
        print(f"Workflow stage: {result.get('workflow_stage', 'N/A')}")

        # clarificationì´ íŠ¸ë¦¬ê±°ë˜ì—ˆê±°ë‚˜ ì •ìƒ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        answer = result.get("answer", "")
        assert len(answer) > 0, "No answer or clarification message"

        if result.get("workflow_stage") == "awaiting_clarification":
            print("âœ… Clarification was triggered as expected")
        else:
            print("âœ… Normal answer generated (confidence was high enough)")

    except Exception as e:
        pytest.skip(f"Clarification workflow test failed: {e}")


@pytest.mark.asyncio
async def test_retrieval_metadata_tracking(hybrid_retriever, llm, reasoning_llm):
    """
    ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì¶”ì  í…ŒìŠ¤íŠ¸.

    retrieval_filters_applied, retrieval_fallback_used ë“±
    ë©”íƒ€ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì¶”ì ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    print("\n" + "=" * 80)
    print("Retrieval Metadata Tracking Test")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    query = "Slackì—ì„œ í•™ìŠµë¥  ì„¤ì • ê´€ë ¨ ì§ˆë¬¸"

    try:
        result = await graph.ainvoke(
            {"question": query, "max_retries": 1},
            config=RunnableConfig(run_name="test_metadata_tracking"),
        )

        print(f"Query: {query}")

        # ë©”íƒ€ë°ì´í„° í•„ë“œ í™•ì¸
        metadata_fields = [
            "retrieval_filters",
            "retrieval_filters_applied",
            "retrieval_fallback_used",
            "retrieval_strategy",
            "filter_confidence",
        ]

        for field in metadata_fields:
            value = result.get(field)
            print(f"  {field}: {value}")

        # ê¸°ë³¸ ê²€ì¦
        assert "retrieval_strategy" in result, "Retrieval strategy not tracked"
        print("âœ… Metadata tracking test passed")

    except Exception as e:
        pytest.skip(f"Metadata tracking test failed: {e}")


@pytest.mark.asyncio
async def test_edge_case_empty_query(hybrid_retriever, llm, reasoning_llm):
    """
    ì—£ì§€ ì¼€ì´ìŠ¤: ë¹ˆ ì§ˆì˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸.
    """
    print("\n" + "=" * 80)
    print("Edge Case: Empty Query Test")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    try:
        result = await graph.ainvoke(
            {"question": "", "max_retries": 1},
            config=RunnableConfig(run_name="test_empty_query"),
        )

        # ë¹ˆ ì§ˆì˜ë„ ì—ëŸ¬ ì—†ì´ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
        assert "answer" in result, "Should handle empty query gracefully"
        print("âœ… Empty query handled gracefully")

    except Exception as e:
        # ë¹ˆ ì§ˆì˜ëŠ” ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìžˆìŒ (í—ˆìš©)
        print(f"Empty query raised exception (acceptable): {e}")


@pytest.mark.asyncio
async def test_edge_case_very_long_query(hybrid_retriever, llm, reasoning_llm):
    """
    ì—£ì§€ ì¼€ì´ìŠ¤: ë§¤ìš° ê¸´ ì§ˆì˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸.
    """
    print("\n" + "=" * 80)
    print("Edge Case: Very Long Query Test")
    print("=" * 80)

    from naver_connect_chatbot.service.graph import build_adaptive_rag_graph

    graph = build_adaptive_rag_graph(
        retriever=hybrid_retriever,
        llm=llm,
        reasoning_llm=reasoning_llm,
        debug=True,
    )

    # ë§¤ìš° ê¸´ ì§ˆì˜ (500ìž ì´ìƒ)
    long_query = (
        "PyTorchì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œê°€ ë°œìƒí•˜ë©´ "
        "ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•˜ë‚˜ìš”? ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ëŠ” ê²ƒ ì™¸ì— gradient accumulation, "
        "mixed precision training (FP16), gradient checkpointing ë“±ì˜ ê¸°ë²•ì„ "
        "ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤ê³  ë“¤ì—ˆëŠ”ë° ê°ê°ì˜ ìž¥ë‹¨ì ê³¼ êµ¬í˜„ ë°©ë²•, ê·¸ë¦¬ê³  ì–´ë–¤ ìƒí™©ì—ì„œ "
        "ì–´ë–¤ ê¸°ë²•ì„ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ ìžì„¸ížˆ ì•Œë ¤ì£¼ì„¸ìš”. íŠ¹ížˆ Vision Transformerë‚˜ "
        "ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ê°™ì€ í° ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ íš¨ê³¼ì ì¸ ë°©ë²•ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤."
    )

    try:
        result = await graph.ainvoke(
            {"question": long_query, "max_retries": 1},
            config=RunnableConfig(run_name="test_long_query"),
        )

        assert "answer" in result, "Should handle long query"
        assert len(result.get("answer", "")) > 0, "Answer should not be empty"
        print(f"Query length: {len(long_query)} characters")
        print(f"Answer length: {len(result.get('answer', ''))} characters")
        print("âœ… Long query handled successfully")

    except Exception as e:
        pytest.skip(f"Long query test failed: {e}")


if __name__ == "__main__":
    # pytest ì‹¤í–‰
    pytest.main([__file__, "-v", "-s"])
